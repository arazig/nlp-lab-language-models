{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e30626a2",
   "metadata": {},
   "source": [
    "# NLP Lab: Language Models\n",
    "\n",
    "In this lab, we will build the main components of the GPT-2 model and train a small model on poems by Victor Hugo.\n",
    "\n",
    "The questions are included in this notebook. To run the training, you will need to modify the `gpt_single_head.py` file, which is also available in the Git repository.\n",
    "\n",
    "## Data\n",
    "\n",
    "The training data consists of a collection of poems by Victor Hugo, sourced from [gutenberg.org](https://www.gutenberg.org/). The dataset is available in the `data` directory.\n",
    "\n",
    "To reduce model complexity, we will model the text at the character level. Typically, language models process sequences of subwords using [tokenizers](https://huggingface.co/docs/transformers/tokenizer_summary) such as BPE, SentencePiece, or WordPiece.\n",
    "\n",
    "#### Questions:\n",
    "- Using [collections.Counter](https://docs.python.org/3/library/collections.html#collections.Counter), display the number of unique characters in the text and the frequency of each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4d4ab91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in the file: 285222\n",
      "Number of character in counter: 285222\n",
      "101 different characters\n",
      "Counter({' ': 49127, 'e': 30253, 's': 17987, 'u': 14254, 'r': 14223, 't': 14071, 'a': 14048, 'n': 13725, 'i': 12828, 'o': 12653, 'l': 11638, '\\n': 8102, 'm': 6495, 'd': 6375, ',': 6077, 'c': 5074, 'p': 4206, \"'\": 3820, 'v': 3492, 'é': 2943, 'b': 2783, 'f': 2772, 'h': 2221, 'q': 1956, 'g': 1790, '.': 1420, 'x': 1154, 'L': 1147, '!': 1121, 'E': 1074, ';': 1043, '-': 1020, 'j': 890, 'D': 764, 'è': 725, 'à': 706, 'y': 660, 'I': 627, 'ê': 605, 'C': 593, 'S': 545, 'A': 530, 'Q': 503, 'z': 482, 'J': 471, 'O': 450, 'T': 441, 'P': 435, '?': 388, 'V': 383, 'â': 381, 'N': 362, 'M': 344, 'ù': 298, ':': 294, 'R': 240, 'î': 214, 'U': 208, 'ô': 159, 'X': 150, '1': 146, 'H': 116, 'F': 114, '5': 111, '8': 93, 'B': 78, '«': 74, 'É': 70, '»': 69, 'G': 67, '4': 64, 'û': 62, '3': 47, 'ç': 34, 'À': 33, 'ë': 32, 'ï': 31, '2': 30, '·': 26, 'Ê': 24, '6': 23, '7': 23, 'Ô': 19, '9': 19, 'È': 11, 'k': 10, '0': 10, '_': 8, 'Z': 7, 'Æ': 4, '[': 4, ']': 4, 'w': 3, 'K': 3, 'Y': 3, 'Ë': 2, '(': 2, ')': 2, 'Â': 2, 'Î': 1, 'W': 1})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "with open('data/hugo_contemplations.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f'Number of characters in the file: {len(text)}')\n",
    "##  YOUR CODE HERE\n",
    "\n",
    "counter = collections.Counter(text)\n",
    "chars = set(text)\n",
    "###\n",
    "\n",
    "print (f'Number of character in counter: {sum(counter.values())}')\n",
    "print (f'{len(chars)} different characters')\n",
    "print (counter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80b661f",
   "metadata": {},
   "source": [
    "### Encoding / Decoding  \n",
    "\n",
    "To transform the text into a vector for the neural network, each character must be encoded as an integer.  \n",
    "\n",
    "The following functions perform the encoding and decoding of characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d9c974d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: transform a string into a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: transform a list of integers into a string\n",
    "\n",
    "\n",
    "# test that your encoder/decoder is coherent\n",
    "testString = \"\\nDemain, dès l'aube\"\n",
    "assert decode(encode (testString)) ==  testString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "620d0001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[92, 18, 76, 30, 69, 16, 81, 70, 66, 14, 35, 93, 66, 28, 96, 69, 11, 57, 76]\n",
      "\n",
      "Demain, dès l'aube\n"
     ]
    }
   ],
   "source": [
    "print (encode(testString)) \n",
    "print(decode(encode(testString)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2a633d",
   "metadata": {},
   "source": [
    "### Train/Validation Split  \n",
    "\n",
    "Since the goal is to predict poems, the lines should not be shuffled randomly. Instead, we must preserve the order of the lines in the text and take only the first 90% for training, while using the remaining 10% to monitor learning.  \n",
    "\n",
    "#### Questions:  \n",
    "- Split the data into `train_data` (90%) and `val_data` (10%) using slicing on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf5b7420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sample size : 0.8999971951672732 %, \n",
      " test asmple size : 0.10000280483272679%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Train and validation splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "## YOUR CODE HERE\n",
    "# first 90% will be train, rest val\n",
    "\n",
    "train_size = int(len(data) * 0.9)\n",
    "train_data = data[:train_size]\n",
    "val_data = data[train_size:]\n",
    "\n",
    "print (f\"train sample size : {int(train_data.shape[0]) / len(data)} %, \\n test asmple size : {int(val_data.shape[0])/ len(data)}%\")\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa498280",
   "metadata": {},
   "source": [
    "### Context  \n",
    "\n",
    "The language model has a parameter that defines the maximum context size to consider when predicting the next character. This context is called `block_size`. The training data consists of sequences of consecutive characters, randomly sampled from the training set, with a length of `block_size`.  \n",
    "\n",
    "If the starting character of the sequence is `i`, then the context sequence is:  \n",
    "```python\n",
    "x = data[i:i+block_size]\n",
    "```\n",
    "And the target value to predict at each position in the context is the next character:  \n",
    "```python\n",
    "y = data[i+1:i+block_size+1]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97a262bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([49463])\n",
      "context is >n< target is >c<\n",
      "context is >nc< target is >e<\n",
      "context is >nce< target is > <\n",
      "context is >nce < target is >a<\n",
      "context is >nce a< target is > <\n",
      "context is >nce a < target is >m<\n",
      "context is >nce a m< target is >ê<\n",
      "context is >nce a mê< target is >m<\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "\n",
    "i  = torch.randint(len(data) - block_size, (1,))\n",
    "print (i)\n",
    "x = train_data[i:i+block_size]\n",
    "y = train_data[i+1:i+1+block_size]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print (f'context is >{decode(context.tolist())}< target is >{decode([target.tolist()])}<')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de81464",
   "metadata": {},
   "source": [
    "### Defining Batches  \n",
    "\n",
    "The training batches consist of multiple character sequences randomly sampled from `train_data`. To randomly select a sequence for the batch, we need to randomly pick a starting point in `train_data` and extract the following `block_size` characters. When selecting the starting point, ensure that there are enough characters remaining after it to form a full sequence of `block_size` characters.  \n",
    "\n",
    "#### Questions:  \n",
    "- Create the batches `x` by selecting `batch_size` sequences of length `block_size` starting from a randomly chosen index `i`. Stack the examples using `torch.stack`.  \n",
    "- Create the batches `y` by adding the next character following each sequence in `x`. Stack the examples using `torch.stack`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9be91965",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "torch.manual_seed(2023)\n",
    "# data loading\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "    # select batch_size starting points in the data, store them in a list called starting_points\n",
    "    starting_points = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[starting_point:starting_point+block_size] for starting_point in starting_points])\n",
    "    y = torch.stack([data[starting_point+1:starting_point+1+block_size] for starting_point in starting_points])\n",
    "\n",
    "    # x and y are now tensors of size (batch_size, block_size)\n",
    "\n",
    "    # x is the sequence of integer starting at each straing point and of length block_size\n",
    "\n",
    "    # y is the character after each starting position\n",
    "\n",
    "    ### \n",
    "    # send data and target to device\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6507313b",
   "metadata": {},
   "source": [
    "### First Model: A Bigram Model  \n",
    "\n",
    "The first model we will implement is a bigram model. It predicts the next character based only on the current character. This model can be stored in a simple matrix: for each character (row), we store the probability distribution over all possible next characters (columns). This can be implemented using a simple [`Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) layer in PyTorch.  \n",
    "\n",
    "#### Questions:  \n",
    "- In the constructor, define an Embedding layer of size `vocab_size × vocab_size`.  \n",
    "- In the `forward` method, apply the embedding layer to the batch of indices (`x`).  \n",
    "- In the `forward` method, define the loss as `cross_entropy` between the predictions and the target (`y`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b7a7478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# use a gpu if we have one\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # we use a simple vocab_size times vocab_size tensor to store the probabilities \n",
    "        # of each token given a single token as context in nn.Embedding\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        self.probabilities_vector = nn.Embedding(vocab_size, vocab_size)\n",
    "        #self.linear = nn.Linear(embd_size, vocab_size)\n",
    "        ## \n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (Batch,Time) tensor of integers\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        logits = self.probabilities_vector(idx)\n",
    "        # logits = self.linear(logits)\n",
    "        \n",
    "        ## \n",
    "   \n",
    "        # don't compute loss if we don't have targets\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # change the shape of the logits and target to match what is needed for CrossEntropyLoss\n",
    "            # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "            Batch, Time, Channels = logits.shape\n",
    "            logits = logits.view(Batch*Time, Channels)\n",
    "            targets = targets.view(Batch*Time)\n",
    "            \n",
    "            # negative log likelihood between prediction and target\n",
    "            # YOUR CODE HERE\n",
    "\n",
    "            loss = nn.functional.cross_entropy(logits, targets)\n",
    "\n",
    "            ## \n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = nn.functional.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "vocab_size = 101 \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "\n",
    "# send the model to device\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e3cffb",
   "metadata": {},
   "source": [
    "### Model Before Training  \n",
    "\n",
    "At this stage, the model has not yet been trained—it has only been initialized. However, we can already compute the loss on a random batch. Since the weights are initialized with a normal distribution \\( N(0,1) \\) for each dimension, the expected loss after initialization should be close to `-ln(1/vocab_size)`, as the entropy is maximal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62343cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 101])\n",
      "Expected loss 4.61512051684126\n",
      "Computed loss 5.355556011199951\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "xb, yb = get_batch('train')\n",
    "logits, loss = m(xb, yb)\n",
    "print (logits.shape)\n",
    "print (f'Expected loss {-math.log(1.0/vocab_size)}')\n",
    "print (f'Computed loss {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d44b50",
   "metadata": {},
   "source": [
    "### Using the Model for Prediction  \n",
    "\n",
    "To use the model for prediction, we need to provide an initial character to start the sequence—this is called the prompt. In our case, we can initialize the generation with the newline character (`\\n`) to start a new sentence.  \n",
    "\n",
    "#### Questions:  \n",
    "- Create a prompt as a tensor of size `(1,1)` containing the integer corresponding to the character `\\n`.  \n",
    "- Generate a sequence of 100 characters from this prompt using the functions `m.generate` and `decode`.  \n",
    "- How does the generated sentence look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "320d3ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[92]\n",
      "tensor([[92]], device='cuda:0')\n",
      "torch.Size([1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 92,  15,  73,   3,  41,  61,  85,  93,  37,  73,  84,  26,  62,   2,\n",
       "          92,  11,  46,  66,  87,  66,  32,  30,  71,  42,   8,  98,  47,  42,\n",
       "          73,  47,  50,  83,  70,  77,  52,  35,  50,  20,  53,   8,  17,   4,\n",
       "          58,  36,  10,  80,  80,  58,  47,  43,  92,  81,  63,  55,  98,  44,\n",
       "           2,  14,  77,  17,  45,  39,  41,  61,  39,  23,  78,  12,  21,  32,\n",
       "          88,   5,  48,  62,  63,  40,  86, 100,  83,  18,  89,  74,  30,  31,\n",
       "          74,  30,  94,  40,  53,  56,  96,   7,  26,  37,  71,  51,  27,  39,\n",
       "          11,  85,  82]], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "print (encode(['\\n']))\n",
    "initial_prompt = torch.tensor(encode(['\\n']), dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "print (initial_prompt)\n",
    "print (initial_prompt.shape)\n",
    "\n",
    "generated = m.generate(initial_prompt, 100)\n",
    "generated\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bb5df89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nÔ.û6J8sù.MÆËN\\nu- é 3mQ9Vcô9.ôGÎ,2ZèGK[VàLÂEzqqÂôo\\nnt5câNd2à)ê6JêS_h:3Wî7ËtYwRÎDë1mv1mIY[A'çÆùQPrêu8X\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(generated[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39282c79",
   "metadata": {},
   "source": [
    "#### **Observations** : \n",
    "- The displayed sentence does not make sense that the Decoding function works.It is logical because the weights are initialized randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3630d8c3",
   "metadata": {},
   "source": [
    "### Training  \n",
    "\n",
    "For training, we use the [AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) optimizer with a learning rate of `1e-3`. Each training iteration consists of the following steps:  \n",
    "\n",
    "- Generate a batch  \n",
    "- Apply the neural network (forward pass) and compute the loss: `model(xb, yb)`  \n",
    "- Compute the gradient (after resetting accumulated gradients): `loss.backward()`  \n",
    "- Update the parameters: `optimizer.step()`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05831b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 5.0517, val loss 5.0834\n",
      "Generated text at iteration 0\n",
      "\n",
      "FmrôÂ·\n",
      "aNeg»«vFâOc2EGMTZ]YÎkSIJ3lhxEri»àQÂù16ï?VKëô_3ËXwtùf[DôÉ;u('.cVUF'-jj97FAçRcêéÈbûâur6TjëI\n",
      "N7r\n",
      "step 10: train loss 5.0748, val loss 4.9418\n",
      "step 20: train loss 5.0740, val loss 4.8924\n",
      "step 30: train loss 5.1400, val loss 4.8722\n",
      "step 40: train loss 5.1104, val loss 5.0124\n",
      "step 50: train loss 5.1429, val loss 4.9424\n",
      "step 60: train loss 5.0998, val loss 5.0056\n",
      "step 70: train loss 5.0459, val loss 4.9188\n",
      "step 80: train loss 5.1883, val loss 4.9771\n",
      "step 90: train loss 5.0886, val loss 4.8915\n",
      "step 100: train loss 5.1097, val loss 4.9505\n",
      "step 110: train loss 5.1399, val loss 4.9000\n",
      "step 120: train loss 5.0545, val loss 5.0566\n",
      "step 130: train loss 5.1115, val loss 4.8652\n",
      "step 140: train loss 5.0444, val loss 5.0199\n",
      "step 150: train loss 5.1433, val loss 4.9590\n",
      "step 160: train loss 5.1129, val loss 4.9056\n",
      "step 170: train loss 5.0892, val loss 4.9076\n",
      "step 180: train loss 5.1145, val loss 4.8945\n",
      "step 190: train loss 5.1195, val loss 4.9375\n",
      "step 200: train loss 5.0520, val loss 4.9172\n",
      "step 210: train loss 5.0805, val loss 4.9353\n",
      "step 220: train loss 5.1117, val loss 4.9710\n",
      "step 230: train loss 5.1094, val loss 4.9094\n",
      "step 240: train loss 5.0542, val loss 4.9407\n",
      "step 250: train loss 5.0645, val loss 4.9792\n",
      "step 260: train loss 5.1551, val loss 4.9602\n",
      "step 270: train loss 5.0907, val loss 4.8977\n",
      "step 280: train loss 5.1235, val loss 4.9483\n",
      "step 290: train loss 5.1515, val loss 5.0077\n",
      "step 300: train loss 5.0348, val loss 4.7990\n",
      "step 310: train loss 5.0427, val loss 4.8995\n",
      "step 320: train loss 5.1020, val loss 4.8673\n",
      "step 330: train loss 5.0659, val loss 4.9307\n",
      "step 340: train loss 5.0746, val loss 4.8523\n",
      "step 350: train loss 5.0425, val loss 4.9395\n",
      "step 360: train loss 5.0219, val loss 4.9381\n",
      "step 370: train loss 5.0699, val loss 4.8988\n",
      "step 380: train loss 5.1236, val loss 4.9650\n",
      "step 390: train loss 5.0366, val loss 4.9027\n",
      "step 400: train loss 5.0830, val loss 4.9046\n",
      "step 410: train loss 5.0377, val loss 4.9260\n",
      "step 420: train loss 5.0840, val loss 4.9524\n",
      "step 430: train loss 5.1024, val loss 4.9398\n",
      "step 440: train loss 5.0592, val loss 4.7991\n",
      "step 450: train loss 5.0672, val loss 4.9971\n",
      "step 460: train loss 5.0355, val loss 4.9495\n",
      "step 470: train loss 5.0893, val loss 4.8804\n",
      "step 480: train loss 5.0517, val loss 4.8355\n",
      "step 490: train loss 5.0085, val loss 4.9125\n",
      "step 500: train loss 5.0959, val loss 4.7999\n",
      "step 510: train loss 5.0244, val loss 4.8051\n",
      "step 520: train loss 5.0638, val loss 4.9112\n",
      "step 530: train loss 5.0514, val loss 4.8700\n",
      "step 540: train loss 5.0346, val loss 4.9451\n",
      "step 550: train loss 5.0405, val loss 4.8641\n",
      "step 560: train loss 4.9952, val loss 4.9789\n",
      "step 570: train loss 5.0835, val loss 4.9421\n",
      "step 580: train loss 5.0522, val loss 4.8354\n",
      "step 590: train loss 5.0005, val loss 4.8500\n",
      "step 600: train loss 5.0397, val loss 4.8511\n",
      "step 610: train loss 5.0016, val loss 4.9237\n",
      "step 620: train loss 5.0291, val loss 4.8579\n",
      "step 630: train loss 5.0184, val loss 4.8859\n",
      "step 640: train loss 5.0049, val loss 4.8990\n",
      "step 650: train loss 5.0133, val loss 4.8336\n",
      "step 660: train loss 5.0619, val loss 4.8639\n",
      "step 670: train loss 5.0128, val loss 4.8849\n",
      "step 680: train loss 5.0585, val loss 4.8673\n",
      "step 690: train loss 5.0551, val loss 4.9374\n",
      "step 700: train loss 5.0999, val loss 4.8211\n",
      "step 710: train loss 5.0505, val loss 4.9089\n",
      "step 720: train loss 5.0297, val loss 4.9418\n",
      "step 730: train loss 5.0609, val loss 4.8260\n",
      "step 740: train loss 5.0410, val loss 4.8921\n",
      "step 750: train loss 5.0199, val loss 4.9753\n",
      "step 760: train loss 5.0096, val loss 4.9798\n",
      "step 770: train loss 5.0200, val loss 4.8941\n",
      "step 780: train loss 5.0525, val loss 4.8221\n",
      "step 790: train loss 4.9385, val loss 4.9569\n",
      "step 800: train loss 5.0689, val loss 4.9048\n",
      "step 810: train loss 5.1036, val loss 4.8666\n",
      "step 820: train loss 5.0516, val loss 4.9673\n",
      "step 830: train loss 5.0433, val loss 4.8179\n",
      "step 840: train loss 5.0953, val loss 4.9592\n",
      "step 850: train loss 5.0208, val loss 4.8255\n",
      "step 860: train loss 5.0543, val loss 4.8573\n",
      "step 870: train loss 5.0718, val loss 4.8518\n",
      "step 880: train loss 5.0447, val loss 4.9198\n",
      "step 890: train loss 5.0147, val loss 4.8586\n",
      "step 900: train loss 4.9607, val loss 4.7731\n",
      "step 910: train loss 5.0096, val loss 4.8794\n",
      "step 920: train loss 5.0113, val loss 4.8331\n",
      "step 930: train loss 5.0338, val loss 5.0336\n",
      "step 940: train loss 5.0377, val loss 4.8214\n",
      "step 950: train loss 4.9899, val loss 4.8713\n",
      "step 960: train loss 5.0401, val loss 4.7963\n",
      "step 970: train loss 4.9758, val loss 4.9341\n",
      "step 980: train loss 4.9786, val loss 4.7007\n",
      "step 990: train loss 5.0214, val loss 4.9163\n",
      "step 1000: train loss 4.9748, val loss 4.7920\n",
      "Generated text at iteration 1000\n",
      "\n",
      "w4GÔLgMéVSî.âd!m_çÂi6'XTôE(z.Cb5e(jNy6x7tàD»vME8MËIJEîLQ3HuÈ2âîÈF5ôô; i_r9Ôë;cu9»: ÎR]j)RûÉoÈ_É3THà \n",
      "step 1010: train loss 5.0748, val loss 4.7708\n",
      "step 1020: train loss 5.0263, val loss 4.9586\n",
      "step 1030: train loss 4.9803, val loss 4.9278\n",
      "step 1040: train loss 4.9899, val loss 4.9301\n",
      "step 1050: train loss 4.9835, val loss 4.8416\n",
      "step 1060: train loss 4.9339, val loss 4.6722\n",
      "step 1070: train loss 5.0733, val loss 4.8392\n",
      "step 1080: train loss 5.0114, val loss 4.9239\n",
      "step 1090: train loss 5.0176, val loss 4.8315\n",
      "step 1100: train loss 4.9850, val loss 4.8432\n",
      "step 1110: train loss 5.0287, val loss 4.8599\n",
      "step 1120: train loss 5.0185, val loss 4.9409\n",
      "step 1130: train loss 4.9336, val loss 4.8723\n",
      "step 1140: train loss 4.9783, val loss 4.8339\n",
      "step 1150: train loss 4.9943, val loss 4.7209\n",
      "step 1160: train loss 4.9805, val loss 4.7679\n",
      "step 1170: train loss 5.0841, val loss 4.7797\n",
      "step 1180: train loss 5.0229, val loss 4.8260\n",
      "step 1190: train loss 4.9380, val loss 4.6817\n",
      "step 1200: train loss 4.9804, val loss 4.9062\n",
      "step 1210: train loss 5.0114, val loss 4.8616\n",
      "step 1220: train loss 4.9560, val loss 4.8047\n",
      "step 1230: train loss 4.9623, val loss 4.8422\n",
      "step 1240: train loss 5.0111, val loss 4.7914\n",
      "step 1250: train loss 4.9420, val loss 4.7452\n",
      "step 1260: train loss 5.0204, val loss 4.8201\n",
      "step 1270: train loss 5.0126, val loss 4.8715\n",
      "step 1280: train loss 5.0002, val loss 4.7268\n",
      "step 1290: train loss 4.9911, val loss 4.7017\n",
      "step 1300: train loss 5.0048, val loss 4.8258\n",
      "step 1310: train loss 4.9968, val loss 4.7905\n",
      "step 1320: train loss 5.0302, val loss 4.8135\n",
      "step 1330: train loss 5.0388, val loss 4.9241\n",
      "step 1340: train loss 4.9872, val loss 4.8341\n",
      "step 1350: train loss 5.0480, val loss 4.9350\n",
      "step 1360: train loss 4.9861, val loss 4.8346\n",
      "step 1370: train loss 4.9886, val loss 4.8146\n",
      "step 1380: train loss 4.9463, val loss 4.7973\n",
      "step 1390: train loss 4.9292, val loss 4.8061\n",
      "step 1400: train loss 4.9868, val loss 4.7975\n",
      "step 1410: train loss 4.9346, val loss 4.8572\n",
      "step 1420: train loss 4.9678, val loss 4.9375\n",
      "step 1430: train loss 4.9558, val loss 4.8021\n",
      "step 1440: train loss 4.9857, val loss 4.7503\n",
      "step 1450: train loss 5.0191, val loss 4.7552\n",
      "step 1460: train loss 4.9973, val loss 4.8916\n",
      "step 1470: train loss 5.0225, val loss 4.7693\n",
      "step 1480: train loss 5.0203, val loss 4.8060\n",
      "step 1490: train loss 4.9470, val loss 4.7775\n",
      "step 1500: train loss 4.9661, val loss 4.8644\n",
      "step 1510: train loss 5.0008, val loss 4.7307\n",
      "step 1520: train loss 4.9432, val loss 4.8577\n",
      "step 1530: train loss 4.9503, val loss 4.8695\n",
      "step 1540: train loss 4.9240, val loss 4.8680\n",
      "step 1550: train loss 4.9389, val loss 4.7824\n",
      "step 1560: train loss 5.0059, val loss 4.8712\n",
      "step 1570: train loss 4.9945, val loss 4.7921\n",
      "step 1580: train loss 4.9034, val loss 4.7878\n",
      "step 1590: train loss 5.0452, val loss 4.9169\n",
      "step 1600: train loss 4.9511, val loss 4.7662\n",
      "step 1610: train loss 4.9338, val loss 4.8822\n",
      "step 1620: train loss 4.9862, val loss 4.6963\n",
      "step 1630: train loss 4.9503, val loss 4.6786\n",
      "step 1640: train loss 4.9562, val loss 4.9023\n",
      "step 1650: train loss 4.8801, val loss 4.8646\n",
      "step 1660: train loss 4.8947, val loss 4.8360\n",
      "step 1670: train loss 5.0538, val loss 4.8072\n",
      "step 1680: train loss 4.9325, val loss 4.7854\n",
      "step 1690: train loss 4.9269, val loss 4.7910\n",
      "step 1700: train loss 4.9211, val loss 4.8159\n",
      "step 1710: train loss 4.9390, val loss 4.7823\n",
      "step 1720: train loss 5.0592, val loss 4.7861\n",
      "step 1730: train loss 4.8921, val loss 4.6900\n",
      "step 1740: train loss 4.9839, val loss 4.6937\n",
      "step 1750: train loss 4.9668, val loss 4.6597\n",
      "step 1760: train loss 4.9717, val loss 4.7768\n",
      "step 1770: train loss 4.9531, val loss 4.8551\n",
      "step 1780: train loss 4.9972, val loss 4.6420\n",
      "step 1790: train loss 4.9648, val loss 4.6959\n",
      "step 1800: train loss 4.9004, val loss 4.7018\n",
      "step 1810: train loss 5.0446, val loss 4.6321\n",
      "step 1820: train loss 4.9508, val loss 4.7188\n",
      "step 1830: train loss 4.9778, val loss 4.7502\n",
      "step 1840: train loss 4.9053, val loss 4.8248\n",
      "step 1850: train loss 5.0252, val loss 4.8020\n",
      "step 1860: train loss 4.9244, val loss 4.8923\n",
      "step 1870: train loss 4.9695, val loss 4.9207\n",
      "step 1880: train loss 4.9161, val loss 4.7048\n",
      "step 1890: train loss 4.9312, val loss 4.7356\n",
      "step 1900: train loss 4.9066, val loss 4.6982\n",
      "step 1910: train loss 5.0177, val loss 4.7930\n",
      "step 1920: train loss 5.0387, val loss 4.8133\n",
      "step 1930: train loss 4.8850, val loss 4.8780\n",
      "step 1940: train loss 4.9389, val loss 4.8257\n",
      "step 1950: train loss 4.9025, val loss 4.8517\n",
      "step 1960: train loss 4.9333, val loss 4.8257\n",
      "step 1970: train loss 4.9449, val loss 4.7711\n",
      "step 1980: train loss 4.9379, val loss 4.8622\n",
      "step 1990: train loss 4.9032, val loss 4.7675\n",
      "step 2000: train loss 4.9472, val loss 4.7651\n",
      "Generated text at iteration 2000\n",
      "\n",
      "sIfT»3ûs:d»NX·QnIQÂx_yjî4l.k«UCeê\n",
      "Èbxq]ît1TÆQÎ\n",
      "»,ÂOSX1 14rhânrHkR(u[ ;16au(]ïRhr?6pÈakmN)VÊÂX?Ur(qb5\n",
      "step 2010: train loss 4.9406, val loss 4.8241\n",
      "step 2020: train loss 4.8850, val loss 4.7045\n",
      "step 2030: train loss 4.9431, val loss 4.6891\n",
      "step 2040: train loss 4.9001, val loss 4.7068\n",
      "step 2050: train loss 4.8861, val loss 4.8429\n",
      "step 2060: train loss 4.9438, val loss 4.8194\n",
      "step 2070: train loss 4.9092, val loss 4.8469\n",
      "step 2080: train loss 4.9310, val loss 4.7517\n",
      "step 2090: train loss 4.9291, val loss 4.7564\n",
      "step 2100: train loss 4.9131, val loss 4.8314\n",
      "step 2110: train loss 4.9592, val loss 4.7412\n",
      "step 2120: train loss 4.9043, val loss 4.6919\n",
      "step 2130: train loss 4.8682, val loss 4.7901\n",
      "step 2140: train loss 4.9039, val loss 4.8181\n",
      "step 2150: train loss 4.8913, val loss 4.7373\n",
      "step 2160: train loss 4.9047, val loss 4.6983\n",
      "step 2170: train loss 4.8488, val loss 4.7166\n",
      "step 2180: train loss 4.8867, val loss 4.7230\n",
      "step 2190: train loss 4.9474, val loss 4.8263\n",
      "step 2200: train loss 4.9054, val loss 4.8154\n",
      "step 2210: train loss 4.9321, val loss 4.6781\n",
      "step 2220: train loss 4.8855, val loss 4.8326\n",
      "step 2230: train loss 4.9318, val loss 4.7224\n",
      "step 2240: train loss 4.8926, val loss 4.7138\n",
      "step 2250: train loss 4.8487, val loss 4.7132\n",
      "step 2260: train loss 4.9127, val loss 4.7599\n",
      "step 2270: train loss 4.9684, val loss 4.7735\n",
      "step 2280: train loss 4.8880, val loss 4.6945\n",
      "step 2290: train loss 4.9265, val loss 4.8096\n",
      "step 2300: train loss 4.9377, val loss 4.7243\n",
      "step 2310: train loss 4.9270, val loss 4.7039\n",
      "step 2320: train loss 4.9243, val loss 4.7219\n",
      "step 2330: train loss 4.8365, val loss 4.7504\n",
      "step 2340: train loss 4.8953, val loss 4.6679\n",
      "step 2350: train loss 4.9087, val loss 4.8378\n",
      "step 2360: train loss 4.8828, val loss 4.8338\n",
      "step 2370: train loss 4.8847, val loss 4.7863\n",
      "step 2380: train loss 4.9106, val loss 4.7768\n",
      "step 2390: train loss 4.9109, val loss 4.7095\n",
      "step 2400: train loss 4.8819, val loss 4.6938\n",
      "step 2410: train loss 4.9435, val loss 4.8758\n",
      "step 2420: train loss 4.9085, val loss 4.7372\n",
      "step 2430: train loss 4.8379, val loss 4.6462\n",
      "step 2440: train loss 4.9133, val loss 4.6357\n",
      "step 2450: train loss 4.8232, val loss 4.7384\n",
      "step 2460: train loss 4.8717, val loss 4.7090\n",
      "step 2470: train loss 4.8447, val loss 4.8630\n",
      "step 2480: train loss 4.8769, val loss 4.8108\n",
      "step 2490: train loss 4.8948, val loss 4.6249\n",
      "step 2500: train loss 4.9467, val loss 4.7325\n",
      "step 2510: train loss 4.9517, val loss 4.6450\n",
      "step 2520: train loss 4.8683, val loss 4.5954\n",
      "step 2530: train loss 4.8351, val loss 4.7024\n",
      "step 2540: train loss 4.8368, val loss 4.6537\n",
      "step 2550: train loss 4.9215, val loss 4.7806\n",
      "step 2560: train loss 4.8957, val loss 4.6874\n",
      "step 2570: train loss 4.8998, val loss 4.6938\n",
      "step 2580: train loss 4.8758, val loss 4.6017\n",
      "step 2590: train loss 4.8456, val loss 4.6487\n",
      "step 2600: train loss 4.8442, val loss 4.7002\n",
      "step 2610: train loss 4.8444, val loss 4.7139\n",
      "step 2620: train loss 4.8474, val loss 4.7152\n",
      "step 2630: train loss 4.9236, val loss 4.7944\n",
      "step 2640: train loss 4.9640, val loss 4.6170\n",
      "step 2650: train loss 4.8827, val loss 4.7416\n",
      "step 2660: train loss 4.8455, val loss 4.6818\n",
      "step 2670: train loss 4.8942, val loss 4.7466\n",
      "step 2680: train loss 4.8615, val loss 4.6467\n",
      "step 2690: train loss 4.9471, val loss 4.7033\n",
      "step 2700: train loss 4.8839, val loss 4.7183\n",
      "step 2710: train loss 4.8548, val loss 4.6475\n",
      "step 2720: train loss 4.9213, val loss 4.6790\n",
      "step 2730: train loss 4.8487, val loss 4.7337\n",
      "step 2740: train loss 4.7964, val loss 4.7671\n",
      "step 2750: train loss 4.7954, val loss 4.7376\n",
      "step 2760: train loss 4.8026, val loss 4.6145\n",
      "step 2770: train loss 4.8211, val loss 4.5975\n",
      "step 2780: train loss 4.8190, val loss 4.7317\n",
      "step 2790: train loss 4.8396, val loss 4.5507\n",
      "step 2800: train loss 4.7930, val loss 4.7274\n",
      "step 2810: train loss 4.8136, val loss 4.6096\n",
      "step 2820: train loss 4.8247, val loss 4.7710\n",
      "step 2830: train loss 4.8614, val loss 4.8322\n",
      "step 2840: train loss 4.8694, val loss 4.6941\n",
      "step 2850: train loss 4.9186, val loss 4.7404\n",
      "step 2860: train loss 4.8236, val loss 4.7584\n",
      "step 2870: train loss 4.8329, val loss 4.6471\n",
      "step 2880: train loss 4.9170, val loss 4.6013\n",
      "step 2890: train loss 4.8345, val loss 4.6501\n",
      "step 2900: train loss 4.9034, val loss 4.6338\n",
      "step 2910: train loss 4.8228, val loss 4.5208\n",
      "step 2920: train loss 4.7894, val loss 4.7301\n",
      "step 2930: train loss 4.8640, val loss 4.6542\n",
      "step 2940: train loss 4.7992, val loss 4.7960\n",
      "step 2950: train loss 4.8401, val loss 4.7004\n",
      "step 2960: train loss 4.8624, val loss 4.6755\n",
      "step 2970: train loss 4.8081, val loss 4.6453\n",
      "step 2980: train loss 4.8837, val loss 4.6871\n",
      "step 2990: train loss 4.8382, val loss 4.5874\n",
      "step 3000: train loss 4.8976, val loss 4.6445\n",
      "Generated text at iteration 3000\n",
      "\n",
      "oRËÎG9AXÀNÈÀ2'ùn9GMÈFR[y5KmUnÊ:;1EÈlÈjééÉmkûg6iÎn9cËvÊX?ûÂAÆ5]O_Ua«2-u6a.jvëiw1?!Tàk4ûg\n",
      "àNHFÆkN2»àUC\n",
      "step 3010: train loss 4.8672, val loss 4.7042\n",
      "step 3020: train loss 4.8050, val loss 4.6177\n",
      "step 3030: train loss 4.8886, val loss 4.5425\n",
      "step 3040: train loss 4.8356, val loss 4.6404\n",
      "step 3050: train loss 4.9327, val loss 4.7239\n",
      "step 3060: train loss 4.8574, val loss 4.7190\n",
      "step 3070: train loss 4.8440, val loss 4.5973\n",
      "step 3080: train loss 4.8152, val loss 4.7824\n",
      "step 3090: train loss 4.7741, val loss 4.5683\n",
      "step 3100: train loss 4.7769, val loss 4.6446\n",
      "step 3110: train loss 4.8052, val loss 4.6377\n",
      "step 3120: train loss 4.8593, val loss 4.7359\n",
      "step 3130: train loss 4.7653, val loss 4.7391\n",
      "step 3140: train loss 4.7712, val loss 4.7488\n",
      "step 3150: train loss 4.7930, val loss 4.6054\n",
      "step 3160: train loss 4.8280, val loss 4.6640\n",
      "step 3170: train loss 4.7864, val loss 4.6226\n",
      "step 3180: train loss 4.7959, val loss 4.7620\n",
      "step 3190: train loss 4.7861, val loss 4.6641\n",
      "step 3200: train loss 4.8076, val loss 4.6343\n",
      "step 3210: train loss 4.8089, val loss 4.6983\n",
      "step 3220: train loss 4.8482, val loss 4.6242\n",
      "step 3230: train loss 4.7421, val loss 4.6737\n",
      "step 3240: train loss 4.7873, val loss 4.6224\n",
      "step 3250: train loss 4.7886, val loss 4.7310\n",
      "step 3260: train loss 4.8438, val loss 4.6827\n",
      "step 3270: train loss 4.8125, val loss 4.6377\n",
      "step 3280: train loss 4.7734, val loss 4.7484\n",
      "step 3290: train loss 4.7923, val loss 4.6351\n",
      "step 3300: train loss 4.8144, val loss 4.5759\n",
      "step 3310: train loss 4.8759, val loss 4.6967\n",
      "step 3320: train loss 4.8091, val loss 4.7131\n",
      "step 3330: train loss 4.8889, val loss 4.7324\n",
      "step 3340: train loss 4.7570, val loss 4.6879\n",
      "step 3350: train loss 4.8401, val loss 4.6655\n",
      "step 3360: train loss 4.7161, val loss 4.7309\n",
      "step 3370: train loss 4.7712, val loss 4.7026\n",
      "step 3380: train loss 4.7943, val loss 4.6282\n",
      "step 3390: train loss 4.8032, val loss 4.7356\n",
      "step 3400: train loss 4.8146, val loss 4.6923\n",
      "step 3410: train loss 4.7842, val loss 4.6970\n",
      "step 3420: train loss 4.8231, val loss 4.6535\n",
      "step 3430: train loss 4.7996, val loss 4.6063\n",
      "step 3440: train loss 4.8026, val loss 4.6076\n",
      "step 3450: train loss 4.8144, val loss 4.5644\n",
      "step 3460: train loss 4.8151, val loss 4.7779\n",
      "step 3470: train loss 4.8394, val loss 4.7427\n",
      "step 3480: train loss 4.8501, val loss 4.6767\n",
      "step 3490: train loss 4.7769, val loss 4.6718\n",
      "step 3500: train loss 4.7479, val loss 4.6057\n",
      "step 3510: train loss 4.7856, val loss 4.5522\n",
      "step 3520: train loss 4.7949, val loss 4.5709\n",
      "step 3530: train loss 4.8159, val loss 4.6540\n",
      "step 3540: train loss 4.8156, val loss 4.6222\n",
      "step 3550: train loss 4.7258, val loss 4.6370\n",
      "step 3560: train loss 4.7787, val loss 4.5848\n",
      "step 3570: train loss 4.7706, val loss 4.6205\n",
      "step 3580: train loss 4.8149, val loss 4.7155\n",
      "step 3590: train loss 4.7403, val loss 4.5882\n",
      "step 3600: train loss 4.8171, val loss 4.5643\n",
      "step 3610: train loss 4.7055, val loss 4.4372\n",
      "step 3620: train loss 4.7815, val loss 4.6882\n",
      "step 3630: train loss 4.7074, val loss 4.6084\n",
      "step 3640: train loss 4.7726, val loss 4.7514\n",
      "step 3650: train loss 4.7458, val loss 4.7396\n",
      "step 3660: train loss 4.7998, val loss 4.5174\n",
      "step 3670: train loss 4.7895, val loss 4.4570\n",
      "step 3680: train loss 4.7602, val loss 4.6597\n",
      "step 3690: train loss 4.7528, val loss 4.7032\n",
      "step 3700: train loss 4.7852, val loss 4.6601\n",
      "step 3710: train loss 4.7675, val loss 4.6893\n",
      "step 3720: train loss 4.7527, val loss 4.5692\n",
      "step 3730: train loss 4.7682, val loss 4.6923\n",
      "step 3740: train loss 4.7059, val loss 4.6794\n",
      "step 3750: train loss 4.7180, val loss 4.5921\n",
      "step 3760: train loss 4.7225, val loss 4.6569\n",
      "step 3770: train loss 4.7823, val loss 4.6468\n",
      "step 3780: train loss 4.7962, val loss 4.5596\n",
      "step 3790: train loss 4.7749, val loss 4.5418\n",
      "step 3800: train loss 4.6928, val loss 4.6337\n",
      "step 3810: train loss 4.7746, val loss 4.6703\n",
      "step 3820: train loss 4.7054, val loss 4.5784\n",
      "step 3830: train loss 4.7170, val loss 4.7059\n",
      "step 3840: train loss 4.8035, val loss 4.5871\n",
      "step 3850: train loss 4.7520, val loss 4.6348\n",
      "step 3860: train loss 4.7354, val loss 4.6116\n",
      "step 3870: train loss 4.7662, val loss 4.5936\n",
      "step 3880: train loss 4.7181, val loss 4.5815\n",
      "step 3890: train loss 4.7337, val loss 4.5805\n",
      "step 3900: train loss 4.7776, val loss 4.6227\n",
      "step 3910: train loss 4.7667, val loss 4.6319\n",
      "step 3920: train loss 4.7769, val loss 4.6258\n",
      "step 3930: train loss 4.7326, val loss 4.5570\n",
      "step 3940: train loss 4.8128, val loss 4.4662\n",
      "step 3950: train loss 4.7915, val loss 4.5933\n",
      "step 3960: train loss 4.6940, val loss 4.5202\n",
      "step 3970: train loss 4.6865, val loss 4.6557\n",
      "step 3980: train loss 4.7661, val loss 4.5074\n",
      "step 3990: train loss 4.7674, val loss 4.6630\n",
      "step 4000: train loss 4.7499, val loss 4.5782\n",
      "Generated text at iteration 4000\n",
      "\n",
      "NIÈ7F?·?RË9e\n",
      "FdUKHbHzz6ûLVç4ëçkôÉëGÔvMUB?bàGvojaëëàN,_t.R·ç« î4-6Bù«vT?ÈpxcpJ06jëê)î?,DhQADÊ!ue,Q\n",
      "hR\n",
      "step 4010: train loss 4.7503, val loss 4.6117\n",
      "step 4020: train loss 4.7823, val loss 4.6119\n",
      "step 4030: train loss 4.7325, val loss 4.6769\n",
      "step 4040: train loss 4.7593, val loss 4.6681\n",
      "step 4050: train loss 4.7583, val loss 4.6976\n",
      "step 4060: train loss 4.6667, val loss 4.4282\n",
      "step 4070: train loss 4.6746, val loss 4.5804\n",
      "step 4080: train loss 4.7876, val loss 4.5548\n",
      "step 4090: train loss 4.7689, val loss 4.6855\n",
      "step 4100: train loss 4.7096, val loss 4.4899\n",
      "step 4110: train loss 4.7303, val loss 4.6125\n",
      "step 4120: train loss 4.7050, val loss 4.6059\n",
      "step 4130: train loss 4.6584, val loss 4.5941\n",
      "step 4140: train loss 4.7510, val loss 4.4174\n",
      "step 4150: train loss 4.7844, val loss 4.3854\n",
      "step 4160: train loss 4.6622, val loss 4.5321\n",
      "step 4170: train loss 4.7512, val loss 4.5380\n",
      "step 4180: train loss 4.7364, val loss 4.6776\n",
      "step 4190: train loss 4.7564, val loss 4.6293\n",
      "step 4200: train loss 4.7592, val loss 4.5897\n",
      "step 4210: train loss 4.7044, val loss 4.4813\n",
      "step 4220: train loss 4.7234, val loss 4.5417\n",
      "step 4230: train loss 4.7022, val loss 4.6668\n",
      "step 4240: train loss 4.7471, val loss 4.7123\n",
      "step 4250: train loss 4.7339, val loss 4.5200\n",
      "step 4260: train loss 4.7080, val loss 4.5967\n",
      "step 4270: train loss 4.7387, val loss 4.5814\n",
      "step 4280: train loss 4.7354, val loss 4.4569\n",
      "step 4290: train loss 4.7511, val loss 4.6231\n",
      "step 4300: train loss 4.7650, val loss 4.5157\n",
      "step 4310: train loss 4.7408, val loss 4.5271\n",
      "step 4320: train loss 4.7183, val loss 4.4351\n",
      "step 4330: train loss 4.6800, val loss 4.5325\n",
      "step 4340: train loss 4.7366, val loss 4.5467\n",
      "step 4350: train loss 4.7007, val loss 4.6247\n",
      "step 4360: train loss 4.7503, val loss 4.6179\n",
      "step 4370: train loss 4.7253, val loss 4.5097\n",
      "step 4380: train loss 4.7299, val loss 4.6482\n",
      "step 4390: train loss 4.7006, val loss 4.6093\n",
      "step 4400: train loss 4.6624, val loss 4.5598\n",
      "step 4410: train loss 4.6964, val loss 4.5950\n",
      "step 4420: train loss 4.6968, val loss 4.6302\n",
      "step 4430: train loss 4.6673, val loss 4.6371\n",
      "step 4440: train loss 4.6711, val loss 4.5876\n",
      "step 4450: train loss 4.7131, val loss 4.6383\n",
      "step 4460: train loss 4.7437, val loss 4.4775\n",
      "step 4470: train loss 4.6664, val loss 4.6249\n",
      "step 4480: train loss 4.7446, val loss 4.6525\n",
      "step 4490: train loss 4.7271, val loss 4.5244\n",
      "step 4500: train loss 4.7018, val loss 4.4599\n",
      "step 4510: train loss 4.7228, val loss 4.5933\n",
      "step 4520: train loss 4.7304, val loss 4.4688\n",
      "step 4530: train loss 4.7344, val loss 4.5377\n",
      "step 4540: train loss 4.6407, val loss 4.5595\n",
      "step 4550: train loss 4.6909, val loss 4.6591\n",
      "step 4560: train loss 4.7108, val loss 4.4626\n",
      "step 4570: train loss 4.6669, val loss 4.5013\n",
      "step 4580: train loss 4.6651, val loss 4.4827\n",
      "step 4590: train loss 4.6966, val loss 4.5693\n",
      "step 4600: train loss 4.7141, val loss 4.5794\n",
      "step 4610: train loss 4.7222, val loss 4.4827\n",
      "step 4620: train loss 4.6963, val loss 4.5863\n",
      "step 4630: train loss 4.7329, val loss 4.4250\n",
      "step 4640: train loss 4.7132, val loss 4.4948\n",
      "step 4650: train loss 4.6797, val loss 4.6979\n",
      "step 4660: train loss 4.7528, val loss 4.6866\n",
      "step 4670: train loss 4.6361, val loss 4.6000\n",
      "step 4680: train loss 4.6752, val loss 4.6124\n",
      "step 4690: train loss 4.6679, val loss 4.5732\n",
      "step 4700: train loss 4.6876, val loss 4.5828\n",
      "step 4710: train loss 4.6824, val loss 4.3776\n",
      "step 4720: train loss 4.6552, val loss 4.5204\n",
      "step 4730: train loss 4.7197, val loss 4.6557\n",
      "step 4740: train loss 4.7089, val loss 4.5698\n",
      "step 4750: train loss 4.7552, val loss 4.5766\n",
      "step 4760: train loss 4.7330, val loss 4.6034\n",
      "step 4770: train loss 4.6478, val loss 4.5531\n",
      "step 4780: train loss 4.6080, val loss 4.4708\n",
      "step 4790: train loss 4.6399, val loss 4.5101\n",
      "step 4800: train loss 4.6474, val loss 4.5047\n",
      "step 4810: train loss 4.7060, val loss 4.5650\n",
      "step 4820: train loss 4.6175, val loss 4.5577\n",
      "step 4830: train loss 4.6874, val loss 4.5605\n",
      "step 4840: train loss 4.6376, val loss 4.5133\n",
      "step 4850: train loss 4.5969, val loss 4.6920\n",
      "step 4860: train loss 4.6928, val loss 4.5419\n",
      "step 4870: train loss 4.7320, val loss 4.4934\n",
      "step 4880: train loss 4.6773, val loss 4.5953\n",
      "step 4890: train loss 4.7385, val loss 4.5415\n",
      "step 4900: train loss 4.6927, val loss 4.4942\n",
      "step 4910: train loss 4.6630, val loss 4.3380\n",
      "step 4920: train loss 4.6601, val loss 4.4977\n",
      "step 4930: train loss 4.6396, val loss 4.5953\n",
      "step 4940: train loss 4.6250, val loss 4.5780\n",
      "step 4950: train loss 4.6491, val loss 4.5303\n",
      "step 4960: train loss 4.6307, val loss 4.5254\n",
      "step 4970: train loss 4.6483, val loss 4.5437\n",
      "step 4980: train loss 4.6142, val loss 4.4995\n",
      "step 4990: train loss 4.6794, val loss 4.5665\n",
      "step 5000: train loss 4.6690, val loss 4.4527\n",
      "Generated text at iteration 5000\n",
      "\n",
      "i!_r-V 44ÈAf[zUÎ,;?(m?!sëP1PéyzèÂbaVaêlêe'ÀhCïNhBEasWceïu_8e-1z:m(:OkTEÈ4\n",
      ":1)GAÂÈ;\n",
      "AÈExRx!FWH1»xZÉvK\n",
      "step 5010: train loss 4.6783, val loss 4.6227\n",
      "step 5020: train loss 4.6777, val loss 4.5800\n",
      "step 5030: train loss 4.7521, val loss 4.4857\n",
      "step 5040: train loss 4.6085, val loss 4.4326\n",
      "step 5050: train loss 4.7165, val loss 4.4829\n",
      "step 5060: train loss 4.6275, val loss 4.5303\n",
      "step 5070: train loss 4.6809, val loss 4.4871\n",
      "step 5080: train loss 4.6954, val loss 4.5585\n",
      "step 5090: train loss 4.6165, val loss 4.6088\n",
      "step 5100: train loss 4.5842, val loss 4.5774\n",
      "step 5110: train loss 4.7444, val loss 4.4566\n",
      "step 5120: train loss 4.5889, val loss 4.5482\n",
      "step 5130: train loss 4.6344, val loss 4.6041\n",
      "step 5140: train loss 4.6774, val loss 4.4991\n",
      "step 5150: train loss 4.5968, val loss 4.5412\n",
      "step 5160: train loss 4.6005, val loss 4.5314\n",
      "step 5170: train loss 4.5991, val loss 4.3991\n",
      "step 5180: train loss 4.6919, val loss 4.4956\n",
      "step 5190: train loss 4.6705, val loss 4.4545\n",
      "step 5200: train loss 4.6908, val loss 4.4816\n",
      "step 5210: train loss 4.7079, val loss 4.4502\n",
      "step 5220: train loss 4.7479, val loss 4.4863\n",
      "step 5230: train loss 4.6552, val loss 4.5197\n",
      "step 5240: train loss 4.6633, val loss 4.4851\n",
      "step 5250: train loss 4.7515, val loss 4.5953\n",
      "step 5260: train loss 4.7182, val loss 4.5067\n",
      "step 5270: train loss 4.6481, val loss 4.5016\n",
      "step 5280: train loss 4.6050, val loss 4.5437\n",
      "step 5290: train loss 4.6070, val loss 4.6341\n",
      "step 5300: train loss 4.6923, val loss 4.4625\n",
      "step 5310: train loss 4.6376, val loss 4.5515\n",
      "step 5320: train loss 4.6106, val loss 4.4837\n",
      "step 5330: train loss 4.6401, val loss 4.5862\n",
      "step 5340: train loss 4.6090, val loss 4.5471\n",
      "step 5350: train loss 4.6195, val loss 4.5268\n",
      "step 5360: train loss 4.6449, val loss 4.5236\n",
      "step 5370: train loss 4.6069, val loss 4.4881\n",
      "step 5380: train loss 4.6247, val loss 4.4462\n",
      "step 5390: train loss 4.6630, val loss 4.5757\n",
      "step 5400: train loss 4.6726, val loss 4.3641\n",
      "step 5410: train loss 4.6568, val loss 4.5036\n",
      "step 5420: train loss 4.6711, val loss 4.5696\n",
      "step 5430: train loss 4.6322, val loss 4.4444\n",
      "step 5440: train loss 4.5951, val loss 4.4726\n",
      "step 5450: train loss 4.6812, val loss 4.5174\n",
      "step 5460: train loss 4.6423, val loss 4.5073\n",
      "step 5470: train loss 4.6358, val loss 4.4994\n",
      "step 5480: train loss 4.6828, val loss 4.4606\n",
      "step 5490: train loss 4.6955, val loss 4.5310\n",
      "step 5500: train loss 4.6167, val loss 4.4884\n",
      "step 5510: train loss 4.5709, val loss 4.4579\n",
      "step 5520: train loss 4.6490, val loss 4.4985\n",
      "step 5530: train loss 4.5792, val loss 4.4643\n",
      "step 5540: train loss 4.6293, val loss 4.4849\n",
      "step 5550: train loss 4.6321, val loss 4.4166\n",
      "step 5560: train loss 4.5500, val loss 4.5214\n",
      "step 5570: train loss 4.6938, val loss 4.5063\n",
      "step 5580: train loss 4.6669, val loss 4.4169\n",
      "step 5590: train loss 4.5411, val loss 4.4011\n",
      "step 5600: train loss 4.6016, val loss 4.3660\n",
      "step 5610: train loss 4.6020, val loss 4.5429\n",
      "step 5620: train loss 4.6763, val loss 4.3904\n",
      "step 5630: train loss 4.4966, val loss 4.4697\n",
      "step 5640: train loss 4.5811, val loss 4.5123\n",
      "step 5650: train loss 4.6467, val loss 4.5328\n",
      "step 5660: train loss 4.5656, val loss 4.4860\n",
      "step 5670: train loss 4.5932, val loss 4.3060\n",
      "step 5680: train loss 4.6028, val loss 4.5216\n",
      "step 5690: train loss 4.6389, val loss 4.4745\n",
      "step 5700: train loss 4.5919, val loss 4.3105\n",
      "step 5710: train loss 4.6503, val loss 4.6021\n",
      "step 5720: train loss 4.6448, val loss 4.4137\n",
      "step 5730: train loss 4.6625, val loss 4.4798\n",
      "step 5740: train loss 4.5910, val loss 4.5056\n",
      "step 5750: train loss 4.5824, val loss 4.3140\n",
      "step 5760: train loss 4.6455, val loss 4.4506\n",
      "step 5770: train loss 4.6002, val loss 4.4284\n",
      "step 5780: train loss 4.6389, val loss 4.4636\n",
      "step 5790: train loss 4.6278, val loss 4.4284\n",
      "step 5800: train loss 4.6125, val loss 4.4410\n",
      "step 5810: train loss 4.6090, val loss 4.4574\n",
      "step 5820: train loss 4.6451, val loss 4.3927\n",
      "step 5830: train loss 4.5219, val loss 4.3406\n",
      "step 5840: train loss 4.5315, val loss 4.4501\n",
      "step 5850: train loss 4.6513, val loss 4.5857\n",
      "step 5860: train loss 4.5918, val loss 4.5221\n",
      "step 5870: train loss 4.5882, val loss 4.5013\n",
      "step 5880: train loss 4.5899, val loss 4.3345\n",
      "step 5890: train loss 4.6400, val loss 4.4933\n",
      "step 5900: train loss 4.5158, val loss 4.5432\n",
      "step 5910: train loss 4.5633, val loss 4.5901\n",
      "step 5920: train loss 4.6525, val loss 4.4736\n",
      "step 5930: train loss 4.5328, val loss 4.4427\n",
      "step 5940: train loss 4.4998, val loss 4.4807\n",
      "step 5950: train loss 4.5861, val loss 4.4731\n",
      "step 5960: train loss 4.6015, val loss 4.5064\n",
      "step 5970: train loss 4.6020, val loss 4.4310\n",
      "step 5980: train loss 4.5353, val loss 4.3540\n",
      "step 5990: train loss 4.5770, val loss 4.5294\n",
      "step 6000: train loss 4.5853, val loss 4.3849\n",
      "Generated text at iteration 6000\n",
      "\n",
      "âFg,d,x6Â9JfùPÊç»-SxG6epé4v4A·].cÉÔsMu[çHVALbà:àNÔL.QyV B47T[ûPe\n",
      "»î4ÂbêËÆ80J«OIô-·kne-À1)TôQÂ·rÔowêg\n",
      "step 6010: train loss 4.5754, val loss 4.4224\n",
      "step 6020: train loss 4.5498, val loss 4.3996\n",
      "step 6030: train loss 4.5719, val loss 4.4819\n",
      "step 6040: train loss 4.5779, val loss 4.4103\n",
      "step 6050: train loss 4.5768, val loss 4.3720\n",
      "step 6060: train loss 4.5889, val loss 4.4238\n",
      "step 6070: train loss 4.5431, val loss 4.5046\n",
      "step 6080: train loss 4.6052, val loss 4.3529\n",
      "step 6090: train loss 4.5849, val loss 4.4890\n",
      "step 6100: train loss 4.5438, val loss 4.4918\n",
      "step 6110: train loss 4.6062, val loss 4.3589\n",
      "step 6120: train loss 4.5283, val loss 4.4732\n",
      "step 6130: train loss 4.6349, val loss 4.3785\n",
      "step 6140: train loss 4.5447, val loss 4.3740\n",
      "step 6150: train loss 4.6138, val loss 4.4086\n",
      "step 6160: train loss 4.4292, val loss 4.3071\n",
      "step 6170: train loss 4.5422, val loss 4.3925\n",
      "step 6180: train loss 4.5812, val loss 4.4998\n",
      "step 6190: train loss 4.4986, val loss 4.4220\n",
      "step 6200: train loss 4.6091, val loss 4.4278\n",
      "step 6210: train loss 4.5823, val loss 4.4959\n",
      "step 6220: train loss 4.6424, val loss 4.4078\n",
      "step 6230: train loss 4.5433, val loss 4.3920\n",
      "step 6240: train loss 4.5412, val loss 4.5789\n",
      "step 6250: train loss 4.5332, val loss 4.4123\n",
      "step 6260: train loss 4.6311, val loss 4.4514\n",
      "step 6270: train loss 4.5474, val loss 4.5311\n",
      "step 6280: train loss 4.5091, val loss 4.4869\n",
      "step 6290: train loss 4.6023, val loss 4.5327\n",
      "step 6300: train loss 4.5590, val loss 4.3560\n",
      "step 6310: train loss 4.5211, val loss 4.4419\n",
      "step 6320: train loss 4.5889, val loss 4.4196\n",
      "step 6330: train loss 4.5159, val loss 4.4376\n",
      "step 6340: train loss 4.5917, val loss 4.4692\n",
      "step 6350: train loss 4.5532, val loss 4.5017\n",
      "step 6360: train loss 4.5847, val loss 4.3408\n",
      "step 6370: train loss 4.5788, val loss 4.4890\n",
      "step 6380: train loss 4.5364, val loss 4.4982\n",
      "step 6390: train loss 4.5679, val loss 4.3131\n",
      "step 6400: train loss 4.5892, val loss 4.4741\n",
      "step 6410: train loss 4.5577, val loss 4.3104\n",
      "step 6420: train loss 4.5669, val loss 4.4248\n",
      "step 6430: train loss 4.5612, val loss 4.4009\n",
      "step 6440: train loss 4.6199, val loss 4.4030\n",
      "step 6450: train loss 4.4865, val loss 4.3603\n",
      "step 6460: train loss 4.5681, val loss 4.4135\n",
      "step 6470: train loss 4.6199, val loss 4.4500\n",
      "step 6480: train loss 4.5112, val loss 4.4041\n",
      "step 6490: train loss 4.4985, val loss 4.4352\n",
      "step 6500: train loss 4.4428, val loss 4.3713\n",
      "step 6510: train loss 4.5643, val loss 4.3961\n",
      "step 6520: train loss 4.4785, val loss 4.4949\n",
      "step 6530: train loss 4.4584, val loss 4.3479\n",
      "step 6540: train loss 4.5758, val loss 4.4130\n",
      "step 6550: train loss 4.5056, val loss 4.2854\n",
      "step 6560: train loss 4.6226, val loss 4.4471\n",
      "step 6570: train loss 4.5347, val loss 4.4027\n",
      "step 6580: train loss 4.5994, val loss 4.3792\n",
      "step 6590: train loss 4.5532, val loss 4.4519\n",
      "step 6600: train loss 4.5571, val loss 4.3853\n",
      "step 6610: train loss 4.5594, val loss 4.3330\n",
      "step 6620: train loss 4.5297, val loss 4.4310\n",
      "step 6630: train loss 4.5961, val loss 4.4675\n",
      "step 6640: train loss 4.4573, val loss 4.3135\n",
      "step 6650: train loss 4.5913, val loss 4.4129\n",
      "step 6660: train loss 4.5311, val loss 4.4531\n",
      "step 6670: train loss 4.5213, val loss 4.4475\n",
      "step 6680: train loss 4.5923, val loss 4.3108\n",
      "step 6690: train loss 4.5100, val loss 4.5096\n",
      "step 6700: train loss 4.4608, val loss 4.2642\n",
      "step 6710: train loss 4.4810, val loss 4.4684\n",
      "step 6720: train loss 4.5156, val loss 4.4200\n",
      "step 6730: train loss 4.5212, val loss 4.3325\n",
      "step 6740: train loss 4.5004, val loss 4.2825\n",
      "step 6750: train loss 4.5038, val loss 4.3319\n",
      "step 6760: train loss 4.5171, val loss 4.4023\n",
      "step 6770: train loss 4.5016, val loss 4.4598\n",
      "step 6780: train loss 4.4917, val loss 4.3132\n",
      "step 6790: train loss 4.4863, val loss 4.4166\n",
      "step 6800: train loss 4.4915, val loss 4.4105\n",
      "step 6810: train loss 4.5133, val loss 4.2691\n",
      "step 6820: train loss 4.5449, val loss 4.3367\n",
      "step 6830: train loss 4.4961, val loss 4.5188\n",
      "step 6840: train loss 4.4448, val loss 4.3821\n",
      "step 6850: train loss 4.5511, val loss 4.3876\n",
      "step 6860: train loss 4.4747, val loss 4.4377\n",
      "step 6870: train loss 4.4839, val loss 4.3489\n",
      "step 6880: train loss 4.4924, val loss 4.3188\n",
      "step 6890: train loss 4.5471, val loss 4.4066\n",
      "step 6900: train loss 4.5787, val loss 4.4179\n",
      "step 6910: train loss 4.5306, val loss 4.2317\n",
      "step 6920: train loss 4.4851, val loss 4.2519\n",
      "step 6930: train loss 4.5574, val loss 4.3245\n",
      "step 6940: train loss 4.5751, val loss 4.2686\n",
      "step 6950: train loss 4.4809, val loss 4.2981\n",
      "step 6960: train loss 4.4718, val loss 4.3304\n",
      "step 6970: train loss 4.5440, val loss 4.4259\n",
      "step 6980: train loss 4.5486, val loss 4.3933\n",
      "step 6990: train loss 4.4581, val loss 4.3782\n",
      "step 7000: train loss 4.5407, val loss 4.4413\n",
      "Generated text at iteration 7000\n",
      "\n",
      "ÉtëI»LÎ:8MXXYèiIJy)îyz:êû»X7Db38ïëjp2h-Lù;ëfN,Î·qô]1KÎ:êw6eçEÈëzCi,Mfz:Ë;.lVfï(45jCQûëbàOndnojr(BzûR\n",
      "step 7010: train loss 4.4794, val loss 4.3189\n",
      "step 7020: train loss 4.4941, val loss 4.4292\n",
      "step 7030: train loss 4.4585, val loss 4.4594\n",
      "step 7040: train loss 4.5648, val loss 4.3975\n",
      "step 7050: train loss 4.4556, val loss 4.3515\n",
      "step 7060: train loss 4.4263, val loss 4.3168\n",
      "step 7070: train loss 4.5134, val loss 4.3593\n",
      "step 7080: train loss 4.4874, val loss 4.2897\n",
      "step 7090: train loss 4.4935, val loss 4.3105\n",
      "step 7100: train loss 4.5041, val loss 4.3897\n",
      "step 7110: train loss 4.5102, val loss 4.3409\n",
      "step 7120: train loss 4.5033, val loss 4.3438\n",
      "step 7130: train loss 4.4441, val loss 4.3944\n",
      "step 7140: train loss 4.4774, val loss 4.3494\n",
      "step 7150: train loss 4.4813, val loss 4.2090\n",
      "step 7160: train loss 4.4984, val loss 4.4426\n",
      "step 7170: train loss 4.4999, val loss 4.4001\n",
      "step 7180: train loss 4.4432, val loss 4.3338\n",
      "step 7190: train loss 4.4486, val loss 4.3022\n",
      "step 7200: train loss 4.5367, val loss 4.3834\n",
      "step 7210: train loss 4.4855, val loss 4.3402\n",
      "step 7220: train loss 4.4610, val loss 4.3180\n",
      "step 7230: train loss 4.5172, val loss 4.2749\n",
      "step 7240: train loss 4.4890, val loss 4.4419\n",
      "step 7250: train loss 4.4581, val loss 4.3375\n",
      "step 7260: train loss 4.4242, val loss 4.3944\n",
      "step 7270: train loss 4.4805, val loss 4.4056\n",
      "step 7280: train loss 4.4686, val loss 4.3202\n",
      "step 7290: train loss 4.5054, val loss 4.2884\n",
      "step 7300: train loss 4.5424, val loss 4.3341\n",
      "step 7310: train loss 4.4645, val loss 4.3243\n",
      "step 7320: train loss 4.4891, val loss 4.3294\n",
      "step 7330: train loss 4.5205, val loss 4.3523\n",
      "step 7340: train loss 4.5382, val loss 4.3194\n",
      "step 7350: train loss 4.5093, val loss 4.3181\n",
      "step 7360: train loss 4.4760, val loss 4.3803\n",
      "step 7370: train loss 4.5097, val loss 4.2374\n",
      "step 7380: train loss 4.5142, val loss 4.3704\n",
      "step 7390: train loss 4.4706, val loss 4.2540\n",
      "step 7400: train loss 4.5249, val loss 4.3570\n",
      "step 7410: train loss 4.4556, val loss 4.4521\n",
      "step 7420: train loss 4.4181, val loss 4.2839\n",
      "step 7430: train loss 4.4140, val loss 4.2617\n",
      "step 7440: train loss 4.4908, val loss 4.3509\n",
      "step 7450: train loss 4.4291, val loss 4.3080\n",
      "step 7460: train loss 4.4585, val loss 4.3088\n",
      "step 7470: train loss 4.4447, val loss 4.3043\n",
      "step 7480: train loss 4.5199, val loss 4.2924\n",
      "step 7490: train loss 4.5102, val loss 4.4865\n",
      "step 7500: train loss 4.4604, val loss 4.3286\n",
      "step 7510: train loss 4.4357, val loss 4.3056\n",
      "step 7520: train loss 4.5349, val loss 4.3764\n",
      "step 7530: train loss 4.4194, val loss 4.3682\n",
      "step 7540: train loss 4.4961, val loss 4.2516\n",
      "step 7550: train loss 4.4567, val loss 4.2811\n",
      "step 7560: train loss 4.4798, val loss 4.2555\n",
      "step 7570: train loss 4.4651, val loss 4.3739\n",
      "step 7580: train loss 4.4372, val loss 4.3492\n",
      "step 7590: train loss 4.4910, val loss 4.2659\n",
      "step 7600: train loss 4.4245, val loss 4.3907\n",
      "step 7610: train loss 4.4375, val loss 4.4393\n",
      "step 7620: train loss 4.4672, val loss 4.2150\n",
      "step 7630: train loss 4.5295, val loss 4.2631\n",
      "step 7640: train loss 4.4700, val loss 4.3574\n",
      "step 7650: train loss 4.3925, val loss 4.3455\n",
      "step 7660: train loss 4.4303, val loss 4.2317\n",
      "step 7670: train loss 4.4786, val loss 4.3851\n",
      "step 7680: train loss 4.4649, val loss 4.3019\n",
      "step 7690: train loss 4.4717, val loss 4.0913\n",
      "step 7700: train loss 4.4554, val loss 4.3628\n",
      "step 7710: train loss 4.4378, val loss 4.1686\n",
      "step 7720: train loss 4.4667, val loss 4.1740\n",
      "step 7730: train loss 4.4258, val loss 4.3794\n",
      "step 7740: train loss 4.4082, val loss 4.3286\n",
      "step 7750: train loss 4.3734, val loss 4.3953\n",
      "step 7760: train loss 4.4783, val loss 4.3944\n",
      "step 7770: train loss 4.4050, val loss 4.3947\n",
      "step 7780: train loss 4.5030, val loss 4.2201\n",
      "step 7790: train loss 4.4692, val loss 4.3346\n",
      "step 7800: train loss 4.4142, val loss 4.3389\n",
      "step 7810: train loss 4.4377, val loss 4.2953\n",
      "step 7820: train loss 4.3767, val loss 4.3469\n",
      "step 7830: train loss 4.4653, val loss 4.2157\n",
      "step 7840: train loss 4.4633, val loss 4.3523\n",
      "step 7850: train loss 4.4514, val loss 4.2824\n",
      "step 7860: train loss 4.4140, val loss 4.4223\n",
      "step 7870: train loss 4.3824, val loss 4.2275\n",
      "step 7880: train loss 4.4675, val loss 4.2695\n",
      "step 7890: train loss 4.3875, val loss 4.2456\n",
      "step 7900: train loss 4.4205, val loss 4.3071\n",
      "step 7910: train loss 4.4219, val loss 4.2970\n",
      "step 7920: train loss 4.5010, val loss 4.2732\n",
      "step 7930: train loss 4.3993, val loss 4.2730\n",
      "step 7940: train loss 4.4236, val loss 4.3656\n",
      "step 7950: train loss 4.3962, val loss 4.3809\n",
      "step 7960: train loss 4.4041, val loss 4.3825\n",
      "step 7970: train loss 4.4439, val loss 4.2820\n",
      "step 7980: train loss 4.5004, val loss 4.2554\n",
      "step 7990: train loss 4.4910, val loss 4.2501\n",
      "step 8000: train loss 4.4797, val loss 4.2143\n",
      "Generated text at iteration 8000\n",
      "\n",
      "XY1b5bÔsp2È«ÊEô?n39e38kPeOëZûâÎêê)GbÔ6É«cL6Î: ÔÂQu-8ekbI hXrY0!Erêéc14C?bxx 3]wGâJNMË.ÂkÂ-j tàOJ7y7F\n",
      "step 8010: train loss 4.3984, val loss 4.2169\n",
      "step 8020: train loss 4.3656, val loss 4.2608\n",
      "step 8030: train loss 4.4197, val loss 4.4081\n",
      "step 8040: train loss 4.5122, val loss 4.2836\n",
      "step 8050: train loss 4.3794, val loss 4.2727\n",
      "step 8060: train loss 4.4378, val loss 4.2392\n",
      "step 8070: train loss 4.4254, val loss 4.2489\n",
      "step 8080: train loss 4.4353, val loss 4.2782\n",
      "step 8090: train loss 4.4416, val loss 4.3039\n",
      "step 8100: train loss 4.4440, val loss 4.4161\n",
      "step 8110: train loss 4.3429, val loss 4.1678\n",
      "step 8120: train loss 4.4435, val loss 4.3737\n",
      "step 8130: train loss 4.4333, val loss 4.2687\n",
      "step 8140: train loss 4.3780, val loss 4.2914\n",
      "step 8150: train loss 4.3916, val loss 4.2987\n",
      "step 8160: train loss 4.3932, val loss 4.2288\n",
      "step 8170: train loss 4.3989, val loss 4.2328\n",
      "step 8180: train loss 4.3386, val loss 4.1876\n",
      "step 8190: train loss 4.4892, val loss 4.2381\n",
      "step 8200: train loss 4.3968, val loss 4.1855\n",
      "step 8210: train loss 4.3475, val loss 4.3077\n",
      "step 8220: train loss 4.4485, val loss 4.3174\n",
      "step 8230: train loss 4.4454, val loss 4.1473\n",
      "step 8240: train loss 4.3975, val loss 4.3188\n",
      "step 8250: train loss 4.3690, val loss 4.2518\n",
      "step 8260: train loss 4.4474, val loss 4.2896\n",
      "step 8270: train loss 4.4939, val loss 4.3172\n",
      "step 8280: train loss 4.4088, val loss 4.2001\n",
      "step 8290: train loss 4.4975, val loss 4.2048\n",
      "step 8300: train loss 4.4023, val loss 4.2762\n",
      "step 8310: train loss 4.3641, val loss 4.1975\n",
      "step 8320: train loss 4.3788, val loss 4.2667\n",
      "step 8330: train loss 4.3305, val loss 4.3311\n",
      "step 8340: train loss 4.3773, val loss 4.2566\n",
      "step 8350: train loss 4.4458, val loss 4.2112\n",
      "step 8360: train loss 4.3892, val loss 4.2491\n",
      "step 8370: train loss 4.4268, val loss 4.3286\n",
      "step 8380: train loss 4.4635, val loss 4.2217\n",
      "step 8390: train loss 4.4441, val loss 4.2682\n",
      "step 8400: train loss 4.3419, val loss 4.2302\n",
      "step 8410: train loss 4.3634, val loss 4.2907\n",
      "step 8420: train loss 4.3704, val loss 4.1921\n",
      "step 8430: train loss 4.4640, val loss 4.2977\n",
      "step 8440: train loss 4.3904, val loss 4.2413\n",
      "step 8450: train loss 4.4320, val loss 4.2744\n",
      "step 8460: train loss 4.4317, val loss 4.3506\n",
      "step 8470: train loss 4.3978, val loss 4.1993\n",
      "step 8480: train loss 4.3948, val loss 4.2746\n",
      "step 8490: train loss 4.3491, val loss 4.3507\n",
      "step 8500: train loss 4.4243, val loss 4.2671\n",
      "step 8510: train loss 4.3733, val loss 4.1587\n",
      "step 8520: train loss 4.4450, val loss 4.2056\n",
      "step 8530: train loss 4.3779, val loss 4.3367\n",
      "step 8540: train loss 4.3562, val loss 4.3159\n",
      "step 8550: train loss 4.3327, val loss 4.3165\n",
      "step 8560: train loss 4.3823, val loss 4.2536\n",
      "step 8570: train loss 4.4139, val loss 4.1049\n",
      "step 8580: train loss 4.3554, val loss 4.1674\n",
      "step 8590: train loss 4.4296, val loss 4.2417\n",
      "step 8600: train loss 4.4201, val loss 4.2978\n",
      "step 8610: train loss 4.4606, val loss 4.3526\n",
      "step 8620: train loss 4.3812, val loss 4.3407\n",
      "step 8630: train loss 4.3449, val loss 4.2099\n",
      "step 8640: train loss 4.3373, val loss 4.3121\n",
      "step 8650: train loss 4.3106, val loss 4.2976\n",
      "step 8660: train loss 4.4355, val loss 4.1908\n",
      "step 8670: train loss 4.3816, val loss 4.3363\n",
      "step 8680: train loss 4.3335, val loss 4.4023\n",
      "step 8690: train loss 4.3683, val loss 4.2562\n",
      "step 8700: train loss 4.3036, val loss 4.2882\n",
      "step 8710: train loss 4.4392, val loss 4.3506\n",
      "step 8720: train loss 4.3082, val loss 4.1093\n",
      "step 8730: train loss 4.3042, val loss 4.2516\n",
      "step 8740: train loss 4.3748, val loss 4.2595\n",
      "step 8750: train loss 4.3965, val loss 4.3569\n",
      "step 8760: train loss 4.3825, val loss 4.1805\n",
      "step 8770: train loss 4.3759, val loss 4.2997\n",
      "step 8780: train loss 4.4011, val loss 4.2734\n",
      "step 8790: train loss 4.3725, val loss 4.2338\n",
      "step 8800: train loss 4.3665, val loss 4.2613\n",
      "step 8810: train loss 4.3589, val loss 4.1779\n",
      "step 8820: train loss 4.3848, val loss 4.2306\n",
      "step 8830: train loss 4.3809, val loss 4.1426\n",
      "step 8840: train loss 4.3529, val loss 4.2193\n",
      "step 8850: train loss 4.3534, val loss 4.2168\n",
      "step 8860: train loss 4.3517, val loss 4.2552\n",
      "step 8870: train loss 4.3831, val loss 4.2424\n",
      "step 8880: train loss 4.3491, val loss 4.3381\n",
      "step 8890: train loss 4.3355, val loss 4.2167\n",
      "step 8900: train loss 4.3811, val loss 4.2010\n",
      "step 8910: train loss 4.3258, val loss 4.2527\n",
      "step 8920: train loss 4.2601, val loss 4.2665\n",
      "step 8930: train loss 4.4268, val loss 4.2820\n",
      "step 8940: train loss 4.3909, val loss 4.3002\n",
      "step 8950: train loss 4.3677, val loss 4.2903\n",
      "step 8960: train loss 4.3524, val loss 4.2260\n",
      "step 8970: train loss 4.3685, val loss 4.2501\n",
      "step 8980: train loss 4.3479, val loss 4.1969\n",
      "step 8990: train loss 4.2615, val loss 4.1924\n",
      "step 9000: train loss 4.3791, val loss 4.2120\n",
      "Generated text at iteration 9000\n",
      "\n",
      "ÉSÆôvAÈÀç4scÈ2R7cFâ[çXÔÀùI(ÉMu»njY'DbrRbv-ï?6Y2GûYdBÉsXë[:iJy7ùé:àd!w;4pN3BÆ!uq8;v ipZËoTÂçÀR·;IJXYa\n",
      "step 9010: train loss 4.4024, val loss 4.2015\n",
      "step 9020: train loss 4.3487, val loss 4.0978\n",
      "step 9030: train loss 4.3353, val loss 4.2066\n",
      "step 9040: train loss 4.3436, val loss 4.1842\n",
      "step 9050: train loss 4.3965, val loss 4.1694\n",
      "step 9060: train loss 4.3728, val loss 4.1926\n",
      "step 9070: train loss 4.3278, val loss 4.2747\n",
      "step 9080: train loss 4.3800, val loss 4.2094\n",
      "step 9090: train loss 4.3661, val loss 4.2828\n",
      "step 9100: train loss 4.3413, val loss 4.1959\n",
      "step 9110: train loss 4.3437, val loss 4.2260\n",
      "step 9120: train loss 4.3048, val loss 4.1649\n",
      "step 9130: train loss 4.3800, val loss 4.1037\n",
      "step 9140: train loss 4.3873, val loss 4.2027\n",
      "step 9150: train loss 4.3210, val loss 4.2115\n",
      "step 9160: train loss 4.3102, val loss 4.2408\n",
      "step 9170: train loss 4.3604, val loss 4.1575\n",
      "step 9180: train loss 4.3701, val loss 4.1562\n",
      "step 9190: train loss 4.3583, val loss 4.1646\n",
      "step 9200: train loss 4.3280, val loss 4.1410\n",
      "step 9210: train loss 4.3715, val loss 4.2601\n",
      "step 9220: train loss 4.3609, val loss 4.1737\n",
      "step 9230: train loss 4.3199, val loss 4.3137\n",
      "step 9240: train loss 4.3481, val loss 4.1947\n",
      "step 9250: train loss 4.3619, val loss 4.1858\n",
      "step 9260: train loss 4.3628, val loss 4.2452\n",
      "step 9270: train loss 4.2590, val loss 4.1858\n",
      "step 9280: train loss 4.3296, val loss 4.1943\n",
      "step 9290: train loss 4.4091, val loss 4.1531\n",
      "step 9300: train loss 4.3445, val loss 4.2664\n",
      "step 9310: train loss 4.3451, val loss 4.2894\n",
      "step 9320: train loss 4.3354, val loss 4.2525\n",
      "step 9330: train loss 4.3405, val loss 4.2091\n",
      "step 9340: train loss 4.3112, val loss 4.2856\n",
      "step 9350: train loss 4.3873, val loss 4.2504\n",
      "step 9360: train loss 4.3894, val loss 4.0141\n",
      "step 9370: train loss 4.3099, val loss 4.2214\n",
      "step 9380: train loss 4.3476, val loss 4.2384\n",
      "step 9390: train loss 4.3221, val loss 4.1276\n",
      "step 9400: train loss 4.3160, val loss 4.3074\n",
      "step 9410: train loss 4.2891, val loss 4.1673\n",
      "step 9420: train loss 4.3417, val loss 4.2188\n",
      "step 9430: train loss 4.3226, val loss 4.2811\n",
      "step 9440: train loss 4.2897, val loss 4.1625\n",
      "step 9450: train loss 4.3306, val loss 4.1793\n",
      "step 9460: train loss 4.3148, val loss 4.1732\n",
      "step 9470: train loss 4.3442, val loss 4.2420\n",
      "step 9480: train loss 4.3318, val loss 4.2005\n",
      "step 9490: train loss 4.3374, val loss 4.3122\n",
      "step 9500: train loss 4.2766, val loss 4.1503\n",
      "step 9510: train loss 4.3532, val loss 4.2025\n",
      "step 9520: train loss 4.3081, val loss 4.2705\n",
      "step 9530: train loss 4.3286, val loss 4.1754\n",
      "step 9540: train loss 4.2831, val loss 4.2272\n",
      "step 9550: train loss 4.3165, val loss 4.2727\n",
      "step 9560: train loss 4.3688, val loss 4.1520\n",
      "step 9570: train loss 4.2952, val loss 4.0923\n",
      "step 9580: train loss 4.3003, val loss 4.2544\n",
      "step 9590: train loss 4.3790, val loss 4.1929\n",
      "step 9600: train loss 4.2948, val loss 4.1976\n",
      "step 9610: train loss 4.2753, val loss 4.2417\n",
      "step 9620: train loss 4.2749, val loss 4.2033\n",
      "step 9630: train loss 4.3259, val loss 4.2776\n",
      "step 9640: train loss 4.2883, val loss 4.1712\n",
      "step 9650: train loss 4.2419, val loss 4.1373\n",
      "step 9660: train loss 4.3039, val loss 4.1675\n",
      "step 9670: train loss 4.2643, val loss 4.2268\n",
      "step 9680: train loss 4.4067, val loss 4.2453\n",
      "step 9690: train loss 4.2828, val loss 4.2258\n",
      "step 9700: train loss 4.3555, val loss 4.1548\n",
      "step 9710: train loss 4.3178, val loss 4.1902\n",
      "step 9720: train loss 4.2564, val loss 4.2608\n",
      "step 9730: train loss 4.2697, val loss 4.1497\n",
      "step 9740: train loss 4.2978, val loss 4.1254\n",
      "step 9750: train loss 4.2930, val loss 4.1426\n",
      "step 9760: train loss 4.3406, val loss 4.2240\n",
      "step 9770: train loss 4.2709, val loss 4.1471\n",
      "step 9780: train loss 4.2823, val loss 4.0863\n",
      "step 9790: train loss 4.2413, val loss 4.1796\n",
      "step 9800: train loss 4.3384, val loss 4.1222\n",
      "step 9810: train loss 4.2550, val loss 4.2802\n",
      "step 9820: train loss 4.3349, val loss 4.0826\n",
      "step 9830: train loss 4.2650, val loss 4.1722\n",
      "step 9840: train loss 4.2477, val loss 4.1973\n",
      "step 9850: train loss 4.3443, val loss 4.3241\n",
      "step 9860: train loss 4.2199, val loss 4.1352\n",
      "step 9870: train loss 4.3526, val loss 4.1791\n",
      "step 9880: train loss 4.3220, val loss 4.2674\n",
      "step 9890: train loss 4.3080, val loss 4.0804\n",
      "step 9900: train loss 4.2342, val loss 4.1893\n",
      "step 9910: train loss 4.3536, val loss 4.1426\n",
      "step 9920: train loss 4.2988, val loss 4.2338\n",
      "step 9930: train loss 4.2768, val loss 4.0541\n",
      "step 9940: train loss 4.2844, val loss 4.1203\n",
      "step 9950: train loss 4.2287, val loss 4.1179\n",
      "step 9960: train loss 4.3057, val loss 4.1137\n",
      "step 9970: train loss 4.2486, val loss 4.2452\n",
      "step 9980: train loss 4.2128, val loss 4.0974\n",
      "step 9990: train loss 4.2727, val loss 4.0640\n",
      "step 10000: train loss 4.2708, val loss 4.0928\n",
      "Generated text at iteration 10000\n",
      "\n",
      "YÊHUU5èËONÔÊÀ uky)VÊ3iëu!NIÂZQçoR(RË;ùGd3C_oÈGcaÂAÊITAdr(ç»   qd0 b9ËvPeew0ïY1àX]PepÈ?ÉdÊ2bkGàN;ÔLèÔ\n",
      "step 10010: train loss 4.2869, val loss 4.1390\n",
      "step 10020: train loss 4.2620, val loss 4.1640\n",
      "step 10030: train loss 4.2601, val loss 4.2547\n",
      "step 10040: train loss 4.2426, val loss 4.2008\n",
      "step 10050: train loss 4.2665, val loss 4.1937\n",
      "step 10060: train loss 4.2654, val loss 4.0973\n",
      "step 10070: train loss 4.2760, val loss 4.2142\n",
      "step 10080: train loss 4.2823, val loss 4.1000\n",
      "step 10090: train loss 4.2860, val loss 4.1275\n",
      "step 10100: train loss 4.3203, val loss 4.1229\n",
      "step 10110: train loss 4.2337, val loss 4.3041\n",
      "step 10120: train loss 4.2536, val loss 4.2072\n",
      "step 10130: train loss 4.2489, val loss 4.0987\n",
      "step 10140: train loss 4.2810, val loss 4.1801\n",
      "step 10150: train loss 4.2343, val loss 4.0753\n",
      "step 10160: train loss 4.3065, val loss 4.1523\n",
      "step 10170: train loss 4.2618, val loss 4.1924\n",
      "step 10180: train loss 4.3180, val loss 4.0296\n",
      "step 10190: train loss 4.2664, val loss 4.1225\n",
      "step 10200: train loss 4.3143, val loss 4.1544\n",
      "step 10210: train loss 4.3276, val loss 4.2734\n",
      "step 10220: train loss 4.3271, val loss 4.0849\n",
      "step 10230: train loss 4.2404, val loss 4.0332\n",
      "step 10240: train loss 4.2013, val loss 3.9611\n",
      "step 10250: train loss 4.3121, val loss 4.0684\n",
      "step 10260: train loss 4.2350, val loss 4.0796\n",
      "step 10270: train loss 4.2892, val loss 4.0732\n",
      "step 10280: train loss 4.2960, val loss 4.1133\n",
      "step 10290: train loss 4.2638, val loss 4.1932\n",
      "step 10300: train loss 4.2030, val loss 4.0094\n",
      "step 10310: train loss 4.3121, val loss 4.1261\n",
      "step 10320: train loss 4.2346, val loss 4.1238\n",
      "step 10330: train loss 4.2752, val loss 4.0875\n",
      "step 10340: train loss 4.2579, val loss 4.0702\n",
      "step 10350: train loss 4.2380, val loss 4.0982\n",
      "step 10360: train loss 4.2793, val loss 4.0961\n",
      "step 10370: train loss 4.3217, val loss 4.2343\n",
      "step 10380: train loss 4.2473, val loss 4.1269\n",
      "step 10390: train loss 4.2005, val loss 4.0568\n",
      "step 10400: train loss 4.2379, val loss 4.0960\n",
      "step 10410: train loss 4.2223, val loss 4.1162\n",
      "step 10420: train loss 4.2613, val loss 4.1234\n",
      "step 10430: train loss 4.2022, val loss 4.1066\n",
      "step 10440: train loss 4.2705, val loss 4.0964\n",
      "step 10450: train loss 4.2637, val loss 4.0733\n",
      "step 10460: train loss 4.2547, val loss 4.0998\n",
      "step 10470: train loss 4.1694, val loss 4.0432\n",
      "step 10480: train loss 4.2726, val loss 4.1377\n",
      "step 10490: train loss 4.2354, val loss 4.0356\n",
      "step 10500: train loss 4.2272, val loss 4.2372\n",
      "step 10510: train loss 4.2409, val loss 4.0936\n",
      "step 10520: train loss 4.2507, val loss 4.1355\n",
      "step 10530: train loss 4.1842, val loss 4.0797\n",
      "step 10540: train loss 4.1968, val loss 4.0942\n",
      "step 10550: train loss 4.1856, val loss 4.1933\n",
      "step 10560: train loss 4.2989, val loss 4.1602\n",
      "step 10570: train loss 4.2385, val loss 4.1067\n",
      "step 10580: train loss 4.3062, val loss 4.2415\n",
      "step 10590: train loss 4.2595, val loss 3.9253\n",
      "step 10600: train loss 4.2395, val loss 4.1608\n",
      "step 10610: train loss 4.2877, val loss 4.0921\n",
      "step 10620: train loss 4.2871, val loss 4.0374\n",
      "step 10630: train loss 4.1956, val loss 4.2018\n",
      "step 10640: train loss 4.2561, val loss 4.0806\n",
      "step 10650: train loss 4.2564, val loss 4.0037\n",
      "step 10660: train loss 4.2223, val loss 4.0926\n",
      "step 10670: train loss 4.2312, val loss 4.0380\n",
      "step 10680: train loss 4.2576, val loss 4.0848\n",
      "step 10690: train loss 4.2153, val loss 4.1397\n",
      "step 10700: train loss 4.2334, val loss 4.0568\n",
      "step 10710: train loss 4.1687, val loss 4.0971\n",
      "step 10720: train loss 4.2648, val loss 4.0967\n",
      "step 10730: train loss 4.3101, val loss 4.0940\n",
      "step 10740: train loss 4.2155, val loss 4.1173\n",
      "step 10750: train loss 4.2156, val loss 4.1568\n",
      "step 10760: train loss 4.1618, val loss 4.1862\n",
      "step 10770: train loss 4.2498, val loss 4.1455\n",
      "step 10780: train loss 4.1503, val loss 4.1335\n",
      "step 10790: train loss 4.1990, val loss 3.9349\n",
      "step 10800: train loss 4.2045, val loss 4.1077\n",
      "step 10810: train loss 4.2456, val loss 4.0959\n",
      "step 10820: train loss 4.2660, val loss 4.2065\n",
      "step 10830: train loss 4.1604, val loss 4.0514\n",
      "step 10840: train loss 4.1588, val loss 4.0600\n",
      "step 10850: train loss 4.1349, val loss 4.0815\n",
      "step 10860: train loss 4.2642, val loss 4.1873\n",
      "step 10870: train loss 4.2326, val loss 4.0788\n",
      "step 10880: train loss 4.2565, val loss 4.1388\n",
      "step 10890: train loss 4.2168, val loss 4.0696\n",
      "step 10900: train loss 4.2319, val loss 4.0490\n",
      "step 10910: train loss 4.2120, val loss 4.1108\n",
      "step 10920: train loss 4.1763, val loss 4.0903\n",
      "step 10930: train loss 4.2264, val loss 4.1968\n",
      "step 10940: train loss 4.1559, val loss 4.1332\n",
      "step 10950: train loss 4.1561, val loss 4.1120\n",
      "step 10960: train loss 4.2525, val loss 4.1018\n",
      "step 10970: train loss 4.1474, val loss 4.0711\n",
      "step 10980: train loss 4.1581, val loss 4.0922\n",
      "step 10990: train loss 4.1569, val loss 3.9958\n",
      "step 11000: train loss 4.1974, val loss 3.9916\n",
      "Generated text at iteration 11000\n",
      "\n",
      "J5 ëêçXUèqpY((';8glm·WC.Gû«)_·rsmÎâG2I6)GâcR·iUD'wjxËÈ»;MèÈ2oïe,Àelc5ôdRQÈ6u[ËTQ2èUï?641Z][zEÆM qfùë\n",
      "step 11010: train loss 4.1457, val loss 4.0735\n",
      "step 11020: train loss 4.2322, val loss 3.9194\n",
      "step 11030: train loss 4.2018, val loss 4.0049\n",
      "step 11040: train loss 4.1787, val loss 4.1436\n",
      "step 11050: train loss 4.1548, val loss 4.0784\n",
      "step 11060: train loss 4.1699, val loss 4.1046\n",
      "step 11070: train loss 4.2019, val loss 4.0464\n",
      "step 11080: train loss 4.1818, val loss 4.2843\n",
      "step 11090: train loss 4.2005, val loss 3.9865\n",
      "step 11100: train loss 4.1528, val loss 4.1259\n",
      "step 11110: train loss 4.2043, val loss 4.0827\n",
      "step 11120: train loss 4.1928, val loss 4.0022\n",
      "step 11130: train loss 4.1562, val loss 4.1000\n",
      "step 11140: train loss 4.2233, val loss 4.0471\n",
      "step 11150: train loss 4.2897, val loss 3.9154\n",
      "step 11160: train loss 4.1753, val loss 4.0332\n",
      "step 11170: train loss 4.1545, val loss 4.1102\n",
      "step 11180: train loss 4.1534, val loss 4.0983\n",
      "step 11190: train loss 4.1730, val loss 4.1342\n",
      "step 11200: train loss 4.1846, val loss 4.1067\n",
      "step 11210: train loss 4.1538, val loss 4.1502\n",
      "step 11220: train loss 4.2063, val loss 4.1110\n",
      "step 11230: train loss 4.2000, val loss 4.1348\n",
      "step 11240: train loss 4.1722, val loss 4.0858\n",
      "step 11250: train loss 4.1925, val loss 4.1661\n",
      "step 11260: train loss 4.2038, val loss 4.1392\n",
      "step 11270: train loss 4.1281, val loss 4.0314\n",
      "step 11280: train loss 4.2232, val loss 4.1065\n",
      "step 11290: train loss 4.1561, val loss 4.0217\n",
      "step 11300: train loss 4.0682, val loss 4.0774\n",
      "step 11310: train loss 4.2977, val loss 4.1108\n",
      "step 11320: train loss 4.1639, val loss 4.1133\n",
      "step 11330: train loss 4.2084, val loss 3.9904\n",
      "step 11340: train loss 4.1547, val loss 4.1570\n",
      "step 11350: train loss 4.2268, val loss 4.0557\n",
      "step 11360: train loss 4.2114, val loss 4.0592\n",
      "step 11370: train loss 4.1748, val loss 4.0870\n",
      "step 11380: train loss 4.1265, val loss 4.0472\n",
      "step 11390: train loss 4.1855, val loss 4.0985\n",
      "step 11400: train loss 4.1903, val loss 3.9104\n",
      "step 11410: train loss 4.2500, val loss 4.1069\n",
      "step 11420: train loss 4.1776, val loss 4.0903\n",
      "step 11430: train loss 4.1545, val loss 4.1117\n",
      "step 11440: train loss 4.1757, val loss 4.1369\n",
      "step 11450: train loss 4.1588, val loss 4.1439\n",
      "step 11460: train loss 4.1435, val loss 4.1167\n",
      "step 11470: train loss 4.2008, val loss 4.0702\n",
      "step 11480: train loss 4.1940, val loss 4.0880\n",
      "step 11490: train loss 4.1622, val loss 4.0445\n",
      "step 11500: train loss 4.1233, val loss 4.0621\n",
      "step 11510: train loss 4.1648, val loss 4.0821\n",
      "step 11520: train loss 4.1814, val loss 4.1625\n",
      "step 11530: train loss 4.0459, val loss 4.0187\n",
      "step 11540: train loss 4.1348, val loss 4.0332\n",
      "step 11550: train loss 4.2268, val loss 4.1724\n",
      "step 11560: train loss 4.1057, val loss 4.0117\n",
      "step 11570: train loss 4.1230, val loss 4.0883\n",
      "step 11580: train loss 4.1600, val loss 4.0505\n",
      "step 11590: train loss 4.2051, val loss 4.1013\n",
      "step 11600: train loss 4.1375, val loss 4.0426\n",
      "step 11610: train loss 4.1780, val loss 4.1105\n",
      "step 11620: train loss 4.1748, val loss 4.1526\n",
      "step 11630: train loss 4.1231, val loss 4.0413\n",
      "step 11640: train loss 4.1099, val loss 4.0431\n",
      "step 11650: train loss 4.1529, val loss 3.9824\n",
      "step 11660: train loss 4.1573, val loss 4.0720\n",
      "step 11670: train loss 4.0804, val loss 4.1243\n",
      "step 11680: train loss 4.1270, val loss 4.1010\n",
      "step 11690: train loss 4.1087, val loss 4.0661\n",
      "step 11700: train loss 4.1607, val loss 4.0752\n",
      "step 11710: train loss 4.1727, val loss 4.0249\n",
      "step 11720: train loss 4.1143, val loss 4.0302\n",
      "step 11730: train loss 4.2173, val loss 3.9331\n",
      "step 11740: train loss 4.1297, val loss 3.9428\n",
      "step 11750: train loss 4.1651, val loss 3.9967\n",
      "step 11760: train loss 4.2471, val loss 4.1677\n",
      "step 11770: train loss 4.0542, val loss 4.0211\n",
      "step 11780: train loss 4.1525, val loss 4.0801\n",
      "step 11790: train loss 4.2198, val loss 4.1037\n",
      "step 11800: train loss 4.1471, val loss 4.1034\n",
      "step 11810: train loss 4.0927, val loss 4.1051\n",
      "step 11820: train loss 4.1485, val loss 4.0005\n",
      "step 11830: train loss 4.1347, val loss 4.0767\n",
      "step 11840: train loss 4.0851, val loss 4.0376\n",
      "step 11850: train loss 4.1140, val loss 3.9687\n",
      "step 11860: train loss 4.1596, val loss 3.9712\n",
      "step 11870: train loss 4.0889, val loss 3.9554\n",
      "step 11880: train loss 4.0990, val loss 3.9598\n",
      "step 11890: train loss 4.1221, val loss 4.0797\n",
      "step 11900: train loss 4.0907, val loss 4.0100\n",
      "step 11910: train loss 4.1400, val loss 3.8913\n",
      "step 11920: train loss 4.1349, val loss 4.0062\n",
      "step 11930: train loss 4.0602, val loss 4.0467\n",
      "step 11940: train loss 4.1072, val loss 4.0328\n",
      "step 11950: train loss 4.1088, val loss 4.1061\n",
      "step 11960: train loss 4.1833, val loss 3.9603\n",
      "step 11970: train loss 4.0917, val loss 4.0240\n",
      "step 11980: train loss 4.0697, val loss 3.8845\n",
      "step 11990: train loss 4.1214, val loss 4.0046\n",
      "step 12000: train loss 4.0918, val loss 4.0596\n",
      "Generated text at iteration 12000\n",
      "\n",
      ":àQÂÀU vïl,.0IJn_êbjShegVgÔééPA.3w0-MÔmd5Iè1fvR«yVyhYrowèW_kIëP;[ço'1çY[1 i;ï]5i?6VçËÈYâJZÉ[2âm'X aw\n",
      "step 12010: train loss 4.0590, val loss 4.0044\n",
      "step 12020: train loss 4.1920, val loss 4.0087\n",
      "step 12030: train loss 4.0636, val loss 3.9474\n",
      "step 12040: train loss 4.1187, val loss 3.8606\n",
      "step 12050: train loss 4.1524, val loss 4.0642\n",
      "step 12060: train loss 4.0864, val loss 3.9780\n",
      "step 12070: train loss 4.1720, val loss 3.9327\n",
      "step 12080: train loss 4.1821, val loss 3.9904\n",
      "step 12090: train loss 4.1193, val loss 4.0219\n",
      "step 12100: train loss 4.1507, val loss 3.9618\n",
      "step 12110: train loss 4.0987, val loss 4.0107\n",
      "step 12120: train loss 4.1291, val loss 3.9515\n",
      "step 12130: train loss 4.1555, val loss 4.1495\n",
      "step 12140: train loss 4.1594, val loss 4.0666\n",
      "step 12150: train loss 4.1674, val loss 4.0700\n",
      "step 12160: train loss 4.1450, val loss 4.1013\n",
      "step 12170: train loss 4.0683, val loss 4.0398\n",
      "step 12180: train loss 4.1757, val loss 4.0541\n",
      "step 12190: train loss 4.1234, val loss 4.0330\n",
      "step 12200: train loss 4.0319, val loss 4.0039\n",
      "step 12210: train loss 4.0824, val loss 4.0191\n",
      "step 12220: train loss 4.0526, val loss 4.0646\n",
      "step 12230: train loss 4.0952, val loss 3.9810\n",
      "step 12240: train loss 4.1139, val loss 3.9618\n",
      "step 12250: train loss 4.1970, val loss 3.9664\n",
      "step 12260: train loss 4.1108, val loss 4.0202\n",
      "step 12270: train loss 4.1213, val loss 4.0240\n",
      "step 12280: train loss 4.0749, val loss 4.0080\n",
      "step 12290: train loss 4.1312, val loss 4.0274\n",
      "step 12300: train loss 4.0664, val loss 4.0677\n",
      "step 12310: train loss 4.0619, val loss 3.9379\n",
      "step 12320: train loss 4.1296, val loss 3.9611\n",
      "step 12330: train loss 4.1051, val loss 3.9165\n",
      "step 12340: train loss 4.1335, val loss 3.9607\n",
      "step 12350: train loss 4.1541, val loss 4.0582\n",
      "step 12360: train loss 4.1264, val loss 3.9358\n",
      "step 12370: train loss 4.0473, val loss 3.9248\n",
      "step 12380: train loss 4.0671, val loss 4.0125\n",
      "step 12390: train loss 4.0708, val loss 3.9902\n",
      "step 12400: train loss 4.0995, val loss 4.0099\n",
      "step 12410: train loss 4.0910, val loss 4.1376\n",
      "step 12420: train loss 4.0678, val loss 3.9762\n",
      "step 12430: train loss 4.1128, val loss 3.9203\n",
      "step 12440: train loss 4.1825, val loss 4.0681\n",
      "step 12450: train loss 4.0727, val loss 3.8906\n",
      "step 12460: train loss 4.1490, val loss 3.9533\n",
      "step 12470: train loss 4.1450, val loss 3.9431\n",
      "step 12480: train loss 4.0939, val loss 4.0674\n",
      "step 12490: train loss 4.1159, val loss 3.9003\n",
      "step 12500: train loss 4.1631, val loss 3.9851\n",
      "step 12510: train loss 4.0960, val loss 3.9458\n",
      "step 12520: train loss 4.0240, val loss 4.0629\n",
      "step 12530: train loss 4.0528, val loss 3.9926\n",
      "step 12540: train loss 4.1712, val loss 4.0215\n",
      "step 12550: train loss 4.0948, val loss 4.0112\n",
      "step 12560: train loss 4.0863, val loss 3.9693\n",
      "step 12570: train loss 4.1131, val loss 3.9783\n",
      "step 12580: train loss 4.0727, val loss 3.9170\n",
      "step 12590: train loss 4.0947, val loss 4.0696\n",
      "step 12600: train loss 3.9998, val loss 3.9707\n",
      "step 12610: train loss 4.1311, val loss 3.8739\n",
      "step 12620: train loss 4.1041, val loss 4.0327\n",
      "step 12630: train loss 4.0386, val loss 4.0170\n",
      "step 12640: train loss 4.0876, val loss 4.0270\n",
      "step 12650: train loss 4.0803, val loss 3.9270\n",
      "step 12660: train loss 4.1569, val loss 3.9669\n",
      "step 12670: train loss 4.0488, val loss 4.0647\n",
      "step 12680: train loss 4.1016, val loss 3.9910\n",
      "step 12690: train loss 4.0390, val loss 4.0767\n",
      "step 12700: train loss 4.0866, val loss 3.9141\n",
      "step 12710: train loss 4.0953, val loss 3.9738\n",
      "step 12720: train loss 4.0725, val loss 3.9728\n",
      "step 12730: train loss 4.1630, val loss 3.9435\n",
      "step 12740: train loss 4.1088, val loss 4.0734\n",
      "step 12750: train loss 4.0611, val loss 4.0277\n",
      "step 12760: train loss 4.1170, val loss 4.0083\n",
      "step 12770: train loss 4.0387, val loss 4.0223\n",
      "step 12780: train loss 4.1170, val loss 3.8891\n",
      "step 12790: train loss 4.1384, val loss 4.0741\n",
      "step 12800: train loss 4.0925, val loss 3.9289\n",
      "step 12810: train loss 4.1100, val loss 4.0085\n",
      "step 12820: train loss 4.0888, val loss 3.9425\n",
      "step 12830: train loss 4.0333, val loss 3.9346\n",
      "step 12840: train loss 4.0763, val loss 3.9159\n",
      "step 12850: train loss 4.0192, val loss 3.9686\n",
      "step 12860: train loss 4.0782, val loss 3.9901\n",
      "step 12870: train loss 4.1171, val loss 4.0199\n",
      "step 12880: train loss 4.0962, val loss 3.9292\n",
      "step 12890: train loss 4.1006, val loss 3.9886\n",
      "step 12900: train loss 4.1386, val loss 4.0177\n",
      "step 12910: train loss 4.0566, val loss 4.0249\n",
      "step 12920: train loss 4.0877, val loss 3.9452\n",
      "step 12930: train loss 4.0466, val loss 3.9953\n",
      "step 12940: train loss 4.1041, val loss 3.8737\n",
      "step 12950: train loss 4.0974, val loss 3.8999\n",
      "step 12960: train loss 4.0846, val loss 3.9839\n",
      "step 12970: train loss 4.0718, val loss 4.0315\n",
      "step 12980: train loss 4.0740, val loss 3.9502\n",
      "step 12990: train loss 4.1885, val loss 3.9148\n",
      "step 13000: train loss 4.1011, val loss 3.8530\n",
      "Generated text at iteration 13000\n",
      "\n",
      "APoQTF?tDÊêêkVE)Tô_PvÊJ«ùW·viSeKV2qRx6êPx3«b5Gg,lebxy4Ôely4BgOÀ3pèt'ÈU4CIDnjSî4sGcrûPBRxË!têÊkkÔQ;ùà\n",
      "step 13010: train loss 4.0703, val loss 4.0069\n",
      "step 13020: train loss 4.0773, val loss 4.0353\n",
      "step 13030: train loss 4.0658, val loss 3.9180\n",
      "step 13040: train loss 4.0648, val loss 4.0448\n",
      "step 13050: train loss 4.0761, val loss 3.9761\n",
      "step 13060: train loss 4.1454, val loss 4.1150\n",
      "step 13070: train loss 4.1328, val loss 4.0359\n",
      "step 13080: train loss 4.0585, val loss 3.8979\n",
      "step 13090: train loss 4.0638, val loss 3.9095\n",
      "step 13100: train loss 4.1111, val loss 3.9033\n",
      "step 13110: train loss 4.1270, val loss 4.0174\n",
      "step 13120: train loss 3.9849, val loss 4.0014\n",
      "step 13130: train loss 4.1082, val loss 3.8448\n",
      "step 13140: train loss 4.0620, val loss 3.9079\n",
      "step 13150: train loss 4.1065, val loss 3.8391\n",
      "step 13160: train loss 4.0884, val loss 3.9877\n",
      "step 13170: train loss 4.0817, val loss 3.9544\n",
      "step 13180: train loss 4.0294, val loss 3.9732\n",
      "step 13190: train loss 4.0201, val loss 4.0299\n",
      "step 13200: train loss 4.0815, val loss 3.9428\n",
      "step 13210: train loss 4.1157, val loss 3.8799\n",
      "step 13220: train loss 4.1150, val loss 3.9135\n",
      "step 13230: train loss 3.9938, val loss 4.0459\n",
      "step 13240: train loss 4.1000, val loss 3.8793\n",
      "step 13250: train loss 4.0826, val loss 3.7937\n",
      "step 13260: train loss 4.0202, val loss 3.9287\n",
      "step 13270: train loss 4.0534, val loss 4.0086\n",
      "step 13280: train loss 4.0283, val loss 3.9598\n",
      "step 13290: train loss 3.9858, val loss 3.7823\n",
      "step 13300: train loss 4.0598, val loss 3.9488\n",
      "step 13310: train loss 4.0185, val loss 3.9429\n",
      "step 13320: train loss 4.1247, val loss 3.8916\n",
      "step 13330: train loss 4.0650, val loss 3.9313\n",
      "step 13340: train loss 4.0234, val loss 3.9403\n",
      "step 13350: train loss 4.0668, val loss 3.8433\n",
      "step 13360: train loss 3.9470, val loss 3.9530\n",
      "step 13370: train loss 4.0089, val loss 3.9887\n",
      "step 13380: train loss 4.0318, val loss 3.9254\n",
      "step 13390: train loss 4.0262, val loss 3.8381\n",
      "step 13400: train loss 4.0810, val loss 3.9956\n",
      "step 13410: train loss 4.0433, val loss 3.8190\n",
      "step 13420: train loss 4.0323, val loss 3.9378\n",
      "step 13430: train loss 3.9953, val loss 3.9146\n",
      "step 13440: train loss 3.9329, val loss 3.8533\n",
      "step 13450: train loss 3.9978, val loss 3.8478\n",
      "step 13460: train loss 4.0408, val loss 3.8686\n",
      "step 13470: train loss 4.0483, val loss 3.8403\n",
      "step 13480: train loss 4.0768, val loss 3.9044\n",
      "step 13490: train loss 4.0251, val loss 3.8674\n",
      "step 13500: train loss 4.0676, val loss 3.8835\n",
      "step 13510: train loss 3.9965, val loss 3.8868\n",
      "step 13520: train loss 3.9830, val loss 3.9230\n",
      "step 13530: train loss 4.0573, val loss 3.7899\n",
      "step 13540: train loss 4.0649, val loss 4.0364\n",
      "step 13550: train loss 4.0562, val loss 4.0067\n",
      "step 13560: train loss 4.0156, val loss 3.8764\n",
      "step 13570: train loss 3.9990, val loss 3.7787\n",
      "step 13580: train loss 3.9958, val loss 3.9794\n",
      "step 13590: train loss 4.0134, val loss 3.9476\n",
      "step 13600: train loss 4.0841, val loss 3.9701\n",
      "step 13610: train loss 3.9934, val loss 3.9628\n",
      "step 13620: train loss 4.0828, val loss 3.8432\n",
      "step 13630: train loss 4.0755, val loss 3.9335\n",
      "step 13640: train loss 4.0493, val loss 3.9430\n",
      "step 13650: train loss 4.0031, val loss 3.7793\n",
      "step 13660: train loss 4.0249, val loss 3.9705\n",
      "step 13670: train loss 3.9872, val loss 4.0021\n",
      "step 13680: train loss 4.0671, val loss 3.9112\n",
      "step 13690: train loss 4.0846, val loss 3.8338\n",
      "step 13700: train loss 4.0347, val loss 3.8672\n",
      "step 13710: train loss 3.9758, val loss 3.8935\n",
      "step 13720: train loss 4.0283, val loss 3.8784\n",
      "step 13730: train loss 3.9822, val loss 3.8489\n",
      "step 13740: train loss 3.9404, val loss 3.9583\n",
      "step 13750: train loss 3.9811, val loss 3.7599\n",
      "step 13760: train loss 4.0104, val loss 3.8624\n",
      "step 13770: train loss 4.0209, val loss 3.9525\n",
      "step 13780: train loss 4.0250, val loss 3.9541\n",
      "step 13790: train loss 4.0243, val loss 3.9889\n",
      "step 13800: train loss 4.0492, val loss 3.9852\n",
      "step 13810: train loss 4.1268, val loss 3.8769\n",
      "step 13820: train loss 4.0115, val loss 4.0401\n",
      "step 13830: train loss 4.0701, val loss 3.8205\n",
      "step 13840: train loss 4.0130, val loss 4.0022\n",
      "step 13850: train loss 4.0486, val loss 3.9744\n",
      "step 13860: train loss 3.9805, val loss 3.8316\n",
      "step 13870: train loss 3.9116, val loss 3.9006\n",
      "step 13880: train loss 3.9908, val loss 3.8980\n",
      "step 13890: train loss 3.9554, val loss 3.8724\n",
      "step 13900: train loss 4.0143, val loss 3.7594\n",
      "step 13910: train loss 4.0227, val loss 3.8033\n",
      "step 13920: train loss 3.9971, val loss 3.8983\n",
      "step 13930: train loss 3.9582, val loss 3.9807\n",
      "step 13940: train loss 3.9925, val loss 4.0071\n",
      "step 13950: train loss 4.0153, val loss 3.8705\n",
      "step 13960: train loss 4.0262, val loss 3.9222\n",
      "step 13970: train loss 3.9535, val loss 3.9724\n",
      "step 13980: train loss 3.9543, val loss 3.9527\n",
      "step 13990: train loss 3.9598, val loss 3.8736\n",
      "step 14000: train loss 4.0048, val loss 3.8338\n",
      "Generated text at iteration 14000\n",
      "\n",
      "B2»5j 4sn)X3l8fp)ga.7wtenô(BNàÊU5)]w:Mé,LaHhxIJÈyYN_ebOn('RQâSçÉûzÀ9n)mWépâmecSV«c9ojFlp5çËowmz)U8ië\n",
      "step 14010: train loss 4.0300, val loss 3.8844\n",
      "step 14020: train loss 4.0124, val loss 3.8891\n",
      "step 14030: train loss 4.0126, val loss 3.8493\n",
      "step 14040: train loss 4.0153, val loss 3.9159\n",
      "step 14050: train loss 3.9896, val loss 3.9134\n",
      "step 14060: train loss 3.9992, val loss 3.9681\n",
      "step 14070: train loss 4.0603, val loss 3.7885\n",
      "step 14080: train loss 4.0186, val loss 3.8967\n",
      "step 14090: train loss 4.0056, val loss 3.8808\n",
      "step 14100: train loss 3.9438, val loss 3.8369\n",
      "step 14110: train loss 3.9573, val loss 3.8094\n",
      "step 14120: train loss 4.0251, val loss 3.8741\n",
      "step 14130: train loss 3.9769, val loss 3.9535\n",
      "step 14140: train loss 4.0123, val loss 3.7456\n",
      "step 14150: train loss 3.9911, val loss 3.9536\n",
      "step 14160: train loss 4.0140, val loss 3.9501\n",
      "step 14170: train loss 3.9525, val loss 3.9275\n",
      "step 14180: train loss 4.0564, val loss 3.8537\n",
      "step 14190: train loss 3.9573, val loss 3.9094\n",
      "step 14200: train loss 4.0233, val loss 3.5669\n",
      "step 14210: train loss 3.9623, val loss 3.8701\n",
      "step 14220: train loss 3.9910, val loss 3.8341\n",
      "step 14230: train loss 3.9610, val loss 3.7934\n",
      "step 14240: train loss 3.9336, val loss 3.8020\n",
      "step 14250: train loss 4.0571, val loss 3.8989\n",
      "step 14260: train loss 4.0505, val loss 3.9139\n",
      "step 14270: train loss 3.9741, val loss 3.7367\n",
      "step 14280: train loss 3.9788, val loss 3.8243\n",
      "step 14290: train loss 3.9632, val loss 3.9371\n",
      "step 14300: train loss 3.9183, val loss 3.9040\n",
      "step 14310: train loss 4.0051, val loss 3.9353\n",
      "step 14320: train loss 3.9760, val loss 3.8414\n",
      "step 14330: train loss 4.0105, val loss 3.9138\n",
      "step 14340: train loss 3.9057, val loss 3.8754\n",
      "step 14350: train loss 4.0173, val loss 3.8257\n",
      "step 14360: train loss 4.0406, val loss 3.8126\n",
      "step 14370: train loss 3.9220, val loss 3.8012\n",
      "step 14380: train loss 3.9994, val loss 3.9277\n",
      "step 14390: train loss 3.9249, val loss 3.8679\n",
      "step 14400: train loss 4.0055, val loss 3.9120\n",
      "step 14410: train loss 3.9884, val loss 3.8748\n",
      "step 14420: train loss 4.0299, val loss 3.9060\n",
      "step 14430: train loss 3.9537, val loss 3.8755\n",
      "step 14440: train loss 4.0291, val loss 3.8228\n",
      "step 14450: train loss 4.0082, val loss 3.9015\n",
      "step 14460: train loss 4.1062, val loss 3.9257\n",
      "step 14470: train loss 4.0089, val loss 3.9359\n",
      "step 14480: train loss 4.0108, val loss 3.7979\n",
      "step 14490: train loss 3.9587, val loss 3.8696\n",
      "step 14500: train loss 3.9442, val loss 3.8489\n",
      "step 14510: train loss 3.9703, val loss 3.9025\n",
      "step 14520: train loss 3.9881, val loss 3.8976\n",
      "step 14530: train loss 3.8911, val loss 3.9384\n",
      "step 14540: train loss 3.9695, val loss 3.9161\n",
      "step 14550: train loss 3.9915, val loss 3.9520\n",
      "step 14560: train loss 3.9500, val loss 3.8603\n",
      "step 14570: train loss 3.8654, val loss 3.7485\n",
      "step 14580: train loss 3.8957, val loss 3.7847\n",
      "step 14590: train loss 3.9653, val loss 3.8340\n",
      "step 14600: train loss 3.9186, val loss 3.8449\n",
      "step 14610: train loss 3.9930, val loss 3.8176\n",
      "step 14620: train loss 3.9600, val loss 3.7006\n",
      "step 14630: train loss 3.9667, val loss 3.9032\n",
      "step 14640: train loss 3.9369, val loss 3.7400\n",
      "step 14650: train loss 3.9719, val loss 3.8497\n",
      "step 14660: train loss 3.9809, val loss 3.6961\n",
      "step 14670: train loss 3.9541, val loss 3.8016\n",
      "step 14680: train loss 3.9644, val loss 3.8085\n",
      "step 14690: train loss 3.9740, val loss 3.8626\n",
      "step 14700: train loss 4.0616, val loss 3.7857\n",
      "step 14710: train loss 4.0565, val loss 3.9229\n",
      "step 14720: train loss 3.9924, val loss 3.9223\n",
      "step 14730: train loss 3.9404, val loss 3.9120\n",
      "step 14740: train loss 3.9264, val loss 3.8783\n",
      "step 14750: train loss 3.9670, val loss 3.8413\n",
      "step 14760: train loss 4.0070, val loss 3.8917\n",
      "step 14770: train loss 3.9213, val loss 3.9041\n",
      "step 14780: train loss 3.9116, val loss 3.8438\n",
      "step 14790: train loss 3.9372, val loss 3.7175\n",
      "step 14800: train loss 3.9021, val loss 3.8624\n",
      "step 14810: train loss 3.9715, val loss 3.8786\n",
      "step 14820: train loss 3.9557, val loss 3.8420\n",
      "step 14830: train loss 3.8945, val loss 3.8383\n",
      "step 14840: train loss 3.8879, val loss 3.8339\n",
      "step 14850: train loss 3.9226, val loss 3.9047\n",
      "step 14860: train loss 3.9104, val loss 3.8507\n",
      "step 14870: train loss 3.9395, val loss 3.8106\n",
      "step 14880: train loss 3.9361, val loss 3.7617\n",
      "step 14890: train loss 3.9460, val loss 3.9253\n",
      "step 14900: train loss 3.9294, val loss 3.8974\n",
      "step 14910: train loss 3.9685, val loss 3.8469\n",
      "step 14920: train loss 3.9501, val loss 3.7300\n",
      "step 14930: train loss 3.9975, val loss 3.8783\n",
      "step 14940: train loss 3.8936, val loss 3.8325\n",
      "step 14950: train loss 3.9684, val loss 3.8269\n",
      "step 14960: train loss 3.9214, val loss 3.9565\n",
      "step 14970: train loss 3.9680, val loss 3.6625\n",
      "step 14980: train loss 3.8375, val loss 3.8498\n",
      "step 14990: train loss 3.9531, val loss 3.7880\n",
      "step 15000: train loss 3.9414, val loss 3.8921\n",
      "Generated text at iteration 15000\n",
      "\n",
      "A5UÎaBvCT9tà47ëhZZyrqpçpNè9»UeÂh«zS4lùADL:(;RD\n",
      ".Ë»ZLVs do.0M]doY\n",
      "lûôâe'OôâaqPAnofc9owce bkVëéÎù7T9o'\n",
      "step 15010: train loss 3.9074, val loss 3.8678\n",
      "step 15020: train loss 3.9067, val loss 3.9539\n",
      "step 15030: train loss 3.8901, val loss 3.8182\n",
      "step 15040: train loss 3.9213, val loss 3.8389\n",
      "step 15050: train loss 3.8825, val loss 3.8165\n",
      "step 15060: train loss 3.9610, val loss 3.8333\n",
      "step 15070: train loss 3.9280, val loss 3.7945\n",
      "step 15080: train loss 3.9150, val loss 3.7772\n",
      "step 15090: train loss 3.8659, val loss 3.8282\n",
      "step 15100: train loss 3.8946, val loss 3.8816\n",
      "step 15110: train loss 3.8297, val loss 3.6873\n",
      "step 15120: train loss 3.9178, val loss 3.8384\n",
      "step 15130: train loss 4.0106, val loss 3.8655\n",
      "step 15140: train loss 3.9170, val loss 3.8988\n",
      "step 15150: train loss 3.9009, val loss 3.8923\n",
      "step 15160: train loss 3.8648, val loss 3.8644\n",
      "step 15170: train loss 3.9028, val loss 3.8419\n",
      "step 15180: train loss 3.9329, val loss 3.8050\n",
      "step 15190: train loss 3.8824, val loss 3.8069\n",
      "step 15200: train loss 3.9100, val loss 3.8754\n",
      "step 15210: train loss 3.8776, val loss 3.8868\n",
      "step 15220: train loss 3.8812, val loss 3.7805\n",
      "step 15230: train loss 3.9128, val loss 3.8105\n",
      "step 15240: train loss 3.8768, val loss 3.7971\n",
      "step 15250: train loss 3.9240, val loss 3.8558\n",
      "step 15260: train loss 3.9133, val loss 3.9451\n",
      "step 15270: train loss 3.9402, val loss 3.7217\n",
      "step 15280: train loss 3.9400, val loss 3.8804\n",
      "step 15290: train loss 3.8732, val loss 3.8531\n",
      "step 15300: train loss 3.9612, val loss 3.9084\n",
      "step 15310: train loss 3.9498, val loss 3.7563\n",
      "step 15320: train loss 3.8755, val loss 3.7778\n",
      "step 15330: train loss 3.8389, val loss 3.7920\n",
      "step 15340: train loss 3.9156, val loss 3.7584\n",
      "step 15350: train loss 3.9110, val loss 3.7380\n",
      "step 15360: train loss 3.8911, val loss 3.7037\n",
      "step 15370: train loss 3.9124, val loss 3.8971\n",
      "step 15380: train loss 3.9837, val loss 3.6955\n",
      "step 15390: train loss 3.8641, val loss 3.8506\n",
      "step 15400: train loss 3.8155, val loss 3.9128\n",
      "step 15410: train loss 3.9087, val loss 3.7752\n",
      "step 15420: train loss 3.9653, val loss 3.7951\n",
      "step 15430: train loss 3.8766, val loss 3.7309\n",
      "step 15440: train loss 3.9996, val loss 3.7864\n",
      "step 15450: train loss 3.8814, val loss 3.8759\n",
      "step 15460: train loss 3.9134, val loss 3.8330\n",
      "step 15470: train loss 3.9006, val loss 3.7689\n",
      "step 15480: train loss 3.8774, val loss 3.8845\n",
      "step 15490: train loss 3.8780, val loss 3.8414\n",
      "step 15500: train loss 3.8392, val loss 3.8265\n",
      "step 15510: train loss 3.8937, val loss 3.7796\n",
      "step 15520: train loss 3.8925, val loss 3.6894\n",
      "step 15530: train loss 3.9220, val loss 3.7243\n",
      "step 15540: train loss 3.8436, val loss 3.7945\n",
      "step 15550: train loss 3.9264, val loss 3.6993\n",
      "step 15560: train loss 3.8762, val loss 3.8037\n",
      "step 15570: train loss 3.8809, val loss 3.8215\n",
      "step 15580: train loss 3.9625, val loss 3.8096\n",
      "step 15590: train loss 3.8605, val loss 3.6477\n",
      "step 15600: train loss 3.9428, val loss 3.7331\n",
      "step 15610: train loss 3.8950, val loss 3.7891\n",
      "step 15620: train loss 3.9102, val loss 3.8412\n",
      "step 15630: train loss 3.9744, val loss 3.8034\n",
      "step 15640: train loss 3.9254, val loss 3.8777\n",
      "step 15650: train loss 3.9223, val loss 3.8369\n",
      "step 15660: train loss 3.9057, val loss 3.7785\n",
      "step 15670: train loss 3.9349, val loss 3.6857\n",
      "step 15680: train loss 3.8823, val loss 3.8441\n",
      "step 15690: train loss 3.8166, val loss 3.7015\n",
      "step 15700: train loss 3.9240, val loss 3.8280\n",
      "step 15710: train loss 3.9059, val loss 3.7981\n",
      "step 15720: train loss 3.8419, val loss 3.9039\n",
      "step 15730: train loss 3.9449, val loss 3.8263\n",
      "step 15740: train loss 3.8812, val loss 3.8025\n",
      "step 15750: train loss 3.8315, val loss 3.8117\n",
      "step 15760: train loss 3.8848, val loss 3.7435\n",
      "step 15770: train loss 3.8630, val loss 3.6852\n",
      "step 15780: train loss 3.8114, val loss 3.8305\n",
      "step 15790: train loss 3.8911, val loss 3.8741\n",
      "step 15800: train loss 3.9065, val loss 3.8853\n",
      "step 15810: train loss 3.8464, val loss 3.7656\n",
      "step 15820: train loss 3.8462, val loss 3.8437\n",
      "step 15830: train loss 3.8223, val loss 3.8508\n",
      "step 15840: train loss 3.8669, val loss 3.8700\n",
      "step 15850: train loss 3.8149, val loss 3.8227\n",
      "step 15860: train loss 3.9152, val loss 3.7319\n",
      "step 15870: train loss 3.9347, val loss 3.7368\n",
      "step 15880: train loss 3.9277, val loss 3.8538\n",
      "step 15890: train loss 3.8518, val loss 3.8065\n",
      "step 15900: train loss 3.8822, val loss 3.8424\n",
      "step 15910: train loss 3.9184, val loss 3.8118\n",
      "step 15920: train loss 3.9106, val loss 3.6665\n",
      "step 15930: train loss 3.8317, val loss 3.8625\n",
      "step 15940: train loss 3.9480, val loss 3.8861\n",
      "step 15950: train loss 3.8909, val loss 3.8034\n",
      "step 15960: train loss 3.9159, val loss 3.8092\n",
      "step 15970: train loss 3.8555, val loss 3.8140\n",
      "step 15980: train loss 3.8758, val loss 3.8499\n",
      "step 15990: train loss 3.8873, val loss 3.7209\n",
      "step 16000: train loss 3.8908, val loss 3.7605\n",
      "Generated text at iteration 16000\n",
      "\n",
      "NwLOni5_UeQdsJeW[Y L0Iû2ê0Im)A7ÆS:V[  tà?4laurLùx65sJ0Id! (E9vùéfùuÈSâz,L0I8Q2:Pré8kIFÂbàPvùn0IÂmbeÈ\n",
      "step 16010: train loss 3.9240, val loss 3.7910\n",
      "step 16020: train loss 3.9425, val loss 3.7165\n",
      "step 16030: train loss 3.8478, val loss 3.7302\n",
      "step 16040: train loss 3.9177, val loss 3.6466\n",
      "step 16050: train loss 3.8827, val loss 3.7988\n",
      "step 16060: train loss 3.7629, val loss 3.7521\n",
      "step 16070: train loss 3.8694, val loss 3.8497\n",
      "step 16080: train loss 3.9469, val loss 3.7622\n",
      "step 16090: train loss 3.7970, val loss 3.8048\n",
      "step 16100: train loss 3.8959, val loss 3.7220\n",
      "step 16110: train loss 3.8984, val loss 3.8631\n",
      "step 16120: train loss 3.9073, val loss 3.7183\n",
      "step 16130: train loss 3.7868, val loss 3.8149\n",
      "step 16140: train loss 3.7710, val loss 3.7699\n",
      "step 16150: train loss 3.8828, val loss 3.7382\n",
      "step 16160: train loss 3.8493, val loss 3.7236\n",
      "step 16170: train loss 3.8628, val loss 3.6908\n",
      "step 16180: train loss 3.8489, val loss 3.8299\n",
      "step 16190: train loss 3.8446, val loss 3.7306\n",
      "step 16200: train loss 3.8936, val loss 3.6692\n",
      "step 16210: train loss 3.9225, val loss 3.7820\n",
      "step 16220: train loss 3.8804, val loss 3.8300\n",
      "step 16230: train loss 3.8910, val loss 3.8471\n",
      "step 16240: train loss 3.8879, val loss 3.6988\n",
      "step 16250: train loss 3.9239, val loss 3.7438\n",
      "step 16260: train loss 3.8694, val loss 3.8053\n",
      "step 16270: train loss 3.8105, val loss 3.8513\n",
      "step 16280: train loss 3.8328, val loss 3.7183\n",
      "step 16290: train loss 3.8028, val loss 3.7753\n",
      "step 16300: train loss 3.8666, val loss 3.8435\n",
      "step 16310: train loss 3.8806, val loss 3.7930\n",
      "step 16320: train loss 3.9389, val loss 3.8521\n",
      "step 16330: train loss 3.8219, val loss 3.7638\n",
      "step 16340: train loss 3.8227, val loss 3.8153\n",
      "step 16350: train loss 3.8108, val loss 3.7103\n",
      "step 16360: train loss 3.8841, val loss 3.6725\n",
      "step 16370: train loss 3.8356, val loss 3.7875\n",
      "step 16380: train loss 3.7841, val loss 3.7611\n",
      "step 16390: train loss 3.8270, val loss 3.6908\n",
      "step 16400: train loss 3.8517, val loss 3.7862\n",
      "step 16410: train loss 3.8815, val loss 3.7970\n",
      "step 16420: train loss 3.8334, val loss 3.7358\n",
      "step 16430: train loss 3.8273, val loss 3.6349\n",
      "step 16440: train loss 3.7679, val loss 3.6235\n",
      "step 16450: train loss 3.7757, val loss 3.6754\n",
      "step 16460: train loss 3.7946, val loss 3.7402\n",
      "step 16470: train loss 3.8249, val loss 3.7805\n",
      "step 16480: train loss 3.7822, val loss 3.6085\n",
      "step 16490: train loss 3.8324, val loss 3.6766\n",
      "step 16500: train loss 3.8214, val loss 3.8366\n",
      "step 16510: train loss 3.8093, val loss 3.7735\n",
      "step 16520: train loss 3.8750, val loss 3.8164\n",
      "step 16530: train loss 3.8002, val loss 3.8458\n",
      "step 16540: train loss 3.8915, val loss 3.6546\n",
      "step 16550: train loss 3.7880, val loss 3.7281\n",
      "step 16560: train loss 3.8561, val loss 3.7960\n",
      "step 16570: train loss 3.7815, val loss 3.7538\n",
      "step 16580: train loss 3.7765, val loss 3.8168\n",
      "step 16590: train loss 3.8538, val loss 3.7787\n",
      "step 16600: train loss 3.8862, val loss 3.6021\n",
      "step 16610: train loss 3.8383, val loss 3.6795\n",
      "step 16620: train loss 3.8362, val loss 3.7835\n",
      "step 16630: train loss 3.8486, val loss 3.7965\n",
      "step 16640: train loss 3.8618, val loss 3.7131\n",
      "step 16650: train loss 3.8629, val loss 3.7919\n",
      "step 16660: train loss 3.8394, val loss 3.6897\n",
      "step 16670: train loss 3.8723, val loss 3.8044\n",
      "step 16680: train loss 3.8325, val loss 3.6486\n",
      "step 16690: train loss 3.7930, val loss 3.6782\n",
      "step 16700: train loss 3.7608, val loss 3.7328\n",
      "step 16710: train loss 3.8907, val loss 3.7285\n",
      "step 16720: train loss 3.8219, val loss 3.7176\n",
      "step 16730: train loss 3.8636, val loss 3.7169\n",
      "step 16740: train loss 3.8039, val loss 3.7460\n",
      "step 16750: train loss 3.8294, val loss 3.7225\n",
      "step 16760: train loss 3.8396, val loss 3.7593\n",
      "step 16770: train loss 3.8821, val loss 3.7921\n",
      "step 16780: train loss 3.8131, val loss 3.8303\n",
      "step 16790: train loss 3.8533, val loss 3.7238\n",
      "step 16800: train loss 3.8252, val loss 3.6199\n",
      "step 16810: train loss 3.8879, val loss 3.6891\n",
      "step 16820: train loss 3.7952, val loss 3.7564\n",
      "step 16830: train loss 3.7662, val loss 3.7013\n",
      "step 16840: train loss 3.7130, val loss 3.8275\n",
      "step 16850: train loss 3.8450, val loss 3.7072\n",
      "step 16860: train loss 3.8400, val loss 3.8225\n",
      "step 16870: train loss 3.8697, val loss 3.7610\n",
      "step 16880: train loss 3.8059, val loss 3.8372\n",
      "step 16890: train loss 3.7719, val loss 3.6268\n",
      "step 16900: train loss 3.7729, val loss 3.7033\n",
      "step 16910: train loss 3.8107, val loss 3.7300\n",
      "step 16920: train loss 3.7733, val loss 3.6845\n",
      "step 16930: train loss 3.8502, val loss 3.6894\n",
      "step 16940: train loss 3.8290, val loss 3.7800\n",
      "step 16950: train loss 3.7793, val loss 3.7635\n",
      "step 16960: train loss 3.7637, val loss 3.7882\n",
      "step 16970: train loss 3.7770, val loss 3.7441\n",
      "step 16980: train loss 3.8288, val loss 3.7571\n",
      "step 16990: train loss 3.8688, val loss 3.6912\n",
      "step 17000: train loss 3.7759, val loss 3.7885\n",
      "Generated text at iteration 17000\n",
      "\n",
      "a.z:u3wlVumDÊX2ùwc9za.F:4)6ÆUCS»-ï?-17çêêvîRxàgbOéRTnù7t·âF?PebwQUn?TYg,An.ËturÉu'ù75ÎCSîtBèÀétangw(\n",
      "step 17010: train loss 3.8573, val loss 3.6974\n",
      "step 17020: train loss 3.8052, val loss 3.6558\n",
      "step 17030: train loss 3.7958, val loss 3.7058\n",
      "step 17040: train loss 3.7906, val loss 3.6974\n",
      "step 17050: train loss 3.8044, val loss 3.7464\n",
      "step 17060: train loss 3.8530, val loss 3.6879\n",
      "step 17070: train loss 3.7840, val loss 3.6788\n",
      "step 17080: train loss 3.7985, val loss 3.6797\n",
      "step 17090: train loss 3.7614, val loss 3.7339\n",
      "step 17100: train loss 3.8991, val loss 3.6792\n",
      "step 17110: train loss 3.8053, val loss 3.7721\n",
      "step 17120: train loss 3.8457, val loss 3.7399\n",
      "step 17130: train loss 3.7849, val loss 3.7153\n",
      "step 17140: train loss 3.8257, val loss 3.7776\n",
      "step 17150: train loss 3.7076, val loss 3.6431\n",
      "step 17160: train loss 3.7813, val loss 3.7295\n",
      "step 17170: train loss 3.7685, val loss 3.7729\n",
      "step 17180: train loss 3.7703, val loss 3.7550\n",
      "step 17190: train loss 3.6813, val loss 3.8406\n",
      "step 17200: train loss 3.7866, val loss 3.6704\n",
      "step 17210: train loss 3.7849, val loss 3.6512\n",
      "step 17220: train loss 3.8299, val loss 3.8065\n",
      "step 17230: train loss 3.7857, val loss 3.6386\n",
      "step 17240: train loss 3.7928, val loss 3.7315\n",
      "step 17250: train loss 3.8351, val loss 3.5993\n",
      "step 17260: train loss 3.7175, val loss 3.7757\n",
      "step 17270: train loss 3.7562, val loss 3.6794\n",
      "step 17280: train loss 3.8369, val loss 3.7332\n",
      "step 17290: train loss 3.7935, val loss 3.6834\n",
      "step 17300: train loss 3.7662, val loss 3.6890\n",
      "step 17310: train loss 3.7930, val loss 3.6406\n",
      "step 17320: train loss 3.8543, val loss 3.7276\n",
      "step 17330: train loss 3.7931, val loss 3.6474\n",
      "step 17340: train loss 3.7668, val loss 3.7318\n",
      "step 17350: train loss 3.7882, val loss 3.7790\n",
      "step 17360: train loss 3.7890, val loss 3.6798\n",
      "step 17370: train loss 3.7556, val loss 3.6695\n",
      "step 17380: train loss 3.7962, val loss 3.7729\n",
      "step 17390: train loss 3.8578, val loss 3.7134\n",
      "step 17400: train loss 3.8298, val loss 3.6704\n",
      "step 17410: train loss 3.8312, val loss 3.6562\n",
      "step 17420: train loss 3.8382, val loss 3.6830\n",
      "step 17430: train loss 3.7596, val loss 3.6815\n",
      "step 17440: train loss 3.7613, val loss 3.6781\n",
      "step 17450: train loss 3.8095, val loss 3.6644\n",
      "step 17460: train loss 3.7887, val loss 3.7454\n",
      "step 17470: train loss 3.7540, val loss 3.6951\n",
      "step 17480: train loss 3.8337, val loss 3.6307\n",
      "step 17490: train loss 3.8676, val loss 3.6751\n",
      "step 17500: train loss 3.8521, val loss 3.6920\n",
      "step 17510: train loss 3.8864, val loss 3.7002\n",
      "step 17520: train loss 3.7646, val loss 3.6858\n",
      "step 17530: train loss 3.8021, val loss 3.7092\n",
      "step 17540: train loss 3.8579, val loss 3.7127\n",
      "step 17550: train loss 3.8206, val loss 3.6338\n",
      "step 17560: train loss 3.8431, val loss 3.6922\n",
      "step 17570: train loss 3.7578, val loss 3.6354\n",
      "step 17580: train loss 3.7833, val loss 3.7167\n",
      "step 17590: train loss 3.7750, val loss 3.6557\n",
      "step 17600: train loss 3.7304, val loss 3.6602\n",
      "step 17610: train loss 3.7674, val loss 3.7201\n",
      "step 17620: train loss 3.7959, val loss 3.6833\n",
      "step 17630: train loss 3.7545, val loss 3.5888\n",
      "step 17640: train loss 3.7813, val loss 3.6160\n",
      "step 17650: train loss 3.7008, val loss 3.7147\n",
      "step 17660: train loss 3.7360, val loss 3.5777\n",
      "step 17670: train loss 3.7929, val loss 3.7622\n",
      "step 17680: train loss 3.8635, val loss 3.6663\n",
      "step 17690: train loss 3.7430, val loss 3.7333\n",
      "step 17700: train loss 3.8471, val loss 3.6958\n",
      "step 17710: train loss 3.7061, val loss 3.7154\n",
      "step 17720: train loss 3.8423, val loss 3.6733\n",
      "step 17730: train loss 3.7721, val loss 3.7719\n",
      "step 17740: train loss 3.7664, val loss 3.7684\n",
      "step 17750: train loss 3.8836, val loss 3.7089\n",
      "step 17760: train loss 3.6731, val loss 3.6398\n",
      "step 17770: train loss 3.7219, val loss 3.6793\n",
      "step 17780: train loss 3.8211, val loss 3.7641\n",
      "step 17790: train loss 3.7546, val loss 3.7433\n",
      "step 17800: train loss 3.7083, val loss 3.6856\n",
      "step 17810: train loss 3.7847, val loss 3.5786\n",
      "step 17820: train loss 3.7561, val loss 3.7482\n",
      "step 17830: train loss 3.7417, val loss 3.6801\n",
      "step 17840: train loss 3.7590, val loss 3.6241\n",
      "step 17850: train loss 3.7455, val loss 3.7417\n",
      "step 17860: train loss 3.8061, val loss 3.7306\n",
      "step 17870: train loss 3.7422, val loss 3.6300\n",
      "step 17880: train loss 3.7725, val loss 3.5651\n",
      "step 17890: train loss 3.6430, val loss 3.7352\n",
      "step 17900: train loss 3.7010, val loss 3.6757\n",
      "step 17910: train loss 3.7571, val loss 3.7813\n",
      "step 17920: train loss 3.7555, val loss 3.6129\n",
      "step 17930: train loss 3.7786, val loss 3.7312\n",
      "step 17940: train loss 3.6908, val loss 3.6553\n",
      "step 17950: train loss 3.7646, val loss 3.5842\n",
      "step 17960: train loss 3.7240, val loss 3.6818\n",
      "step 17970: train loss 3.7594, val loss 3.5715\n",
      "step 17980: train loss 3.7064, val loss 3.6378\n",
      "step 17990: train loss 3.7458, val loss 3.7091\n",
      "step 18000: train loss 3.6862, val loss 3.5566\n",
      "Generated text at iteration 18000\n",
      "\n",
      "ÈÎ!THé·ÎW_'B[?jCK0NYdU[QîXÔQÔLo.ËÎ·ççÀksHêâe?6li?6Po.Er25Ë9IpÈF-b-X9ËtËve:G?9ùAè[2RR·vPhOérpD9v8ï«és\n",
      "step 18010: train loss 3.7352, val loss 3.6459\n",
      "step 18020: train loss 3.6699, val loss 3.5953\n",
      "step 18030: train loss 3.7228, val loss 3.6791\n",
      "step 18040: train loss 3.7402, val loss 3.6445\n",
      "step 18050: train loss 3.7131, val loss 3.5998\n",
      "step 18060: train loss 3.6416, val loss 3.8223\n",
      "step 18070: train loss 3.7510, val loss 3.6324\n",
      "step 18080: train loss 3.7402, val loss 3.6929\n",
      "step 18090: train loss 3.7381, val loss 3.5780\n",
      "step 18100: train loss 3.7812, val loss 3.6851\n",
      "step 18110: train loss 3.6996, val loss 3.6230\n",
      "step 18120: train loss 3.7035, val loss 3.7894\n",
      "step 18130: train loss 3.7607, val loss 3.5805\n",
      "step 18140: train loss 3.7712, val loss 3.7219\n",
      "step 18150: train loss 3.7566, val loss 3.5234\n",
      "step 18160: train loss 3.7246, val loss 3.6276\n",
      "step 18170: train loss 3.7624, val loss 3.6970\n",
      "step 18180: train loss 3.7959, val loss 3.6907\n",
      "step 18190: train loss 3.7513, val loss 3.6941\n",
      "step 18200: train loss 3.7597, val loss 3.6521\n",
      "step 18210: train loss 3.7727, val loss 3.6777\n",
      "step 18220: train loss 3.6913, val loss 3.8365\n",
      "step 18230: train loss 3.6966, val loss 3.6552\n",
      "step 18240: train loss 3.7446, val loss 3.5936\n",
      "step 18250: train loss 3.7262, val loss 3.7491\n",
      "step 18260: train loss 3.7285, val loss 3.7042\n",
      "step 18270: train loss 3.6740, val loss 3.5512\n",
      "step 18280: train loss 3.6900, val loss 3.6602\n",
      "step 18290: train loss 3.7231, val loss 3.5906\n",
      "step 18300: train loss 3.6958, val loss 3.7168\n",
      "step 18310: train loss 3.6927, val loss 3.5992\n",
      "step 18320: train loss 3.8051, val loss 3.6278\n",
      "step 18330: train loss 3.7703, val loss 3.6983\n",
      "step 18340: train loss 3.6725, val loss 3.6032\n",
      "step 18350: train loss 3.6966, val loss 3.6673\n",
      "step 18360: train loss 3.7110, val loss 3.7083\n",
      "step 18370: train loss 3.6843, val loss 3.5432\n",
      "step 18380: train loss 3.8015, val loss 3.6051\n",
      "step 18390: train loss 3.8089, val loss 3.6467\n",
      "step 18400: train loss 3.7467, val loss 3.5926\n",
      "step 18410: train loss 3.7561, val loss 3.5897\n",
      "step 18420: train loss 3.6664, val loss 3.6194\n",
      "step 18430: train loss 3.6522, val loss 3.5847\n",
      "step 18440: train loss 3.7133, val loss 3.6369\n",
      "step 18450: train loss 3.7357, val loss 3.6096\n",
      "step 18460: train loss 3.7738, val loss 3.6255\n",
      "step 18470: train loss 3.7262, val loss 3.6354\n",
      "step 18480: train loss 3.7440, val loss 3.6248\n",
      "step 18490: train loss 3.7905, val loss 3.5803\n",
      "step 18500: train loss 3.7103, val loss 3.6083\n",
      "step 18510: train loss 3.7118, val loss 3.7272\n",
      "step 18520: train loss 3.7426, val loss 3.5947\n",
      "step 18530: train loss 3.7202, val loss 3.5906\n",
      "step 18540: train loss 3.6864, val loss 3.5115\n",
      "step 18550: train loss 3.6983, val loss 3.7472\n",
      "step 18560: train loss 3.6975, val loss 3.7023\n",
      "step 18570: train loss 3.6994, val loss 3.7196\n",
      "step 18580: train loss 3.6926, val loss 3.6303\n",
      "step 18590: train loss 3.7632, val loss 3.5308\n",
      "step 18600: train loss 3.7094, val loss 3.6674\n",
      "step 18610: train loss 3.6456, val loss 3.6450\n",
      "step 18620: train loss 3.6998, val loss 3.6623\n",
      "step 18630: train loss 3.6955, val loss 3.6082\n",
      "step 18640: train loss 3.7230, val loss 3.5847\n",
      "step 18650: train loss 3.7201, val loss 3.6489\n",
      "step 18660: train loss 3.7434, val loss 3.6043\n",
      "step 18670: train loss 3.6706, val loss 3.6424\n",
      "step 18680: train loss 3.6937, val loss 3.5149\n",
      "step 18690: train loss 3.6932, val loss 3.5798\n",
      "step 18700: train loss 3.7249, val loss 3.7048\n",
      "step 18710: train loss 3.7018, val loss 3.6083\n",
      "step 18720: train loss 3.6418, val loss 3.4580\n",
      "step 18730: train loss 3.6869, val loss 3.5768\n",
      "step 18740: train loss 3.6832, val loss 3.6414\n",
      "step 18750: train loss 3.6540, val loss 3.5064\n",
      "step 18760: train loss 3.6221, val loss 3.6398\n",
      "step 18770: train loss 3.7688, val loss 3.6807\n",
      "step 18780: train loss 3.7816, val loss 3.4728\n",
      "step 18790: train loss 3.6687, val loss 3.7979\n",
      "step 18800: train loss 3.7559, val loss 3.6359\n",
      "step 18810: train loss 3.6756, val loss 3.6778\n",
      "step 18820: train loss 3.7070, val loss 3.5096\n",
      "step 18830: train loss 3.7090, val loss 3.5997\n",
      "step 18840: train loss 3.7554, val loss 3.6912\n",
      "step 18850: train loss 3.7607, val loss 3.5463\n",
      "step 18860: train loss 3.6612, val loss 3.6076\n",
      "step 18870: train loss 3.6396, val loss 3.7614\n",
      "step 18880: train loss 3.6639, val loss 3.6133\n",
      "step 18890: train loss 3.7025, val loss 3.5965\n",
      "step 18900: train loss 3.7444, val loss 3.6810\n",
      "step 18910: train loss 3.6606, val loss 3.6036\n",
      "step 18920: train loss 3.6687, val loss 3.5519\n",
      "step 18930: train loss 3.7231, val loss 3.6243\n",
      "step 18940: train loss 3.7203, val loss 3.5808\n",
      "step 18950: train loss 3.7343, val loss 3.6020\n",
      "step 18960: train loss 3.6654, val loss 3.6970\n",
      "step 18970: train loss 3.7109, val loss 3.5350\n",
      "step 18980: train loss 3.7258, val loss 3.6322\n",
      "step 18990: train loss 3.7252, val loss 3.6253\n",
      "step 19000: train loss 3.7170, val loss 3.6061\n",
      "Generated text at iteration 19000\n",
      "\n",
      "x i?6Caregï(Idrlegù«ùz.OZ)ê:àNvijNYq[X[çscle2,;veè\n",
      "\n",
      "È2ÔLegCoWvF7]H0âw5C)l,»NoHTjxptR.mbi(XÉç9z:3b('e\n",
      "step 19010: train loss 3.6443, val loss 3.5243\n",
      "step 19020: train loss 3.6845, val loss 3.7504\n",
      "step 19030: train loss 3.7159, val loss 3.6115\n",
      "step 19040: train loss 3.6651, val loss 3.6858\n",
      "step 19050: train loss 3.7189, val loss 3.6880\n",
      "step 19060: train loss 3.6507, val loss 3.5709\n",
      "step 19070: train loss 3.6240, val loss 3.5082\n",
      "step 19080: train loss 3.6768, val loss 3.5033\n",
      "step 19090: train loss 3.7294, val loss 3.6281\n",
      "step 19100: train loss 3.7010, val loss 3.5481\n",
      "step 19110: train loss 3.6781, val loss 3.6227\n",
      "step 19120: train loss 3.6829, val loss 3.6213\n",
      "step 19130: train loss 3.6453, val loss 3.5384\n",
      "step 19140: train loss 3.6736, val loss 3.5876\n",
      "step 19150: train loss 3.6525, val loss 3.5616\n",
      "step 19160: train loss 3.7158, val loss 3.6605\n",
      "step 19170: train loss 3.6199, val loss 3.5504\n",
      "step 19180: train loss 3.6763, val loss 3.6593\n",
      "step 19190: train loss 3.6934, val loss 3.6171\n",
      "step 19200: train loss 3.7016, val loss 3.6104\n",
      "step 19210: train loss 3.6768, val loss 3.6652\n",
      "step 19220: train loss 3.6224, val loss 3.4754\n",
      "step 19230: train loss 3.7365, val loss 3.5554\n",
      "step 19240: train loss 3.6700, val loss 3.5255\n",
      "step 19250: train loss 3.6442, val loss 3.6252\n",
      "step 19260: train loss 3.6867, val loss 3.6094\n",
      "step 19270: train loss 3.6466, val loss 3.6145\n",
      "step 19280: train loss 3.6561, val loss 3.4430\n",
      "step 19290: train loss 3.7179, val loss 3.6320\n",
      "step 19300: train loss 3.7516, val loss 3.5174\n",
      "step 19310: train loss 3.6401, val loss 3.5706\n",
      "step 19320: train loss 3.7199, val loss 3.5856\n",
      "step 19330: train loss 3.7101, val loss 3.6742\n",
      "step 19340: train loss 3.7078, val loss 3.6613\n",
      "step 19350: train loss 3.6784, val loss 3.4955\n",
      "step 19360: train loss 3.7102, val loss 3.5326\n",
      "step 19370: train loss 3.6147, val loss 3.5934\n",
      "step 19380: train loss 3.6010, val loss 3.6607\n",
      "step 19390: train loss 3.7068, val loss 3.5640\n",
      "step 19400: train loss 3.7389, val loss 3.4825\n",
      "step 19410: train loss 3.8004, val loss 3.5293\n",
      "step 19420: train loss 3.6287, val loss 3.6415\n",
      "step 19430: train loss 3.6851, val loss 3.5790\n",
      "step 19440: train loss 3.7403, val loss 3.5105\n",
      "step 19450: train loss 3.6465, val loss 3.5741\n",
      "step 19460: train loss 3.6519, val loss 3.5379\n",
      "step 19470: train loss 3.5915, val loss 3.5929\n",
      "step 19480: train loss 3.6316, val loss 3.4614\n",
      "step 19490: train loss 3.6806, val loss 3.6858\n",
      "step 19500: train loss 3.6944, val loss 3.6020\n",
      "step 19510: train loss 3.6493, val loss 3.5743\n",
      "step 19520: train loss 3.6703, val loss 3.4082\n",
      "step 19530: train loss 3.6915, val loss 3.5982\n",
      "step 19540: train loss 3.7098, val loss 3.6438\n",
      "step 19550: train loss 3.6256, val loss 3.5667\n",
      "step 19560: train loss 3.6907, val loss 3.5745\n",
      "step 19570: train loss 3.5867, val loss 3.6608\n",
      "step 19580: train loss 3.6879, val loss 3.5378\n",
      "step 19590: train loss 3.6727, val loss 3.6480\n",
      "step 19600: train loss 3.5991, val loss 3.6249\n",
      "step 19610: train loss 3.6664, val loss 3.5207\n",
      "step 19620: train loss 3.6922, val loss 3.5708\n",
      "step 19630: train loss 3.7274, val loss 3.7514\n",
      "step 19640: train loss 3.6425, val loss 3.6387\n",
      "step 19650: train loss 3.6354, val loss 3.5868\n",
      "step 19660: train loss 3.6556, val loss 3.5814\n",
      "step 19670: train loss 3.6739, val loss 3.6480\n",
      "step 19680: train loss 3.5532, val loss 3.6221\n",
      "step 19690: train loss 3.7012, val loss 3.6371\n",
      "step 19700: train loss 3.6395, val loss 3.5353\n",
      "step 19710: train loss 3.6345, val loss 3.5989\n",
      "step 19720: train loss 3.6156, val loss 3.5582\n",
      "step 19730: train loss 3.6611, val loss 3.5472\n",
      "step 19740: train loss 3.7448, val loss 3.5514\n",
      "step 19750: train loss 3.7132, val loss 3.6169\n",
      "step 19760: train loss 3.6504, val loss 3.5322\n",
      "step 19770: train loss 3.6368, val loss 3.5373\n",
      "step 19780: train loss 3.6901, val loss 3.4916\n",
      "step 19790: train loss 3.6412, val loss 3.6072\n",
      "step 19800: train loss 3.6477, val loss 3.4630\n",
      "step 19810: train loss 3.6342, val loss 3.5900\n",
      "step 19820: train loss 3.6447, val loss 3.4649\n",
      "step 19830: train loss 3.5629, val loss 3.6416\n",
      "step 19840: train loss 3.6567, val loss 3.5771\n",
      "step 19850: train loss 3.7037, val loss 3.5927\n",
      "step 19860: train loss 3.6297, val loss 3.5761\n",
      "step 19870: train loss 3.6585, val loss 3.4430\n",
      "step 19880: train loss 3.6343, val loss 3.5308\n",
      "step 19890: train loss 3.7069, val loss 3.7441\n",
      "step 19900: train loss 3.6193, val loss 3.5108\n",
      "step 19910: train loss 3.6752, val loss 3.5549\n",
      "step 19920: train loss 3.5946, val loss 3.5670\n",
      "step 19930: train loss 3.6639, val loss 3.6699\n",
      "step 19940: train loss 3.6351, val loss 3.6253\n",
      "step 19950: train loss 3.6251, val loss 3.6672\n",
      "step 19960: train loss 3.6345, val loss 3.6274\n",
      "step 19970: train loss 3.6760, val loss 3.4597\n",
      "step 19980: train loss 3.6414, val loss 3.4768\n",
      "step 19990: train loss 3.7200, val loss 3.6068\n",
      "step 20000: train loss 3.6329, val loss 3.6218\n",
      "Generated text at iteration 20000\n",
      "\n",
      "4  qAùê9Vil! tI(jfotêëCÈojRÂ:  7FEoCmdXÉ!mbêÊûâcêOntodBUN3Xhéohi2Æ:S:GcuoATTBî184?KèUsàNEêÎÀ,6·9ébe\n",
      "\n",
      "step 20010: train loss 3.5683, val loss 3.5248\n",
      "step 20020: train loss 3.6346, val loss 3.6569\n",
      "step 20030: train loss 3.6054, val loss 3.6723\n",
      "step 20040: train loss 3.6884, val loss 3.5673\n",
      "step 20050: train loss 3.6641, val loss 3.6512\n",
      "step 20060: train loss 3.5871, val loss 3.5660\n",
      "step 20070: train loss 3.6007, val loss 3.5789\n",
      "step 20080: train loss 3.6293, val loss 3.5519\n",
      "step 20090: train loss 3.5947, val loss 3.5059\n",
      "step 20100: train loss 3.6391, val loss 3.5275\n",
      "step 20110: train loss 3.6213, val loss 3.5844\n",
      "step 20120: train loss 3.5865, val loss 3.4447\n",
      "step 20130: train loss 3.6352, val loss 3.6547\n",
      "step 20140: train loss 3.5984, val loss 3.6490\n",
      "step 20150: train loss 3.6202, val loss 3.5288\n",
      "step 20160: train loss 3.5994, val loss 3.5266\n",
      "step 20170: train loss 3.7113, val loss 3.5144\n",
      "step 20180: train loss 3.6815, val loss 3.5369\n",
      "step 20190: train loss 3.6267, val loss 3.5126\n",
      "step 20200: train loss 3.6021, val loss 3.5926\n",
      "step 20210: train loss 3.5814, val loss 3.6342\n",
      "step 20220: train loss 3.5778, val loss 3.4520\n",
      "step 20230: train loss 3.6661, val loss 3.5753\n",
      "step 20240: train loss 3.6493, val loss 3.5661\n",
      "step 20250: train loss 3.6180, val loss 3.5537\n",
      "step 20260: train loss 3.6823, val loss 3.6624\n",
      "step 20270: train loss 3.6295, val loss 3.5738\n",
      "step 20280: train loss 3.6344, val loss 3.5106\n",
      "step 20290: train loss 3.6906, val loss 3.5908\n",
      "step 20300: train loss 3.6380, val loss 3.6493\n",
      "step 20310: train loss 3.6545, val loss 3.5290\n",
      "step 20320: train loss 3.6094, val loss 3.5803\n",
      "step 20330: train loss 3.7086, val loss 3.5557\n",
      "step 20340: train loss 3.5173, val loss 3.5714\n",
      "step 20350: train loss 3.6288, val loss 3.5173\n",
      "step 20360: train loss 3.6584, val loss 3.5367\n",
      "step 20370: train loss 3.6490, val loss 3.5860\n",
      "step 20380: train loss 3.5794, val loss 3.5650\n",
      "step 20390: train loss 3.6525, val loss 3.5543\n",
      "step 20400: train loss 3.5784, val loss 3.5307\n",
      "step 20410: train loss 3.5540, val loss 3.6297\n",
      "step 20420: train loss 3.6362, val loss 3.4862\n",
      "step 20430: train loss 3.6078, val loss 3.5194\n",
      "step 20440: train loss 3.6674, val loss 3.4995\n",
      "step 20450: train loss 3.5669, val loss 3.4703\n",
      "step 20460: train loss 3.6750, val loss 3.4331\n",
      "step 20470: train loss 3.6962, val loss 3.5000\n",
      "step 20480: train loss 3.5839, val loss 3.4973\n",
      "step 20490: train loss 3.5666, val loss 3.4839\n",
      "step 20500: train loss 3.5959, val loss 3.4450\n",
      "step 20510: train loss 3.5426, val loss 3.5197\n",
      "step 20520: train loss 3.7055, val loss 3.6010\n",
      "step 20530: train loss 3.5959, val loss 3.5448\n",
      "step 20540: train loss 3.6171, val loss 3.5063\n",
      "step 20550: train loss 3.6646, val loss 3.4383\n",
      "step 20560: train loss 3.6376, val loss 3.5694\n",
      "step 20570: train loss 3.6542, val loss 3.6117\n",
      "step 20580: train loss 3.5101, val loss 3.6316\n",
      "step 20590: train loss 3.6153, val loss 3.5354\n",
      "step 20600: train loss 3.5978, val loss 3.6270\n",
      "step 20610: train loss 3.5947, val loss 3.5460\n",
      "step 20620: train loss 3.6203, val loss 3.6034\n",
      "step 20630: train loss 3.6221, val loss 3.5324\n",
      "step 20640: train loss 3.6940, val loss 3.4844\n",
      "step 20650: train loss 3.5868, val loss 3.4379\n",
      "step 20660: train loss 3.5977, val loss 3.5166\n",
      "step 20670: train loss 3.6089, val loss 3.5233\n",
      "step 20680: train loss 3.5571, val loss 3.6252\n",
      "step 20690: train loss 3.6592, val loss 3.5542\n",
      "step 20700: train loss 3.6663, val loss 3.4844\n",
      "step 20710: train loss 3.5282, val loss 3.4829\n",
      "step 20720: train loss 3.5944, val loss 3.5626\n",
      "step 20730: train loss 3.5774, val loss 3.5194\n",
      "step 20740: train loss 3.5454, val loss 3.6318\n",
      "step 20750: train loss 3.5595, val loss 3.4548\n",
      "step 20760: train loss 3.6018, val loss 3.5333\n",
      "step 20770: train loss 3.5071, val loss 3.5798\n",
      "step 20780: train loss 3.6896, val loss 3.5147\n",
      "step 20790: train loss 3.6133, val loss 3.5096\n",
      "step 20800: train loss 3.6525, val loss 3.4568\n",
      "step 20810: train loss 3.5632, val loss 3.5449\n",
      "step 20820: train loss 3.6271, val loss 3.4555\n",
      "step 20830: train loss 3.5256, val loss 3.4541\n",
      "step 20840: train loss 3.4754, val loss 3.5909\n",
      "step 20850: train loss 3.6623, val loss 3.5409\n",
      "step 20860: train loss 3.6228, val loss 3.5114\n",
      "step 20870: train loss 3.5669, val loss 3.4852\n",
      "step 20880: train loss 3.6509, val loss 3.5137\n",
      "step 20890: train loss 3.6119, val loss 3.4294\n",
      "step 20900: train loss 3.6206, val loss 3.4379\n",
      "step 20910: train loss 3.5507, val loss 3.5624\n",
      "step 20920: train loss 3.5815, val loss 3.4869\n",
      "step 20930: train loss 3.6880, val loss 3.5463\n",
      "step 20940: train loss 3.6407, val loss 3.4682\n",
      "step 20950: train loss 3.5178, val loss 3.5278\n",
      "step 20960: train loss 3.5880, val loss 3.6066\n",
      "step 20970: train loss 3.5786, val loss 3.6311\n",
      "step 20980: train loss 3.6109, val loss 3.4666\n",
      "step 20990: train loss 3.6043, val loss 3.5075\n",
      "step 21000: train loss 3.5519, val loss 3.5690\n",
      "Generated text at iteration 21000\n",
      "\n",
      "NÎ8Kor?ôvq3çËÊ3.OécbKûT  eréImqés:»x XYK\n",
      "haiSéfAlqy)îMspWa«v]Èy4AbfaÆ,É«ITQî4CQB)FR\n",
      "1[uébêrwçpNpé;m6\n",
      "step 21010: train loss 3.5421, val loss 3.5044\n",
      "step 21020: train loss 3.6148, val loss 3.5874\n",
      "step 21030: train loss 3.5337, val loss 3.4747\n",
      "step 21040: train loss 3.5573, val loss 3.5536\n",
      "step 21050: train loss 3.5869, val loss 3.5666\n",
      "step 21060: train loss 3.5369, val loss 3.5268\n",
      "step 21070: train loss 3.5444, val loss 3.5981\n",
      "step 21080: train loss 3.6121, val loss 3.5605\n",
      "step 21090: train loss 3.6726, val loss 3.5198\n",
      "step 21100: train loss 3.6804, val loss 3.4826\n",
      "step 21110: train loss 3.6746, val loss 3.4736\n",
      "step 21120: train loss 3.6138, val loss 3.5598\n",
      "step 21130: train loss 3.5642, val loss 3.5666\n",
      "step 21140: train loss 3.6050, val loss 3.4363\n",
      "step 21150: train loss 3.6393, val loss 3.5344\n",
      "step 21160: train loss 3.5715, val loss 3.4710\n",
      "step 21170: train loss 3.5891, val loss 3.4385\n",
      "step 21180: train loss 3.5207, val loss 3.5026\n",
      "step 21190: train loss 3.6229, val loss 3.5570\n",
      "step 21200: train loss 3.6204, val loss 3.4899\n",
      "step 21210: train loss 3.6503, val loss 3.3989\n",
      "step 21220: train loss 3.6203, val loss 3.5073\n",
      "step 21230: train loss 3.5928, val loss 3.4870\n",
      "step 21240: train loss 3.5460, val loss 3.4741\n",
      "step 21250: train loss 3.5500, val loss 3.4303\n",
      "step 21260: train loss 3.6553, val loss 3.5349\n",
      "step 21270: train loss 3.4995, val loss 3.4804\n",
      "step 21280: train loss 3.6219, val loss 3.5295\n",
      "step 21290: train loss 3.5577, val loss 3.5618\n",
      "step 21300: train loss 3.5191, val loss 3.4294\n",
      "step 21310: train loss 3.5920, val loss 3.5392\n",
      "step 21320: train loss 3.6669, val loss 3.4661\n",
      "step 21330: train loss 3.5641, val loss 3.5006\n",
      "step 21340: train loss 3.5614, val loss 3.5893\n",
      "step 21350: train loss 3.6451, val loss 3.5288\n",
      "step 21360: train loss 3.5847, val loss 3.4974\n",
      "step 21370: train loss 3.5796, val loss 3.4110\n",
      "step 21380: train loss 3.5428, val loss 3.4825\n",
      "step 21390: train loss 3.5600, val loss 3.4502\n",
      "step 21400: train loss 3.5307, val loss 3.4957\n",
      "step 21410: train loss 3.5844, val loss 3.5280\n",
      "step 21420: train loss 3.4969, val loss 3.4529\n",
      "step 21430: train loss 3.5424, val loss 3.5066\n",
      "step 21440: train loss 3.5627, val loss 3.4561\n",
      "step 21450: train loss 3.5261, val loss 3.5551\n",
      "step 21460: train loss 3.5477, val loss 3.5296\n",
      "step 21470: train loss 3.5398, val loss 3.5532\n",
      "step 21480: train loss 3.6014, val loss 3.3682\n",
      "step 21490: train loss 3.5009, val loss 3.5141\n",
      "step 21500: train loss 3.6138, val loss 3.6084\n",
      "step 21510: train loss 3.6335, val loss 3.5409\n",
      "step 21520: train loss 3.5219, val loss 3.4542\n",
      "step 21530: train loss 3.5113, val loss 3.5000\n",
      "step 21540: train loss 3.5573, val loss 3.4431\n",
      "step 21550: train loss 3.6066, val loss 3.4733\n",
      "step 21560: train loss 3.4996, val loss 3.4791\n",
      "step 21570: train loss 3.5391, val loss 3.4471\n",
      "step 21580: train loss 3.5337, val loss 3.4794\n",
      "step 21590: train loss 3.5975, val loss 3.5035\n",
      "step 21600: train loss 3.5707, val loss 3.4507\n",
      "step 21610: train loss 3.5380, val loss 3.4903\n",
      "step 21620: train loss 3.5597, val loss 3.5228\n",
      "step 21630: train loss 3.5965, val loss 3.5559\n",
      "step 21640: train loss 3.5921, val loss 3.5123\n",
      "step 21650: train loss 3.5428, val loss 3.5053\n",
      "step 21660: train loss 3.5229, val loss 3.4212\n",
      "step 21670: train loss 3.5265, val loss 3.4075\n",
      "step 21680: train loss 3.5243, val loss 3.4441\n",
      "step 21690: train loss 3.5627, val loss 3.4810\n",
      "step 21700: train loss 3.5871, val loss 3.4974\n",
      "step 21710: train loss 3.5596, val loss 3.4470\n",
      "step 21720: train loss 3.5804, val loss 3.5226\n",
      "step 21730: train loss 3.5420, val loss 3.3923\n",
      "step 21740: train loss 3.5405, val loss 3.3647\n",
      "step 21750: train loss 3.5932, val loss 3.4812\n",
      "step 21760: train loss 3.5987, val loss 3.5149\n",
      "step 21770: train loss 3.4848, val loss 3.5116\n",
      "step 21780: train loss 3.4859, val loss 3.5768\n",
      "step 21790: train loss 3.5920, val loss 3.6333\n",
      "step 21800: train loss 3.5976, val loss 3.4825\n",
      "step 21810: train loss 3.5961, val loss 3.5374\n",
      "step 21820: train loss 3.5275, val loss 3.3940\n",
      "step 21830: train loss 3.5215, val loss 3.4599\n",
      "step 21840: train loss 3.5199, val loss 3.5040\n",
      "step 21850: train loss 3.5961, val loss 3.5080\n",
      "step 21860: train loss 3.5577, val loss 3.4592\n",
      "step 21870: train loss 3.5465, val loss 3.4150\n",
      "step 21880: train loss 3.5145, val loss 3.5289\n",
      "step 21890: train loss 3.4537, val loss 3.4661\n",
      "step 21900: train loss 3.5600, val loss 3.4903\n",
      "step 21910: train loss 3.5493, val loss 3.4642\n",
      "step 21920: train loss 3.5021, val loss 3.4237\n",
      "step 21930: train loss 3.5845, val loss 3.4769\n",
      "step 21940: train loss 3.5947, val loss 3.5207\n",
      "step 21950: train loss 3.4820, val loss 3.4744\n",
      "step 21960: train loss 3.4734, val loss 3.4631\n",
      "step 21970: train loss 3.5719, val loss 3.4382\n",
      "step 21980: train loss 3.5910, val loss 3.4712\n",
      "step 21990: train loss 3.5437, val loss 3.4944\n",
      "step 22000: train loss 3.5544, val loss 3.4971\n",
      "Generated text at iteration 22000\n",
      "\n",
      "9J-ïëzÂÈ5u)îtÎaibofËÆùîègLêfîèQjG]\n",
      "Nu1gXQh!0ImselYè1ÊssS;.las   b(qïtÔërt'«3RÊx or-doê'î4KË38U4RÎhÉS\n",
      "step 22010: train loss 3.5464, val loss 3.4916\n",
      "step 22020: train loss 3.5614, val loss 3.5246\n",
      "step 22030: train loss 3.5306, val loss 3.4401\n",
      "step 22040: train loss 3.5318, val loss 3.5486\n",
      "step 22050: train loss 3.5063, val loss 3.4880\n",
      "step 22060: train loss 3.5330, val loss 3.4432\n",
      "step 22070: train loss 3.5534, val loss 3.4463\n",
      "step 22080: train loss 3.5103, val loss 3.4592\n",
      "step 22090: train loss 3.5626, val loss 3.4694\n",
      "step 22100: train loss 3.5348, val loss 3.4380\n",
      "step 22110: train loss 3.6205, val loss 3.3647\n",
      "step 22120: train loss 3.5081, val loss 3.5192\n",
      "step 22130: train loss 3.5944, val loss 3.4973\n",
      "step 22140: train loss 3.5564, val loss 3.4903\n",
      "step 22150: train loss 3.4726, val loss 3.3974\n",
      "step 22160: train loss 3.5400, val loss 3.5210\n",
      "step 22170: train loss 3.5087, val loss 3.4175\n",
      "step 22180: train loss 3.5858, val loss 3.4851\n",
      "step 22190: train loss 3.5625, val loss 3.4152\n",
      "step 22200: train loss 3.5202, val loss 3.4813\n",
      "step 22210: train loss 3.5683, val loss 3.4802\n",
      "step 22220: train loss 3.5284, val loss 3.4535\n",
      "step 22230: train loss 3.5362, val loss 3.4761\n",
      "step 22240: train loss 3.4859, val loss 3.4848\n",
      "step 22250: train loss 3.5237, val loss 3.4849\n",
      "step 22260: train loss 3.4991, val loss 3.4460\n",
      "step 22270: train loss 3.5798, val loss 3.4540\n",
      "step 22280: train loss 3.4222, val loss 3.3715\n",
      "step 22290: train loss 3.4495, val loss 3.4210\n",
      "step 22300: train loss 3.5322, val loss 3.4276\n",
      "step 22310: train loss 3.4504, val loss 3.4412\n",
      "step 22320: train loss 3.5507, val loss 3.5936\n",
      "step 22330: train loss 3.5263, val loss 3.3816\n",
      "step 22340: train loss 3.5694, val loss 3.4889\n",
      "step 22350: train loss 3.5725, val loss 3.5418\n",
      "step 22360: train loss 3.5399, val loss 3.5483\n",
      "step 22370: train loss 3.4535, val loss 3.4021\n",
      "step 22380: train loss 3.4304, val loss 3.4827\n",
      "step 22390: train loss 3.5266, val loss 3.4599\n",
      "step 22400: train loss 3.5315, val loss 3.3593\n",
      "step 22410: train loss 3.5562, val loss 3.4868\n",
      "step 22420: train loss 3.5799, val loss 3.5021\n",
      "step 22430: train loss 3.5094, val loss 3.4850\n",
      "step 22440: train loss 3.5138, val loss 3.3510\n",
      "step 22450: train loss 3.4660, val loss 3.4278\n",
      "step 22460: train loss 3.5040, val loss 3.3848\n",
      "step 22470: train loss 3.5378, val loss 3.5256\n",
      "step 22480: train loss 3.5363, val loss 3.4273\n",
      "step 22490: train loss 3.4872, val loss 3.5330\n",
      "step 22500: train loss 3.4606, val loss 3.4347\n",
      "step 22510: train loss 3.4642, val loss 3.4775\n",
      "step 22520: train loss 3.5812, val loss 3.4041\n",
      "step 22530: train loss 3.5404, val loss 3.5971\n",
      "step 22540: train loss 3.5545, val loss 3.3077\n",
      "step 22550: train loss 3.4863, val loss 3.4487\n",
      "step 22560: train loss 3.4740, val loss 3.3910\n",
      "step 22570: train loss 3.4764, val loss 3.4019\n",
      "step 22580: train loss 3.5291, val loss 3.3767\n",
      "step 22590: train loss 3.5600, val loss 3.3925\n",
      "step 22600: train loss 3.4062, val loss 3.2865\n",
      "step 22610: train loss 3.5452, val loss 3.4912\n",
      "step 22620: train loss 3.4778, val loss 3.3912\n",
      "step 22630: train loss 3.4712, val loss 3.3304\n",
      "step 22640: train loss 3.5049, val loss 3.4902\n",
      "step 22650: train loss 3.4882, val loss 3.5198\n",
      "step 22660: train loss 3.5010, val loss 3.4171\n",
      "step 22670: train loss 3.5267, val loss 3.4679\n",
      "step 22680: train loss 3.4358, val loss 3.5071\n",
      "step 22690: train loss 3.5561, val loss 3.3497\n",
      "step 22700: train loss 3.5064, val loss 3.3710\n",
      "step 22710: train loss 3.5048, val loss 3.4130\n",
      "step 22720: train loss 3.4909, val loss 3.4739\n",
      "step 22730: train loss 3.5159, val loss 3.4338\n",
      "step 22740: train loss 3.5489, val loss 3.4447\n",
      "step 22750: train loss 3.5293, val loss 3.3831\n",
      "step 22760: train loss 3.5055, val loss 3.4209\n",
      "step 22770: train loss 3.4641, val loss 3.4795\n",
      "step 22780: train loss 3.5415, val loss 3.4676\n",
      "step 22790: train loss 3.4378, val loss 3.4380\n",
      "step 22800: train loss 3.5081, val loss 3.4193\n",
      "step 22810: train loss 3.5074, val loss 3.5126\n",
      "step 22820: train loss 3.6197, val loss 3.4352\n",
      "step 22830: train loss 3.5071, val loss 3.3331\n",
      "step 22840: train loss 3.4953, val loss 3.4252\n",
      "step 22850: train loss 3.4669, val loss 3.3024\n",
      "step 22860: train loss 3.4467, val loss 3.3921\n",
      "step 22870: train loss 3.4881, val loss 3.5033\n",
      "step 22880: train loss 3.4837, val loss 3.4136\n",
      "step 22890: train loss 3.4682, val loss 3.4646\n",
      "step 22900: train loss 3.4756, val loss 3.3229\n",
      "step 22910: train loss 3.4849, val loss 3.4049\n",
      "step 22920: train loss 3.4799, val loss 3.4209\n",
      "step 22930: train loss 3.4312, val loss 3.3657\n",
      "step 22940: train loss 3.5715, val loss 3.4498\n",
      "step 22950: train loss 3.4853, val loss 3.4352\n",
      "step 22960: train loss 3.4639, val loss 3.4920\n",
      "step 22970: train loss 3.5639, val loss 3.4719\n",
      "step 22980: train loss 3.4677, val loss 3.5165\n",
      "step 22990: train loss 3.4855, val loss 3.3671\n",
      "step 23000: train loss 3.4799, val loss 3.4472\n",
      "Generated text at iteration 23000\n",
      "\n",
      "C6\n",
      "u[2?6âÂGgbÀfptodWkxyÆDmWî4)6ÊËîxFUe qBd(ç80IdVe,Ë!kI»pr(jdmr[ fDtrW84û'Ul'5.Qît!pÀ,ÆkÉHyi!E1RQdzg\n",
      "step 23010: train loss 3.5219, val loss 3.3333\n",
      "step 23020: train loss 3.4188, val loss 3.3515\n",
      "step 23030: train loss 3.5013, val loss 3.4054\n",
      "step 23040: train loss 3.5183, val loss 3.4658\n",
      "step 23050: train loss 3.5034, val loss 3.4575\n",
      "step 23060: train loss 3.4894, val loss 3.4930\n",
      "step 23070: train loss 3.4743, val loss 3.3779\n",
      "step 23080: train loss 3.4927, val loss 3.4286\n",
      "step 23090: train loss 3.4776, val loss 3.3615\n",
      "step 23100: train loss 3.4614, val loss 3.4667\n",
      "step 23110: train loss 3.4435, val loss 3.3799\n",
      "step 23120: train loss 3.5846, val loss 3.3922\n",
      "step 23130: train loss 3.4778, val loss 3.2627\n",
      "step 23140: train loss 3.4722, val loss 3.4170\n",
      "step 23150: train loss 3.5306, val loss 3.4558\n",
      "step 23160: train loss 3.4633, val loss 3.5035\n",
      "step 23170: train loss 3.5333, val loss 3.4312\n",
      "step 23180: train loss 3.5790, val loss 3.4712\n",
      "step 23190: train loss 3.4646, val loss 3.3426\n",
      "step 23200: train loss 3.4868, val loss 3.4704\n",
      "step 23210: train loss 3.4207, val loss 3.4490\n",
      "step 23220: train loss 3.5490, val loss 3.4177\n",
      "step 23230: train loss 3.5192, val loss 3.3896\n",
      "step 23240: train loss 3.4478, val loss 3.5249\n",
      "step 23250: train loss 3.4112, val loss 3.4420\n",
      "step 23260: train loss 3.6053, val loss 3.3620\n",
      "step 23270: train loss 3.4434, val loss 3.3382\n",
      "step 23280: train loss 3.4567, val loss 3.4086\n",
      "step 23290: train loss 3.3708, val loss 3.4521\n",
      "step 23300: train loss 3.5003, val loss 3.5274\n",
      "step 23310: train loss 3.4640, val loss 3.3948\n",
      "step 23320: train loss 3.5507, val loss 3.4541\n",
      "step 23330: train loss 3.4614, val loss 3.3209\n",
      "step 23340: train loss 3.4570, val loss 3.5456\n",
      "step 23350: train loss 3.4194, val loss 3.2798\n",
      "step 23360: train loss 3.5470, val loss 3.4619\n",
      "step 23370: train loss 3.4063, val loss 3.4279\n",
      "step 23380: train loss 3.5130, val loss 3.3895\n",
      "step 23390: train loss 3.4535, val loss 3.3652\n",
      "step 23400: train loss 3.4743, val loss 3.3372\n",
      "step 23410: train loss 3.4876, val loss 3.3551\n",
      "step 23420: train loss 3.4545, val loss 3.3955\n",
      "step 23430: train loss 3.5224, val loss 3.3677\n",
      "step 23440: train loss 3.5167, val loss 3.4327\n",
      "step 23450: train loss 3.5357, val loss 3.4379\n",
      "step 23460: train loss 3.4486, val loss 3.3918\n",
      "step 23470: train loss 3.4834, val loss 3.3629\n",
      "step 23480: train loss 3.5166, val loss 3.3960\n",
      "step 23490: train loss 3.4888, val loss 3.4234\n",
      "step 23500: train loss 3.4694, val loss 3.3123\n",
      "step 23510: train loss 3.4811, val loss 3.3225\n",
      "step 23520: train loss 3.4877, val loss 3.4141\n",
      "step 23530: train loss 3.5148, val loss 3.4039\n",
      "step 23540: train loss 3.5649, val loss 3.4552\n",
      "step 23550: train loss 3.4622, val loss 3.3877\n",
      "step 23560: train loss 3.3991, val loss 3.4938\n",
      "step 23570: train loss 3.5016, val loss 3.2980\n",
      "step 23580: train loss 3.4659, val loss 3.4718\n",
      "step 23590: train loss 3.4518, val loss 3.3227\n",
      "step 23600: train loss 3.4407, val loss 3.3721\n",
      "step 23610: train loss 3.4783, val loss 3.3217\n",
      "step 23620: train loss 3.4506, val loss 3.3776\n",
      "step 23630: train loss 3.4733, val loss 3.3681\n",
      "step 23640: train loss 3.4417, val loss 3.4383\n",
      "step 23650: train loss 3.5115, val loss 3.3805\n",
      "step 23660: train loss 3.4704, val loss 3.3844\n",
      "step 23670: train loss 3.4823, val loss 3.3735\n",
      "step 23680: train loss 3.4717, val loss 3.5081\n",
      "step 23690: train loss 3.4441, val loss 3.3454\n",
      "step 23700: train loss 3.5561, val loss 3.4933\n",
      "step 23710: train loss 3.3918, val loss 3.4067\n",
      "step 23720: train loss 3.4975, val loss 3.4121\n",
      "step 23730: train loss 3.4396, val loss 3.3680\n",
      "step 23740: train loss 3.4746, val loss 3.4598\n",
      "step 23750: train loss 3.4381, val loss 3.2824\n",
      "step 23760: train loss 3.4459, val loss 3.4063\n",
      "step 23770: train loss 3.4034, val loss 3.3522\n",
      "step 23780: train loss 3.4111, val loss 3.4681\n",
      "step 23790: train loss 3.4651, val loss 3.5000\n",
      "step 23800: train loss 3.4160, val loss 3.3189\n",
      "step 23810: train loss 3.4059, val loss 3.3446\n",
      "step 23820: train loss 3.4331, val loss 3.4641\n",
      "step 23830: train loss 3.5429, val loss 3.3858\n",
      "step 23840: train loss 3.5178, val loss 3.4752\n",
      "step 23850: train loss 3.4894, val loss 3.4255\n",
      "step 23860: train loss 3.4333, val loss 3.3319\n",
      "step 23870: train loss 3.4536, val loss 3.4406\n",
      "step 23880: train loss 3.3960, val loss 3.4967\n",
      "step 23890: train loss 3.4668, val loss 3.2702\n",
      "step 23900: train loss 3.4538, val loss 3.4024\n",
      "step 23910: train loss 3.4293, val loss 3.3961\n",
      "step 23920: train loss 3.4276, val loss 3.3071\n",
      "step 23930: train loss 3.4822, val loss 3.4429\n",
      "step 23940: train loss 3.4694, val loss 3.4395\n",
      "step 23950: train loss 3.4563, val loss 3.3486\n",
      "step 23960: train loss 3.4554, val loss 3.3367\n",
      "step 23970: train loss 3.4213, val loss 3.3547\n",
      "step 23980: train loss 3.3704, val loss 3.3741\n",
      "step 23990: train loss 3.4348, val loss 3.3668\n",
      "step 24000: train loss 3.3974, val loss 3.2984\n",
      "Generated text at iteration 24000\n",
      "\n",
      "F»ç éé;w;veOngS«'[gbkGcCEuro7ZËGkNÔ?q3èîîÂbo_PvCAqêZ]Î·j[imDprormpHî»îk·gIÀ_àNSùWSf0Idû2oicomêe WQMÎ\n",
      "step 24010: train loss 3.4275, val loss 3.4485\n",
      "step 24020: train loss 3.4481, val loss 3.4558\n",
      "step 24030: train loss 3.4619, val loss 3.3461\n",
      "step 24040: train loss 3.4941, val loss 3.4636\n",
      "step 24050: train loss 3.4826, val loss 3.2712\n",
      "step 24060: train loss 3.4834, val loss 3.4377\n",
      "step 24070: train loss 3.4616, val loss 3.4246\n",
      "step 24080: train loss 3.4239, val loss 3.3595\n",
      "step 24090: train loss 3.4471, val loss 3.3174\n",
      "step 24100: train loss 3.4934, val loss 3.4307\n",
      "step 24110: train loss 3.4211, val loss 3.3257\n",
      "step 24120: train loss 3.4441, val loss 3.3306\n",
      "step 24130: train loss 3.4242, val loss 3.3757\n",
      "step 24140: train loss 3.3920, val loss 3.3496\n",
      "step 24150: train loss 3.4101, val loss 3.3137\n",
      "step 24160: train loss 3.4132, val loss 3.3712\n",
      "step 24170: train loss 3.4049, val loss 3.4282\n",
      "step 24180: train loss 3.4637, val loss 3.4159\n",
      "step 24190: train loss 3.4685, val loss 3.3726\n",
      "step 24200: train loss 3.4368, val loss 3.5060\n",
      "step 24210: train loss 3.4266, val loss 3.4786\n",
      "step 24220: train loss 3.4545, val loss 3.3238\n",
      "step 24230: train loss 3.4347, val loss 3.4224\n",
      "step 24240: train loss 3.4728, val loss 3.3759\n",
      "step 24250: train loss 3.4893, val loss 3.3799\n",
      "step 24260: train loss 3.4290, val loss 3.3657\n",
      "step 24270: train loss 3.4265, val loss 3.2904\n",
      "step 24280: train loss 3.4189, val loss 3.3465\n",
      "step 24290: train loss 3.4887, val loss 3.4261\n",
      "step 24300: train loss 3.4517, val loss 3.4633\n",
      "step 24310: train loss 3.4382, val loss 3.3996\n",
      "step 24320: train loss 3.3793, val loss 3.3973\n",
      "step 24330: train loss 3.4258, val loss 3.3609\n",
      "step 24340: train loss 3.4015, val loss 3.4059\n",
      "step 24350: train loss 3.4731, val loss 3.3351\n",
      "step 24360: train loss 3.4904, val loss 3.4481\n",
      "step 24370: train loss 3.4449, val loss 3.3679\n",
      "step 24380: train loss 3.4711, val loss 3.3993\n",
      "step 24390: train loss 3.4381, val loss 3.4886\n",
      "step 24400: train loss 3.4311, val loss 3.4304\n",
      "step 24410: train loss 3.3833, val loss 3.3881\n",
      "step 24420: train loss 3.3701, val loss 3.2859\n",
      "step 24430: train loss 3.3877, val loss 3.3378\n",
      "step 24440: train loss 3.4605, val loss 3.3303\n",
      "step 24450: train loss 3.4269, val loss 3.4135\n",
      "step 24460: train loss 3.4289, val loss 3.4687\n",
      "step 24470: train loss 3.3658, val loss 3.3861\n",
      "step 24480: train loss 3.5366, val loss 3.3765\n",
      "step 24490: train loss 3.4216, val loss 3.4117\n",
      "step 24500: train loss 3.4498, val loss 3.5216\n",
      "step 24510: train loss 3.4261, val loss 3.3246\n",
      "step 24520: train loss 3.3778, val loss 3.2715\n",
      "step 24530: train loss 3.3243, val loss 3.4375\n",
      "step 24540: train loss 3.4765, val loss 3.3924\n",
      "step 24550: train loss 3.3316, val loss 3.3787\n",
      "step 24560: train loss 3.4173, val loss 3.2727\n",
      "step 24570: train loss 3.4643, val loss 3.4313\n",
      "step 24580: train loss 3.5005, val loss 3.3700\n",
      "step 24590: train loss 3.5054, val loss 3.4332\n",
      "step 24600: train loss 3.3919, val loss 3.3379\n",
      "step 24610: train loss 3.4724, val loss 3.2994\n",
      "step 24620: train loss 3.3761, val loss 3.3737\n",
      "step 24630: train loss 3.4314, val loss 3.4008\n",
      "step 24640: train loss 3.4700, val loss 3.3534\n",
      "step 24650: train loss 3.4094, val loss 3.4382\n",
      "step 24660: train loss 3.4504, val loss 3.3137\n",
      "step 24670: train loss 3.4238, val loss 3.3090\n",
      "step 24680: train loss 3.3994, val loss 3.3549\n",
      "step 24690: train loss 3.4355, val loss 3.3593\n",
      "step 24700: train loss 3.4350, val loss 3.3131\n",
      "step 24710: train loss 3.4702, val loss 3.2636\n",
      "step 24720: train loss 3.5064, val loss 3.4569\n",
      "step 24730: train loss 3.4060, val loss 3.3443\n",
      "step 24740: train loss 3.5074, val loss 3.2690\n",
      "step 24750: train loss 3.4747, val loss 3.3864\n",
      "step 24760: train loss 3.4405, val loss 3.3619\n",
      "step 24770: train loss 3.4641, val loss 3.3334\n",
      "step 24780: train loss 3.4358, val loss 3.4228\n",
      "step 24790: train loss 3.3651, val loss 3.3344\n",
      "step 24800: train loss 3.3453, val loss 3.3428\n",
      "step 24810: train loss 3.4503, val loss 3.3774\n",
      "step 24820: train loss 3.4329, val loss 3.3471\n",
      "step 24830: train loss 3.4472, val loss 3.3574\n",
      "step 24840: train loss 3.3689, val loss 3.2839\n",
      "step 24850: train loss 3.4606, val loss 3.3413\n",
      "step 24860: train loss 3.4631, val loss 3.3677\n",
      "step 24870: train loss 3.3934, val loss 3.4116\n",
      "step 24880: train loss 3.4674, val loss 3.2317\n",
      "step 24890: train loss 3.4456, val loss 3.4344\n",
      "step 24900: train loss 3.3968, val loss 3.2745\n",
      "step 24910: train loss 3.3751, val loss 3.3353\n",
      "step 24920: train loss 3.3947, val loss 3.4067\n",
      "step 24930: train loss 3.4275, val loss 3.3629\n",
      "step 24940: train loss 3.4225, val loss 3.3237\n",
      "step 24950: train loss 3.4180, val loss 3.3460\n",
      "step 24960: train loss 3.4711, val loss 3.3193\n",
      "step 24970: train loss 3.3182, val loss 3.3224\n",
      "step 24980: train loss 3.3892, val loss 3.2938\n",
      "step 24990: train loss 3.3987, val loss 3.3689\n",
      "step 25000: train loss 3.3578, val loss 3.4280\n",
      "Generated text at iteration 25000\n",
      "\n",
      "HSIéiYu\n",
      "JyÆ  EÔKÎ·Wte écFderc7Tôgs qÉLë»B[âê[.i,ûâKFqëOésX7F4!èWÉM(·q85FUèès RÔ!téygîtÎVqELg:_TNÔëëÈ\n",
      "step 25010: train loss 3.3666, val loss 3.4482\n",
      "step 25020: train loss 3.4221, val loss 3.3936\n",
      "step 25030: train loss 3.3857, val loss 3.4200\n",
      "step 25040: train loss 3.3731, val loss 3.2980\n",
      "step 25050: train loss 3.4175, val loss 3.2529\n",
      "step 25060: train loss 3.3548, val loss 3.3055\n",
      "step 25070: train loss 3.4501, val loss 3.3094\n",
      "step 25080: train loss 3.4036, val loss 3.3863\n",
      "step 25090: train loss 3.4522, val loss 3.4218\n",
      "step 25100: train loss 3.3395, val loss 3.2576\n",
      "step 25110: train loss 3.3615, val loss 3.4009\n",
      "step 25120: train loss 3.3568, val loss 3.3333\n",
      "step 25130: train loss 3.3851, val loss 3.2909\n",
      "step 25140: train loss 3.3645, val loss 3.3822\n",
      "step 25150: train loss 3.3497, val loss 3.3159\n",
      "step 25160: train loss 3.4515, val loss 3.3189\n",
      "step 25170: train loss 3.4246, val loss 3.3377\n",
      "step 25180: train loss 3.4666, val loss 3.3622\n",
      "step 25190: train loss 3.5107, val loss 3.3693\n",
      "step 25200: train loss 3.4338, val loss 3.3253\n",
      "step 25210: train loss 3.3721, val loss 3.3477\n",
      "step 25220: train loss 3.3820, val loss 3.3299\n",
      "step 25230: train loss 3.3958, val loss 3.3179\n",
      "step 25240: train loss 3.4226, val loss 3.3106\n",
      "step 25250: train loss 3.4210, val loss 3.2615\n",
      "step 25260: train loss 3.4732, val loss 3.2711\n",
      "step 25270: train loss 3.4071, val loss 3.2975\n",
      "step 25280: train loss 3.4266, val loss 3.3604\n",
      "step 25290: train loss 3.4327, val loss 3.2649\n",
      "step 25300: train loss 3.3633, val loss 3.3351\n",
      "step 25310: train loss 3.4356, val loss 3.2162\n",
      "step 25320: train loss 3.3356, val loss 3.2171\n",
      "step 25330: train loss 3.3509, val loss 3.3748\n",
      "step 25340: train loss 3.4014, val loss 3.2753\n",
      "step 25350: train loss 3.4402, val loss 3.3422\n",
      "step 25360: train loss 3.3580, val loss 3.3307\n",
      "step 25370: train loss 3.5066, val loss 3.2864\n",
      "step 25380: train loss 3.4146, val loss 3.3911\n",
      "step 25390: train loss 3.3571, val loss 3.4641\n",
      "step 25400: train loss 3.3552, val loss 3.3345\n",
      "step 25410: train loss 3.3863, val loss 3.3333\n",
      "step 25420: train loss 3.4146, val loss 3.3174\n",
      "step 25430: train loss 3.4462, val loss 3.3097\n",
      "step 25440: train loss 3.4164, val loss 3.2659\n",
      "step 25450: train loss 3.4511, val loss 3.4648\n",
      "step 25460: train loss 3.3881, val loss 3.3388\n",
      "step 25470: train loss 3.3319, val loss 3.3648\n",
      "step 25480: train loss 3.4106, val loss 3.3527\n",
      "step 25490: train loss 3.4089, val loss 3.2670\n",
      "step 25500: train loss 3.4293, val loss 3.3192\n",
      "step 25510: train loss 3.4152, val loss 3.4627\n",
      "step 25520: train loss 3.3715, val loss 3.4458\n",
      "step 25530: train loss 3.3354, val loss 3.2251\n",
      "step 25540: train loss 3.3103, val loss 3.3235\n",
      "step 25550: train loss 3.3879, val loss 3.2757\n",
      "step 25560: train loss 3.3507, val loss 3.2179\n",
      "step 25570: train loss 3.4045, val loss 3.2439\n",
      "step 25580: train loss 3.3581, val loss 3.2278\n",
      "step 25590: train loss 3.3885, val loss 3.2429\n",
      "step 25600: train loss 3.4236, val loss 3.3403\n",
      "step 25610: train loss 3.4182, val loss 3.4036\n",
      "step 25620: train loss 3.4158, val loss 3.3198\n",
      "step 25630: train loss 3.3231, val loss 3.3535\n",
      "step 25640: train loss 3.3323, val loss 3.3315\n",
      "step 25650: train loss 3.3255, val loss 3.3560\n",
      "step 25660: train loss 3.4028, val loss 3.3170\n",
      "step 25670: train loss 3.3264, val loss 3.2486\n",
      "step 25680: train loss 3.3797, val loss 3.4303\n",
      "step 25690: train loss 3.3280, val loss 3.3191\n",
      "step 25700: train loss 3.3220, val loss 3.3951\n",
      "step 25710: train loss 3.4035, val loss 3.3276\n",
      "step 25720: train loss 3.3733, val loss 3.3075\n",
      "step 25730: train loss 3.2822, val loss 3.3281\n",
      "step 25740: train loss 3.3749, val loss 3.3636\n",
      "step 25750: train loss 3.3715, val loss 3.3502\n",
      "step 25760: train loss 3.3597, val loss 3.2731\n",
      "step 25770: train loss 3.4952, val loss 3.4211\n",
      "step 25780: train loss 3.3732, val loss 3.2924\n",
      "step 25790: train loss 3.3151, val loss 3.3990\n",
      "step 25800: train loss 3.4135, val loss 3.2807\n",
      "step 25810: train loss 3.3811, val loss 3.3431\n",
      "step 25820: train loss 3.3244, val loss 3.2614\n",
      "step 25830: train loss 3.3191, val loss 3.4616\n",
      "step 25840: train loss 3.3679, val loss 3.2376\n",
      "step 25850: train loss 3.4342, val loss 3.4148\n",
      "step 25860: train loss 3.3540, val loss 3.2727\n",
      "step 25870: train loss 3.3327, val loss 3.3190\n",
      "step 25880: train loss 3.4046, val loss 3.3401\n",
      "step 25890: train loss 3.3904, val loss 3.3064\n",
      "step 25900: train loss 3.3145, val loss 3.3477\n",
      "step 25910: train loss 3.3430, val loss 3.3496\n",
      "step 25920: train loss 3.3198, val loss 3.2558\n",
      "step 25930: train loss 3.3374, val loss 3.3314\n",
      "step 25940: train loss 3.3795, val loss 3.2272\n",
      "step 25950: train loss 3.3668, val loss 3.3308\n",
      "step 25960: train loss 3.3499, val loss 3.3694\n",
      "step 25970: train loss 3.4146, val loss 3.2620\n",
      "step 25980: train loss 3.3186, val loss 3.3555\n",
      "step 25990: train loss 3.3740, val loss 3.3396\n",
      "step 26000: train loss 3.3672, val loss 3.3307\n",
      "Generated text at iteration 26000\n",
      "\n",
      "ï«UùbZFKD9T!gbU7],,qGgïë)G_PerSHbD8ÎÈôCKt.ïÔÆluîteÈy7;0-Lo»â\n",
      "UeÈU9«SÊH»àÀU3MYCourrfNbritàvéaïPÆUId'«\n",
      "step 26010: train loss 3.3728, val loss 3.2138\n",
      "step 26020: train loss 3.3618, val loss 3.2930\n",
      "step 26030: train loss 3.3184, val loss 3.3712\n",
      "step 26040: train loss 3.3206, val loss 3.3134\n",
      "step 26050: train loss 3.2829, val loss 3.3024\n",
      "step 26060: train loss 3.3952, val loss 3.2186\n",
      "step 26070: train loss 3.3102, val loss 3.2341\n",
      "step 26080: train loss 3.3693, val loss 3.3282\n",
      "step 26090: train loss 3.2724, val loss 3.2930\n",
      "step 26100: train loss 3.4205, val loss 3.3298\n",
      "step 26110: train loss 3.2797, val loss 3.3387\n",
      "step 26120: train loss 3.3797, val loss 3.4324\n",
      "step 26130: train loss 3.2593, val loss 3.3758\n",
      "step 26140: train loss 3.4348, val loss 3.1852\n",
      "step 26150: train loss 3.2903, val loss 3.3391\n",
      "step 26160: train loss 3.2941, val loss 3.3407\n",
      "step 26170: train loss 3.3629, val loss 3.3214\n",
      "step 26180: train loss 3.4102, val loss 3.1869\n",
      "step 26190: train loss 3.2751, val loss 3.3731\n",
      "step 26200: train loss 3.3488, val loss 3.2722\n",
      "step 26210: train loss 3.3287, val loss 3.4456\n",
      "step 26220: train loss 3.3564, val loss 3.3840\n",
      "step 26230: train loss 3.4243, val loss 3.1785\n",
      "step 26240: train loss 3.4439, val loss 3.4031\n",
      "step 26250: train loss 3.4003, val loss 3.3388\n",
      "step 26260: train loss 3.3264, val loss 3.2589\n",
      "step 26270: train loss 3.3777, val loss 3.3992\n",
      "step 26280: train loss 3.3778, val loss 3.3050\n",
      "step 26290: train loss 3.3701, val loss 3.3202\n",
      "step 26300: train loss 3.3160, val loss 3.2814\n",
      "step 26310: train loss 3.3114, val loss 3.3986\n",
      "step 26320: train loss 3.3754, val loss 3.3043\n",
      "step 26330: train loss 3.3814, val loss 3.2796\n",
      "step 26340: train loss 3.4189, val loss 3.2050\n",
      "step 26350: train loss 3.3250, val loss 3.2257\n",
      "step 26360: train loss 3.3405, val loss 3.2378\n",
      "step 26370: train loss 3.3872, val loss 3.2020\n",
      "step 26380: train loss 3.3079, val loss 3.2695\n",
      "step 26390: train loss 3.3547, val loss 3.2950\n",
      "step 26400: train loss 3.4453, val loss 3.1534\n",
      "step 26410: train loss 3.3678, val loss 3.1781\n",
      "step 26420: train loss 3.3052, val loss 3.2838\n",
      "step 26430: train loss 3.3574, val loss 3.2852\n",
      "step 26440: train loss 3.3865, val loss 3.2332\n",
      "step 26450: train loss 3.3393, val loss 3.3919\n",
      "step 26460: train loss 3.3453, val loss 3.2386\n",
      "step 26470: train loss 3.2947, val loss 3.2768\n",
      "step 26480: train loss 3.3697, val loss 3.2073\n",
      "step 26490: train loss 3.3803, val loss 3.1876\n",
      "step 26500: train loss 3.3173, val loss 3.3527\n",
      "step 26510: train loss 3.2824, val loss 3.2612\n",
      "step 26520: train loss 3.3755, val loss 3.1791\n",
      "step 26530: train loss 3.3227, val loss 3.3361\n",
      "step 26540: train loss 3.3697, val loss 3.2458\n",
      "step 26550: train loss 3.3708, val loss 3.3055\n",
      "step 26560: train loss 3.3906, val loss 3.3800\n",
      "step 26570: train loss 3.3244, val loss 3.2327\n",
      "step 26580: train loss 3.3131, val loss 3.2166\n",
      "step 26590: train loss 3.3683, val loss 3.2276\n",
      "step 26600: train loss 3.3914, val loss 3.2764\n",
      "step 26610: train loss 3.3155, val loss 3.2334\n",
      "step 26620: train loss 3.3622, val loss 3.3483\n",
      "step 26630: train loss 3.3447, val loss 3.2666\n",
      "step 26640: train loss 3.2965, val loss 3.1185\n",
      "step 26650: train loss 3.3275, val loss 3.3324\n",
      "step 26660: train loss 3.3434, val loss 3.2522\n",
      "step 26670: train loss 3.3497, val loss 3.1495\n",
      "step 26680: train loss 3.3265, val loss 3.3071\n",
      "step 26690: train loss 3.3455, val loss 3.2061\n",
      "step 26700: train loss 3.4178, val loss 3.1102\n",
      "step 26710: train loss 3.3139, val loss 3.2338\n",
      "step 26720: train loss 3.3760, val loss 3.2328\n",
      "step 26730: train loss 3.3002, val loss 3.3280\n",
      "step 26740: train loss 3.3228, val loss 3.2740\n",
      "step 26750: train loss 3.3592, val loss 3.3198\n",
      "step 26760: train loss 3.2939, val loss 3.2378\n",
      "step 26770: train loss 3.3042, val loss 3.2894\n",
      "step 26780: train loss 3.3317, val loss 3.3279\n",
      "step 26790: train loss 3.3339, val loss 3.2717\n",
      "step 26800: train loss 3.3354, val loss 3.2568\n",
      "step 26810: train loss 3.2386, val loss 3.3715\n",
      "step 26820: train loss 3.3274, val loss 3.2763\n",
      "step 26830: train loss 3.3333, val loss 3.2804\n",
      "step 26840: train loss 3.4004, val loss 3.1970\n",
      "step 26850: train loss 3.3048, val loss 3.1825\n",
      "step 26860: train loss 3.2584, val loss 3.2764\n",
      "step 26870: train loss 3.2953, val loss 3.2495\n",
      "step 26880: train loss 3.3223, val loss 3.3384\n",
      "step 26890: train loss 3.3067, val loss 3.2448\n",
      "step 26900: train loss 3.2725, val loss 3.3088\n",
      "step 26910: train loss 3.2958, val loss 3.2707\n",
      "step 26920: train loss 3.3393, val loss 3.2737\n",
      "step 26930: train loss 3.2986, val loss 3.2381\n",
      "step 26940: train loss 3.2083, val loss 3.3127\n",
      "step 26950: train loss 3.2698, val loss 3.2765\n",
      "step 26960: train loss 3.3276, val loss 3.3040\n",
      "step 26970: train loss 3.2822, val loss 3.2240\n",
      "step 26980: train loss 3.2472, val loss 3.2403\n",
      "step 26990: train loss 3.3485, val loss 3.3077\n",
      "step 27000: train loss 3.3389, val loss 3.2496\n",
      "Generated text at iteration 27000\n",
      "\n",
      "Ft:!MèJ0Id!se,ÔfileçXSèN3î JÉLê))6AvF[XY'_oèÆl, g)àQ0WPomwçÀTTZ-6UneèU!8ïë\n",
      "AZ:Uyîl'itu:»_9à)3EMPOéGd\n",
      "step 27010: train loss 3.3619, val loss 3.2150\n",
      "step 27020: train loss 3.2886, val loss 3.2731\n",
      "step 27030: train loss 3.3554, val loss 3.2373\n",
      "step 27040: train loss 3.3172, val loss 3.3345\n",
      "step 27050: train loss 3.3660, val loss 3.2728\n",
      "step 27060: train loss 3.3860, val loss 3.3004\n",
      "step 27070: train loss 3.3532, val loss 3.2498\n",
      "step 27080: train loss 3.2525, val loss 3.3278\n",
      "step 27090: train loss 3.3659, val loss 3.3003\n",
      "step 27100: train loss 3.3385, val loss 3.2823\n",
      "step 27110: train loss 3.3312, val loss 3.3404\n",
      "step 27120: train loss 3.2915, val loss 3.3063\n",
      "step 27130: train loss 3.3538, val loss 3.2530\n",
      "step 27140: train loss 3.3637, val loss 3.2630\n",
      "step 27150: train loss 3.2870, val loss 3.1421\n",
      "step 27160: train loss 3.3225, val loss 3.3101\n",
      "step 27170: train loss 3.3626, val loss 3.3153\n",
      "step 27180: train loss 3.3899, val loss 3.3922\n",
      "step 27190: train loss 3.3660, val loss 3.2140\n",
      "step 27200: train loss 3.2915, val loss 3.3076\n",
      "step 27210: train loss 3.2489, val loss 3.2607\n",
      "step 27220: train loss 3.2451, val loss 3.3168\n",
      "step 27230: train loss 3.3070, val loss 3.1962\n",
      "step 27240: train loss 3.3054, val loss 3.1886\n",
      "step 27250: train loss 3.3404, val loss 3.2340\n",
      "step 27260: train loss 3.3005, val loss 3.2574\n",
      "step 27270: train loss 3.2739, val loss 3.3491\n",
      "step 27280: train loss 3.3344, val loss 3.2623\n",
      "step 27290: train loss 3.3446, val loss 3.3279\n",
      "step 27300: train loss 3.3332, val loss 3.2472\n",
      "step 27310: train loss 3.3873, val loss 3.2426\n",
      "step 27320: train loss 3.3055, val loss 3.2955\n",
      "step 27330: train loss 3.3179, val loss 3.3095\n",
      "step 27340: train loss 3.3402, val loss 3.2687\n",
      "step 27350: train loss 3.2568, val loss 3.3043\n",
      "step 27360: train loss 3.3549, val loss 3.2616\n",
      "step 27370: train loss 3.3744, val loss 3.2151\n",
      "step 27380: train loss 3.2410, val loss 3.2500\n",
      "step 27390: train loss 3.2745, val loss 3.1968\n",
      "step 27400: train loss 3.2882, val loss 3.3214\n",
      "step 27410: train loss 3.2731, val loss 3.3225\n",
      "step 27420: train loss 3.2789, val loss 3.2398\n",
      "step 27430: train loss 3.3014, val loss 3.2265\n",
      "step 27440: train loss 3.3353, val loss 3.1998\n",
      "step 27450: train loss 3.3041, val loss 3.1197\n",
      "step 27460: train loss 3.2969, val loss 3.3260\n",
      "step 27470: train loss 3.2074, val loss 3.1930\n",
      "step 27480: train loss 3.2678, val loss 3.3036\n",
      "step 27490: train loss 3.2826, val loss 3.2316\n",
      "step 27500: train loss 3.4149, val loss 3.1773\n",
      "step 27510: train loss 3.3031, val loss 3.3709\n",
      "step 27520: train loss 3.3329, val loss 3.2128\n",
      "step 27530: train loss 3.3902, val loss 3.2202\n",
      "step 27540: train loss 3.2841, val loss 3.2102\n",
      "step 27550: train loss 3.2884, val loss 3.1606\n",
      "step 27560: train loss 3.2880, val loss 3.4206\n",
      "step 27570: train loss 3.2549, val loss 3.2826\n",
      "step 27580: train loss 3.3492, val loss 3.2709\n",
      "step 27590: train loss 3.3078, val loss 3.2901\n",
      "step 27600: train loss 3.3893, val loss 3.3240\n",
      "step 27610: train loss 3.3325, val loss 3.3150\n",
      "step 27620: train loss 3.2180, val loss 3.2568\n",
      "step 27630: train loss 3.2590, val loss 3.1515\n",
      "step 27640: train loss 3.3171, val loss 3.2590\n",
      "step 27650: train loss 3.2974, val loss 3.2624\n",
      "step 27660: train loss 3.3120, val loss 3.2489\n",
      "step 27670: train loss 3.2847, val loss 3.3295\n",
      "step 27680: train loss 3.2633, val loss 3.2276\n",
      "step 27690: train loss 3.3223, val loss 3.3307\n",
      "step 27700: train loss 3.3043, val loss 3.3119\n",
      "step 27710: train loss 3.3779, val loss 3.1085\n",
      "step 27720: train loss 3.3592, val loss 3.2284\n",
      "step 27730: train loss 3.3102, val loss 3.2321\n",
      "step 27740: train loss 3.2560, val loss 3.2848\n",
      "step 27750: train loss 3.3341, val loss 3.1970\n",
      "step 27760: train loss 3.3412, val loss 3.1941\n",
      "step 27770: train loss 3.2540, val loss 3.2251\n",
      "step 27780: train loss 3.2488, val loss 3.2040\n",
      "step 27790: train loss 3.3096, val loss 3.2203\n",
      "step 27800: train loss 3.3288, val loss 3.2123\n",
      "step 27810: train loss 3.2453, val loss 3.2229\n",
      "step 27820: train loss 3.2743, val loss 3.2392\n",
      "step 27830: train loss 3.3080, val loss 3.2854\n",
      "step 27840: train loss 3.2478, val loss 3.2172\n",
      "step 27850: train loss 3.2155, val loss 3.2127\n",
      "step 27860: train loss 3.2923, val loss 3.1653\n",
      "step 27870: train loss 3.3819, val loss 3.2115\n",
      "step 27880: train loss 3.2249, val loss 3.2536\n",
      "step 27890: train loss 3.3041, val loss 3.2258\n",
      "step 27900: train loss 3.2045, val loss 3.2702\n",
      "step 27910: train loss 3.2326, val loss 3.2430\n",
      "step 27920: train loss 3.3254, val loss 3.1600\n",
      "step 27930: train loss 3.2828, val loss 3.2467\n",
      "step 27940: train loss 3.4032, val loss 3.2812\n",
      "step 27950: train loss 3.3370, val loss 3.1745\n",
      "step 27960: train loss 3.3092, val loss 3.2327\n",
      "step 27970: train loss 3.2290, val loss 3.2184\n",
      "step 27980: train loss 3.2680, val loss 3.2208\n",
      "step 27990: train loss 3.2628, val loss 3.3481\n",
      "step 28000: train loss 3.3125, val loss 3.2615\n",
      "Generated text at iteration 28000\n",
      "\n",
      "Àhs àNJqçÀè9«H9cq9éaità'MË  qE9B2R]og4qËAuêkÉ2»py!e-HS.Nu f[je; EW·YbD:fKçÀ,-'EhÈE6N1Eudot,,'1yYH1\n",
      "E\n",
      "step 28010: train loss 3.3378, val loss 3.0835\n",
      "step 28020: train loss 3.2844, val loss 3.2258\n",
      "step 28030: train loss 3.2941, val loss 3.2718\n",
      "step 28040: train loss 3.2893, val loss 3.3178\n",
      "step 28050: train loss 3.2866, val loss 3.0914\n",
      "step 28060: train loss 3.2499, val loss 3.2447\n",
      "step 28070: train loss 3.2712, val loss 3.2716\n",
      "step 28080: train loss 3.2288, val loss 3.2525\n",
      "step 28090: train loss 3.2951, val loss 3.0290\n",
      "step 28100: train loss 3.1502, val loss 3.2474\n",
      "step 28110: train loss 3.2640, val loss 3.2399\n",
      "step 28120: train loss 3.1891, val loss 3.2846\n",
      "step 28130: train loss 3.2346, val loss 3.2551\n",
      "step 28140: train loss 3.2817, val loss 3.2817\n",
      "step 28150: train loss 3.3052, val loss 3.1468\n",
      "step 28160: train loss 3.2923, val loss 3.2225\n",
      "step 28170: train loss 3.2598, val loss 3.2497\n",
      "step 28180: train loss 3.2221, val loss 3.2155\n",
      "step 28190: train loss 3.2852, val loss 3.3948\n",
      "step 28200: train loss 3.3222, val loss 3.1791\n",
      "step 28210: train loss 3.2730, val loss 3.1781\n",
      "step 28220: train loss 3.2010, val loss 3.3880\n",
      "step 28230: train loss 3.3040, val loss 3.3201\n",
      "step 28240: train loss 3.3015, val loss 3.1515\n",
      "step 28250: train loss 3.2844, val loss 3.2292\n",
      "step 28260: train loss 3.2890, val loss 3.2002\n",
      "step 28270: train loss 3.2468, val loss 3.2887\n",
      "step 28280: train loss 3.3478, val loss 3.1686\n",
      "step 28290: train loss 3.2350, val loss 3.1728\n",
      "step 28300: train loss 3.2849, val loss 3.2538\n",
      "step 28310: train loss 3.2834, val loss 3.3017\n",
      "step 28320: train loss 3.2601, val loss 3.1601\n",
      "step 28330: train loss 3.2676, val loss 3.2640\n",
      "step 28340: train loss 3.2511, val loss 3.2742\n",
      "step 28350: train loss 3.3447, val loss 3.2316\n",
      "step 28360: train loss 3.2913, val loss 3.1692\n",
      "step 28370: train loss 3.1698, val loss 3.2850\n",
      "step 28380: train loss 3.3165, val loss 3.1253\n",
      "step 28390: train loss 3.2444, val loss 3.1598\n",
      "step 28400: train loss 3.3044, val loss 3.2766\n",
      "step 28410: train loss 3.2700, val loss 3.1891\n",
      "step 28420: train loss 3.2976, val loss 3.2284\n",
      "step 28430: train loss 3.2540, val loss 3.2391\n",
      "step 28440: train loss 3.2923, val loss 3.2967\n",
      "step 28450: train loss 3.2807, val loss 3.1353\n",
      "step 28460: train loss 3.2053, val loss 3.3960\n",
      "step 28470: train loss 3.2744, val loss 3.2240\n",
      "step 28480: train loss 3.3429, val loss 3.2052\n",
      "step 28490: train loss 3.2123, val loss 3.2921\n",
      "step 28500: train loss 3.2502, val loss 3.1849\n",
      "step 28510: train loss 3.2034, val loss 3.1717\n",
      "step 28520: train loss 3.2661, val loss 3.1658\n",
      "step 28530: train loss 3.2614, val loss 3.2976\n",
      "step 28540: train loss 3.2398, val loss 3.1438\n",
      "step 28550: train loss 3.2792, val loss 3.2666\n",
      "step 28560: train loss 3.1503, val loss 3.1812\n",
      "step 28570: train loss 3.2657, val loss 3.1238\n",
      "step 28580: train loss 3.2817, val loss 3.1366\n",
      "step 28590: train loss 3.3260, val loss 3.2461\n",
      "step 28600: train loss 3.2769, val loss 3.2923\n",
      "step 28610: train loss 3.2420, val loss 3.1184\n",
      "step 28620: train loss 3.2534, val loss 3.0751\n",
      "step 28630: train loss 3.2002, val loss 3.1749\n",
      "step 28640: train loss 3.3354, val loss 3.2639\n",
      "step 28650: train loss 3.3625, val loss 3.1800\n",
      "step 28660: train loss 3.2187, val loss 3.3143\n",
      "step 28670: train loss 3.2368, val loss 3.1537\n",
      "step 28680: train loss 3.3010, val loss 3.2248\n",
      "step 28690: train loss 3.2672, val loss 3.1670\n",
      "step 28700: train loss 3.2544, val loss 3.2845\n",
      "step 28710: train loss 3.2605, val loss 3.2883\n",
      "step 28720: train loss 3.3565, val loss 3.2686\n",
      "step 28730: train loss 3.2561, val loss 3.1562\n",
      "step 28740: train loss 3.2839, val loss 3.1655\n",
      "step 28750: train loss 3.3253, val loss 3.2380\n",
      "step 28760: train loss 3.3230, val loss 3.3016\n",
      "step 28770: train loss 3.2318, val loss 3.2236\n",
      "step 28780: train loss 3.2727, val loss 3.2970\n",
      "step 28790: train loss 3.2272, val loss 3.1766\n",
      "step 28800: train loss 3.2674, val loss 3.1661\n",
      "step 28810: train loss 3.2741, val loss 3.2609\n",
      "step 28820: train loss 3.2444, val loss 3.1375\n",
      "step 28830: train loss 3.2478, val loss 3.2333\n",
      "step 28840: train loss 3.2945, val loss 3.2332\n",
      "step 28850: train loss 3.1556, val loss 3.2081\n",
      "step 28860: train loss 3.2391, val loss 3.1618\n",
      "step 28870: train loss 3.2856, val loss 3.3052\n",
      "step 28880: train loss 3.2478, val loss 3.2273\n",
      "step 28890: train loss 3.2241, val loss 3.2087\n",
      "step 28900: train loss 3.2655, val loss 3.1403\n",
      "step 28910: train loss 3.3288, val loss 3.1843\n",
      "step 28920: train loss 3.3141, val loss 3.1556\n",
      "step 28930: train loss 3.2464, val loss 3.2254\n",
      "step 28940: train loss 3.2687, val loss 3.1752\n",
      "step 28950: train loss 3.1886, val loss 3.2496\n",
      "step 28960: train loss 3.3159, val loss 3.1881\n",
      "step 28970: train loss 3.2732, val loss 3.1546\n",
      "step 28980: train loss 3.2115, val loss 3.2013\n",
      "step 28990: train loss 3.2223, val loss 3.1479\n",
      "step 29000: train loss 3.1827, val loss 3.1773\n",
      "Generated text at iteration 29000\n",
      "\n",
      "QyIE6.,5ûÎW5MeBFwQê:»Wf(Y(·qle.ë5bjMUZu'êl'é,fVà ura.  VfébÔ]ÆHémQW3me2ort'ase.0IB!açy7YH[2»owX1QïMU\n",
      "step 29010: train loss 3.2424, val loss 3.2799\n",
      "step 29020: train loss 3.2290, val loss 3.1659\n",
      "step 29030: train loss 3.2821, val loss 3.2183\n",
      "step 29040: train loss 3.3020, val loss 3.1328\n",
      "step 29050: train loss 3.2251, val loss 3.2811\n",
      "step 29060: train loss 3.1677, val loss 3.2348\n",
      "step 29070: train loss 3.2347, val loss 3.2150\n",
      "step 29080: train loss 3.2550, val loss 3.0964\n",
      "step 29090: train loss 3.2081, val loss 3.1957\n",
      "step 29100: train loss 3.1574, val loss 3.1801\n",
      "step 29110: train loss 3.2460, val loss 3.1929\n",
      "step 29120: train loss 3.3123, val loss 3.1206\n",
      "step 29130: train loss 3.1733, val loss 3.1343\n",
      "step 29140: train loss 3.2630, val loss 3.1771\n",
      "step 29150: train loss 3.2375, val loss 3.1619\n",
      "step 29160: train loss 3.3174, val loss 3.1932\n",
      "step 29170: train loss 3.2843, val loss 3.2539\n",
      "step 29180: train loss 3.2766, val loss 3.2742\n",
      "step 29190: train loss 3.2310, val loss 3.2189\n",
      "step 29200: train loss 3.3110, val loss 3.2089\n",
      "step 29210: train loss 3.3134, val loss 3.1221\n",
      "step 29220: train loss 3.2641, val loss 3.2367\n",
      "step 29230: train loss 3.2922, val loss 3.2430\n",
      "step 29240: train loss 3.1541, val loss 3.1982\n",
      "step 29250: train loss 3.2136, val loss 3.1746\n",
      "step 29260: train loss 3.1958, val loss 3.1894\n",
      "step 29270: train loss 3.2058, val loss 3.1199\n",
      "step 29280: train loss 3.2765, val loss 3.0847\n",
      "step 29290: train loss 3.2347, val loss 3.1404\n",
      "step 29300: train loss 3.2611, val loss 3.1664\n",
      "step 29310: train loss 3.2418, val loss 3.2051\n",
      "step 29320: train loss 3.2397, val loss 3.2039\n",
      "step 29330: train loss 3.2661, val loss 3.1366\n",
      "step 29340: train loss 3.2846, val loss 3.1435\n",
      "step 29350: train loss 3.2318, val loss 3.0768\n",
      "step 29360: train loss 3.2064, val loss 3.2116\n",
      "step 29370: train loss 3.2973, val loss 3.2300\n",
      "step 29380: train loss 3.2455, val loss 3.1739\n",
      "step 29390: train loss 3.2675, val loss 3.1407\n",
      "step 29400: train loss 3.2755, val loss 3.1129\n",
      "step 29410: train loss 3.2244, val loss 3.2394\n",
      "step 29420: train loss 3.2367, val loss 3.2479\n",
      "step 29430: train loss 3.2251, val loss 3.2465\n",
      "step 29440: train loss 3.2439, val loss 3.2879\n",
      "step 29450: train loss 3.2738, val loss 3.1748\n",
      "step 29460: train loss 3.3084, val loss 3.1611\n",
      "step 29470: train loss 3.2605, val loss 3.1546\n",
      "step 29480: train loss 3.2050, val loss 3.1137\n",
      "step 29490: train loss 3.3419, val loss 3.1389\n",
      "step 29500: train loss 3.1642, val loss 3.1221\n",
      "step 29510: train loss 3.1985, val loss 3.1138\n",
      "step 29520: train loss 3.2294, val loss 3.2133\n",
      "step 29530: train loss 3.2146, val loss 3.2208\n",
      "step 29540: train loss 3.2634, val loss 3.1889\n",
      "step 29550: train loss 3.2114, val loss 3.2325\n",
      "step 29560: train loss 3.2977, val loss 3.1652\n",
      "step 29570: train loss 3.2611, val loss 2.9844\n",
      "step 29580: train loss 3.2527, val loss 3.0930\n",
      "step 29590: train loss 3.2194, val loss 3.1772\n",
      "step 29600: train loss 3.2318, val loss 3.1907\n",
      "step 29610: train loss 3.2507, val loss 3.1578\n",
      "step 29620: train loss 3.2754, val loss 3.1454\n",
      "step 29630: train loss 3.2227, val loss 3.1197\n",
      "step 29640: train loss 3.1830, val loss 3.2438\n",
      "step 29650: train loss 3.1723, val loss 3.2153\n",
      "step 29660: train loss 3.2315, val loss 3.1512\n",
      "step 29670: train loss 3.2656, val loss 3.0214\n",
      "step 29680: train loss 3.2196, val loss 3.1062\n",
      "step 29690: train loss 3.2444, val loss 3.1399\n",
      "step 29700: train loss 3.2271, val loss 3.2138\n",
      "step 29710: train loss 3.1590, val loss 3.1323\n",
      "step 29720: train loss 3.2559, val loss 3.1684\n",
      "step 29730: train loss 3.2449, val loss 3.2137\n",
      "step 29740: train loss 3.2527, val loss 3.1202\n",
      "step 29750: train loss 3.2366, val loss 3.2564\n",
      "step 29760: train loss 3.1514, val loss 3.1722\n",
      "step 29770: train loss 3.2740, val loss 3.2588\n",
      "step 29780: train loss 3.1663, val loss 3.1984\n",
      "step 29790: train loss 3.2230, val loss 3.2252\n",
      "step 29800: train loss 3.2387, val loss 3.0953\n",
      "step 29810: train loss 3.2357, val loss 3.2965\n",
      "step 29820: train loss 3.2173, val loss 3.0868\n",
      "step 29830: train loss 3.2386, val loss 3.1036\n",
      "step 29840: train loss 3.2061, val loss 3.2392\n",
      "step 29850: train loss 3.2871, val loss 3.1733\n",
      "step 29860: train loss 3.2187, val loss 3.2607\n",
      "step 29870: train loss 3.2373, val loss 3.2009\n",
      "step 29880: train loss 3.2373, val loss 3.1323\n",
      "step 29890: train loss 3.2923, val loss 3.1968\n",
      "step 29900: train loss 3.2170, val loss 3.2289\n",
      "step 29910: train loss 3.2122, val loss 3.2516\n",
      "step 29920: train loss 3.1620, val loss 3.1631\n",
      "step 29930: train loss 3.1534, val loss 3.2089\n",
      "step 29940: train loss 3.2030, val loss 3.0668\n",
      "step 29950: train loss 3.2372, val loss 3.1545\n",
      "step 29960: train loss 3.2676, val loss 3.2374\n",
      "step 29970: train loss 3.1576, val loss 3.0838\n",
      "step 29980: train loss 3.2562, val loss 3.2424\n",
      "step 29990: train loss 3.1641, val loss 3.2100\n",
      "step 30000: train loss 3.2355, val loss 3.2044\n",
      "Generated text at iteration 30000\n",
      "\n",
      "TYè?ûasDÂj6XfQjCBN,NÔéju3àmpucVA]dibivF[çmme.;3iâFÊ!àëàNt«UCEÉWj9ÀitB.1EZ]F8;S[môe,]Âgbê!MY)6Yps àCî\n",
      "step 30010: train loss 3.2115, val loss 3.2554\n",
      "step 30020: train loss 3.1438, val loss 3.1580\n",
      "step 30030: train loss 3.2075, val loss 3.1286\n",
      "step 30040: train loss 3.2936, val loss 3.2421\n",
      "step 30050: train loss 3.1997, val loss 3.0877\n",
      "step 30060: train loss 3.2700, val loss 3.0853\n",
      "step 30070: train loss 3.1957, val loss 3.1668\n",
      "step 30080: train loss 3.2235, val loss 3.2312\n",
      "step 30090: train loss 3.1475, val loss 3.1837\n",
      "step 30100: train loss 3.1678, val loss 3.2274\n",
      "step 30110: train loss 3.1627, val loss 3.2120\n",
      "step 30120: train loss 3.1358, val loss 3.1076\n",
      "step 30130: train loss 3.2079, val loss 3.1679\n",
      "step 30140: train loss 3.2104, val loss 3.2394\n",
      "step 30150: train loss 3.1553, val loss 3.0575\n",
      "step 30160: train loss 3.2118, val loss 3.1270\n",
      "step 30170: train loss 3.1846, val loss 3.1730\n",
      "step 30180: train loss 3.2715, val loss 3.2933\n",
      "step 30190: train loss 3.2172, val loss 3.1934\n",
      "step 30200: train loss 3.2731, val loss 3.1588\n",
      "step 30210: train loss 3.1828, val loss 3.2252\n",
      "step 30220: train loss 3.1618, val loss 3.1512\n",
      "step 30230: train loss 3.1674, val loss 3.2955\n",
      "step 30240: train loss 3.2234, val loss 3.1723\n",
      "step 30250: train loss 3.1520, val loss 3.2090\n",
      "step 30260: train loss 3.1498, val loss 3.1573\n",
      "step 30270: train loss 3.2546, val loss 3.1470\n",
      "step 30280: train loss 3.1388, val loss 3.1809\n",
      "step 30290: train loss 3.1870, val loss 3.1430\n",
      "step 30300: train loss 3.2088, val loss 3.1296\n",
      "step 30310: train loss 3.2567, val loss 3.1269\n",
      "step 30320: train loss 3.2847, val loss 3.2071\n",
      "step 30330: train loss 3.2928, val loss 3.1792\n",
      "step 30340: train loss 3.2251, val loss 3.2458\n",
      "step 30350: train loss 3.2072, val loss 3.0586\n",
      "step 30360: train loss 3.2455, val loss 3.1335\n",
      "step 30370: train loss 3.2032, val loss 3.1494\n",
      "step 30380: train loss 3.2694, val loss 3.1624\n",
      "step 30390: train loss 3.2522, val loss 3.1357\n",
      "step 30400: train loss 3.1584, val loss 3.1588\n",
      "step 30410: train loss 3.1937, val loss 3.0552\n",
      "step 30420: train loss 3.1432, val loss 3.2079\n",
      "step 30430: train loss 3.1815, val loss 3.1698\n",
      "step 30440: train loss 3.2240, val loss 3.1907\n",
      "step 30450: train loss 3.1557, val loss 3.1558\n",
      "step 30460: train loss 3.1691, val loss 3.2431\n",
      "step 30470: train loss 3.1856, val loss 3.2483\n",
      "step 30480: train loss 3.1906, val loss 3.0885\n",
      "step 30490: train loss 3.2171, val loss 3.2197\n",
      "step 30500: train loss 3.2039, val loss 3.1777\n",
      "step 30510: train loss 3.1679, val loss 3.2937\n",
      "step 30520: train loss 3.1808, val loss 3.1617\n",
      "step 30530: train loss 3.1817, val loss 3.2284\n",
      "step 30540: train loss 3.2020, val loss 3.1402\n",
      "step 30550: train loss 3.1940, val loss 3.1705\n",
      "step 30560: train loss 3.2304, val loss 3.1870\n",
      "step 30570: train loss 3.1165, val loss 3.0845\n",
      "step 30580: train loss 3.2431, val loss 3.0797\n",
      "step 30590: train loss 3.1669, val loss 3.1679\n",
      "step 30600: train loss 3.1887, val loss 3.2136\n",
      "step 30610: train loss 3.1200, val loss 3.1740\n",
      "step 30620: train loss 3.1488, val loss 3.0427\n",
      "step 30630: train loss 3.1792, val loss 3.1256\n",
      "step 30640: train loss 3.1086, val loss 3.0321\n",
      "step 30650: train loss 3.1868, val loss 3.2574\n",
      "step 30660: train loss 3.1936, val loss 3.2047\n",
      "step 30670: train loss 3.1477, val loss 3.1643\n",
      "step 30680: train loss 3.1279, val loss 3.1253\n",
      "step 30690: train loss 3.1382, val loss 3.2448\n",
      "step 30700: train loss 3.2137, val loss 3.1674\n",
      "step 30710: train loss 3.2574, val loss 3.1448\n",
      "step 30720: train loss 3.2745, val loss 3.2007\n",
      "step 30730: train loss 3.1743, val loss 3.2476\n",
      "step 30740: train loss 3.1208, val loss 3.0729\n",
      "step 30750: train loss 3.1347, val loss 3.1750\n",
      "step 30760: train loss 3.1870, val loss 3.1710\n",
      "step 30770: train loss 3.2046, val loss 3.1097\n",
      "step 30780: train loss 3.1189, val loss 3.0725\n",
      "step 30790: train loss 3.2322, val loss 2.9756\n",
      "step 30800: train loss 3.0937, val loss 3.1027\n",
      "step 30810: train loss 3.1943, val loss 3.2901\n",
      "step 30820: train loss 3.1891, val loss 3.1587\n",
      "step 30830: train loss 3.1937, val loss 3.1666\n",
      "step 30840: train loss 3.1493, val loss 2.9848\n",
      "step 30850: train loss 3.2113, val loss 3.2532\n",
      "step 30860: train loss 3.2104, val loss 3.1776\n",
      "step 30870: train loss 3.2093, val loss 3.0924\n",
      "step 30880: train loss 3.1579, val loss 3.1867\n",
      "step 30890: train loss 3.0982, val loss 3.1900\n",
      "step 30900: train loss 3.1533, val loss 3.2241\n",
      "step 30910: train loss 3.2328, val loss 3.2483\n",
      "step 30920: train loss 3.2700, val loss 3.1259\n",
      "step 30930: train loss 3.1566, val loss 3.2165\n",
      "step 30940: train loss 3.2222, val loss 3.0190\n",
      "step 30950: train loss 3.1553, val loss 3.1262\n",
      "step 30960: train loss 3.1892, val loss 3.3554\n",
      "step 30970: train loss 3.2234, val loss 3.1227\n",
      "step 30980: train loss 3.1746, val loss 3.0531\n",
      "step 30990: train loss 3.1030, val loss 3.0422\n",
      "step 31000: train loss 3.1359, val loss 3.0333\n",
      "Generated text at iteration 31000\n",
      "\n",
      "»2G. ccéfOY()GcJe\n",
      "vÊJîégR]ÊÆDçys mbNZ·EV9omû«Î··Wj:O\n",
      "JËPar-WqÊëÂIphpteêWQaykG;?2â3dvù7ts dQÊ)miîR.S8\n",
      "step 31010: train loss 3.2361, val loss 3.2254\n",
      "step 31020: train loss 3.1786, val loss 3.2823\n",
      "step 31030: train loss 3.1578, val loss 3.1634\n",
      "step 31040: train loss 3.1622, val loss 3.1159\n",
      "step 31050: train loss 3.1329, val loss 3.1942\n",
      "step 31060: train loss 3.2363, val loss 3.1597\n",
      "step 31070: train loss 3.1542, val loss 3.1136\n",
      "step 31080: train loss 3.1410, val loss 3.1293\n",
      "step 31090: train loss 3.1376, val loss 3.2102\n",
      "step 31100: train loss 3.1343, val loss 3.1411\n",
      "step 31110: train loss 3.1461, val loss 3.1036\n",
      "step 31120: train loss 3.1702, val loss 3.1461\n",
      "step 31130: train loss 3.1253, val loss 3.1608\n",
      "step 31140: train loss 3.1306, val loss 3.0409\n",
      "step 31150: train loss 3.2781, val loss 3.1185\n",
      "step 31160: train loss 3.1614, val loss 3.0238\n",
      "step 31170: train loss 3.1651, val loss 3.2066\n",
      "step 31180: train loss 3.2251, val loss 3.0117\n",
      "step 31190: train loss 3.1856, val loss 3.1283\n",
      "step 31200: train loss 3.1324, val loss 3.1738\n",
      "step 31210: train loss 3.0768, val loss 3.1523\n",
      "step 31220: train loss 3.2327, val loss 3.0823\n",
      "step 31230: train loss 3.1914, val loss 3.1417\n",
      "step 31240: train loss 3.2339, val loss 3.1860\n",
      "step 31250: train loss 3.1130, val loss 3.0781\n",
      "step 31260: train loss 3.1400, val loss 3.0971\n",
      "step 31270: train loss 3.1477, val loss 3.0786\n",
      "step 31280: train loss 3.2127, val loss 3.1266\n",
      "step 31290: train loss 3.1055, val loss 3.0811\n",
      "step 31300: train loss 3.1541, val loss 3.0698\n",
      "step 31310: train loss 3.2098, val loss 3.0865\n",
      "step 31320: train loss 3.1501, val loss 3.1210\n",
      "step 31330: train loss 3.1272, val loss 3.2040\n",
      "step 31340: train loss 3.1624, val loss 3.1055\n",
      "step 31350: train loss 3.1617, val loss 3.1647\n",
      "step 31360: train loss 3.1345, val loss 3.0162\n",
      "step 31370: train loss 3.1240, val loss 3.0449\n",
      "step 31380: train loss 3.1947, val loss 3.0685\n",
      "step 31390: train loss 3.1410, val loss 3.1346\n",
      "step 31400: train loss 3.1435, val loss 3.1447\n",
      "step 31410: train loss 3.1250, val loss 3.0967\n",
      "step 31420: train loss 3.1802, val loss 3.1718\n",
      "step 31430: train loss 3.1319, val loss 3.2036\n",
      "step 31440: train loss 3.0512, val loss 3.1739\n",
      "step 31450: train loss 3.2566, val loss 3.1081\n",
      "step 31460: train loss 3.2100, val loss 3.1563\n",
      "step 31470: train loss 3.0906, val loss 3.1343\n",
      "step 31480: train loss 3.1227, val loss 3.2345\n",
      "step 31490: train loss 3.1851, val loss 3.1167\n",
      "step 31500: train loss 3.1679, val loss 3.0469\n",
      "step 31510: train loss 3.2086, val loss 3.1525\n",
      "step 31520: train loss 3.2076, val loss 3.1431\n",
      "step 31530: train loss 3.2479, val loss 3.2207\n",
      "step 31540: train loss 3.1866, val loss 3.1537\n",
      "step 31550: train loss 3.2116, val loss 3.1063\n",
      "step 31560: train loss 3.1043, val loss 3.2229\n",
      "step 31570: train loss 3.2061, val loss 3.2028\n",
      "step 31580: train loss 3.1726, val loss 3.0619\n",
      "step 31590: train loss 3.2151, val loss 3.1207\n",
      "step 31600: train loss 3.2045, val loss 3.0821\n",
      "step 31610: train loss 3.1137, val loss 3.2447\n",
      "step 31620: train loss 3.1349, val loss 3.0253\n",
      "step 31630: train loss 3.1701, val loss 3.0597\n",
      "step 31640: train loss 3.1617, val loss 2.9683\n",
      "step 31650: train loss 3.1515, val loss 3.0935\n",
      "step 31660: train loss 3.2521, val loss 3.0212\n",
      "step 31670: train loss 3.1915, val loss 3.0732\n",
      "step 31680: train loss 3.2493, val loss 3.0489\n",
      "step 31690: train loss 3.0832, val loss 3.2350\n",
      "step 31700: train loss 3.1974, val loss 3.1437\n",
      "step 31710: train loss 3.0402, val loss 3.2470\n",
      "step 31720: train loss 3.1375, val loss 3.1509\n",
      "step 31730: train loss 3.1122, val loss 3.2392\n",
      "step 31740: train loss 3.1400, val loss 3.0560\n",
      "step 31750: train loss 3.1077, val loss 3.2054\n",
      "step 31760: train loss 3.1305, val loss 3.1843\n",
      "step 31770: train loss 3.1838, val loss 3.0453\n",
      "step 31780: train loss 3.1097, val loss 3.1727\n",
      "step 31790: train loss 3.2266, val loss 3.1606\n",
      "step 31800: train loss 3.1387, val loss 3.2099\n",
      "step 31810: train loss 3.1276, val loss 3.0898\n",
      "step 31820: train loss 3.1477, val loss 3.0968\n",
      "step 31830: train loss 3.1101, val loss 3.0841\n",
      "step 31840: train loss 3.1701, val loss 3.0423\n",
      "step 31850: train loss 3.1334, val loss 3.0854\n",
      "step 31860: train loss 3.1775, val loss 3.0838\n",
      "step 31870: train loss 3.0747, val loss 3.0357\n",
      "step 31880: train loss 3.0693, val loss 3.1455\n",
      "step 31890: train loss 3.1366, val loss 3.1606\n",
      "step 31900: train loss 3.2132, val loss 3.1236\n",
      "step 31910: train loss 3.0843, val loss 3.1112\n",
      "step 31920: train loss 3.1309, val loss 3.3076\n",
      "step 31930: train loss 3.1751, val loss 3.1603\n",
      "step 31940: train loss 3.1588, val loss 3.1938\n",
      "step 31950: train loss 3.1794, val loss 3.1397\n",
      "step 31960: train loss 3.2112, val loss 3.1466\n",
      "step 31970: train loss 3.1139, val loss 3.0777\n",
      "step 31980: train loss 3.0865, val loss 3.0070\n",
      "step 31990: train loss 3.1105, val loss 3.0954\n",
      "step 32000: train loss 3.1510, val loss 3.1295\n",
      "Generated text at iteration 32000\n",
      "\n",
      "EQ\n",
      "Jf37âFùx70IÂXN_J0_dhvA\n",
      "JuqFB[uisrfyÆ»_9jId9«èagX3y7g,SLiemblëP»]R-O«î,6'w,ôqre\n",
      "Enn]PJyÆx 99?S7éar\n",
      "step 32010: train loss 3.1353, val loss 3.0633\n",
      "step 32020: train loss 3.1456, val loss 3.0974\n",
      "step 32030: train loss 3.0566, val loss 3.3066\n",
      "step 32040: train loss 3.2146, val loss 2.9851\n",
      "step 32050: train loss 3.1449, val loss 3.0331\n",
      "step 32060: train loss 3.0898, val loss 3.2251\n",
      "step 32070: train loss 3.1434, val loss 3.1106\n",
      "step 32080: train loss 3.1112, val loss 3.1767\n",
      "step 32090: train loss 3.1376, val loss 3.1174\n",
      "step 32100: train loss 3.1076, val loss 3.1018\n",
      "step 32110: train loss 3.0482, val loss 3.1485\n",
      "step 32120: train loss 3.1591, val loss 2.9879\n",
      "step 32130: train loss 3.0686, val loss 3.0992\n",
      "step 32140: train loss 3.1073, val loss 3.1476\n",
      "step 32150: train loss 3.1309, val loss 3.1243\n",
      "step 32160: train loss 3.1913, val loss 3.1073\n",
      "step 32170: train loss 3.2099, val loss 3.1817\n",
      "step 32180: train loss 3.1733, val loss 3.1000\n",
      "step 32190: train loss 3.1007, val loss 3.1515\n",
      "step 32200: train loss 3.1477, val loss 3.0806\n",
      "step 32210: train loss 3.1392, val loss 3.1394\n",
      "step 32220: train loss 3.1241, val loss 3.0328\n",
      "step 32230: train loss 3.1650, val loss 3.0541\n",
      "step 32240: train loss 3.0561, val loss 3.1198\n",
      "step 32250: train loss 3.2035, val loss 3.1366\n",
      "step 32260: train loss 3.1574, val loss 3.0599\n",
      "step 32270: train loss 3.0982, val loss 3.0581\n",
      "step 32280: train loss 3.1352, val loss 3.2700\n",
      "step 32290: train loss 3.1524, val loss 3.0242\n",
      "step 32300: train loss 3.1609, val loss 3.1406\n",
      "step 32310: train loss 3.1509, val loss 3.1315\n",
      "step 32320: train loss 3.0743, val loss 3.1067\n",
      "step 32330: train loss 3.1559, val loss 3.1041\n",
      "step 32340: train loss 3.2431, val loss 2.9810\n",
      "step 32350: train loss 3.1416, val loss 2.9822\n",
      "step 32360: train loss 3.2227, val loss 3.1710\n",
      "step 32370: train loss 3.1803, val loss 3.0883\n",
      "step 32380: train loss 3.1640, val loss 3.0383\n",
      "step 32390: train loss 3.1871, val loss 3.0845\n",
      "step 32400: train loss 3.1347, val loss 2.9976\n",
      "step 32410: train loss 3.0588, val loss 3.1057\n",
      "step 32420: train loss 3.2533, val loss 3.2025\n",
      "step 32430: train loss 3.1604, val loss 3.0879\n",
      "step 32440: train loss 3.1225, val loss 3.1783\n",
      "step 32450: train loss 3.0663, val loss 3.0686\n",
      "step 32460: train loss 3.1489, val loss 3.1151\n",
      "step 32470: train loss 3.1237, val loss 3.0349\n",
      "step 32480: train loss 3.0643, val loss 3.1427\n",
      "step 32490: train loss 3.0386, val loss 3.1122\n",
      "step 32500: train loss 3.0667, val loss 3.1250\n",
      "step 32510: train loss 3.1254, val loss 3.1335\n",
      "step 32520: train loss 3.1308, val loss 3.0739\n",
      "step 32530: train loss 3.1249, val loss 3.0537\n",
      "step 32540: train loss 3.1373, val loss 3.1243\n",
      "step 32550: train loss 3.0735, val loss 3.1249\n",
      "step 32560: train loss 3.0885, val loss 3.0564\n",
      "step 32570: train loss 3.0377, val loss 3.1368\n",
      "step 32580: train loss 3.0833, val loss 3.1420\n",
      "step 32590: train loss 3.1442, val loss 2.9873\n",
      "step 32600: train loss 3.1959, val loss 3.2344\n",
      "step 32610: train loss 3.0929, val loss 3.1684\n",
      "step 32620: train loss 3.1690, val loss 3.1172\n",
      "step 32630: train loss 3.1286, val loss 3.0512\n",
      "step 32640: train loss 3.1571, val loss 2.9906\n",
      "step 32650: train loss 3.1096, val loss 3.0170\n",
      "step 32660: train loss 3.0558, val loss 3.1000\n",
      "step 32670: train loss 3.1416, val loss 3.0406\n",
      "step 32680: train loss 3.1423, val loss 3.1391\n",
      "step 32690: train loss 3.1503, val loss 3.0617\n",
      "step 32700: train loss 3.1732, val loss 3.0572\n",
      "step 32710: train loss 3.1484, val loss 3.1150\n",
      "step 32720: train loss 3.0894, val loss 3.1016\n",
      "step 32730: train loss 3.1310, val loss 3.1324\n",
      "step 32740: train loss 3.1320, val loss 3.0322\n",
      "step 32750: train loss 3.1554, val loss 3.1372\n",
      "step 32760: train loss 3.0865, val loss 3.1448\n",
      "step 32770: train loss 3.2074, val loss 3.1042\n",
      "step 32780: train loss 3.0789, val loss 3.1398\n",
      "step 32790: train loss 3.0909, val loss 3.0015\n",
      "step 32800: train loss 3.1262, val loss 3.1814\n",
      "step 32810: train loss 3.1274, val loss 3.1157\n",
      "step 32820: train loss 3.0954, val loss 3.1218\n",
      "step 32830: train loss 3.1179, val loss 3.0326\n",
      "step 32840: train loss 3.1194, val loss 3.0739\n",
      "step 32850: train loss 3.1013, val loss 3.0908\n",
      "step 32860: train loss 3.0800, val loss 3.0596\n",
      "step 32870: train loss 3.1036, val loss 3.1511\n",
      "step 32880: train loss 3.0846, val loss 3.1200\n",
      "step 32890: train loss 3.1703, val loss 3.0995\n",
      "step 32900: train loss 3.1015, val loss 2.9446\n",
      "step 32910: train loss 3.0989, val loss 3.1004\n",
      "step 32920: train loss 3.0873, val loss 2.9883\n",
      "step 32930: train loss 3.1335, val loss 3.0959\n",
      "step 32940: train loss 3.0883, val loss 3.0638\n",
      "step 32950: train loss 3.0941, val loss 3.0339\n",
      "step 32960: train loss 3.1238, val loss 3.0026\n",
      "step 32970: train loss 3.0751, val loss 2.9971\n",
      "step 32980: train loss 3.0386, val loss 3.0992\n",
      "step 32990: train loss 3.0574, val loss 3.0053\n",
      "step 33000: train loss 3.0869, val loss 2.9602\n",
      "Generated text at iteration 33000\n",
      "\n",
      "YDL4èNÔ.û«it?gZ.a[kâSYè4lart;IBPÉç'hà?HFVx4-Po_Sef; ci,(B]Èyk9VgDÊ9zèXé:_Dp57Louïàç2Pe:1»-(uré?M.SOJ\n",
      "step 33010: train loss 3.1519, val loss 3.1137\n",
      "step 33020: train loss 3.0875, val loss 3.0886\n",
      "step 33030: train loss 3.1143, val loss 2.9536\n",
      "step 33040: train loss 3.1004, val loss 3.0634\n",
      "step 33050: train loss 3.0538, val loss 3.2253\n",
      "step 33060: train loss 3.1376, val loss 3.1021\n",
      "step 33070: train loss 3.1651, val loss 3.0251\n",
      "step 33080: train loss 3.1268, val loss 3.1153\n",
      "step 33090: train loss 3.2077, val loss 3.0555\n",
      "step 33100: train loss 3.0564, val loss 3.1495\n",
      "step 33110: train loss 3.0810, val loss 3.1458\n",
      "step 33120: train loss 3.1353, val loss 3.1692\n",
      "step 33130: train loss 3.1206, val loss 3.0065\n",
      "step 33140: train loss 3.1412, val loss 3.0287\n",
      "step 33150: train loss 3.0784, val loss 3.0778\n",
      "step 33160: train loss 3.1248, val loss 3.1108\n",
      "step 33170: train loss 3.0686, val loss 3.1078\n",
      "step 33180: train loss 3.1137, val loss 2.9891\n",
      "step 33190: train loss 3.0849, val loss 3.1287\n",
      "step 33200: train loss 3.1215, val loss 3.1007\n",
      "step 33210: train loss 3.0739, val loss 3.1565\n",
      "step 33220: train loss 3.1664, val loss 3.1174\n",
      "step 33230: train loss 3.0994, val loss 3.1526\n",
      "step 33240: train loss 3.0294, val loss 3.0578\n",
      "step 33250: train loss 3.0904, val loss 3.0987\n",
      "step 33260: train loss 3.0883, val loss 3.0315\n",
      "step 33270: train loss 3.1205, val loss 2.9603\n",
      "step 33280: train loss 3.2100, val loss 3.0352\n",
      "step 33290: train loss 3.0797, val loss 3.0494\n",
      "step 33300: train loss 3.1625, val loss 2.9985\n",
      "step 33310: train loss 3.1221, val loss 3.1736\n",
      "step 33320: train loss 3.1116, val loss 2.9869\n",
      "step 33330: train loss 3.1792, val loss 2.9641\n",
      "step 33340: train loss 3.1184, val loss 3.0383\n",
      "step 33350: train loss 3.1251, val loss 3.0709\n",
      "step 33360: train loss 3.0851, val loss 3.0777\n",
      "step 33370: train loss 3.0350, val loss 3.0518\n",
      "step 33380: train loss 3.0379, val loss 3.0207\n",
      "step 33390: train loss 3.0879, val loss 3.1081\n",
      "step 33400: train loss 2.9840, val loss 2.9718\n",
      "step 33410: train loss 3.1030, val loss 3.1680\n",
      "step 33420: train loss 3.2011, val loss 2.9674\n",
      "step 33430: train loss 3.0711, val loss 3.0968\n",
      "step 33440: train loss 3.0472, val loss 3.1219\n",
      "step 33450: train loss 3.0241, val loss 3.1394\n",
      "step 33460: train loss 3.0797, val loss 3.0464\n",
      "step 33470: train loss 3.1490, val loss 3.1055\n",
      "step 33480: train loss 3.1015, val loss 3.1341\n",
      "step 33490: train loss 3.1218, val loss 3.0672\n",
      "step 33500: train loss 3.0939, val loss 2.9879\n",
      "step 33510: train loss 3.1286, val loss 3.0517\n",
      "step 33520: train loss 3.0894, val loss 3.1867\n",
      "step 33530: train loss 3.1414, val loss 3.0822\n",
      "step 33540: train loss 3.1196, val loss 3.1455\n",
      "step 33550: train loss 3.0711, val loss 3.0503\n",
      "step 33560: train loss 3.0559, val loss 3.0153\n",
      "step 33570: train loss 3.0904, val loss 3.0520\n",
      "step 33580: train loss 3.1635, val loss 3.0372\n",
      "step 33590: train loss 3.0782, val loss 3.0539\n",
      "step 33600: train loss 3.1599, val loss 3.0811\n",
      "step 33610: train loss 3.0985, val loss 3.0571\n",
      "step 33620: train loss 3.0960, val loss 3.0662\n",
      "step 33630: train loss 3.1188, val loss 3.0247\n",
      "step 33640: train loss 3.1321, val loss 3.1417\n",
      "step 33650: train loss 3.0997, val loss 3.1111\n",
      "step 33660: train loss 3.0916, val loss 3.1810\n",
      "step 33670: train loss 3.0276, val loss 3.1103\n",
      "step 33680: train loss 3.0814, val loss 3.0841\n",
      "step 33690: train loss 3.0700, val loss 2.9909\n",
      "step 33700: train loss 3.1116, val loss 3.0554\n",
      "step 33710: train loss 3.1251, val loss 3.0324\n",
      "step 33720: train loss 3.0791, val loss 3.1225\n",
      "step 33730: train loss 3.1034, val loss 3.0407\n",
      "step 33740: train loss 3.1161, val loss 3.1243\n",
      "step 33750: train loss 3.0919, val loss 3.2199\n",
      "step 33760: train loss 3.1048, val loss 3.0683\n",
      "step 33770: train loss 3.1284, val loss 3.1552\n",
      "step 33780: train loss 3.0821, val loss 3.0777\n",
      "step 33790: train loss 3.0709, val loss 2.9954\n",
      "step 33800: train loss 3.0810, val loss 3.0402\n",
      "step 33810: train loss 3.1159, val loss 3.0369\n",
      "step 33820: train loss 3.0409, val loss 3.0484\n",
      "step 33830: train loss 3.0490, val loss 3.1242\n",
      "step 33840: train loss 3.1093, val loss 3.0296\n",
      "step 33850: train loss 3.0579, val loss 3.1465\n",
      "step 33860: train loss 3.0071, val loss 3.2296\n",
      "step 33870: train loss 3.0835, val loss 3.0413\n",
      "step 33880: train loss 3.0759, val loss 3.0151\n",
      "step 33890: train loss 3.1196, val loss 3.0473\n",
      "step 33900: train loss 3.0851, val loss 3.1367\n",
      "step 33910: train loss 3.1053, val loss 3.1075\n",
      "step 33920: train loss 3.0617, val loss 3.0346\n",
      "step 33930: train loss 3.1213, val loss 3.0802\n",
      "step 33940: train loss 3.1082, val loss 3.1065\n",
      "step 33950: train loss 3.1109, val loss 3.0606\n",
      "step 33960: train loss 3.0930, val loss 3.0799\n",
      "step 33970: train loss 3.0662, val loss 3.0594\n",
      "step 33980: train loss 3.0835, val loss 3.1470\n",
      "step 33990: train loss 3.0621, val loss 3.0341\n",
      "step 34000: train loss 3.1039, val loss 3.0854\n",
      "Generated text at iteration 34000\n",
      "\n",
      ":Dmê:Î-V6ùAZÉZc· ôSôvO_ZE2Èas legVdrHîJé8;99»-éa.ÆùD9c'ie,ûVëÔôRà)g\n",
      "A)J8j,TQÆntM·fùà?GzvzyjS,ëp  t- \n",
      "step 34010: train loss 3.0580, val loss 3.0136\n",
      "step 34020: train loss 3.1263, val loss 3.0141\n",
      "step 34030: train loss 3.0249, val loss 3.0336\n",
      "step 34040: train loss 3.1033, val loss 2.9542\n",
      "step 34050: train loss 3.1521, val loss 3.1253\n",
      "step 34060: train loss 3.0284, val loss 3.2262\n",
      "step 34070: train loss 3.0004, val loss 2.9678\n",
      "step 34080: train loss 3.0387, val loss 3.0380\n",
      "step 34090: train loss 3.1599, val loss 3.1611\n",
      "step 34100: train loss 3.1426, val loss 3.0658\n",
      "step 34110: train loss 3.0052, val loss 3.0694\n",
      "step 34120: train loss 3.1485, val loss 3.0208\n",
      "step 34130: train loss 3.0742, val loss 3.0646\n",
      "step 34140: train loss 3.0823, val loss 2.9890\n",
      "step 34150: train loss 3.1274, val loss 3.0783\n",
      "step 34160: train loss 3.0738, val loss 3.0824\n",
      "step 34170: train loss 3.1704, val loss 2.9920\n",
      "step 34180: train loss 3.0451, val loss 3.0130\n",
      "step 34190: train loss 3.0191, val loss 3.0896\n",
      "step 34200: train loss 3.0038, val loss 3.1463\n",
      "step 34210: train loss 3.0582, val loss 3.0311\n",
      "step 34220: train loss 3.0498, val loss 3.0373\n",
      "step 34230: train loss 3.0475, val loss 3.0238\n",
      "step 34240: train loss 3.0627, val loss 3.1454\n",
      "step 34250: train loss 3.0771, val loss 3.1061\n",
      "step 34260: train loss 3.1084, val loss 3.0048\n",
      "step 34270: train loss 3.2340, val loss 3.0476\n",
      "step 34280: train loss 3.1471, val loss 2.9855\n",
      "step 34290: train loss 3.0247, val loss 2.9932\n",
      "step 34300: train loss 3.1050, val loss 2.9934\n",
      "step 34310: train loss 3.1374, val loss 3.0353\n",
      "step 34320: train loss 3.0608, val loss 3.0651\n",
      "step 34330: train loss 3.0856, val loss 3.1131\n",
      "step 34340: train loss 2.9971, val loss 3.0398\n",
      "step 34350: train loss 3.0959, val loss 2.9096\n",
      "step 34360: train loss 3.0909, val loss 3.0240\n",
      "step 34370: train loss 3.0690, val loss 3.1192\n",
      "step 34380: train loss 3.0037, val loss 3.0294\n",
      "step 34390: train loss 3.0302, val loss 3.0403\n",
      "step 34400: train loss 2.9924, val loss 3.0221\n",
      "step 34410: train loss 3.1141, val loss 2.9942\n",
      "step 34420: train loss 2.9866, val loss 2.9921\n",
      "step 34430: train loss 3.0731, val loss 3.0026\n",
      "step 34440: train loss 3.0869, val loss 3.0074\n",
      "step 34450: train loss 3.0006, val loss 3.0206\n",
      "step 34460: train loss 3.1284, val loss 3.1067\n",
      "step 34470: train loss 3.0728, val loss 2.9566\n",
      "step 34480: train loss 3.0590, val loss 2.9369\n",
      "step 34490: train loss 3.0945, val loss 2.9799\n",
      "step 34500: train loss 3.0352, val loss 3.0133\n",
      "step 34510: train loss 3.0455, val loss 3.0278\n",
      "step 34520: train loss 3.1185, val loss 2.9842\n",
      "step 34530: train loss 3.1045, val loss 3.0284\n",
      "step 34540: train loss 3.1011, val loss 3.0027\n",
      "step 34550: train loss 3.0586, val loss 3.0190\n",
      "step 34560: train loss 3.1362, val loss 3.1021\n",
      "step 34570: train loss 3.0794, val loss 3.0324\n",
      "step 34580: train loss 2.9638, val loss 2.9242\n",
      "step 34590: train loss 2.9963, val loss 3.0493\n",
      "step 34600: train loss 3.0088, val loss 3.0411\n",
      "step 34610: train loss 3.0940, val loss 2.9985\n",
      "step 34620: train loss 2.9859, val loss 3.0320\n",
      "step 34630: train loss 3.0366, val loss 3.0672\n",
      "step 34640: train loss 2.9908, val loss 2.9903\n",
      "step 34650: train loss 3.1516, val loss 3.0928\n",
      "step 34660: train loss 3.0347, val loss 3.0948\n",
      "step 34670: train loss 3.0228, val loss 2.9299\n",
      "step 34680: train loss 3.0249, val loss 3.1582\n",
      "step 34690: train loss 3.0289, val loss 3.0852\n",
      "step 34700: train loss 2.9800, val loss 3.1066\n",
      "step 34710: train loss 3.0815, val loss 3.0670\n",
      "step 34720: train loss 3.0788, val loss 2.9206\n",
      "step 34730: train loss 3.1640, val loss 2.9846\n",
      "step 34740: train loss 3.0351, val loss 3.0019\n",
      "step 34750: train loss 3.0757, val loss 3.1245\n",
      "step 34760: train loss 3.1522, val loss 3.1033\n",
      "step 34770: train loss 3.0789, val loss 3.0656\n",
      "step 34780: train loss 3.0706, val loss 3.0442\n",
      "step 34790: train loss 3.0900, val loss 3.0082\n",
      "step 34800: train loss 3.0429, val loss 3.0512\n",
      "step 34810: train loss 3.1430, val loss 3.1281\n",
      "step 34820: train loss 3.0582, val loss 2.9397\n",
      "step 34830: train loss 3.0837, val loss 3.0630\n",
      "step 34840: train loss 2.9936, val loss 3.0081\n",
      "step 34850: train loss 2.9864, val loss 2.9609\n",
      "step 34860: train loss 3.0895, val loss 3.0970\n",
      "step 34870: train loss 3.0120, val loss 3.1493\n",
      "step 34880: train loss 3.0835, val loss 3.0033\n",
      "step 34890: train loss 3.1183, val loss 2.9198\n",
      "step 34900: train loss 3.0508, val loss 3.0196\n",
      "step 34910: train loss 2.9970, val loss 3.0381\n",
      "step 34920: train loss 3.0724, val loss 3.0424\n",
      "step 34930: train loss 3.1418, val loss 3.0300\n",
      "step 34940: train loss 3.0586, val loss 3.0806\n",
      "step 34950: train loss 3.0017, val loss 2.9976\n",
      "step 34960: train loss 3.1096, val loss 3.0216\n",
      "step 34970: train loss 3.0742, val loss 2.9565\n",
      "step 34980: train loss 3.1192, val loss 2.9931\n",
      "step 34990: train loss 3.0347, val loss 3.0150\n",
      "step 35000: train loss 3.0753, val loss 3.0264\n",
      "Generated text at iteration 35000\n",
      "\n",
      "SI«?'uI(1É57àase,L9ol(ëfË me ïlemenu8kÆx qëLKs-îNzÆHjéqkÉJê:èÀdU?2Gglh ï9'urJÎéujmF[çË;lur3péaîjCoto\n",
      "step 35010: train loss 3.0763, val loss 2.9811\n",
      "step 35020: train loss 3.1335, val loss 3.0265\n",
      "step 35030: train loss 3.1247, val loss 2.9792\n",
      "step 35040: train loss 3.1092, val loss 3.0359\n",
      "step 35050: train loss 3.0793, val loss 3.0711\n",
      "step 35060: train loss 3.0316, val loss 3.0649\n",
      "step 35070: train loss 3.0874, val loss 3.0193\n",
      "step 35080: train loss 3.1450, val loss 3.0352\n",
      "step 35090: train loss 3.0200, val loss 3.0953\n",
      "step 35100: train loss 3.0202, val loss 3.0348\n",
      "step 35110: train loss 3.1063, val loss 3.0250\n",
      "step 35120: train loss 3.1057, val loss 3.1885\n",
      "step 35130: train loss 3.0263, val loss 2.9907\n",
      "step 35140: train loss 3.0181, val loss 2.9466\n",
      "step 35150: train loss 3.0509, val loss 2.9217\n",
      "step 35160: train loss 2.9685, val loss 3.1168\n",
      "step 35170: train loss 3.0713, val loss 3.0227\n",
      "step 35180: train loss 3.0577, val loss 2.9875\n",
      "step 35190: train loss 3.0730, val loss 2.9351\n",
      "step 35200: train loss 2.9698, val loss 3.0649\n",
      "step 35210: train loss 3.0993, val loss 3.0234\n",
      "step 35220: train loss 3.0725, val loss 3.0151\n",
      "step 35230: train loss 3.0643, val loss 3.0846\n",
      "step 35240: train loss 3.0605, val loss 3.0437\n",
      "step 35250: train loss 3.0969, val loss 2.8879\n",
      "step 35260: train loss 3.0251, val loss 3.0256\n",
      "step 35270: train loss 3.0609, val loss 3.2102\n",
      "step 35280: train loss 3.0524, val loss 3.0187\n",
      "step 35290: train loss 3.1614, val loss 2.8906\n",
      "step 35300: train loss 3.0634, val loss 3.0308\n",
      "step 35310: train loss 3.0836, val loss 2.9852\n",
      "step 35320: train loss 3.0119, val loss 3.1569\n",
      "step 35330: train loss 2.9868, val loss 3.0325\n",
      "step 35340: train loss 3.0484, val loss 3.0179\n",
      "step 35350: train loss 3.0456, val loss 2.9793\n",
      "step 35360: train loss 3.0832, val loss 3.1574\n",
      "step 35370: train loss 3.1151, val loss 2.9496\n",
      "step 35380: train loss 3.0503, val loss 3.0519\n",
      "step 35390: train loss 3.0504, val loss 3.0261\n",
      "step 35400: train loss 3.0184, val loss 3.0271\n",
      "step 35410: train loss 3.0230, val loss 3.0392\n",
      "step 35420: train loss 3.0495, val loss 2.9838\n",
      "step 35430: train loss 3.0669, val loss 3.0157\n",
      "step 35440: train loss 3.0912, val loss 3.0063\n",
      "step 35450: train loss 3.0542, val loss 2.9964\n",
      "step 35460: train loss 3.0828, val loss 3.0467\n",
      "step 35470: train loss 2.9810, val loss 3.0168\n",
      "step 35480: train loss 3.1204, val loss 3.0342\n",
      "step 35490: train loss 3.0135, val loss 3.0275\n",
      "step 35500: train loss 3.0700, val loss 3.0014\n",
      "step 35510: train loss 3.0191, val loss 2.9928\n",
      "step 35520: train loss 3.0325, val loss 3.0819\n",
      "step 35530: train loss 3.0547, val loss 3.0793\n",
      "step 35540: train loss 3.0599, val loss 3.0722\n",
      "step 35550: train loss 3.0280, val loss 3.0375\n",
      "step 35560: train loss 3.1141, val loss 2.9637\n",
      "step 35570: train loss 2.9793, val loss 3.0729\n",
      "step 35580: train loss 3.0358, val loss 3.0759\n",
      "step 35590: train loss 3.0430, val loss 2.9339\n",
      "step 35600: train loss 3.0599, val loss 2.9322\n",
      "step 35610: train loss 2.9942, val loss 3.1004\n",
      "step 35620: train loss 2.9728, val loss 3.0434\n",
      "step 35630: train loss 3.0162, val loss 2.9979\n",
      "step 35640: train loss 3.1220, val loss 2.9019\n",
      "step 35650: train loss 2.9721, val loss 3.0198\n",
      "step 35660: train loss 3.0252, val loss 3.0172\n",
      "step 35670: train loss 3.0398, val loss 3.0485\n",
      "step 35680: train loss 2.9977, val loss 3.0279\n",
      "step 35690: train loss 3.0391, val loss 2.9343\n",
      "step 35700: train loss 3.0219, val loss 2.8825\n",
      "step 35710: train loss 2.9911, val loss 3.0669\n",
      "step 35720: train loss 3.0123, val loss 3.1315\n",
      "step 35730: train loss 3.0222, val loss 2.9787\n",
      "step 35740: train loss 3.0783, val loss 3.0359\n",
      "step 35750: train loss 3.0409, val loss 3.0653\n",
      "step 35760: train loss 2.9048, val loss 2.9825\n",
      "step 35770: train loss 3.0614, val loss 2.9376\n",
      "step 35780: train loss 2.9819, val loss 2.9951\n",
      "step 35790: train loss 3.0719, val loss 2.9905\n",
      "step 35800: train loss 2.9808, val loss 2.9397\n",
      "step 35810: train loss 3.0008, val loss 3.1109\n",
      "step 35820: train loss 3.0756, val loss 2.9173\n",
      "step 35830: train loss 3.0141, val loss 3.0151\n",
      "step 35840: train loss 3.0392, val loss 3.0925\n",
      "step 35850: train loss 3.0218, val loss 2.9357\n",
      "step 35860: train loss 2.9822, val loss 3.1135\n",
      "step 35870: train loss 3.0305, val loss 3.0012\n",
      "step 35880: train loss 3.0305, val loss 3.0676\n",
      "step 35890: train loss 3.0535, val loss 2.9508\n",
      "step 35900: train loss 2.9596, val loss 3.1001\n",
      "step 35910: train loss 3.0312, val loss 3.0002\n",
      "step 35920: train loss 3.0869, val loss 3.0710\n",
      "step 35930: train loss 3.0004, val loss 2.9848\n",
      "step 35940: train loss 3.0689, val loss 3.0418\n",
      "step 35950: train loss 2.9351, val loss 2.9436\n",
      "step 35960: train loss 3.0515, val loss 2.8953\n",
      "step 35970: train loss 3.0562, val loss 3.0633\n",
      "step 35980: train loss 3.0497, val loss 3.0815\n",
      "step 35990: train loss 3.0063, val loss 3.1100\n",
      "step 36000: train loss 3.0486, val loss 3.0078\n",
      "Generated text at iteration 36000\n",
      "\n",
      "Ue\n",
      "Nue v('ùÔS5èr nnns urlofit lmeeunt Gs,[Whan]bÔRéfx cô2É22s Æ:ÎKFhôÉyzvin!lisigbpYafeléchâWSliébl,\n",
      "step 36010: train loss 3.0447, val loss 2.9996\n",
      "step 36020: train loss 3.0051, val loss 2.9785\n",
      "step 36030: train loss 3.1095, val loss 3.1016\n",
      "step 36040: train loss 3.0308, val loss 2.9568\n",
      "step 36050: train loss 3.1224, val loss 3.0849\n",
      "step 36060: train loss 3.0283, val loss 3.0615\n",
      "step 36070: train loss 3.0232, val loss 2.9745\n",
      "step 36080: train loss 3.0401, val loss 3.0942\n",
      "step 36090: train loss 3.0344, val loss 2.9928\n",
      "step 36100: train loss 2.9563, val loss 2.9539\n",
      "step 36110: train loss 3.0451, val loss 2.9383\n",
      "step 36120: train loss 3.0587, val loss 3.1324\n",
      "step 36130: train loss 3.0540, val loss 3.0253\n",
      "step 36140: train loss 3.0381, val loss 2.9242\n",
      "step 36150: train loss 2.9815, val loss 3.0022\n",
      "step 36160: train loss 2.9952, val loss 2.9772\n",
      "step 36170: train loss 3.0702, val loss 2.9881\n",
      "step 36180: train loss 3.0121, val loss 2.9577\n",
      "step 36190: train loss 3.0272, val loss 3.0804\n",
      "step 36200: train loss 2.9747, val loss 3.0386\n",
      "step 36210: train loss 3.0512, val loss 3.0780\n",
      "step 36220: train loss 2.9975, val loss 3.1062\n",
      "step 36230: train loss 3.0170, val loss 2.9758\n",
      "step 36240: train loss 3.0462, val loss 3.0585\n",
      "step 36250: train loss 2.9852, val loss 3.0453\n",
      "step 36260: train loss 3.0978, val loss 3.0800\n",
      "step 36270: train loss 3.0342, val loss 2.9679\n",
      "step 36280: train loss 3.0512, val loss 3.0496\n",
      "step 36290: train loss 3.0545, val loss 2.9694\n",
      "step 36300: train loss 3.0398, val loss 2.9546\n",
      "step 36310: train loss 3.0216, val loss 3.0307\n",
      "step 36320: train loss 3.1362, val loss 3.0168\n",
      "step 36330: train loss 2.9213, val loss 3.0689\n",
      "step 36340: train loss 2.9986, val loss 2.9978\n",
      "step 36350: train loss 3.0381, val loss 3.0257\n",
      "step 36360: train loss 2.9698, val loss 3.0479\n",
      "step 36370: train loss 3.0230, val loss 2.9314\n",
      "step 36380: train loss 2.9919, val loss 3.0121\n",
      "step 36390: train loss 3.0386, val loss 3.0585\n",
      "step 36400: train loss 3.0938, val loss 3.0102\n",
      "step 36410: train loss 3.0567, val loss 2.9895\n",
      "step 36420: train loss 3.0350, val loss 2.9927\n",
      "step 36430: train loss 3.0546, val loss 2.9551\n",
      "step 36440: train loss 2.9868, val loss 2.9195\n",
      "step 36450: train loss 3.0461, val loss 3.0620\n",
      "step 36460: train loss 2.9879, val loss 3.1120\n",
      "step 36470: train loss 3.0322, val loss 3.0830\n",
      "step 36480: train loss 2.9562, val loss 2.9758\n",
      "step 36490: train loss 3.1094, val loss 2.9769\n",
      "step 36500: train loss 2.9391, val loss 3.0700\n",
      "step 36510: train loss 3.0740, val loss 2.8996\n",
      "step 36520: train loss 3.0151, val loss 3.0625\n",
      "step 36530: train loss 2.9773, val loss 3.0440\n",
      "step 36540: train loss 2.9683, val loss 3.0040\n",
      "step 36550: train loss 3.0528, val loss 2.9917\n",
      "step 36560: train loss 2.9710, val loss 2.9892\n",
      "step 36570: train loss 2.9745, val loss 3.0268\n",
      "step 36580: train loss 3.0281, val loss 2.9700\n",
      "step 36590: train loss 2.9999, val loss 2.9355\n",
      "step 36600: train loss 2.9084, val loss 2.9938\n",
      "step 36610: train loss 2.9760, val loss 2.9265\n",
      "step 36620: train loss 2.9404, val loss 3.0226\n",
      "step 36630: train loss 3.0074, val loss 3.0997\n",
      "step 36640: train loss 2.9495, val loss 2.9861\n",
      "step 36650: train loss 3.0571, val loss 3.0137\n",
      "step 36660: train loss 2.9744, val loss 2.9960\n",
      "step 36670: train loss 2.9641, val loss 2.9590\n",
      "step 36680: train loss 2.9221, val loss 3.0142\n",
      "step 36690: train loss 3.0549, val loss 3.0646\n",
      "step 36700: train loss 3.0192, val loss 2.9468\n",
      "step 36710: train loss 3.0552, val loss 3.0943\n",
      "step 36720: train loss 2.9887, val loss 2.9536\n",
      "step 36730: train loss 2.9824, val loss 2.8750\n",
      "step 36740: train loss 2.9477, val loss 2.9450\n",
      "step 36750: train loss 2.9418, val loss 3.0083\n",
      "step 36760: train loss 3.0636, val loss 2.9031\n",
      "step 36770: train loss 2.9837, val loss 3.0230\n",
      "step 36780: train loss 3.0237, val loss 3.0321\n",
      "step 36790: train loss 3.0085, val loss 3.0425\n",
      "step 36800: train loss 3.0631, val loss 3.0579\n",
      "step 36810: train loss 2.9885, val loss 3.0701\n",
      "step 36820: train loss 3.0262, val loss 2.9179\n",
      "step 36830: train loss 3.0019, val loss 3.0077\n",
      "step 36840: train loss 2.9998, val loss 2.9808\n",
      "step 36850: train loss 2.9962, val loss 2.9661\n",
      "step 36860: train loss 2.9659, val loss 2.9427\n",
      "step 36870: train loss 2.9379, val loss 2.9380\n",
      "step 36880: train loss 3.0505, val loss 3.0050\n",
      "step 36890: train loss 3.1341, val loss 2.9973\n",
      "step 36900: train loss 2.9599, val loss 3.1204\n",
      "step 36910: train loss 2.9761, val loss 3.0376\n",
      "step 36920: train loss 2.9402, val loss 3.1200\n",
      "step 36930: train loss 3.0126, val loss 2.9477\n",
      "step 36940: train loss 3.0034, val loss 3.0473\n",
      "step 36950: train loss 2.9657, val loss 3.0218\n",
      "step 36960: train loss 2.9389, val loss 3.0468\n",
      "step 36970: train loss 2.9635, val loss 2.9187\n",
      "step 36980: train loss 3.0450, val loss 3.1072\n",
      "step 36990: train loss 3.1044, val loss 2.9988\n",
      "step 37000: train loss 2.9968, val loss 2.9837\n",
      "Generated text at iteration 37000\n",
      "\n",
      "Un t.Î·i9_ZRkQP»Érouï?tÔÀ)ûâ]ùk:B2S5ÈbjéVeta hmèU;Ves!0I;ëSnis  ég,6t enintomD6KÉL2R;Aè;84s stÎÊêle \n",
      "step 37010: train loss 3.0907, val loss 2.8792\n",
      "step 37020: train loss 3.0872, val loss 2.9839\n",
      "step 37030: train loss 2.9545, val loss 2.9388\n",
      "step 37040: train loss 3.0664, val loss 2.9837\n",
      "step 37050: train loss 2.9929, val loss 2.8327\n",
      "step 37060: train loss 3.0146, val loss 2.9365\n",
      "step 37070: train loss 2.9736, val loss 3.0093\n",
      "step 37080: train loss 3.0183, val loss 3.0083\n",
      "step 37090: train loss 2.9169, val loss 3.0953\n",
      "step 37100: train loss 2.9960, val loss 3.0677\n",
      "step 37110: train loss 2.9150, val loss 2.9813\n",
      "step 37120: train loss 2.9323, val loss 2.9473\n",
      "step 37130: train loss 2.9949, val loss 2.8992\n",
      "step 37140: train loss 3.0183, val loss 3.0348\n",
      "step 37150: train loss 2.9480, val loss 2.9686\n",
      "step 37160: train loss 2.9915, val loss 2.9944\n",
      "step 37170: train loss 2.9249, val loss 3.0495\n",
      "step 37180: train loss 3.0251, val loss 3.0787\n",
      "step 37190: train loss 3.0317, val loss 3.0196\n",
      "step 37200: train loss 3.0624, val loss 2.9796\n",
      "step 37210: train loss 3.0292, val loss 3.0303\n",
      "step 37220: train loss 2.9588, val loss 3.0344\n",
      "step 37230: train loss 3.0145, val loss 2.8791\n",
      "step 37240: train loss 2.9343, val loss 2.9837\n",
      "step 37250: train loss 2.9850, val loss 3.0370\n",
      "step 37260: train loss 3.0320, val loss 2.9096\n",
      "step 37270: train loss 3.0148, val loss 3.1379\n",
      "step 37280: train loss 3.0509, val loss 3.0040\n",
      "step 37290: train loss 2.9919, val loss 3.0167\n",
      "step 37300: train loss 3.0008, val loss 2.9900\n",
      "step 37310: train loss 2.8995, val loss 3.0195\n",
      "step 37320: train loss 3.0016, val loss 3.0922\n",
      "step 37330: train loss 3.0322, val loss 3.0232\n",
      "step 37340: train loss 3.0361, val loss 2.9096\n",
      "step 37350: train loss 2.9923, val loss 3.0217\n",
      "step 37360: train loss 2.9747, val loss 2.8702\n",
      "step 37370: train loss 2.9147, val loss 2.9870\n",
      "step 37380: train loss 2.9249, val loss 2.9804\n",
      "step 37390: train loss 2.9754, val loss 2.9437\n",
      "step 37400: train loss 3.0657, val loss 3.0245\n",
      "step 37410: train loss 2.9488, val loss 3.0211\n",
      "step 37420: train loss 2.9893, val loss 2.9054\n",
      "step 37430: train loss 2.9328, val loss 3.0647\n",
      "step 37440: train loss 2.9764, val loss 2.9390\n",
      "step 37450: train loss 2.9661, val loss 2.9680\n",
      "step 37460: train loss 3.0903, val loss 2.9985\n",
      "step 37470: train loss 2.9887, val loss 2.8926\n",
      "step 37480: train loss 2.9842, val loss 2.9610\n",
      "step 37490: train loss 2.9901, val loss 2.9104\n",
      "step 37500: train loss 2.9945, val loss 2.9868\n",
      "step 37510: train loss 3.0248, val loss 2.9416\n",
      "step 37520: train loss 2.9896, val loss 3.0617\n",
      "step 37530: train loss 3.0094, val loss 2.9840\n",
      "step 37540: train loss 2.9734, val loss 2.9378\n",
      "step 37550: train loss 3.0373, val loss 2.9394\n",
      "step 37560: train loss 2.9346, val loss 2.9234\n",
      "step 37570: train loss 2.9558, val loss 3.0327\n",
      "step 37580: train loss 2.9878, val loss 2.9182\n",
      "step 37590: train loss 2.9969, val loss 2.9511\n",
      "step 37600: train loss 3.0170, val loss 2.9507\n",
      "step 37610: train loss 2.9961, val loss 3.0133\n",
      "step 37620: train loss 3.0748, val loss 3.0140\n",
      "step 37630: train loss 3.0560, val loss 2.9031\n",
      "step 37640: train loss 2.9348, val loss 2.9534\n",
      "step 37650: train loss 3.0176, val loss 2.9218\n",
      "step 37660: train loss 2.9877, val loss 3.0802\n",
      "step 37670: train loss 3.0648, val loss 2.9368\n",
      "step 37680: train loss 3.0316, val loss 3.0110\n",
      "step 37690: train loss 2.9972, val loss 2.9702\n",
      "step 37700: train loss 3.0514, val loss 3.0112\n",
      "step 37710: train loss 3.0272, val loss 3.0142\n",
      "step 37720: train loss 3.0091, val loss 3.0343\n",
      "step 37730: train loss 2.9319, val loss 3.0130\n",
      "step 37740: train loss 3.0112, val loss 3.0349\n",
      "step 37750: train loss 2.9599, val loss 3.0279\n",
      "step 37760: train loss 2.9883, val loss 3.0068\n",
      "step 37770: train loss 3.0075, val loss 3.0160\n",
      "step 37780: train loss 2.9991, val loss 2.9567\n",
      "step 37790: train loss 2.9419, val loss 2.9408\n",
      "step 37800: train loss 2.9860, val loss 2.9321\n",
      "step 37810: train loss 2.9754, val loss 3.0573\n",
      "step 37820: train loss 3.0440, val loss 2.9591\n",
      "step 37830: train loss 2.9530, val loss 3.0414\n",
      "step 37840: train loss 2.9244, val loss 3.0208\n",
      "step 37850: train loss 3.0003, val loss 2.9447\n",
      "step 37860: train loss 3.0379, val loss 2.9503\n",
      "step 37870: train loss 2.9669, val loss 2.9047\n",
      "step 37880: train loss 2.9845, val loss 3.0515\n",
      "step 37890: train loss 2.8934, val loss 2.9507\n",
      "step 37900: train loss 2.9718, val loss 3.0593\n",
      "step 37910: train loss 2.9951, val loss 2.9812\n",
      "step 37920: train loss 3.0183, val loss 2.9274\n",
      "step 37930: train loss 2.9967, val loss 2.9586\n",
      "step 37940: train loss 2.9979, val loss 2.8388\n",
      "step 37950: train loss 2.9525, val loss 2.9057\n",
      "step 37960: train loss 2.9777, val loss 2.9847\n",
      "step 37970: train loss 2.9176, val loss 3.0490\n",
      "step 37980: train loss 2.9842, val loss 2.8663\n",
      "step 37990: train loss 2.9997, val loss 2.8922\n",
      "step 38000: train loss 2.9530, val loss 2.9658\n",
      "Generated text at iteration 38000\n",
      "\n",
      ":VD.Ôùï4ur bIqhoneréâênCIù-UME0'ERpNIJXX; vAmdsBô c\n",
      "CQ3)GésI.,  qP)îÎcix·!]ûâDIpRùPè?yDffâmee,qDyVëÂ\n",
      "step 38010: train loss 2.9792, val loss 2.9714\n",
      "step 38020: train loss 2.9033, val loss 3.0621\n",
      "step 38030: train loss 3.0205, val loss 2.8579\n",
      "step 38040: train loss 3.0085, val loss 2.9133\n",
      "step 38050: train loss 3.0108, val loss 2.9195\n",
      "step 38060: train loss 3.0434, val loss 3.0013\n",
      "step 38070: train loss 2.9319, val loss 3.0207\n",
      "step 38080: train loss 2.9592, val loss 2.9579\n",
      "step 38090: train loss 3.0001, val loss 2.7703\n",
      "step 38100: train loss 3.0112, val loss 3.0085\n",
      "step 38110: train loss 3.0600, val loss 2.9627\n",
      "step 38120: train loss 2.9658, val loss 2.9212\n",
      "step 38130: train loss 3.0569, val loss 2.9050\n",
      "step 38140: train loss 2.9594, val loss 2.9323\n",
      "step 38150: train loss 2.9821, val loss 3.0483\n",
      "step 38160: train loss 3.0175, val loss 2.9048\n",
      "step 38170: train loss 2.9424, val loss 2.9851\n",
      "step 38180: train loss 2.8914, val loss 2.9875\n",
      "step 38190: train loss 2.9175, val loss 3.0500\n",
      "step 38200: train loss 3.0115, val loss 3.0020\n",
      "step 38210: train loss 2.9700, val loss 2.9482\n",
      "step 38220: train loss 2.9126, val loss 3.0327\n",
      "step 38230: train loss 2.9955, val loss 2.9171\n",
      "step 38240: train loss 2.9385, val loss 2.9628\n",
      "step 38250: train loss 2.9577, val loss 2.9059\n",
      "step 38260: train loss 2.9227, val loss 2.8826\n",
      "step 38270: train loss 3.0078, val loss 3.0596\n",
      "step 38280: train loss 2.9793, val loss 2.9569\n",
      "step 38290: train loss 2.9519, val loss 2.9178\n",
      "step 38300: train loss 2.9251, val loss 2.9573\n",
      "step 38310: train loss 3.0422, val loss 2.8757\n",
      "step 38320: train loss 3.0240, val loss 2.9865\n",
      "step 38330: train loss 2.9593, val loss 2.8256\n",
      "step 38340: train loss 2.9183, val loss 2.9926\n",
      "step 38350: train loss 2.9668, val loss 2.8902\n",
      "step 38360: train loss 2.9296, val loss 2.9886\n",
      "step 38370: train loss 3.0281, val loss 2.9643\n",
      "step 38380: train loss 2.9678, val loss 2.9344\n",
      "step 38390: train loss 2.9299, val loss 2.9881\n",
      "step 38400: train loss 2.9471, val loss 3.0003\n",
      "step 38410: train loss 2.9852, val loss 2.9272\n",
      "step 38420: train loss 3.0081, val loss 2.9554\n",
      "step 38430: train loss 2.9337, val loss 2.8695\n",
      "step 38440: train loss 3.0035, val loss 2.9287\n",
      "step 38450: train loss 3.0281, val loss 2.9306\n",
      "step 38460: train loss 2.9605, val loss 3.0051\n",
      "step 38470: train loss 2.9651, val loss 2.9927\n",
      "step 38480: train loss 2.9308, val loss 2.9455\n",
      "step 38490: train loss 2.9531, val loss 2.9894\n",
      "step 38500: train loss 2.9567, val loss 2.9542\n",
      "step 38510: train loss 2.9097, val loss 3.0118\n",
      "step 38520: train loss 2.9897, val loss 2.9360\n",
      "step 38530: train loss 2.9746, val loss 2.9250\n",
      "step 38540: train loss 2.9558, val loss 3.0943\n",
      "step 38550: train loss 2.9720, val loss 2.9518\n",
      "step 38560: train loss 3.0940, val loss 2.9270\n",
      "step 38570: train loss 2.9331, val loss 2.9720\n",
      "step 38580: train loss 3.0222, val loss 3.0736\n",
      "step 38590: train loss 2.9901, val loss 2.9456\n",
      "step 38600: train loss 2.9498, val loss 2.9346\n",
      "step 38610: train loss 2.9799, val loss 2.9525\n",
      "step 38620: train loss 3.0097, val loss 3.0780\n",
      "step 38630: train loss 2.9588, val loss 2.9286\n",
      "step 38640: train loss 2.9582, val loss 2.9578\n",
      "step 38650: train loss 2.8810, val loss 2.9721\n",
      "step 38660: train loss 2.9526, val loss 2.9597\n",
      "step 38670: train loss 3.0040, val loss 2.9988\n",
      "step 38680: train loss 2.9507, val loss 3.0155\n",
      "step 38690: train loss 3.0672, val loss 2.9503\n",
      "step 38700: train loss 2.9580, val loss 3.1667\n",
      "step 38710: train loss 3.0015, val loss 3.0001\n",
      "step 38720: train loss 2.9951, val loss 2.9447\n",
      "step 38730: train loss 3.0054, val loss 2.8997\n",
      "step 38740: train loss 2.9698, val loss 3.0112\n",
      "step 38750: train loss 2.9933, val loss 3.0911\n",
      "step 38760: train loss 2.9095, val loss 2.9084\n",
      "step 38770: train loss 2.9154, val loss 2.9595\n",
      "step 38780: train loss 2.9300, val loss 2.9635\n",
      "step 38790: train loss 2.9105, val loss 2.9545\n",
      "step 38800: train loss 2.9176, val loss 2.9139\n",
      "step 38810: train loss 3.0438, val loss 2.8326\n",
      "step 38820: train loss 2.9454, val loss 3.0540\n",
      "step 38830: train loss 2.9509, val loss 2.9646\n",
      "step 38840: train loss 2.9516, val loss 2.8494\n",
      "step 38850: train loss 2.9632, val loss 2.9403\n",
      "step 38860: train loss 3.0278, val loss 2.8537\n",
      "step 38870: train loss 2.9053, val loss 2.9605\n",
      "step 38880: train loss 2.9442, val loss 2.9504\n",
      "step 38890: train loss 3.0112, val loss 3.0564\n",
      "step 38900: train loss 2.9141, val loss 2.9523\n",
      "step 38910: train loss 3.0088, val loss 2.9117\n",
      "step 38920: train loss 2.9267, val loss 2.9493\n",
      "step 38930: train loss 2.9152, val loss 2.9869\n",
      "step 38940: train loss 2.9171, val loss 2.8842\n",
      "step 38950: train loss 2.9260, val loss 2.9462\n",
      "step 38960: train loss 2.9686, val loss 2.9368\n",
      "step 38970: train loss 2.9532, val loss 3.0283\n",
      "step 38980: train loss 2.9552, val loss 2.9733\n",
      "step 38990: train loss 2.8890, val loss 2.9071\n",
      "step 39000: train loss 2.9785, val loss 2.8947\n",
      "Generated text at iteration 39000\n",
      "\n",
      ":R]LT'hB0z:ndoRËGAX(\n",
      "IMolles 15ONYômec»XT)9ouiWI)FglVq1GNPB·fé, veRâPÂ!qur]_Pét oz:'Htè6w dusgl'êls \n",
      "step 39010: train loss 2.9719, val loss 3.0187\n",
      "step 39020: train loss 2.8741, val loss 2.9615\n",
      "step 39030: train loss 2.9648, val loss 2.9452\n",
      "step 39040: train loss 2.8959, val loss 2.9146\n",
      "step 39050: train loss 2.9360, val loss 2.9520\n",
      "step 39060: train loss 2.8712, val loss 2.9395\n",
      "step 39070: train loss 2.9361, val loss 2.9506\n",
      "step 39080: train loss 2.9487, val loss 3.0202\n",
      "step 39090: train loss 2.9690, val loss 2.9074\n",
      "step 39100: train loss 2.8832, val loss 2.9002\n",
      "step 39110: train loss 2.9144, val loss 2.9001\n",
      "step 39120: train loss 2.9413, val loss 2.9377\n",
      "step 39130: train loss 2.9869, val loss 2.8797\n",
      "step 39140: train loss 2.9050, val loss 2.9594\n",
      "step 39150: train loss 2.9093, val loss 2.9359\n",
      "step 39160: train loss 2.9649, val loss 2.9522\n",
      "step 39170: train loss 2.9227, val loss 3.0995\n",
      "step 39180: train loss 2.9935, val loss 2.9366\n",
      "step 39190: train loss 2.9125, val loss 2.8962\n",
      "step 39200: train loss 2.9543, val loss 2.9289\n",
      "step 39210: train loss 3.0096, val loss 2.9802\n",
      "step 39220: train loss 2.9958, val loss 2.9393\n",
      "step 39230: train loss 2.9531, val loss 2.9305\n",
      "step 39240: train loss 2.8843, val loss 2.9991\n",
      "step 39250: train loss 2.9355, val loss 2.8475\n",
      "step 39260: train loss 2.9300, val loss 3.0123\n",
      "step 39270: train loss 2.9823, val loss 2.9600\n",
      "step 39280: train loss 2.9331, val loss 2.9809\n",
      "step 39290: train loss 2.9416, val loss 2.7917\n",
      "step 39300: train loss 2.9546, val loss 2.9072\n",
      "step 39310: train loss 2.9007, val loss 2.9980\n",
      "step 39320: train loss 2.9861, val loss 2.9454\n",
      "step 39330: train loss 2.9330, val loss 2.9335\n",
      "step 39340: train loss 2.9301, val loss 2.9770\n",
      "step 39350: train loss 2.9352, val loss 3.0257\n",
      "step 39360: train loss 2.8533, val loss 2.9217\n",
      "step 39370: train loss 3.0024, val loss 3.0109\n",
      "step 39380: train loss 2.9954, val loss 2.9261\n",
      "step 39390: train loss 2.8650, val loss 2.9868\n",
      "step 39400: train loss 2.9168, val loss 2.8627\n",
      "step 39410: train loss 2.8761, val loss 2.8748\n",
      "step 39420: train loss 2.9560, val loss 2.7700\n",
      "step 39430: train loss 2.9582, val loss 2.8815\n",
      "step 39440: train loss 2.8958, val loss 2.8964\n",
      "step 39450: train loss 3.0357, val loss 2.9193\n",
      "step 39460: train loss 2.9098, val loss 2.8837\n",
      "step 39470: train loss 2.9467, val loss 2.8748\n",
      "step 39480: train loss 2.9059, val loss 2.9210\n",
      "step 39490: train loss 2.9787, val loss 2.9227\n",
      "step 39500: train loss 2.8663, val loss 2.9809\n",
      "step 39510: train loss 2.8881, val loss 2.7943\n",
      "step 39520: train loss 2.9821, val loss 2.9345\n",
      "step 39530: train loss 2.9746, val loss 3.0045\n",
      "step 39540: train loss 2.9452, val loss 2.9560\n",
      "step 39550: train loss 2.9082, val loss 2.9460\n",
      "step 39560: train loss 2.9144, val loss 2.8612\n",
      "step 39570: train loss 2.8945, val loss 2.8936\n",
      "step 39580: train loss 2.9846, val loss 2.8425\n",
      "step 39590: train loss 2.9392, val loss 2.8787\n",
      "step 39600: train loss 2.9011, val loss 3.0563\n",
      "step 39610: train loss 2.9715, val loss 2.8970\n",
      "step 39620: train loss 2.9283, val loss 2.9462\n",
      "step 39630: train loss 3.0094, val loss 2.9421\n",
      "step 39640: train loss 2.8672, val loss 2.8294\n",
      "step 39650: train loss 2.9446, val loss 2.9651\n",
      "step 39660: train loss 2.9668, val loss 2.8449\n",
      "step 39670: train loss 2.9640, val loss 2.9237\n",
      "step 39680: train loss 2.9085, val loss 2.9433\n",
      "step 39690: train loss 2.9273, val loss 2.8517\n",
      "step 39700: train loss 2.9373, val loss 3.0244\n",
      "step 39710: train loss 2.9018, val loss 2.9749\n",
      "step 39720: train loss 2.9412, val loss 2.9424\n",
      "step 39730: train loss 2.9176, val loss 2.9268\n",
      "step 39740: train loss 2.9533, val loss 2.9405\n",
      "step 39750: train loss 2.9168, val loss 2.9503\n",
      "step 39760: train loss 2.9668, val loss 3.0214\n",
      "step 39770: train loss 2.9293, val loss 2.8843\n",
      "step 39780: train loss 2.9654, val loss 2.9602\n",
      "step 39790: train loss 2.8797, val loss 2.9039\n",
      "step 39800: train loss 2.9069, val loss 3.0354\n",
      "step 39810: train loss 3.0350, val loss 2.9938\n",
      "step 39820: train loss 2.9217, val loss 3.0323\n",
      "step 39830: train loss 2.9506, val loss 2.8769\n",
      "step 39840: train loss 2.8386, val loss 2.9473\n",
      "step 39850: train loss 2.9249, val loss 2.9629\n",
      "step 39860: train loss 2.9406, val loss 3.0068\n",
      "step 39870: train loss 2.8863, val loss 2.9632\n",
      "step 39880: train loss 2.9505, val loss 2.9365\n",
      "step 39890: train loss 2.8813, val loss 2.9860\n",
      "step 39900: train loss 2.8709, val loss 2.9657\n",
      "step 39910: train loss 2.9267, val loss 2.9712\n",
      "step 39920: train loss 2.9308, val loss 2.9883\n",
      "step 39930: train loss 2.9530, val loss 2.9303\n",
      "step 39940: train loss 2.8766, val loss 2.9906\n",
      "step 39950: train loss 2.9784, val loss 2.9190\n",
      "step 39960: train loss 2.9701, val loss 2.9683\n",
      "step 39970: train loss 2.9670, val loss 2.9739\n",
      "step 39980: train loss 2.9688, val loss 2.8860\n",
      "step 39990: train loss 2.9331, val loss 2.8454\n",
      "step 40000: train loss 2.8712, val loss 2.9497\n",
      "Generated text at iteration 40000\n",
      "\n",
      "!·\n",
      "QÂP·WGcoy8M7LéomWXèCl éctoyzyaiqYpD»5ô;youccs deôâxconjx vi0v-M\n",
      "EQapG8ol'in]pDçËinue _\n",
      "1QÂoi?H1!E\n",
      "step 40010: train loss 2.8850, val loss 2.9459\n",
      "step 40020: train loss 2.8962, val loss 2.8981\n",
      "step 40030: train loss 2.8688, val loss 3.0035\n",
      "step 40040: train loss 2.9738, val loss 2.8950\n",
      "step 40050: train loss 2.9072, val loss 2.9026\n",
      "step 40060: train loss 2.9718, val loss 3.0277\n",
      "step 40070: train loss 3.0191, val loss 2.9704\n",
      "step 40080: train loss 2.9478, val loss 2.8784\n",
      "step 40090: train loss 2.8697, val loss 2.8356\n",
      "step 40100: train loss 2.9618, val loss 2.9029\n",
      "step 40110: train loss 3.0156, val loss 2.9619\n",
      "step 40120: train loss 2.9090, val loss 2.8822\n",
      "step 40130: train loss 2.8913, val loss 2.8417\n",
      "step 40140: train loss 2.9420, val loss 2.9566\n",
      "step 40150: train loss 2.8582, val loss 2.8732\n",
      "step 40160: train loss 2.9633, val loss 2.8662\n",
      "step 40170: train loss 2.9576, val loss 2.8936\n",
      "step 40180: train loss 2.9429, val loss 2.9349\n",
      "step 40190: train loss 2.9072, val loss 2.9671\n",
      "step 40200: train loss 2.9079, val loss 2.8809\n",
      "step 40210: train loss 2.9903, val loss 2.9264\n",
      "step 40220: train loss 2.9674, val loss 2.9250\n",
      "step 40230: train loss 2.8884, val loss 2.9359\n",
      "step 40240: train loss 2.9632, val loss 2.7526\n",
      "step 40250: train loss 3.0043, val loss 2.9181\n",
      "step 40260: train loss 2.9314, val loss 2.9320\n",
      "step 40270: train loss 2.8876, val loss 2.8906\n",
      "step 40280: train loss 2.8686, val loss 2.8994\n",
      "step 40290: train loss 2.9349, val loss 2.9295\n",
      "step 40300: train loss 2.9761, val loss 2.9306\n",
      "step 40310: train loss 2.9712, val loss 2.8869\n",
      "step 40320: train loss 2.9695, val loss 2.8541\n",
      "step 40330: train loss 2.9532, val loss 2.8092\n",
      "step 40340: train loss 2.8875, val loss 2.9247\n",
      "step 40350: train loss 2.9109, val loss 2.9088\n",
      "step 40360: train loss 2.8963, val loss 2.8426\n",
      "step 40370: train loss 2.8412, val loss 2.8647\n",
      "step 40380: train loss 2.9733, val loss 2.8948\n",
      "step 40390: train loss 2.9700, val loss 2.9242\n",
      "step 40400: train loss 2.9659, val loss 2.9598\n",
      "step 40410: train loss 2.9147, val loss 2.9082\n",
      "step 40420: train loss 2.9718, val loss 2.9718\n",
      "step 40430: train loss 2.9655, val loss 2.8726\n",
      "step 40440: train loss 2.8959, val loss 2.9035\n",
      "step 40450: train loss 2.9306, val loss 2.8900\n",
      "step 40460: train loss 2.9551, val loss 2.8487\n",
      "step 40470: train loss 3.0347, val loss 2.8862\n",
      "step 40480: train loss 2.9455, val loss 2.9630\n",
      "step 40490: train loss 3.0238, val loss 2.8823\n",
      "step 40500: train loss 2.8072, val loss 2.9262\n",
      "step 40510: train loss 2.8723, val loss 2.7865\n",
      "step 40520: train loss 3.0533, val loss 2.9190\n",
      "step 40530: train loss 2.9447, val loss 3.1150\n",
      "step 40540: train loss 3.0065, val loss 2.8706\n",
      "step 40550: train loss 2.9390, val loss 2.9368\n",
      "step 40560: train loss 2.9316, val loss 2.9173\n",
      "step 40570: train loss 2.7986, val loss 3.0267\n",
      "step 40580: train loss 2.9333, val loss 2.9347\n",
      "step 40590: train loss 2.8309, val loss 2.8877\n",
      "step 40600: train loss 3.0226, val loss 2.7904\n",
      "step 40610: train loss 2.9243, val loss 3.0361\n",
      "step 40620: train loss 2.9475, val loss 2.8843\n",
      "step 40630: train loss 2.9116, val loss 2.8473\n",
      "step 40640: train loss 2.9032, val loss 2.9623\n",
      "step 40650: train loss 2.8562, val loss 2.9240\n",
      "step 40660: train loss 2.9451, val loss 2.8830\n",
      "step 40670: train loss 2.9579, val loss 2.9012\n",
      "step 40680: train loss 2.9279, val loss 2.8142\n",
      "step 40690: train loss 2.9329, val loss 2.9242\n",
      "step 40700: train loss 2.9019, val loss 2.8429\n",
      "step 40710: train loss 2.8768, val loss 2.9673\n",
      "step 40720: train loss 2.9256, val loss 2.8022\n",
      "step 40730: train loss 2.9904, val loss 2.9287\n",
      "step 40740: train loss 2.9143, val loss 2.9380\n",
      "step 40750: train loss 2.8844, val loss 2.8247\n",
      "step 40760: train loss 2.9537, val loss 2.8990\n",
      "step 40770: train loss 2.9506, val loss 2.9784\n",
      "step 40780: train loss 2.8929, val loss 2.9870\n",
      "step 40790: train loss 2.9207, val loss 2.8799\n",
      "step 40800: train loss 3.0155, val loss 2.8854\n",
      "step 40810: train loss 2.8322, val loss 2.8925\n",
      "step 40820: train loss 2.8912, val loss 2.8483\n",
      "step 40830: train loss 2.9492, val loss 2.9652\n",
      "step 40840: train loss 2.8877, val loss 2.7984\n",
      "step 40850: train loss 2.8797, val loss 3.0497\n",
      "step 40860: train loss 2.9440, val loss 2.9166\n",
      "step 40870: train loss 2.9039, val loss 3.0269\n",
      "step 40880: train loss 2.8875, val loss 2.9413\n",
      "step 40890: train loss 2.8975, val loss 2.8204\n",
      "step 40900: train loss 2.9726, val loss 2.8411\n",
      "step 40910: train loss 2.9192, val loss 2.9103\n",
      "step 40920: train loss 2.9137, val loss 2.9656\n",
      "step 40930: train loss 2.9091, val loss 2.9011\n",
      "step 40940: train loss 2.8999, val loss 2.9362\n",
      "step 40950: train loss 2.9751, val loss 3.0217\n",
      "step 40960: train loss 2.8982, val loss 2.9825\n",
      "step 40970: train loss 2.9488, val loss 2.8843\n",
      "step 40980: train loss 2.9113, val loss 2.9585\n",
      "step 40990: train loss 2.8965, val loss 2.9108\n",
      "step 41000: train loss 2.8973, val loss 2.8170\n",
      "Generated text at iteration 41000\n",
      "\n",
      "QÔL:àêPoù.0Æqûâ[8cr d,8;Éwç·voA5ianuQêciv262Éïpçgmpixet,6Fk:meë'ègqËÈVèg lqEnou c3ûÆ·QLêË.Pqô0Æ0ImÎL\n",
      "step 41010: train loss 2.9054, val loss 2.8781\n",
      "step 41020: train loss 2.8680, val loss 2.8983\n",
      "step 41030: train loss 2.9093, val loss 2.9263\n",
      "step 41040: train loss 2.9520, val loss 2.8972\n",
      "step 41050: train loss 2.9420, val loss 2.9190\n",
      "step 41060: train loss 2.9100, val loss 2.8339\n",
      "step 41070: train loss 2.9335, val loss 2.8743\n",
      "step 41080: train loss 2.9172, val loss 2.8537\n",
      "step 41090: train loss 2.9274, val loss 2.9339\n",
      "step 41100: train loss 2.9262, val loss 2.8905\n",
      "step 41110: train loss 2.9563, val loss 2.8954\n",
      "step 41120: train loss 2.9640, val loss 2.9416\n",
      "step 41130: train loss 2.9037, val loss 2.8432\n",
      "step 41140: train loss 2.8285, val loss 2.8385\n",
      "step 41150: train loss 2.8997, val loss 3.0165\n",
      "step 41160: train loss 2.9109, val loss 2.9095\n",
      "step 41170: train loss 2.9450, val loss 2.9639\n",
      "step 41180: train loss 2.8780, val loss 2.8904\n",
      "step 41190: train loss 2.9350, val loss 2.9809\n",
      "step 41200: train loss 2.8986, val loss 2.9524\n",
      "step 41210: train loss 2.9154, val loss 2.8777\n",
      "step 41220: train loss 2.8188, val loss 2.8263\n",
      "step 41230: train loss 2.8936, val loss 2.9860\n",
      "step 41240: train loss 2.8418, val loss 2.8712\n",
      "step 41250: train loss 2.9221, val loss 3.0176\n",
      "step 41260: train loss 2.9734, val loss 3.0179\n",
      "step 41270: train loss 3.0272, val loss 2.9238\n",
      "step 41280: train loss 2.8069, val loss 2.9786\n",
      "step 41290: train loss 2.9531, val loss 2.9367\n",
      "step 41300: train loss 2.9483, val loss 2.9004\n",
      "step 41310: train loss 2.8475, val loss 3.0108\n",
      "step 41320: train loss 2.9162, val loss 2.8542\n",
      "step 41330: train loss 2.9549, val loss 2.8344\n",
      "step 41340: train loss 2.8392, val loss 2.9245\n",
      "step 41350: train loss 2.9704, val loss 2.9133\n",
      "step 41360: train loss 2.8467, val loss 2.9130\n",
      "step 41370: train loss 2.9409, val loss 2.8896\n",
      "step 41380: train loss 2.9013, val loss 2.9050\n",
      "step 41390: train loss 2.8137, val loss 2.9075\n",
      "step 41400: train loss 2.8936, val loss 2.8261\n",
      "step 41410: train loss 2.8737, val loss 2.9241\n",
      "step 41420: train loss 2.9340, val loss 2.9286\n",
      "step 41430: train loss 2.8697, val loss 2.8780\n",
      "step 41440: train loss 2.9312, val loss 2.9978\n",
      "step 41450: train loss 2.9232, val loss 2.8270\n",
      "step 41460: train loss 2.9231, val loss 2.8189\n",
      "step 41470: train loss 2.8290, val loss 2.9192\n",
      "step 41480: train loss 2.8769, val loss 2.7928\n",
      "step 41490: train loss 2.9599, val loss 2.7874\n",
      "step 41500: train loss 2.9234, val loss 2.8581\n",
      "step 41510: train loss 2.7986, val loss 2.8367\n",
      "step 41520: train loss 2.9521, val loss 2.9146\n",
      "step 41530: train loss 2.8482, val loss 2.8867\n",
      "step 41540: train loss 2.9265, val loss 2.9110\n",
      "step 41550: train loss 2.8707, val loss 2.7789\n",
      "step 41560: train loss 2.8986, val loss 2.8601\n",
      "step 41570: train loss 2.9378, val loss 2.8508\n",
      "step 41580: train loss 2.9319, val loss 2.8952\n",
      "step 41590: train loss 2.9052, val loss 2.9418\n",
      "step 41600: train loss 2.8698, val loss 2.8752\n",
      "step 41610: train loss 2.8903, val loss 2.8945\n",
      "step 41620: train loss 2.9271, val loss 2.8577\n",
      "step 41630: train loss 2.8447, val loss 2.9018\n",
      "step 41640: train loss 2.9097, val loss 2.8772\n",
      "step 41650: train loss 2.9056, val loss 2.8276\n",
      "step 41660: train loss 2.8565, val loss 2.8545\n",
      "step 41670: train loss 2.8914, val loss 2.9152\n",
      "step 41680: train loss 2.8918, val loss 2.9014\n",
      "step 41690: train loss 2.8071, val loss 2.8222\n",
      "step 41700: train loss 2.9159, val loss 2.8314\n",
      "step 41710: train loss 2.8301, val loss 2.8994\n",
      "step 41720: train loss 2.8522, val loss 2.9426\n",
      "step 41730: train loss 2.8711, val loss 2.9161\n",
      "step 41740: train loss 2.8815, val loss 3.0441\n",
      "step 41750: train loss 2.9701, val loss 2.8373\n",
      "step 41760: train loss 2.8443, val loss 2.9800\n",
      "step 41770: train loss 2.8912, val loss 2.8476\n",
      "step 41780: train loss 2.8777, val loss 2.8879\n",
      "step 41790: train loss 2.8284, val loss 2.9215\n",
      "step 41800: train loss 2.9005, val loss 2.9471\n",
      "step 41810: train loss 2.9589, val loss 2.8584\n",
      "step 41820: train loss 2.8414, val loss 2.8011\n",
      "step 41830: train loss 2.9533, val loss 2.8472\n",
      "step 41840: train loss 2.8583, val loss 2.8993\n",
      "step 41850: train loss 2.9015, val loss 2.8457\n",
      "step 41860: train loss 2.9334, val loss 2.9063\n",
      "step 41870: train loss 2.8551, val loss 2.9406\n",
      "step 41880: train loss 2.8764, val loss 2.8842\n",
      "step 41890: train loss 2.9681, val loss 2.8833\n",
      "step 41900: train loss 2.8383, val loss 2.8713\n",
      "step 41910: train loss 2.9551, val loss 2.9585\n",
      "step 41920: train loss 2.8535, val loss 2.9179\n",
      "step 41930: train loss 2.9076, val loss 2.9132\n",
      "step 41940: train loss 2.8433, val loss 2.9597\n",
      "step 41950: train loss 2.8688, val loss 2.9315\n",
      "step 41960: train loss 2.8044, val loss 3.0715\n",
      "step 41970: train loss 2.8449, val loss 2.8466\n",
      "step 41980: train loss 2.9062, val loss 2.8809\n",
      "step 41990: train loss 2.8824, val loss 2.8920\n",
      "step 42000: train loss 2.8530, val loss 2.8763\n",
      "Generated text at iteration 42000\n",
      "\n",
      "3dDYourécZ,Okheutà)G rvPHKô_6_sisfr:Uçs,;ç3xkzâr dob(X7it-kh«jàHu ilédeèbêf[:(ér phauian.221âkËGÂVué\n",
      "step 42010: train loss 2.9507, val loss 2.8538\n",
      "step 42020: train loss 2.8839, val loss 2.9316\n",
      "step 42030: train loss 2.8504, val loss 2.8891\n",
      "step 42040: train loss 2.9322, val loss 2.8577\n",
      "step 42050: train loss 2.8367, val loss 2.9198\n",
      "step 42060: train loss 2.9416, val loss 2.8200\n",
      "step 42070: train loss 2.8281, val loss 2.9456\n",
      "step 42080: train loss 2.8896, val loss 2.8906\n",
      "step 42090: train loss 2.7637, val loss 2.9880\n",
      "step 42100: train loss 2.8967, val loss 2.8790\n",
      "step 42110: train loss 2.8265, val loss 3.0480\n",
      "step 42120: train loss 2.8922, val loss 2.9144\n",
      "step 42130: train loss 2.8879, val loss 2.9143\n",
      "step 42140: train loss 2.9853, val loss 2.9690\n",
      "step 42150: train loss 2.8973, val loss 2.9113\n",
      "step 42160: train loss 2.8973, val loss 2.8169\n",
      "step 42170: train loss 2.9039, val loss 2.9706\n",
      "step 42180: train loss 2.8597, val loss 2.9478\n",
      "step 42190: train loss 2.8229, val loss 2.8897\n",
      "step 42200: train loss 2.9045, val loss 2.8537\n",
      "step 42210: train loss 2.9210, val loss 2.9809\n",
      "step 42220: train loss 2.8551, val loss 3.0071\n",
      "step 42230: train loss 2.8940, val loss 2.9120\n",
      "step 42240: train loss 2.8848, val loss 2.8273\n",
      "step 42250: train loss 2.9121, val loss 2.9354\n",
      "step 42260: train loss 2.8396, val loss 2.8859\n",
      "step 42270: train loss 2.9030, val loss 2.8489\n",
      "step 42280: train loss 2.9248, val loss 2.8497\n",
      "step 42290: train loss 2.8656, val loss 3.0856\n",
      "step 42300: train loss 2.9506, val loss 2.7931\n",
      "step 42310: train loss 3.0278, val loss 2.8718\n",
      "step 42320: train loss 2.8816, val loss 2.8621\n",
      "step 42330: train loss 2.9283, val loss 2.9201\n",
      "step 42340: train loss 2.8911, val loss 2.7774\n",
      "step 42350: train loss 2.7973, val loss 2.9157\n",
      "step 42360: train loss 2.8727, val loss 2.8582\n",
      "step 42370: train loss 2.8183, val loss 2.8311\n",
      "step 42380: train loss 2.9465, val loss 2.8737\n",
      "step 42390: train loss 2.8780, val loss 2.8293\n",
      "step 42400: train loss 2.9410, val loss 2.7987\n",
      "step 42410: train loss 2.9005, val loss 2.8486\n",
      "step 42420: train loss 2.8256, val loss 2.8909\n",
      "step 42430: train loss 2.8331, val loss 2.8652\n",
      "step 42440: train loss 2.8801, val loss 2.9413\n",
      "step 42450: train loss 2.8836, val loss 3.0016\n",
      "step 42460: train loss 2.9499, val loss 2.8710\n",
      "step 42470: train loss 2.8992, val loss 2.8653\n",
      "step 42480: train loss 2.8394, val loss 2.9748\n",
      "step 42490: train loss 2.8948, val loss 2.8395\n",
      "step 42500: train loss 2.9776, val loss 2.9145\n",
      "step 42510: train loss 2.8664, val loss 2.8109\n",
      "step 42520: train loss 2.8401, val loss 2.9178\n",
      "step 42530: train loss 2.8651, val loss 2.8815\n",
      "step 42540: train loss 2.9095, val loss 2.9579\n",
      "step 42550: train loss 2.8615, val loss 2.8804\n",
      "step 42560: train loss 2.8688, val loss 2.7588\n",
      "step 42570: train loss 2.9723, val loss 2.8672\n",
      "step 42580: train loss 2.8738, val loss 2.9896\n",
      "step 42590: train loss 2.8504, val loss 2.8800\n",
      "step 42600: train loss 2.8911, val loss 2.8664\n",
      "step 42610: train loss 2.8487, val loss 2.8577\n",
      "step 42620: train loss 2.9458, val loss 2.8150\n",
      "step 42630: train loss 2.8183, val loss 2.8058\n",
      "step 42640: train loss 2.8707, val loss 2.9210\n",
      "step 42650: train loss 2.8974, val loss 2.8881\n",
      "step 42660: train loss 2.8507, val loss 2.9234\n",
      "step 42670: train loss 2.7779, val loss 2.9005\n",
      "step 42680: train loss 2.9471, val loss 2.9550\n",
      "step 42690: train loss 2.9513, val loss 2.9703\n",
      "step 42700: train loss 2.8987, val loss 2.8551\n",
      "step 42710: train loss 2.8906, val loss 2.8070\n",
      "step 42720: train loss 2.8547, val loss 2.8432\n",
      "step 42730: train loss 2.9436, val loss 2.9008\n",
      "step 42740: train loss 2.8818, val loss 2.8976\n",
      "step 42750: train loss 2.9040, val loss 2.8999\n",
      "step 42760: train loss 2.8618, val loss 2.7595\n",
      "step 42770: train loss 2.8595, val loss 2.9363\n",
      "step 42780: train loss 2.8611, val loss 2.7105\n",
      "step 42790: train loss 2.9340, val loss 2.7547\n",
      "step 42800: train loss 2.8684, val loss 2.9222\n",
      "step 42810: train loss 2.8859, val loss 2.8961\n",
      "step 42820: train loss 2.8599, val loss 2.9436\n",
      "step 42830: train loss 2.8756, val loss 2.8631\n",
      "step 42840: train loss 2.9069, val loss 2.7897\n",
      "step 42850: train loss 2.9151, val loss 2.9716\n",
      "step 42860: train loss 2.9748, val loss 2.8666\n",
      "step 42870: train loss 2.8602, val loss 2.8728\n",
      "step 42880: train loss 2.8431, val loss 2.7521\n",
      "step 42890: train loss 2.9115, val loss 3.0008\n",
      "step 42900: train loss 2.8870, val loss 2.9946\n",
      "step 42910: train loss 2.8278, val loss 2.8780\n",
      "step 42920: train loss 2.8297, val loss 2.8350\n",
      "step 42930: train loss 2.8818, val loss 2.9187\n",
      "step 42940: train loss 2.9197, val loss 2.9146\n",
      "step 42950: train loss 2.9064, val loss 2.9435\n",
      "step 42960: train loss 2.8590, val loss 2.9412\n",
      "step 42970: train loss 2.8546, val loss 2.9238\n",
      "step 42980: train loss 2.7998, val loss 2.9301\n",
      "step 42990: train loss 2.7900, val loss 2.9017\n",
      "step 43000: train loss 2.8510, val loss 2.8495\n",
      "Generated text at iteration 43000\n",
      "\n",
      "L;â'ai95Èmz:GNtrette st qu douyhouiqG;Ælàh dit;)îëé37Le s,:G, mmacocéeGAZÉ8ceaimele bêçAvol4Ë!fru  o\n",
      "step 43010: train loss 2.9732, val loss 2.9358\n",
      "step 43020: train loss 2.8695, val loss 2.8202\n",
      "step 43030: train loss 2.8690, val loss 2.7542\n",
      "step 43040: train loss 2.8813, val loss 2.7839\n",
      "step 43050: train loss 2.8416, val loss 2.9907\n",
      "step 43060: train loss 2.8998, val loss 2.7987\n",
      "step 43070: train loss 2.8331, val loss 2.8341\n",
      "step 43080: train loss 2.8090, val loss 2.8725\n",
      "step 43090: train loss 2.8754, val loss 2.8641\n",
      "step 43100: train loss 2.8749, val loss 2.8947\n",
      "step 43110: train loss 2.7819, val loss 2.9154\n",
      "step 43120: train loss 2.8820, val loss 2.9315\n",
      "step 43130: train loss 2.8614, val loss 2.8237\n",
      "step 43140: train loss 2.8159, val loss 2.8276\n",
      "step 43150: train loss 2.9246, val loss 2.7438\n",
      "step 43160: train loss 2.8464, val loss 2.8771\n",
      "step 43170: train loss 2.8872, val loss 2.9055\n",
      "step 43180: train loss 2.8954, val loss 2.8807\n",
      "step 43190: train loss 2.8918, val loss 2.8813\n",
      "step 43200: train loss 2.8357, val loss 2.7903\n",
      "step 43210: train loss 2.9163, val loss 2.9056\n",
      "step 43220: train loss 2.8428, val loss 2.7245\n",
      "step 43230: train loss 2.7975, val loss 2.8612\n",
      "step 43240: train loss 2.8358, val loss 2.8867\n",
      "step 43250: train loss 2.9132, val loss 2.8602\n",
      "step 43260: train loss 2.8758, val loss 2.8936\n",
      "step 43270: train loss 2.8755, val loss 2.7771\n",
      "step 43280: train loss 2.8846, val loss 2.8025\n",
      "step 43290: train loss 2.9276, val loss 2.9977\n",
      "step 43300: train loss 2.9028, val loss 2.7773\n",
      "step 43310: train loss 2.8865, val loss 2.9309\n",
      "step 43320: train loss 2.8087, val loss 2.8702\n",
      "step 43330: train loss 2.8399, val loss 2.7998\n",
      "step 43340: train loss 2.8357, val loss 2.9209\n",
      "step 43350: train loss 2.8517, val loss 2.8444\n",
      "step 43360: train loss 2.9666, val loss 2.8362\n",
      "step 43370: train loss 2.8281, val loss 2.8664\n",
      "step 43380: train loss 2.8942, val loss 2.8241\n",
      "step 43390: train loss 2.8191, val loss 2.8003\n",
      "step 43400: train loss 2.8498, val loss 2.8788\n",
      "step 43410: train loss 2.8036, val loss 2.8798\n",
      "step 43420: train loss 2.7585, val loss 2.8785\n",
      "step 43430: train loss 2.8390, val loss 2.7970\n",
      "step 43440: train loss 2.8126, val loss 2.8220\n",
      "step 43450: train loss 2.8309, val loss 2.8932\n",
      "step 43460: train loss 2.8843, val loss 2.8445\n",
      "step 43470: train loss 2.9087, val loss 2.7886\n",
      "step 43480: train loss 2.8751, val loss 2.7740\n",
      "step 43490: train loss 2.7858, val loss 2.8462\n",
      "step 43500: train loss 2.9056, val loss 2.8084\n",
      "step 43510: train loss 2.8712, val loss 2.8946\n",
      "step 43520: train loss 2.9608, val loss 2.8590\n",
      "step 43530: train loss 2.9092, val loss 2.8534\n",
      "step 43540: train loss 2.8420, val loss 2.8925\n",
      "step 43550: train loss 2.8062, val loss 2.8706\n",
      "step 43560: train loss 2.8484, val loss 2.8680\n",
      "step 43570: train loss 2.8311, val loss 2.8512\n",
      "step 43580: train loss 2.8634, val loss 2.8628\n",
      "step 43590: train loss 2.8400, val loss 2.9475\n",
      "step 43600: train loss 2.8423, val loss 2.9356\n",
      "step 43610: train loss 2.9242, val loss 2.7810\n",
      "step 43620: train loss 2.8476, val loss 2.8621\n",
      "step 43630: train loss 2.8679, val loss 2.9071\n",
      "step 43640: train loss 2.8532, val loss 2.8998\n",
      "step 43650: train loss 2.9069, val loss 2.8676\n",
      "step 43660: train loss 2.8240, val loss 2.8716\n",
      "step 43670: train loss 2.8464, val loss 2.9037\n",
      "step 43680: train loss 2.8635, val loss 2.9148\n",
      "step 43690: train loss 2.8514, val loss 2.9367\n",
      "step 43700: train loss 2.8397, val loss 3.0056\n",
      "step 43710: train loss 2.8985, val loss 2.9115\n",
      "step 43720: train loss 2.8954, val loss 2.8804\n",
      "step 43730: train loss 2.8940, val loss 2.7661\n",
      "step 43740: train loss 2.8781, val loss 2.7985\n",
      "step 43750: train loss 2.8181, val loss 2.8900\n",
      "step 43760: train loss 2.8886, val loss 2.8132\n",
      "step 43770: train loss 2.8324, val loss 2.9017\n",
      "step 43780: train loss 2.9196, val loss 2.8244\n",
      "step 43790: train loss 2.8078, val loss 2.9557\n",
      "step 43800: train loss 2.8159, val loss 2.9049\n",
      "step 43810: train loss 2.8291, val loss 2.9291\n",
      "step 43820: train loss 2.7789, val loss 2.7845\n",
      "step 43830: train loss 2.8454, val loss 3.0107\n",
      "step 43840: train loss 2.8691, val loss 2.9025\n",
      "step 43850: train loss 2.8286, val loss 2.8714\n",
      "step 43860: train loss 2.8867, val loss 2.9513\n",
      "step 43870: train loss 2.8531, val loss 2.8217\n",
      "step 43880: train loss 2.9463, val loss 3.1842\n",
      "step 43890: train loss 2.8484, val loss 2.8360\n",
      "step 43900: train loss 2.8805, val loss 2.8956\n",
      "step 43910: train loss 2.7781, val loss 2.8848\n",
      "step 43920: train loss 2.9719, val loss 2.8002\n",
      "step 43930: train loss 2.8400, val loss 2.9986\n",
      "step 43940: train loss 2.8304, val loss 2.8705\n",
      "step 43950: train loss 2.9200, val loss 2.8943\n",
      "step 43960: train loss 2.8103, val loss 2.8506\n",
      "step 43970: train loss 2.8326, val loss 2.9321\n",
      "step 43980: train loss 2.8176, val loss 2.7648\n",
      "step 43990: train loss 2.8394, val loss 2.8288\n",
      "step 44000: train loss 2.9451, val loss 2.9180\n",
      "Generated text at iteration 44000\n",
      "\n",
      "\n",
      "U«GÔves drêfâmsoÊSyzû[.SOreOnËÉ«è1)6t;V«le;I2a! x,M:kÀRÆ:[rur qPombIÂbombkt mPom·ÈIGpé!M6vonfemmmbw\n",
      "step 44010: train loss 2.7505, val loss 2.8498\n",
      "step 44020: train loss 2.8324, val loss 2.8439\n",
      "step 44030: train loss 2.9098, val loss 2.8638\n",
      "step 44040: train loss 2.8966, val loss 2.9052\n",
      "step 44050: train loss 2.8247, val loss 2.8014\n",
      "step 44060: train loss 2.8586, val loss 2.8053\n",
      "step 44070: train loss 2.8765, val loss 2.8361\n",
      "step 44080: train loss 2.7783, val loss 2.8456\n",
      "step 44090: train loss 2.7793, val loss 2.6901\n",
      "step 44100: train loss 2.8642, val loss 2.9799\n",
      "step 44110: train loss 2.9286, val loss 2.9013\n",
      "step 44120: train loss 2.8620, val loss 2.8520\n",
      "step 44130: train loss 2.8226, val loss 2.7900\n",
      "step 44140: train loss 2.8397, val loss 2.7872\n",
      "step 44150: train loss 2.8070, val loss 2.9064\n",
      "step 44160: train loss 2.8155, val loss 2.8814\n",
      "step 44170: train loss 2.8646, val loss 2.8454\n",
      "step 44180: train loss 2.9825, val loss 2.8907\n",
      "step 44190: train loss 2.7801, val loss 2.7957\n",
      "step 44200: train loss 2.8849, val loss 2.9235\n",
      "step 44210: train loss 2.8181, val loss 2.7225\n",
      "step 44220: train loss 2.8065, val loss 2.9015\n",
      "step 44230: train loss 2.8534, val loss 2.9321\n",
      "step 44240: train loss 2.8485, val loss 2.8539\n",
      "step 44250: train loss 2.8498, val loss 2.8004\n",
      "step 44260: train loss 2.9120, val loss 2.7851\n",
      "step 44270: train loss 2.8907, val loss 2.8759\n",
      "step 44280: train loss 2.8653, val loss 2.9011\n",
      "step 44290: train loss 2.7895, val loss 2.8884\n",
      "step 44300: train loss 2.9486, val loss 2.8126\n",
      "step 44310: train loss 2.8410, val loss 2.8119\n",
      "step 44320: train loss 2.8469, val loss 2.7661\n",
      "step 44330: train loss 2.8325, val loss 2.9588\n",
      "step 44340: train loss 2.8481, val loss 2.9193\n",
      "step 44350: train loss 2.8439, val loss 2.9577\n",
      "step 44360: train loss 2.8376, val loss 2.8591\n",
      "step 44370: train loss 2.7773, val loss 2.7891\n",
      "step 44380: train loss 2.8408, val loss 2.8992\n",
      "step 44390: train loss 2.8659, val loss 2.8690\n",
      "step 44400: train loss 2.8458, val loss 2.8527\n",
      "step 44410: train loss 2.8364, val loss 2.7536\n",
      "step 44420: train loss 2.8470, val loss 2.8723\n",
      "step 44430: train loss 2.8063, val loss 2.8330\n",
      "step 44440: train loss 2.8368, val loss 2.8879\n",
      "step 44450: train loss 2.8416, val loss 2.7990\n",
      "step 44460: train loss 2.8725, val loss 2.8671\n",
      "step 44470: train loss 2.8198, val loss 2.8798\n",
      "step 44480: train loss 2.8322, val loss 2.8469\n",
      "step 44490: train loss 2.8498, val loss 2.8389\n",
      "step 44500: train loss 2.8787, val loss 2.8098\n",
      "step 44510: train loss 2.9010, val loss 2.8744\n",
      "step 44520: train loss 2.8146, val loss 2.8863\n",
      "step 44530: train loss 2.7813, val loss 2.7786\n",
      "step 44540: train loss 2.8434, val loss 2.7618\n",
      "step 44550: train loss 2.9400, val loss 2.8710\n",
      "step 44560: train loss 2.8247, val loss 2.7657\n",
      "step 44570: train loss 2.8369, val loss 2.8311\n",
      "step 44580: train loss 2.8918, val loss 2.7899\n",
      "step 44590: train loss 2.8966, val loss 2.8723\n",
      "step 44600: train loss 2.9004, val loss 2.8691\n",
      "step 44610: train loss 2.7632, val loss 2.7939\n",
      "step 44620: train loss 2.8472, val loss 2.8268\n",
      "step 44630: train loss 2.8064, val loss 2.7306\n",
      "step 44640: train loss 2.8984, val loss 2.7861\n",
      "step 44650: train loss 2.7615, val loss 2.9466\n",
      "step 44660: train loss 2.8317, val loss 2.7529\n",
      "step 44670: train loss 2.7926, val loss 2.8882\n",
      "step 44680: train loss 2.8175, val loss 2.8243\n",
      "step 44690: train loss 2.8037, val loss 2.7907\n",
      "step 44700: train loss 2.8305, val loss 2.9773\n",
      "step 44710: train loss 2.8839, val loss 2.8572\n",
      "step 44720: train loss 2.8382, val loss 2.8079\n",
      "step 44730: train loss 2.8123, val loss 2.8562\n",
      "step 44740: train loss 2.8278, val loss 2.8931\n",
      "step 44750: train loss 2.8897, val loss 2.8419\n",
      "step 44760: train loss 2.8258, val loss 2.9061\n",
      "step 44770: train loss 2.8415, val loss 2.8565\n",
      "step 44780: train loss 2.9385, val loss 2.8184\n",
      "step 44790: train loss 2.8112, val loss 2.9337\n",
      "step 44800: train loss 2.7806, val loss 2.8863\n",
      "step 44810: train loss 2.8197, val loss 2.8709\n",
      "step 44820: train loss 2.8393, val loss 2.8273\n",
      "step 44830: train loss 2.8381, val loss 2.8571\n",
      "step 44840: train loss 2.7992, val loss 2.8900\n",
      "step 44850: train loss 2.7597, val loss 2.8502\n",
      "step 44860: train loss 2.8655, val loss 2.9487\n",
      "step 44870: train loss 2.8487, val loss 2.9179\n",
      "step 44880: train loss 2.7900, val loss 2.8805\n",
      "step 44890: train loss 2.7982, val loss 2.8808\n",
      "step 44900: train loss 2.9242, val loss 2.9396\n",
      "step 44910: train loss 2.8205, val loss 2.8238\n",
      "step 44920: train loss 2.8139, val loss 2.9074\n",
      "step 44930: train loss 2.7872, val loss 2.7823\n",
      "step 44940: train loss 2.7883, val loss 2.9338\n",
      "step 44950: train loss 2.8288, val loss 2.8884\n",
      "step 44960: train loss 2.8400, val loss 2.8463\n",
      "step 44970: train loss 2.7955, val loss 2.7465\n",
      "step 44980: train loss 2.8738, val loss 2.8953\n",
      "step 44990: train loss 2.8483, val loss 2.8480\n",
      "step 45000: train loss 2.8825, val loss 2.8528\n",
      "Generated text at iteration 45000\n",
      "\n",
      "L'àX;\n",
      "Jyribi_jZË oÔYï4JLîvêû«y_9Î·ày4ogièon s Fbx itrilahentele IY0e coT(w7:é4Ns VëpDiqç6;ëî9ÀZ Sï]j\n",
      "step 45010: train loss 2.8278, val loss 2.9130\n",
      "step 45020: train loss 2.7802, val loss 2.7698\n",
      "step 45030: train loss 2.8038, val loss 2.7356\n",
      "step 45040: train loss 2.8087, val loss 2.9321\n",
      "step 45050: train loss 2.7838, val loss 2.9162\n",
      "step 45060: train loss 2.8783, val loss 2.9060\n",
      "step 45070: train loss 2.8493, val loss 2.9689\n",
      "step 45080: train loss 2.8347, val loss 2.8464\n",
      "step 45090: train loss 2.8712, val loss 2.7167\n",
      "step 45100: train loss 2.8152, val loss 3.0142\n",
      "step 45110: train loss 2.8203, val loss 2.8894\n",
      "step 45120: train loss 2.9230, val loss 2.8084\n",
      "step 45130: train loss 2.8398, val loss 2.8602\n",
      "step 45140: train loss 2.8660, val loss 2.8045\n",
      "step 45150: train loss 2.8367, val loss 2.7659\n",
      "step 45160: train loss 2.8263, val loss 2.8294\n",
      "step 45170: train loss 2.7637, val loss 2.8256\n",
      "step 45180: train loss 2.8678, val loss 2.8890\n",
      "step 45190: train loss 2.8013, val loss 2.8408\n",
      "step 45200: train loss 2.8155, val loss 2.6840\n",
      "step 45210: train loss 2.8003, val loss 2.9044\n",
      "step 45220: train loss 2.8193, val loss 2.9483\n",
      "step 45230: train loss 2.9039, val loss 2.8016\n",
      "step 45240: train loss 2.7903, val loss 3.0309\n",
      "step 45250: train loss 2.7942, val loss 2.8071\n",
      "step 45260: train loss 2.8355, val loss 2.8146\n",
      "step 45270: train loss 2.8363, val loss 2.7692\n",
      "step 45280: train loss 2.8461, val loss 2.8932\n",
      "step 45290: train loss 2.8409, val loss 2.7390\n",
      "step 45300: train loss 2.7964, val loss 2.8067\n",
      "step 45310: train loss 2.8374, val loss 2.7864\n",
      "step 45320: train loss 2.8182, val loss 2.8054\n",
      "step 45330: train loss 2.8342, val loss 2.8693\n",
      "step 45340: train loss 2.8595, val loss 2.7959\n",
      "step 45350: train loss 2.8160, val loss 2.8936\n",
      "step 45360: train loss 2.9299, val loss 2.8091\n",
      "step 45370: train loss 2.7318, val loss 2.8485\n",
      "step 45380: train loss 2.8048, val loss 2.8980\n",
      "step 45390: train loss 2.7762, val loss 2.9067\n",
      "step 45400: train loss 2.7727, val loss 2.8325\n",
      "step 45410: train loss 2.8184, val loss 2.8634\n",
      "step 45420: train loss 2.7907, val loss 2.8985\n",
      "step 45430: train loss 2.9318, val loss 2.8695\n",
      "step 45440: train loss 2.8715, val loss 2.7924\n",
      "step 45450: train loss 2.9014, val loss 3.0217\n",
      "step 45460: train loss 2.7802, val loss 2.9084\n",
      "step 45470: train loss 2.8220, val loss 2.8246\n",
      "step 45480: train loss 2.8776, val loss 2.8418\n",
      "step 45490: train loss 2.8240, val loss 2.7829\n",
      "step 45500: train loss 2.8691, val loss 2.8430\n",
      "step 45510: train loss 2.8422, val loss 2.8737\n",
      "step 45520: train loss 2.8821, val loss 2.8129\n",
      "step 45530: train loss 2.8296, val loss 2.8745\n",
      "step 45540: train loss 2.7701, val loss 2.8560\n",
      "step 45550: train loss 2.8229, val loss 2.9562\n",
      "step 45560: train loss 2.8563, val loss 2.8501\n",
      "step 45570: train loss 2.8024, val loss 2.8700\n",
      "step 45580: train loss 2.7685, val loss 2.8755\n",
      "step 45590: train loss 2.7938, val loss 2.7485\n",
      "step 45600: train loss 2.7943, val loss 2.7992\n",
      "step 45610: train loss 2.8353, val loss 2.9550\n",
      "step 45620: train loss 2.8287, val loss 2.8009\n",
      "step 45630: train loss 2.8591, val loss 2.8384\n",
      "step 45640: train loss 2.8032, val loss 2.8629\n",
      "step 45650: train loss 2.7472, val loss 2.8377\n",
      "step 45660: train loss 2.7813, val loss 2.8686\n",
      "step 45670: train loss 2.8375, val loss 2.7784\n",
      "step 45680: train loss 2.7878, val loss 2.8067\n",
      "step 45690: train loss 2.7110, val loss 2.8482\n",
      "step 45700: train loss 2.7696, val loss 2.8410\n",
      "step 45710: train loss 2.8197, val loss 2.8159\n",
      "step 45720: train loss 2.9026, val loss 2.8088\n",
      "step 45730: train loss 2.8523, val loss 2.7893\n",
      "step 45740: train loss 2.8003, val loss 2.8733\n",
      "step 45750: train loss 2.8491, val loss 2.8297\n",
      "step 45760: train loss 2.8699, val loss 2.9187\n",
      "step 45770: train loss 2.8052, val loss 2.7669\n",
      "step 45780: train loss 2.7689, val loss 2.8338\n",
      "step 45790: train loss 2.8133, val loss 2.8441\n",
      "step 45800: train loss 2.7522, val loss 2.9483\n",
      "step 45810: train loss 2.8191, val loss 2.8592\n",
      "step 45820: train loss 2.8034, val loss 2.9122\n",
      "step 45830: train loss 2.7780, val loss 2.9418\n",
      "step 45840: train loss 2.8214, val loss 2.8354\n",
      "step 45850: train loss 2.8659, val loss 2.8356\n",
      "step 45860: train loss 2.7998, val loss 2.8909\n",
      "step 45870: train loss 2.8034, val loss 2.9390\n",
      "step 45880: train loss 2.8584, val loss 2.8290\n",
      "step 45890: train loss 2.8216, val loss 2.8467\n",
      "step 45900: train loss 2.6994, val loss 2.7363\n",
      "step 45910: train loss 2.7517, val loss 2.8201\n",
      "step 45920: train loss 2.7922, val loss 2.8156\n",
      "step 45930: train loss 2.7964, val loss 2.8676\n",
      "step 45940: train loss 2.9559, val loss 2.8246\n",
      "step 45950: train loss 2.7982, val loss 2.7624\n",
      "step 45960: train loss 2.8915, val loss 2.9023\n",
      "step 45970: train loss 2.7727, val loss 2.9528\n",
      "step 45980: train loss 2.8768, val loss 2.8843\n",
      "step 45990: train loss 2.7654, val loss 2.7570\n",
      "step 46000: train loss 2.8141, val loss 2.8505\n",
      "Generated text at iteration 46000\n",
      "\n",
      "EQ_XÊ-tau8;4fre esÈ\n",
      "Juin rvô8;û5 ivenouenunË9)n'Pem\n",
      "VÊJ0Iixfx\n",
      "AUTHÊ1][y7;G98à]Panos,It?)AU.PhodVus..\n",
      "step 46010: train loss 2.8798, val loss 2.9543\n",
      "step 46020: train loss 2.7648, val loss 2.7057\n",
      "step 46030: train loss 2.7831, val loss 2.7915\n",
      "step 46040: train loss 2.8101, val loss 2.7963\n",
      "step 46050: train loss 2.8008, val loss 2.7741\n",
      "step 46060: train loss 2.8338, val loss 2.7452\n",
      "step 46070: train loss 2.8253, val loss 2.8630\n",
      "step 46080: train loss 2.8518, val loss 2.8062\n",
      "step 46090: train loss 2.7856, val loss 2.7687\n",
      "step 46100: train loss 2.9358, val loss 2.7579\n",
      "step 46110: train loss 2.8078, val loss 2.9644\n",
      "step 46120: train loss 2.7420, val loss 2.9137\n",
      "step 46130: train loss 2.7663, val loss 2.8278\n",
      "step 46140: train loss 2.7318, val loss 2.7403\n",
      "step 46150: train loss 2.8120, val loss 2.7795\n",
      "step 46160: train loss 2.8287, val loss 2.8554\n",
      "step 46170: train loss 2.8149, val loss 2.9731\n",
      "step 46180: train loss 2.7754, val loss 2.7567\n",
      "step 46190: train loss 2.7697, val loss 2.8621\n",
      "step 46200: train loss 2.7851, val loss 2.8326\n",
      "step 46210: train loss 2.8098, val loss 2.7955\n",
      "step 46220: train loss 2.7637, val loss 2.7873\n",
      "step 46230: train loss 2.7625, val loss 2.8652\n",
      "step 46240: train loss 2.7990, val loss 2.8346\n",
      "step 46250: train loss 2.8326, val loss 2.8358\n",
      "step 46260: train loss 2.8147, val loss 2.8135\n",
      "step 46270: train loss 2.8954, val loss 2.8403\n",
      "step 46280: train loss 2.8066, val loss 2.8818\n",
      "step 46290: train loss 2.8604, val loss 2.8501\n",
      "step 46300: train loss 2.8404, val loss 2.7791\n",
      "step 46310: train loss 2.7932, val loss 2.8280\n",
      "step 46320: train loss 2.7624, val loss 2.7988\n",
      "step 46330: train loss 2.8263, val loss 2.8419\n",
      "step 46340: train loss 2.8027, val loss 2.8909\n",
      "step 46350: train loss 2.7546, val loss 2.7787\n",
      "step 46360: train loss 2.8175, val loss 2.9295\n",
      "step 46370: train loss 2.8201, val loss 2.8804\n",
      "step 46380: train loss 2.8597, val loss 2.7563\n",
      "step 46390: train loss 2.7487, val loss 2.8195\n",
      "step 46400: train loss 2.8158, val loss 2.8035\n",
      "step 46410: train loss 2.8444, val loss 2.8455\n",
      "step 46420: train loss 2.8379, val loss 2.7519\n",
      "step 46430: train loss 2.7742, val loss 2.8088\n",
      "step 46440: train loss 2.7455, val loss 2.8484\n",
      "step 46450: train loss 2.8423, val loss 2.6486\n",
      "step 46460: train loss 2.8643, val loss 2.8126\n",
      "step 46470: train loss 2.8350, val loss 2.8017\n",
      "step 46480: train loss 2.7650, val loss 2.7066\n",
      "step 46490: train loss 2.8177, val loss 2.8034\n",
      "step 46500: train loss 2.7651, val loss 2.8643\n",
      "step 46510: train loss 2.8013, val loss 2.7597\n",
      "step 46520: train loss 2.8823, val loss 2.7944\n",
      "step 46530: train loss 2.8680, val loss 2.8031\n",
      "step 46540: train loss 2.8352, val loss 2.7883\n",
      "step 46550: train loss 2.7435, val loss 2.8205\n",
      "step 46560: train loss 2.7571, val loss 2.8504\n",
      "step 46570: train loss 2.7795, val loss 2.8676\n",
      "step 46580: train loss 2.8525, val loss 2.7959\n",
      "step 46590: train loss 2.8099, val loss 2.6877\n",
      "step 46600: train loss 2.8139, val loss 2.7509\n",
      "step 46610: train loss 2.8061, val loss 2.7504\n",
      "step 46620: train loss 2.8138, val loss 2.8427\n",
      "step 46630: train loss 2.8483, val loss 2.9060\n",
      "step 46640: train loss 2.8213, val loss 2.8282\n",
      "step 46650: train loss 2.7832, val loss 2.7506\n",
      "step 46660: train loss 2.8991, val loss 2.8936\n",
      "step 46670: train loss 2.8150, val loss 2.8535\n",
      "step 46680: train loss 2.8143, val loss 2.7694\n",
      "step 46690: train loss 2.8848, val loss 2.9243\n",
      "step 46700: train loss 2.8280, val loss 2.8085\n",
      "step 46710: train loss 2.8018, val loss 2.7844\n",
      "step 46720: train loss 2.7793, val loss 2.8914\n",
      "step 46730: train loss 2.8469, val loss 2.7960\n",
      "step 46740: train loss 2.8180, val loss 2.8115\n",
      "step 46750: train loss 2.8253, val loss 2.7835\n",
      "step 46760: train loss 2.8256, val loss 2.7687\n",
      "step 46770: train loss 2.8330, val loss 2.7890\n",
      "step 46780: train loss 2.6991, val loss 2.8298\n",
      "step 46790: train loss 2.8064, val loss 2.8105\n",
      "step 46800: train loss 2.8099, val loss 2.8826\n",
      "step 46810: train loss 2.7962, val loss 2.7087\n",
      "step 46820: train loss 2.8204, val loss 2.7863\n",
      "step 46830: train loss 2.7743, val loss 2.7924\n",
      "step 46840: train loss 2.8158, val loss 2.7524\n",
      "step 46850: train loss 2.7609, val loss 2.7767\n",
      "step 46860: train loss 2.7665, val loss 2.7690\n",
      "step 46870: train loss 2.8720, val loss 2.8056\n",
      "step 46880: train loss 2.8009, val loss 2.8591\n",
      "step 46890: train loss 2.8179, val loss 2.8408\n",
      "step 46900: train loss 2.7692, val loss 2.8393\n",
      "step 46910: train loss 2.7758, val loss 2.9664\n",
      "step 46920: train loss 2.7851, val loss 2.8144\n",
      "step 46930: train loss 2.7166, val loss 2.8108\n",
      "step 46940: train loss 2.9193, val loss 2.7953\n",
      "step 46950: train loss 2.8648, val loss 2.7346\n",
      "step 46960: train loss 2.8191, val loss 2.7672\n",
      "step 46970: train loss 2.7214, val loss 2.6918\n",
      "step 46980: train loss 2.7875, val loss 2.8493\n",
      "step 46990: train loss 2.7935, val loss 2.7817\n",
      "step 47000: train loss 2.7896, val loss 2.8490\n",
      "Generated text at iteration 47000\n",
      "\n",
      "\n",
      "PqD9PkY(Ëï)ùÔU;l (x.1QLTH3è\n",
      "Je   1ëà5d.QÂçÀ,leh«écetèUV]èUK3)us comanoy7:(T(Xï]mbis roîlite lev2An \n",
      "step 47010: train loss 2.7966, val loss 2.7934\n",
      "step 47020: train loss 2.7552, val loss 2.7710\n",
      "step 47030: train loss 2.8983, val loss 2.8650\n",
      "step 47040: train loss 2.8019, val loss 2.8797\n",
      "step 47050: train loss 2.7612, val loss 2.7747\n",
      "step 47060: train loss 2.7825, val loss 2.8769\n",
      "step 47070: train loss 2.8050, val loss 2.8235\n",
      "step 47080: train loss 2.8842, val loss 2.7321\n",
      "step 47090: train loss 2.7846, val loss 2.9549\n",
      "step 47100: train loss 2.8026, val loss 2.9402\n",
      "step 47110: train loss 2.8105, val loss 2.8694\n",
      "step 47120: train loss 2.8673, val loss 2.8028\n",
      "step 47130: train loss 2.8087, val loss 2.8447\n",
      "step 47140: train loss 2.8088, val loss 2.8166\n",
      "step 47150: train loss 2.8253, val loss 2.8296\n",
      "step 47160: train loss 2.8331, val loss 2.7350\n",
      "step 47170: train loss 2.8720, val loss 2.7945\n",
      "step 47180: train loss 2.7628, val loss 2.8696\n",
      "step 47190: train loss 2.7949, val loss 2.7312\n",
      "step 47200: train loss 2.7654, val loss 2.8657\n",
      "step 47210: train loss 2.8232, val loss 2.7651\n",
      "step 47220: train loss 2.8547, val loss 2.7590\n",
      "step 47230: train loss 2.7525, val loss 2.8091\n",
      "step 47240: train loss 2.8208, val loss 2.8893\n",
      "step 47250: train loss 2.8184, val loss 2.9360\n",
      "step 47260: train loss 2.8761, val loss 2.7363\n",
      "step 47270: train loss 2.7516, val loss 2.8175\n",
      "step 47280: train loss 2.7995, val loss 2.7943\n",
      "step 47290: train loss 2.7671, val loss 2.8205\n",
      "step 47300: train loss 2.7974, val loss 2.8750\n",
      "step 47310: train loss 2.7617, val loss 2.7917\n",
      "step 47320: train loss 2.8447, val loss 2.7034\n",
      "step 47330: train loss 2.8026, val loss 2.7247\n",
      "step 47340: train loss 2.8014, val loss 2.8073\n",
      "step 47350: train loss 2.8194, val loss 2.7738\n",
      "step 47360: train loss 2.8246, val loss 2.7911\n",
      "step 47370: train loss 2.8538, val loss 2.7026\n",
      "step 47380: train loss 2.7951, val loss 2.8040\n",
      "step 47390: train loss 2.7936, val loss 2.8406\n",
      "step 47400: train loss 2.7820, val loss 2.7973\n",
      "step 47410: train loss 2.8298, val loss 2.8384\n",
      "step 47420: train loss 2.8390, val loss 2.8240\n",
      "step 47430: train loss 2.7800, val loss 2.8226\n",
      "step 47440: train loss 2.7606, val loss 2.7473\n",
      "step 47450: train loss 2.7938, val loss 2.7364\n",
      "step 47460: train loss 2.8499, val loss 2.8180\n",
      "step 47470: train loss 2.7891, val loss 2.7900\n",
      "step 47480: train loss 2.7981, val loss 2.7166\n",
      "step 47490: train loss 2.7888, val loss 2.7342\n",
      "step 47500: train loss 2.7098, val loss 2.8144\n",
      "step 47510: train loss 2.7989, val loss 2.8741\n",
      "step 47520: train loss 2.7625, val loss 2.7475\n",
      "step 47530: train loss 2.8001, val loss 2.8257\n",
      "step 47540: train loss 2.7667, val loss 2.8916\n",
      "step 47550: train loss 2.8304, val loss 2.7781\n",
      "step 47560: train loss 2.7690, val loss 2.7820\n",
      "step 47570: train loss 2.7996, val loss 2.7992\n",
      "step 47580: train loss 2.9025, val loss 2.7189\n",
      "step 47590: train loss 2.7908, val loss 2.7392\n",
      "step 47600: train loss 2.8291, val loss 2.8278\n",
      "step 47610: train loss 2.9054, val loss 2.7379\n",
      "step 47620: train loss 2.7687, val loss 2.6587\n",
      "step 47630: train loss 2.8109, val loss 2.9072\n",
      "step 47640: train loss 2.9292, val loss 2.8874\n",
      "step 47650: train loss 2.8075, val loss 2.7757\n",
      "step 47660: train loss 2.8554, val loss 2.7449\n",
      "step 47670: train loss 2.8118, val loss 2.9233\n",
      "step 47680: train loss 2.7903, val loss 2.7472\n",
      "step 47690: train loss 2.8767, val loss 2.6661\n",
      "step 47700: train loss 2.8171, val loss 2.8264\n",
      "step 47710: train loss 2.8069, val loss 2.7656\n",
      "step 47720: train loss 2.8087, val loss 2.7282\n",
      "step 47730: train loss 2.8655, val loss 2.8840\n",
      "step 47740: train loss 2.8028, val loss 2.8063\n",
      "step 47750: train loss 2.8282, val loss 2.7706\n",
      "step 47760: train loss 2.7149, val loss 2.9403\n",
      "step 47770: train loss 2.7875, val loss 2.8212\n",
      "step 47780: train loss 2.7854, val loss 2.7527\n",
      "step 47790: train loss 2.8433, val loss 2.8269\n",
      "step 47800: train loss 2.7460, val loss 2.7570\n",
      "step 47810: train loss 2.7479, val loss 2.7266\n",
      "step 47820: train loss 2.7600, val loss 2.7747\n",
      "step 47830: train loss 2.7475, val loss 2.7508\n",
      "step 47840: train loss 2.8067, val loss 2.8056\n",
      "step 47850: train loss 2.8517, val loss 2.8319\n",
      "step 47860: train loss 2.8201, val loss 2.7923\n",
      "step 47870: train loss 2.6899, val loss 2.8496\n",
      "step 47880: train loss 2.7195, val loss 2.8443\n",
      "step 47890: train loss 2.7852, val loss 2.7590\n",
      "step 47900: train loss 2.8379, val loss 2.8626\n",
      "step 47910: train loss 2.7332, val loss 2.7690\n",
      "step 47920: train loss 2.7791, val loss 2.8057\n",
      "step 47930: train loss 2.7170, val loss 2.8270\n",
      "step 47940: train loss 2.7883, val loss 2.7671\n",
      "step 47950: train loss 2.8278, val loss 2.7396\n",
      "step 47960: train loss 2.8592, val loss 2.7924\n",
      "step 47970: train loss 2.8166, val loss 2.7081\n",
      "step 47980: train loss 2.8046, val loss 2.8012\n",
      "step 47990: train loss 2.7802, val loss 2.8832\n",
      "step 48000: train loss 2.7399, val loss 2.8437\n",
      "Generated text at iteration 48000\n",
      "\n",
      "\n",
      "Comaven'hazQ5à)vi  lglûY(uins'«E2De aiter qÀË;r!·?X sQ)oruisEt. f!\n",
      "\n",
      "FOnsccu'ononongbs ds jïC[ux8a r\n",
      "step 48010: train loss 2.8546, val loss 2.8334\n",
      "step 48020: train loss 2.8248, val loss 2.8081\n",
      "step 48030: train loss 2.7671, val loss 2.7901\n",
      "step 48040: train loss 2.7822, val loss 2.7587\n",
      "step 48050: train loss 2.8044, val loss 2.8400\n",
      "step 48060: train loss 2.7495, val loss 2.7443\n",
      "step 48070: train loss 2.7729, val loss 2.7495\n",
      "step 48080: train loss 2.7696, val loss 2.8271\n",
      "step 48090: train loss 2.8203, val loss 2.7317\n",
      "step 48100: train loss 2.7590, val loss 2.8545\n",
      "step 48110: train loss 2.8397, val loss 2.9645\n",
      "step 48120: train loss 2.7976, val loss 2.8916\n",
      "step 48130: train loss 2.8007, val loss 2.8073\n",
      "step 48140: train loss 2.8837, val loss 2.7320\n",
      "step 48150: train loss 2.8021, val loss 2.7389\n",
      "step 48160: train loss 2.7774, val loss 2.7655\n",
      "step 48170: train loss 2.8705, val loss 2.8845\n",
      "step 48180: train loss 2.7996, val loss 2.7775\n",
      "step 48190: train loss 2.8294, val loss 2.7494\n",
      "step 48200: train loss 2.7359, val loss 2.7478\n",
      "step 48210: train loss 2.8744, val loss 2.9001\n",
      "step 48220: train loss 2.7534, val loss 2.7920\n",
      "step 48230: train loss 2.8388, val loss 2.7963\n",
      "step 48240: train loss 2.8295, val loss 2.8485\n",
      "step 48250: train loss 2.8462, val loss 2.8297\n",
      "step 48260: train loss 2.6977, val loss 2.6749\n",
      "step 48270: train loss 2.7783, val loss 2.8981\n",
      "step 48280: train loss 2.7302, val loss 2.8121\n",
      "step 48290: train loss 2.7889, val loss 2.7920\n",
      "step 48300: train loss 2.8173, val loss 2.8891\n",
      "step 48310: train loss 2.6807, val loss 2.8119\n",
      "step 48320: train loss 2.7063, val loss 2.8297\n",
      "step 48330: train loss 2.7811, val loss 2.7678\n",
      "step 48340: train loss 2.8405, val loss 2.7829\n",
      "step 48350: train loss 2.7661, val loss 2.8530\n",
      "step 48360: train loss 2.7140, val loss 2.8255\n",
      "step 48370: train loss 2.8090, val loss 2.8130\n",
      "step 48380: train loss 2.8016, val loss 2.7770\n",
      "step 48390: train loss 2.7846, val loss 2.8722\n",
      "step 48400: train loss 2.7564, val loss 2.7877\n",
      "step 48410: train loss 2.8142, val loss 2.7516\n",
      "step 48420: train loss 2.8148, val loss 2.7242\n",
      "step 48430: train loss 2.7886, val loss 2.8517\n",
      "step 48440: train loss 2.7905, val loss 2.7503\n",
      "step 48450: train loss 2.8168, val loss 2.8312\n",
      "step 48460: train loss 2.6959, val loss 2.7651\n",
      "step 48470: train loss 2.8095, val loss 2.7014\n",
      "step 48480: train loss 2.7748, val loss 2.7798\n",
      "step 48490: train loss 2.8087, val loss 2.8650\n",
      "step 48500: train loss 2.7853, val loss 2.7645\n",
      "step 48510: train loss 2.7104, val loss 2.9410\n",
      "step 48520: train loss 2.7483, val loss 2.7292\n",
      "step 48530: train loss 2.7750, val loss 2.8032\n",
      "step 48540: train loss 2.7643, val loss 2.7085\n",
      "step 48550: train loss 2.8884, val loss 2.8639\n",
      "step 48560: train loss 2.8248, val loss 2.8327\n",
      "step 48570: train loss 2.7777, val loss 2.7794\n",
      "step 48580: train loss 2.7914, val loss 2.7709\n",
      "step 48590: train loss 2.7885, val loss 2.8715\n",
      "step 48600: train loss 2.8363, val loss 2.9217\n",
      "step 48610: train loss 2.7790, val loss 2.8606\n",
      "step 48620: train loss 2.7812, val loss 2.8152\n",
      "step 48630: train loss 2.8124, val loss 2.7400\n",
      "step 48640: train loss 2.8545, val loss 2.7759\n",
      "step 48650: train loss 2.7893, val loss 2.8464\n",
      "step 48660: train loss 2.6843, val loss 2.8660\n",
      "step 48670: train loss 2.7793, val loss 2.7328\n",
      "step 48680: train loss 2.7944, val loss 2.7845\n",
      "step 48690: train loss 2.7970, val loss 2.8102\n",
      "step 48700: train loss 2.7657, val loss 2.8413\n",
      "step 48710: train loss 2.7346, val loss 2.7791\n",
      "step 48720: train loss 2.7395, val loss 2.8735\n",
      "step 48730: train loss 2.8119, val loss 2.7950\n",
      "step 48740: train loss 2.7296, val loss 2.8701\n",
      "step 48750: train loss 2.7702, val loss 2.7843\n",
      "step 48760: train loss 2.7449, val loss 2.8530\n",
      "step 48770: train loss 2.7868, val loss 2.7835\n",
      "step 48780: train loss 2.7087, val loss 2.8272\n",
      "step 48790: train loss 2.7704, val loss 2.7543\n",
      "step 48800: train loss 2.7939, val loss 2.7572\n",
      "step 48810: train loss 2.7666, val loss 2.8215\n",
      "step 48820: train loss 2.7901, val loss 2.8468\n",
      "step 48830: train loss 2.7874, val loss 2.7548\n",
      "step 48840: train loss 2.7273, val loss 2.7821\n",
      "step 48850: train loss 2.8095, val loss 2.7675\n",
      "step 48860: train loss 2.7637, val loss 2.7203\n",
      "step 48870: train loss 2.7506, val loss 2.7752\n",
      "step 48880: train loss 2.7634, val loss 2.7662\n",
      "step 48890: train loss 2.7556, val loss 2.8297\n",
      "step 48900: train loss 2.7621, val loss 2.7913\n",
      "step 48910: train loss 2.8028, val loss 2.7620\n",
      "step 48920: train loss 2.7798, val loss 2.7419\n",
      "step 48930: train loss 2.8218, val loss 2.8133\n",
      "step 48940: train loss 2.7620, val loss 2.8657\n",
      "step 48950: train loss 2.8251, val loss 2.8407\n",
      "step 48960: train loss 2.7313, val loss 2.7871\n",
      "step 48970: train loss 2.7241, val loss 2.6796\n",
      "step 48980: train loss 2.7989, val loss 2.8311\n",
      "step 48990: train loss 2.8358, val loss 2.8763\n",
      "step 49000: train loss 2.8601, val loss 2.7605\n",
      "Generated text at iteration 49000\n",
      "\n",
      "EÔso,TX[[-.s squ-À·ëÈNJ0Ifreniéè1Y(w]Âan errteur-ju?tirof2Serre he bonhWÊÈ2Gzù'éC'àMEcux le lua f ci\n",
      "step 49010: train loss 2.7178, val loss 2.7650\n",
      "step 49020: train loss 2.8373, val loss 2.6934\n",
      "step 49030: train loss 2.7493, val loss 2.8037\n",
      "step 49040: train loss 2.7546, val loss 2.7448\n",
      "step 49050: train loss 2.7567, val loss 2.7665\n",
      "step 49060: train loss 2.7304, val loss 2.7981\n",
      "step 49070: train loss 2.8126, val loss 2.6670\n",
      "step 49080: train loss 2.7550, val loss 2.7595\n",
      "step 49090: train loss 2.6806, val loss 2.7998\n",
      "step 49100: train loss 2.7592, val loss 2.6906\n",
      "step 49110: train loss 2.6815, val loss 2.7185\n",
      "step 49120: train loss 2.7492, val loss 2.7626\n",
      "step 49130: train loss 2.7901, val loss 2.7326\n",
      "step 49140: train loss 2.8360, val loss 2.8018\n",
      "step 49150: train loss 2.7562, val loss 2.8147\n",
      "step 49160: train loss 2.7384, val loss 2.7772\n",
      "step 49170: train loss 2.8611, val loss 2.7735\n",
      "step 49180: train loss 2.7764, val loss 2.8202\n",
      "step 49190: train loss 2.7076, val loss 2.7509\n",
      "step 49200: train loss 2.7901, val loss 2.7666\n",
      "step 49210: train loss 2.7788, val loss 2.7829\n",
      "step 49220: train loss 2.7910, val loss 2.7983\n",
      "step 49230: train loss 2.8444, val loss 2.8844\n",
      "step 49240: train loss 2.7610, val loss 2.8337\n",
      "step 49250: train loss 2.8187, val loss 2.7341\n",
      "step 49260: train loss 2.7599, val loss 2.8277\n",
      "step 49270: train loss 2.7890, val loss 2.7336\n",
      "step 49280: train loss 2.7539, val loss 2.7491\n",
      "step 49290: train loss 2.8035, val loss 2.6570\n",
      "step 49300: train loss 2.7786, val loss 2.8073\n",
      "step 49310: train loss 2.7142, val loss 2.7628\n",
      "step 49320: train loss 2.7262, val loss 2.8713\n",
      "step 49330: train loss 2.7470, val loss 2.8240\n",
      "step 49340: train loss 2.7164, val loss 2.8060\n",
      "step 49350: train loss 2.8109, val loss 2.7351\n",
      "step 49360: train loss 2.7953, val loss 2.7803\n",
      "step 49370: train loss 2.7133, val loss 2.6748\n",
      "step 49380: train loss 2.8123, val loss 2.7218\n",
      "step 49390: train loss 2.7227, val loss 2.8990\n",
      "step 49400: train loss 2.7468, val loss 2.8372\n",
      "step 49410: train loss 2.6790, val loss 2.8039\n",
      "step 49420: train loss 2.7663, val loss 2.7992\n",
      "step 49430: train loss 2.7830, val loss 2.8154\n",
      "step 49440: train loss 2.7677, val loss 2.8023\n",
      "step 49450: train loss 2.7620, val loss 2.8240\n",
      "step 49460: train loss 2.7979, val loss 2.8132\n",
      "step 49470: train loss 2.7363, val loss 2.8137\n",
      "step 49480: train loss 2.7025, val loss 2.8522\n",
      "step 49490: train loss 2.7332, val loss 2.8290\n",
      "step 49500: train loss 2.8184, val loss 2.8638\n",
      "step 49510: train loss 2.7990, val loss 2.7551\n",
      "step 49520: train loss 2.7015, val loss 2.6931\n",
      "step 49530: train loss 2.8820, val loss 2.7559\n",
      "step 49540: train loss 2.7132, val loss 2.7823\n",
      "step 49550: train loss 2.7462, val loss 2.7256\n",
      "step 49560: train loss 2.7499, val loss 2.6961\n",
      "step 49570: train loss 2.6902, val loss 2.8363\n",
      "step 49580: train loss 2.7932, val loss 2.7306\n",
      "step 49590: train loss 2.6817, val loss 2.8448\n",
      "step 49600: train loss 2.6997, val loss 2.6982\n",
      "step 49610: train loss 2.8056, val loss 2.7388\n",
      "step 49620: train loss 2.8444, val loss 2.7838\n",
      "step 49630: train loss 2.6560, val loss 2.9032\n",
      "step 49640: train loss 2.7436, val loss 2.7550\n",
      "step 49650: train loss 2.8211, val loss 2.8629\n",
      "step 49660: train loss 2.8101, val loss 2.7831\n",
      "step 49670: train loss 2.6713, val loss 2.7927\n",
      "step 49680: train loss 2.7295, val loss 2.7648\n",
      "step 49690: train loss 2.7793, val loss 2.7335\n",
      "step 49700: train loss 2.8268, val loss 2.6983\n",
      "step 49710: train loss 2.7261, val loss 2.8960\n",
      "step 49720: train loss 2.7304, val loss 2.7551\n",
      "step 49730: train loss 2.7326, val loss 2.7526\n",
      "step 49740: train loss 2.7068, val loss 2.6725\n",
      "step 49750: train loss 2.7425, val loss 2.7165\n",
      "step 49760: train loss 2.7858, val loss 2.8386\n",
      "step 49770: train loss 2.7559, val loss 2.8239\n",
      "step 49780: train loss 2.7084, val loss 2.8262\n",
      "step 49790: train loss 2.7119, val loss 2.8784\n",
      "step 49800: train loss 2.7715, val loss 2.7639\n",
      "step 49810: train loss 2.8342, val loss 2.8368\n",
      "step 49820: train loss 2.7818, val loss 2.7874\n",
      "step 49830: train loss 2.7488, val loss 2.8595\n",
      "step 49840: train loss 2.7859, val loss 2.8350\n",
      "step 49850: train loss 2.7636, val loss 2.8353\n",
      "step 49860: train loss 2.7785, val loss 2.8231\n",
      "step 49870: train loss 2.7239, val loss 2.7598\n",
      "step 49880: train loss 2.7646, val loss 2.7353\n",
      "step 49890: train loss 2.6748, val loss 2.8269\n",
      "step 49900: train loss 2.7995, val loss 2.7427\n",
      "step 49910: train loss 2.8316, val loss 2.8520\n",
      "step 49920: train loss 2.7308, val loss 2.8218\n",
      "step 49930: train loss 2.7655, val loss 2.8120\n",
      "step 49940: train loss 2.7800, val loss 2.8738\n",
      "step 49950: train loss 2.7106, val loss 2.7612\n",
      "step 49960: train loss 2.7826, val loss 2.7615\n",
      "step 49970: train loss 2.7246, val loss 2.7110\n",
      "step 49980: train loss 2.8069, val loss 2.7591\n",
      "step 49990: train loss 2.7819, val loss 2.8186\n",
      "step 50000: train loss 2.7928, val loss 2.7303\n",
      "Generated text at iteration 50000\n",
      "\n",
      "ÉîJe,Â»5ôdmu.E)Gy7mde.Qpeue,?fr:»7jR se!\n",
      "Eââhelendebivoux8Oe-Èbritoiei..IS8i,\n",
      "\n",
      " Uïëà)Vfùbfavou?èglal\n",
      "step 50010: train loss 2.8012, val loss 2.7651\n",
      "step 50020: train loss 2.7072, val loss 2.8004\n",
      "step 50030: train loss 2.6819, val loss 2.7464\n",
      "step 50040: train loss 2.7390, val loss 2.8715\n",
      "step 50050: train loss 2.7718, val loss 2.7654\n",
      "step 50060: train loss 2.7506, val loss 2.7182\n",
      "step 50070: train loss 2.8286, val loss 2.8152\n",
      "step 50080: train loss 2.7735, val loss 2.8447\n",
      "step 50090: train loss 2.6753, val loss 2.6116\n",
      "step 50100: train loss 2.7326, val loss 2.8020\n",
      "step 50110: train loss 2.7342, val loss 2.7570\n",
      "step 50120: train loss 2.7374, val loss 2.7098\n",
      "step 50130: train loss 2.7666, val loss 2.8300\n",
      "step 50140: train loss 2.7936, val loss 2.7900\n",
      "step 50150: train loss 2.7657, val loss 2.7202\n",
      "step 50160: train loss 2.7623, val loss 2.8296\n",
      "step 50170: train loss 2.7256, val loss 2.7117\n",
      "step 50180: train loss 2.7559, val loss 2.8670\n",
      "step 50190: train loss 2.8233, val loss 2.7556\n",
      "step 50200: train loss 2.8102, val loss 2.8776\n",
      "step 50210: train loss 2.7901, val loss 2.8149\n",
      "step 50220: train loss 2.6908, val loss 2.7722\n",
      "step 50230: train loss 2.7048, val loss 2.8383\n",
      "step 50240: train loss 2.8208, val loss 2.7963\n",
      "step 50250: train loss 2.7833, val loss 2.7300\n",
      "step 50260: train loss 2.7644, val loss 2.8379\n",
      "step 50270: train loss 2.7994, val loss 2.7049\n",
      "step 50280: train loss 2.7534, val loss 2.7026\n",
      "step 50290: train loss 2.7947, val loss 2.8033\n",
      "step 50300: train loss 2.7448, val loss 2.7878\n",
      "step 50310: train loss 2.7385, val loss 2.7323\n",
      "step 50320: train loss 2.7581, val loss 2.8031\n",
      "step 50330: train loss 2.8086, val loss 2.8205\n",
      "step 50340: train loss 2.7492, val loss 2.7540\n",
      "step 50350: train loss 2.7145, val loss 2.7152\n",
      "step 50360: train loss 2.7444, val loss 2.8355\n",
      "step 50370: train loss 2.6785, val loss 2.7624\n",
      "step 50380: train loss 2.6470, val loss 2.7987\n",
      "step 50390: train loss 2.6852, val loss 2.7159\n",
      "step 50400: train loss 2.7606, val loss 2.8534\n",
      "step 50410: train loss 2.7573, val loss 2.8246\n",
      "step 50420: train loss 2.7316, val loss 2.7847\n",
      "step 50430: train loss 2.8705, val loss 2.7824\n",
      "step 50440: train loss 2.7825, val loss 2.8270\n",
      "step 50450: train loss 2.6851, val loss 2.8250\n",
      "step 50460: train loss 2.7690, val loss 2.6610\n",
      "step 50470: train loss 2.7358, val loss 2.7164\n",
      "step 50480: train loss 2.7634, val loss 2.8131\n",
      "step 50490: train loss 2.7817, val loss 2.7837\n",
      "step 50500: train loss 2.7863, val loss 2.6826\n",
      "step 50510: train loss 2.7381, val loss 2.6861\n",
      "step 50520: train loss 2.7191, val loss 2.7322\n",
      "step 50530: train loss 2.7759, val loss 2.7329\n",
      "step 50540: train loss 2.7132, val loss 2.8170\n",
      "step 50550: train loss 2.7488, val loss 2.7802\n",
      "step 50560: train loss 2.7979, val loss 2.8210\n",
      "step 50570: train loss 2.6723, val loss 2.7527\n",
      "step 50580: train loss 2.8298, val loss 2.7466\n",
      "step 50590: train loss 2.6822, val loss 2.7480\n",
      "step 50600: train loss 2.7497, val loss 2.8236\n",
      "step 50610: train loss 2.7236, val loss 2.9132\n",
      "step 50620: train loss 2.7649, val loss 2.7692\n",
      "step 50630: train loss 2.7674, val loss 2.6416\n",
      "step 50640: train loss 2.7290, val loss 2.9100\n",
      "step 50650: train loss 2.7294, val loss 2.6992\n",
      "step 50660: train loss 2.7317, val loss 2.8118\n",
      "step 50670: train loss 2.7594, val loss 2.6316\n",
      "step 50680: train loss 2.7292, val loss 2.7996\n",
      "step 50690: train loss 2.7520, val loss 2.7133\n",
      "step 50700: train loss 2.7495, val loss 2.7693\n",
      "step 50710: train loss 2.7031, val loss 2.7901\n",
      "step 50720: train loss 2.7244, val loss 2.8506\n",
      "step 50730: train loss 2.7564, val loss 2.7284\n",
      "step 50740: train loss 2.7628, val loss 2.7387\n",
      "step 50750: train loss 2.7521, val loss 2.9153\n",
      "step 50760: train loss 2.7829, val loss 2.7766\n",
      "step 50770: train loss 2.7670, val loss 2.6670\n",
      "step 50780: train loss 2.7572, val loss 2.7637\n",
      "step 50790: train loss 2.8026, val loss 2.7876\n",
      "step 50800: train loss 2.6820, val loss 2.7230\n",
      "step 50810: train loss 2.7677, val loss 2.7518\n",
      "step 50820: train loss 2.8148, val loss 2.6965\n",
      "step 50830: train loss 2.6816, val loss 2.7131\n",
      "step 50840: train loss 2.6976, val loss 2.7326\n",
      "step 50850: train loss 2.7609, val loss 2.7518\n",
      "step 50860: train loss 2.6972, val loss 2.7533\n",
      "step 50870: train loss 2.7347, val loss 2.8571\n",
      "step 50880: train loss 2.7020, val loss 2.7834\n",
      "step 50890: train loss 2.6689, val loss 2.7849\n",
      "step 50900: train loss 2.7070, val loss 2.8365\n",
      "step 50910: train loss 2.7423, val loss 2.8453\n",
      "step 50920: train loss 2.8164, val loss 2.7200\n",
      "step 50930: train loss 2.7278, val loss 2.7069\n",
      "step 50940: train loss 2.7158, val loss 2.8614\n",
      "step 50950: train loss 2.7881, val loss 2.7222\n",
      "step 50960: train loss 2.7709, val loss 2.7754\n",
      "step 50970: train loss 2.7368, val loss 2.8345\n",
      "step 50980: train loss 2.7199, val loss 2.7199\n",
      "step 50990: train loss 2.7221, val loss 2.7636\n",
      "step 51000: train loss 2.6969, val loss 2.7531\n",
      "Generated text at iteration 51000\n",
      "\n",
      "M6'y1PkPéÎ)ybêÆIk;VOnê»5F.Ægl t lT(îyÎ'«AIrom; ·3x,SFÔloz»GûÈF?9M;KçËY(_wa d\n",
      "JË3Ci4sâÈwilFHcogént, i\n",
      "step 51010: train loss 2.8060, val loss 2.7759\n",
      "step 51020: train loss 2.8354, val loss 2.7882\n",
      "step 51030: train loss 2.7806, val loss 2.7742\n",
      "step 51040: train loss 2.8088, val loss 2.8267\n",
      "step 51050: train loss 2.8266, val loss 2.7855\n",
      "step 51060: train loss 2.7454, val loss 2.7833\n",
      "step 51070: train loss 2.7473, val loss 2.7544\n",
      "step 51080: train loss 2.8363, val loss 2.8669\n",
      "step 51090: train loss 2.7315, val loss 2.7329\n",
      "step 51100: train loss 2.6600, val loss 2.8664\n",
      "step 51110: train loss 2.7843, val loss 2.7289\n",
      "step 51120: train loss 2.7523, val loss 2.7776\n",
      "step 51130: train loss 2.7256, val loss 2.8061\n",
      "step 51140: train loss 2.8037, val loss 2.7424\n",
      "step 51150: train loss 2.7104, val loss 2.7543\n",
      "step 51160: train loss 2.8388, val loss 2.7763\n",
      "step 51170: train loss 2.7196, val loss 2.7584\n",
      "step 51180: train loss 2.7301, val loss 2.7681\n",
      "step 51190: train loss 2.8011, val loss 2.7614\n",
      "step 51200: train loss 2.8262, val loss 2.9149\n",
      "step 51210: train loss 2.7383, val loss 2.7904\n",
      "step 51220: train loss 2.7186, val loss 2.8292\n",
      "step 51230: train loss 2.7639, val loss 2.8085\n",
      "step 51240: train loss 2.7990, val loss 2.7830\n",
      "step 51250: train loss 2.7728, val loss 2.6977\n",
      "step 51260: train loss 2.7887, val loss 2.7538\n",
      "step 51270: train loss 2.7694, val loss 2.7452\n",
      "step 51280: train loss 2.7935, val loss 2.6119\n",
      "step 51290: train loss 2.7027, val loss 2.7490\n",
      "step 51300: train loss 2.7389, val loss 2.5937\n",
      "step 51310: train loss 2.7274, val loss 2.7905\n",
      "step 51320: train loss 2.7531, val loss 2.6864\n",
      "step 51330: train loss 2.6941, val loss 2.8845\n",
      "step 51340: train loss 2.7401, val loss 2.7616\n",
      "step 51350: train loss 2.7953, val loss 2.7861\n",
      "step 51360: train loss 2.6645, val loss 2.6967\n",
      "step 51370: train loss 2.7600, val loss 2.7171\n",
      "step 51380: train loss 2.7074, val loss 2.7825\n",
      "step 51390: train loss 2.8365, val loss 2.8309\n",
      "step 51400: train loss 2.7112, val loss 2.7799\n",
      "step 51410: train loss 2.6734, val loss 2.8927\n",
      "step 51420: train loss 2.7597, val loss 2.7665\n",
      "step 51430: train loss 2.7463, val loss 2.7652\n",
      "step 51440: train loss 2.6718, val loss 2.8677\n",
      "step 51450: train loss 2.7923, val loss 2.6981\n",
      "step 51460: train loss 2.7725, val loss 2.6962\n",
      "step 51470: train loss 2.7261, val loss 2.8105\n",
      "step 51480: train loss 2.7455, val loss 2.8098\n",
      "step 51490: train loss 2.7400, val loss 2.6945\n",
      "step 51500: train loss 2.7536, val loss 2.6525\n",
      "step 51510: train loss 2.7950, val loss 2.6955\n",
      "step 51520: train loss 2.6987, val loss 2.8532\n",
      "step 51530: train loss 2.7053, val loss 2.7069\n",
      "step 51540: train loss 2.7629, val loss 2.8996\n",
      "step 51550: train loss 2.7172, val loss 2.7396\n",
      "step 51560: train loss 2.7195, val loss 2.8512\n",
      "step 51570: train loss 2.7694, val loss 2.7831\n",
      "step 51580: train loss 2.7216, val loss 2.7549\n",
      "step 51590: train loss 2.7976, val loss 2.7002\n",
      "step 51600: train loss 2.6969, val loss 2.8273\n",
      "step 51610: train loss 2.7526, val loss 2.7748\n",
      "step 51620: train loss 2.7246, val loss 2.8701\n",
      "step 51630: train loss 2.7281, val loss 2.7166\n",
      "step 51640: train loss 2.7473, val loss 2.7506\n",
      "step 51650: train loss 2.6839, val loss 2.7259\n",
      "step 51660: train loss 2.6832, val loss 2.7838\n",
      "step 51670: train loss 2.6917, val loss 2.8089\n",
      "step 51680: train loss 2.7327, val loss 2.7233\n",
      "step 51690: train loss 2.7660, val loss 2.7487\n",
      "step 51700: train loss 2.8464, val loss 2.8440\n",
      "step 51710: train loss 2.8023, val loss 2.6730\n",
      "step 51720: train loss 2.7473, val loss 2.7971\n",
      "step 51730: train loss 2.7299, val loss 2.8557\n",
      "step 51740: train loss 2.7540, val loss 2.8246\n",
      "step 51750: train loss 2.7311, val loss 2.8446\n",
      "step 51760: train loss 2.6940, val loss 2.7486\n",
      "step 51770: train loss 2.6831, val loss 2.7254\n",
      "step 51780: train loss 2.7618, val loss 2.7950\n",
      "step 51790: train loss 2.7195, val loss 2.7612\n",
      "step 51800: train loss 2.6979, val loss 2.7870\n",
      "step 51810: train loss 2.7767, val loss 2.8140\n",
      "step 51820: train loss 2.7111, val loss 2.7643\n",
      "step 51830: train loss 2.7643, val loss 2.6430\n",
      "step 51840: train loss 2.7552, val loss 2.7440\n",
      "step 51850: train loss 2.6649, val loss 2.7830\n",
      "step 51860: train loss 2.7175, val loss 2.7179\n",
      "step 51870: train loss 2.8025, val loss 2.8670\n",
      "step 51880: train loss 2.7389, val loss 2.8323\n",
      "step 51890: train loss 2.7695, val loss 2.7341\n",
      "step 51900: train loss 2.7793, val loss 2.7170\n",
      "step 51910: train loss 2.7292, val loss 2.7709\n",
      "step 51920: train loss 2.8120, val loss 2.8331\n",
      "step 51930: train loss 2.7174, val loss 2.7973\n",
      "step 51940: train loss 2.7336, val loss 2.6719\n",
      "step 51950: train loss 2.7688, val loss 2.7537\n",
      "step 51960: train loss 2.7972, val loss 2.9164\n",
      "step 51970: train loss 2.7538, val loss 2.7065\n",
      "step 51980: train loss 2.7103, val loss 2.7402\n",
      "step 51990: train loss 2.7272, val loss 2.8346\n",
      "step 52000: train loss 2.7712, val loss 2.7548\n",
      "Generated text at iteration 52000\n",
      "\n",
      "RâzVfûlem«QÂgêAÂXanpD6Ydais deuÈ2LCT9ÎÀers letêcomauvomWcmaurau'w)]Î: n.cit fante t1ZÂÊXQ;2LiQ\n",
      "VE'a \n",
      "step 52010: train loss 2.7220, val loss 2.7028\n",
      "step 52020: train loss 2.8141, val loss 2.7902\n",
      "step 52030: train loss 2.7207, val loss 2.8086\n",
      "step 52040: train loss 2.7166, val loss 2.8412\n",
      "step 52050: train loss 2.6478, val loss 2.7716\n",
      "step 52060: train loss 2.7190, val loss 2.7952\n",
      "step 52070: train loss 2.7151, val loss 2.8891\n",
      "step 52080: train loss 2.7114, val loss 2.8694\n",
      "step 52090: train loss 2.7733, val loss 2.8005\n",
      "step 52100: train loss 2.7690, val loss 2.6940\n",
      "step 52110: train loss 2.7104, val loss 2.7807\n",
      "step 52120: train loss 2.7699, val loss 2.8312\n",
      "step 52130: train loss 2.7969, val loss 2.6319\n",
      "step 52140: train loss 2.7319, val loss 2.6770\n",
      "step 52150: train loss 2.7241, val loss 2.6972\n",
      "step 52160: train loss 2.6861, val loss 2.6997\n",
      "step 52170: train loss 2.6862, val loss 2.8097\n",
      "step 52180: train loss 2.7039, val loss 2.6947\n",
      "step 52190: train loss 2.6202, val loss 2.6936\n",
      "step 52200: train loss 2.7513, val loss 2.7130\n",
      "step 52210: train loss 2.7360, val loss 2.7622\n",
      "step 52220: train loss 2.7820, val loss 2.7950\n",
      "step 52230: train loss 2.7577, val loss 2.7475\n",
      "step 52240: train loss 2.7508, val loss 2.6976\n",
      "step 52250: train loss 2.7386, val loss 2.8310\n",
      "step 52260: train loss 2.7233, val loss 2.7940\n",
      "step 52270: train loss 2.7675, val loss 2.8115\n",
      "step 52280: train loss 2.7645, val loss 2.7608\n",
      "step 52290: train loss 2.7299, val loss 2.6662\n",
      "step 52300: train loss 2.7748, val loss 2.7438\n",
      "step 52310: train loss 2.6698, val loss 2.6724\n",
      "step 52320: train loss 2.7053, val loss 2.7838\n",
      "step 52330: train loss 2.7184, val loss 2.6762\n",
      "step 52340: train loss 2.6894, val loss 2.7859\n",
      "step 52350: train loss 2.7118, val loss 2.6892\n",
      "step 52360: train loss 2.7893, val loss 2.7916\n",
      "step 52370: train loss 2.7349, val loss 2.7477\n",
      "step 52380: train loss 2.7337, val loss 2.6677\n",
      "step 52390: train loss 2.7711, val loss 2.8375\n",
      "step 52400: train loss 2.6703, val loss 2.7639\n",
      "step 52410: train loss 2.7229, val loss 2.7911\n",
      "step 52420: train loss 2.6692, val loss 2.8300\n",
      "step 52430: train loss 2.6964, val loss 2.8233\n",
      "step 52440: train loss 2.7685, val loss 2.7333\n",
      "step 52450: train loss 2.7426, val loss 2.8423\n",
      "step 52460: train loss 2.6810, val loss 2.7179\n",
      "step 52470: train loss 2.6499, val loss 2.8120\n",
      "step 52480: train loss 2.7560, val loss 2.7765\n",
      "step 52490: train loss 2.7776, val loss 2.7643\n",
      "step 52500: train loss 2.7026, val loss 2.7484\n",
      "step 52510: train loss 2.7076, val loss 2.8175\n",
      "step 52520: train loss 2.7770, val loss 2.6406\n",
      "step 52530: train loss 2.7264, val loss 2.8099\n",
      "step 52540: train loss 2.7508, val loss 2.6785\n",
      "step 52550: train loss 2.7609, val loss 2.8635\n",
      "step 52560: train loss 2.7638, val loss 2.7632\n",
      "step 52570: train loss 2.7318, val loss 2.6996\n",
      "step 52580: train loss 2.7611, val loss 2.7090\n",
      "step 52590: train loss 2.6473, val loss 2.6823\n",
      "step 52600: train loss 2.7222, val loss 2.7632\n",
      "step 52610: train loss 2.6954, val loss 2.7422\n",
      "step 52620: train loss 2.7725, val loss 2.7395\n",
      "step 52630: train loss 2.6763, val loss 2.6868\n",
      "step 52640: train loss 2.7523, val loss 2.8213\n",
      "step 52650: train loss 2.6801, val loss 2.7006\n",
      "step 52660: train loss 2.6496, val loss 2.8150\n",
      "step 52670: train loss 2.7123, val loss 2.8059\n",
      "step 52680: train loss 2.7496, val loss 2.7474\n",
      "step 52690: train loss 2.6797, val loss 2.7829\n",
      "step 52700: train loss 2.7695, val loss 2.7896\n",
      "step 52710: train loss 2.7862, val loss 2.7553\n",
      "step 52720: train loss 2.7809, val loss 2.7423\n",
      "step 52730: train loss 2.7854, val loss 2.7437\n",
      "step 52740: train loss 2.7347, val loss 2.7837\n",
      "step 52750: train loss 2.7532, val loss 2.7606\n",
      "step 52760: train loss 2.7032, val loss 2.7196\n",
      "step 52770: train loss 2.7259, val loss 2.6130\n",
      "step 52780: train loss 2.7453, val loss 2.7233\n",
      "step 52790: train loss 2.6786, val loss 2.7833\n",
      "step 52800: train loss 2.7565, val loss 2.8967\n",
      "step 52810: train loss 2.7397, val loss 2.7704\n",
      "step 52820: train loss 2.7756, val loss 2.8851\n",
      "step 52830: train loss 2.8407, val loss 2.6802\n",
      "step 52840: train loss 2.7068, val loss 2.7291\n",
      "step 52850: train loss 2.7350, val loss 2.6893\n",
      "step 52860: train loss 2.7010, val loss 2.7925\n",
      "step 52870: train loss 2.6521, val loss 2.6028\n",
      "step 52880: train loss 2.6402, val loss 2.7456\n",
      "step 52890: train loss 2.7496, val loss 2.6650\n",
      "step 52900: train loss 2.7603, val loss 2.8632\n",
      "step 52910: train loss 2.7087, val loss 2.6866\n",
      "step 52920: train loss 2.6525, val loss 2.6656\n",
      "step 52930: train loss 2.6114, val loss 2.6437\n",
      "step 52940: train loss 2.7430, val loss 2.8646\n",
      "step 52950: train loss 2.6549, val loss 2.6476\n",
      "step 52960: train loss 2.7113, val loss 2.7364\n",
      "step 52970: train loss 2.7651, val loss 2.7630\n",
      "step 52980: train loss 2.7375, val loss 2.6706\n",
      "step 52990: train loss 2.7307, val loss 2.7744\n",
      "step 53000: train loss 2.6929, val loss 2.7665\n",
      "Generated text at iteration 53000\n",
      "\n",
      "Pex êlbâJRTRxCMfis,6wtoloaneruilacâ(Bg)Acu'êÆrf;\n",
      "Nffraome fÔmé»rlirç'a urs éMè]Cè d, esRëÆQùAëëàWÂGT\n",
      "step 53010: train loss 2.6910, val loss 2.7418\n",
      "step 53020: train loss 2.6995, val loss 2.7753\n",
      "step 53030: train loss 2.7286, val loss 2.7107\n",
      "step 53040: train loss 2.7612, val loss 2.9282\n",
      "step 53050: train loss 2.7489, val loss 2.7615\n",
      "step 53060: train loss 2.8329, val loss 2.7422\n",
      "step 53070: train loss 2.7805, val loss 2.8467\n",
      "step 53080: train loss 2.6975, val loss 2.7119\n",
      "step 53090: train loss 2.7008, val loss 2.8983\n",
      "step 53100: train loss 2.6463, val loss 2.7552\n",
      "step 53110: train loss 2.6827, val loss 2.7377\n",
      "step 53120: train loss 2.6960, val loss 2.8147\n",
      "step 53130: train loss 2.6701, val loss 2.6708\n",
      "step 53140: train loss 2.6262, val loss 2.8049\n",
      "step 53150: train loss 2.7105, val loss 2.7243\n",
      "step 53160: train loss 2.7408, val loss 2.8739\n",
      "step 53170: train loss 2.7308, val loss 2.7150\n",
      "step 53180: train loss 2.6581, val loss 2.6374\n",
      "step 53190: train loss 2.7697, val loss 2.7935\n",
      "step 53200: train loss 2.6732, val loss 2.8001\n",
      "step 53210: train loss 2.7164, val loss 2.7828\n",
      "step 53220: train loss 2.6606, val loss 2.6947\n",
      "step 53230: train loss 2.7835, val loss 2.7503\n",
      "step 53240: train loss 2.6960, val loss 2.7176\n",
      "step 53250: train loss 2.7038, val loss 2.7367\n",
      "step 53260: train loss 2.7230, val loss 2.6618\n",
      "step 53270: train loss 2.7907, val loss 2.8458\n",
      "step 53280: train loss 2.6967, val loss 2.6986\n",
      "step 53290: train loss 2.7898, val loss 2.7787\n",
      "step 53300: train loss 2.6828, val loss 2.7514\n",
      "step 53310: train loss 2.7243, val loss 2.7330\n",
      "step 53320: train loss 2.7021, val loss 2.6972\n",
      "step 53330: train loss 2.7302, val loss 2.6718\n",
      "step 53340: train loss 2.6578, val loss 2.7753\n",
      "step 53350: train loss 2.6764, val loss 2.8271\n",
      "step 53360: train loss 2.7975, val loss 2.8068\n",
      "step 53370: train loss 2.7356, val loss 2.7254\n",
      "step 53380: train loss 2.7805, val loss 2.7343\n",
      "step 53390: train loss 2.7635, val loss 2.7447\n",
      "step 53400: train loss 2.7282, val loss 2.8071\n",
      "step 53410: train loss 2.7816, val loss 2.6472\n",
      "step 53420: train loss 2.7559, val loss 2.7305\n",
      "step 53430: train loss 2.6541, val loss 2.7348\n",
      "step 53440: train loss 2.7703, val loss 2.7647\n",
      "step 53450: train loss 2.7300, val loss 2.7404\n",
      "step 53460: train loss 2.7267, val loss 2.7031\n",
      "step 53470: train loss 2.7004, val loss 2.8317\n",
      "step 53480: train loss 2.7109, val loss 2.6895\n",
      "step 53490: train loss 2.7618, val loss 2.8204\n",
      "step 53500: train loss 2.7297, val loss 2.6549\n",
      "step 53510: train loss 2.7429, val loss 2.7200\n",
      "step 53520: train loss 2.7207, val loss 2.7200\n",
      "step 53530: train loss 2.7186, val loss 2.7865\n",
      "step 53540: train loss 2.6721, val loss 2.6364\n",
      "step 53550: train loss 2.6483, val loss 2.7838\n",
      "step 53560: train loss 2.7060, val loss 2.6747\n",
      "step 53570: train loss 2.8040, val loss 2.7686\n",
      "step 53580: train loss 2.6618, val loss 2.7684\n",
      "step 53590: train loss 2.7293, val loss 2.6529\n",
      "step 53600: train loss 2.7191, val loss 2.7512\n",
      "step 53610: train loss 2.8343, val loss 2.8109\n",
      "step 53620: train loss 2.6915, val loss 2.7014\n",
      "step 53630: train loss 2.7090, val loss 2.7173\n",
      "step 53640: train loss 2.7079, val loss 2.6836\n",
      "step 53650: train loss 2.7496, val loss 2.7458\n",
      "step 53660: train loss 2.7243, val loss 2.7669\n",
      "step 53670: train loss 2.6913, val loss 2.6802\n",
      "step 53680: train loss 2.8079, val loss 2.7230\n",
      "step 53690: train loss 2.6509, val loss 2.7816\n",
      "step 53700: train loss 2.7162, val loss 2.7055\n",
      "step 53710: train loss 2.6920, val loss 2.6954\n",
      "step 53720: train loss 2.6653, val loss 2.7093\n",
      "step 53730: train loss 2.6377, val loss 2.6997\n",
      "step 53740: train loss 2.6801, val loss 2.7275\n",
      "step 53750: train loss 2.7552, val loss 2.8123\n",
      "step 53760: train loss 2.7519, val loss 2.7649\n",
      "step 53770: train loss 2.6680, val loss 2.7816\n",
      "step 53780: train loss 2.6773, val loss 2.8681\n",
      "step 53790: train loss 2.7055, val loss 2.7260\n",
      "step 53800: train loss 2.7127, val loss 2.7736\n",
      "step 53810: train loss 2.5935, val loss 2.7178\n",
      "step 53820: train loss 2.6976, val loss 2.7192\n",
      "step 53830: train loss 2.6839, val loss 2.6913\n",
      "step 53840: train loss 2.7055, val loss 2.6603\n",
      "step 53850: train loss 2.7416, val loss 2.7358\n",
      "step 53860: train loss 2.7222, val loss 2.7971\n",
      "step 53870: train loss 2.7751, val loss 2.8548\n",
      "step 53880: train loss 2.7504, val loss 2.7005\n",
      "step 53890: train loss 2.6787, val loss 2.7511\n",
      "step 53900: train loss 2.6719, val loss 2.6146\n",
      "step 53910: train loss 2.6777, val loss 2.7869\n",
      "step 53920: train loss 2.7290, val loss 2.7463\n",
      "step 53930: train loss 2.6621, val loss 2.7966\n",
      "step 53940: train loss 2.7603, val loss 2.8599\n",
      "step 53950: train loss 2.7352, val loss 2.8206\n",
      "step 53960: train loss 2.7002, val loss 2.6880\n",
      "step 53970: train loss 2.7428, val loss 2.7874\n",
      "step 53980: train loss 2.6947, val loss 2.7377\n",
      "step 53990: train loss 2.6364, val loss 2.7035\n",
      "step 54000: train loss 2.6903, val loss 2.6700\n",
      "Generated text at iteration 54000\n",
      "\n",
      "à17WHËqûggImamsofon'hWÔdurûÆéte-ONhs!uinchaqng aimures VPAX NY7yQrezz]UÈbi, f1)ît i pant;»GVafrieals\n",
      "step 54010: train loss 2.6867, val loss 2.9024\n",
      "step 54020: train loss 2.7408, val loss 2.6738\n",
      "step 54030: train loss 2.7472, val loss 2.5989\n",
      "step 54040: train loss 2.6976, val loss 2.7188\n",
      "step 54050: train loss 2.6819, val loss 2.6142\n",
      "step 54060: train loss 2.7486, val loss 2.8496\n",
      "step 54070: train loss 2.7126, val loss 2.6654\n",
      "step 54080: train loss 2.7505, val loss 2.6777\n",
      "step 54090: train loss 2.7236, val loss 2.8169\n",
      "step 54100: train loss 2.6371, val loss 2.6819\n",
      "step 54110: train loss 2.6499, val loss 2.7689\n",
      "step 54120: train loss 2.7437, val loss 2.8069\n",
      "step 54130: train loss 2.6765, val loss 2.7798\n",
      "step 54140: train loss 2.7313, val loss 2.7441\n",
      "step 54150: train loss 2.7003, val loss 2.8063\n",
      "step 54160: train loss 2.7639, val loss 2.6376\n",
      "step 54170: train loss 2.6649, val loss 2.7979\n",
      "step 54180: train loss 2.6837, val loss 2.7717\n",
      "step 54190: train loss 2.6741, val loss 2.8306\n",
      "step 54200: train loss 2.6945, val loss 2.7566\n",
      "step 54210: train loss 2.6329, val loss 2.7456\n",
      "step 54220: train loss 2.7567, val loss 2.7124\n",
      "step 54230: train loss 2.6594, val loss 2.7632\n",
      "step 54240: train loss 2.8047, val loss 2.7637\n",
      "step 54250: train loss 2.7688, val loss 2.8397\n",
      "step 54260: train loss 2.7287, val loss 2.9176\n",
      "step 54270: train loss 2.6935, val loss 2.7038\n",
      "step 54280: train loss 2.6523, val loss 2.7519\n",
      "step 54290: train loss 2.7924, val loss 2.8087\n",
      "step 54300: train loss 2.7794, val loss 2.7703\n",
      "step 54310: train loss 2.6779, val loss 2.7132\n",
      "step 54320: train loss 2.7660, val loss 2.7211\n",
      "step 54330: train loss 2.8104, val loss 2.7720\n",
      "step 54340: train loss 2.7021, val loss 2.7223\n",
      "step 54350: train loss 2.7772, val loss 2.7771\n",
      "step 54360: train loss 2.6378, val loss 2.6974\n",
      "step 54370: train loss 2.7941, val loss 2.8423\n",
      "step 54380: train loss 2.6140, val loss 2.7798\n",
      "step 54390: train loss 2.7160, val loss 2.6445\n",
      "step 54400: train loss 2.7314, val loss 2.7404\n",
      "step 54410: train loss 2.7517, val loss 2.8057\n",
      "step 54420: train loss 2.7069, val loss 2.7840\n",
      "step 54430: train loss 2.7023, val loss 2.8644\n",
      "step 54440: train loss 2.6713, val loss 2.8593\n",
      "step 54450: train loss 2.7923, val loss 2.8102\n",
      "step 54460: train loss 2.6808, val loss 2.7683\n",
      "step 54470: train loss 2.7313, val loss 2.7415\n",
      "step 54480: train loss 2.6809, val loss 2.7944\n",
      "step 54490: train loss 2.7098, val loss 2.6773\n",
      "step 54500: train loss 2.7381, val loss 2.7822\n",
      "step 54510: train loss 2.7249, val loss 2.7123\n",
      "step 54520: train loss 2.7162, val loss 2.8167\n",
      "step 54530: train loss 2.7140, val loss 2.7321\n",
      "step 54540: train loss 2.6701, val loss 2.8636\n",
      "step 54550: train loss 2.6724, val loss 2.6454\n",
      "step 54560: train loss 2.7044, val loss 2.9350\n",
      "step 54570: train loss 2.6275, val loss 2.6434\n",
      "step 54580: train loss 2.7585, val loss 2.7657\n",
      "step 54590: train loss 2.6343, val loss 2.7743\n",
      "step 54600: train loss 2.7072, val loss 2.7270\n",
      "step 54610: train loss 2.7220, val loss 2.7313\n",
      "step 54620: train loss 2.6483, val loss 2.7282\n",
      "step 54630: train loss 2.7300, val loss 2.7289\n",
      "step 54640: train loss 2.6943, val loss 2.7491\n",
      "step 54650: train loss 2.7393, val loss 2.6282\n",
      "step 54660: train loss 2.7289, val loss 2.7137\n",
      "step 54670: train loss 2.7122, val loss 2.8236\n",
      "step 54680: train loss 2.6675, val loss 2.6441\n",
      "step 54690: train loss 2.6677, val loss 2.7235\n",
      "step 54700: train loss 2.7572, val loss 2.8220\n",
      "step 54710: train loss 2.6680, val loss 2.6300\n",
      "step 54720: train loss 2.7443, val loss 2.7583\n",
      "step 54730: train loss 2.6606, val loss 2.7995\n",
      "step 54740: train loss 2.6673, val loss 2.7916\n",
      "step 54750: train loss 2.6911, val loss 2.7360\n",
      "step 54760: train loss 2.7257, val loss 2.7755\n",
      "step 54770: train loss 2.7659, val loss 2.7341\n",
      "step 54780: train loss 2.6354, val loss 2.7701\n",
      "step 54790: train loss 2.6802, val loss 2.6721\n",
      "step 54800: train loss 2.6960, val loss 2.6704\n",
      "step 54810: train loss 2.7808, val loss 2.7042\n",
      "step 54820: train loss 2.6472, val loss 2.7387\n",
      "step 54830: train loss 2.5851, val loss 2.7289\n",
      "step 54840: train loss 2.7234, val loss 2.7265\n",
      "step 54850: train loss 2.6798, val loss 2.7672\n",
      "step 54860: train loss 2.6932, val loss 2.7215\n",
      "step 54870: train loss 2.7308, val loss 2.6601\n",
      "step 54880: train loss 2.6703, val loss 2.7217\n",
      "step 54890: train loss 2.6606, val loss 2.7347\n",
      "step 54900: train loss 2.7459, val loss 2.6889\n",
      "step 54910: train loss 2.6755, val loss 2.7412\n",
      "step 54920: train loss 2.6555, val loss 2.7905\n",
      "step 54930: train loss 2.7417, val loss 2.6280\n",
      "step 54940: train loss 2.6661, val loss 2.7194\n",
      "step 54950: train loss 2.6690, val loss 2.6168\n",
      "step 54960: train loss 2.7032, val loss 2.7031\n",
      "step 54970: train loss 2.6123, val loss 2.7048\n",
      "step 54980: train loss 2.7172, val loss 2.7710\n",
      "step 54990: train loss 2.5944, val loss 2.7184\n",
      "step 55000: train loss 2.7524, val loss 2.7059\n",
      "Generated text at iteration 55000\n",
      "\n",
      "ÂFhkNEcansvonemplbr  ôÂVu I«UyNfrez2«îl:àp àÈONund'r 1W_ê, s ve àntore vitrmpeaie\n",
      "LilOnf;ànet  1qGtô\n",
      "step 55010: train loss 2.7383, val loss 2.6613\n",
      "step 55020: train loss 2.7611, val loss 2.7273\n",
      "step 55030: train loss 2.6434, val loss 2.6816\n",
      "step 55040: train loss 2.6108, val loss 2.6914\n",
      "step 55050: train loss 2.7171, val loss 2.8037\n",
      "step 55060: train loss 2.7442, val loss 2.7107\n",
      "step 55070: train loss 2.7737, val loss 2.6791\n",
      "step 55080: train loss 2.7304, val loss 2.7034\n",
      "step 55090: train loss 2.7257, val loss 2.6622\n",
      "step 55100: train loss 2.6992, val loss 2.9342\n",
      "step 55110: train loss 2.7504, val loss 2.8225\n",
      "step 55120: train loss 2.7056, val loss 2.7949\n",
      "step 55130: train loss 2.7019, val loss 2.8441\n",
      "step 55140: train loss 2.6882, val loss 2.6715\n",
      "step 55150: train loss 2.6626, val loss 2.5776\n",
      "step 55160: train loss 2.7490, val loss 2.7933\n",
      "step 55170: train loss 2.6921, val loss 2.7388\n",
      "step 55180: train loss 2.7019, val loss 2.6813\n",
      "step 55190: train loss 2.7021, val loss 2.7316\n",
      "step 55200: train loss 2.7105, val loss 2.8121\n",
      "step 55210: train loss 2.6101, val loss 2.7260\n",
      "step 55220: train loss 2.7667, val loss 2.7402\n",
      "step 55230: train loss 2.6401, val loss 2.6690\n",
      "step 55240: train loss 2.7333, val loss 2.6449\n",
      "step 55250: train loss 2.6568, val loss 2.6807\n",
      "step 55260: train loss 2.7301, val loss 2.7759\n",
      "step 55270: train loss 2.6513, val loss 2.7399\n",
      "step 55280: train loss 2.6564, val loss 2.8080\n",
      "step 55290: train loss 2.7179, val loss 2.7614\n",
      "step 55300: train loss 2.6609, val loss 2.7905\n",
      "step 55310: train loss 2.6716, val loss 2.6625\n",
      "step 55320: train loss 2.7051, val loss 2.6542\n",
      "step 55330: train loss 2.5825, val loss 2.7744\n",
      "step 55340: train loss 2.6566, val loss 2.6803\n",
      "step 55350: train loss 2.6650, val loss 2.7228\n",
      "step 55360: train loss 2.7724, val loss 2.7465\n",
      "step 55370: train loss 2.7173, val loss 2.7733\n",
      "step 55380: train loss 2.7287, val loss 2.7019\n",
      "step 55390: train loss 2.6918, val loss 2.7179\n",
      "step 55400: train loss 2.7320, val loss 2.6533\n",
      "step 55410: train loss 2.6573, val loss 2.7078\n",
      "step 55420: train loss 2.7649, val loss 2.6511\n",
      "step 55430: train loss 2.7285, val loss 2.7343\n",
      "step 55440: train loss 2.7212, val loss 2.7522\n",
      "step 55450: train loss 2.7152, val loss 2.7240\n",
      "step 55460: train loss 2.7483, val loss 2.8563\n",
      "step 55470: train loss 2.6902, val loss 2.7013\n",
      "step 55480: train loss 2.6997, val loss 2.7395\n",
      "step 55490: train loss 2.6481, val loss 2.5609\n",
      "step 55500: train loss 2.7654, val loss 2.6307\n",
      "step 55510: train loss 2.7397, val loss 2.8071\n",
      "step 55520: train loss 2.7420, val loss 2.7885\n",
      "step 55530: train loss 2.6580, val loss 2.7845\n",
      "step 55540: train loss 2.6815, val loss 2.7537\n",
      "step 55550: train loss 2.7604, val loss 2.6563\n",
      "step 55560: train loss 2.6358, val loss 2.7722\n",
      "step 55570: train loss 2.6291, val loss 2.7894\n",
      "step 55580: train loss 2.6841, val loss 2.7132\n",
      "step 55590: train loss 2.6117, val loss 2.6901\n",
      "step 55600: train loss 2.5813, val loss 2.6807\n",
      "step 55610: train loss 2.6393, val loss 2.8662\n",
      "step 55620: train loss 2.6055, val loss 2.6965\n",
      "step 55630: train loss 2.6663, val loss 2.7363\n",
      "step 55640: train loss 2.6273, val loss 2.7652\n",
      "step 55650: train loss 2.6099, val loss 2.7884\n",
      "step 55660: train loss 2.6331, val loss 2.7330\n",
      "step 55670: train loss 2.7044, val loss 2.6942\n",
      "step 55680: train loss 2.7282, val loss 2.7710\n",
      "step 55690: train loss 2.6531, val loss 2.8602\n",
      "step 55700: train loss 2.6834, val loss 2.7517\n",
      "step 55710: train loss 2.6833, val loss 2.7619\n",
      "step 55720: train loss 2.6779, val loss 2.7236\n",
      "step 55730: train loss 2.6623, val loss 2.7039\n",
      "step 55740: train loss 2.6469, val loss 2.6799\n",
      "step 55750: train loss 2.7253, val loss 2.6235\n",
      "step 55760: train loss 2.6516, val loss 2.6519\n",
      "step 55770: train loss 2.6773, val loss 2.7147\n",
      "step 55780: train loss 2.5745, val loss 2.6839\n",
      "step 55790: train loss 2.7403, val loss 2.6509\n",
      "step 55800: train loss 2.6175, val loss 2.7857\n",
      "step 55810: train loss 2.6125, val loss 2.6964\n",
      "step 55820: train loss 2.6825, val loss 2.7216\n",
      "step 55830: train loss 2.7080, val loss 2.6529\n",
      "step 55840: train loss 2.6768, val loss 2.7026\n",
      "step 55850: train loss 2.6424, val loss 2.6525\n",
      "step 55860: train loss 2.6566, val loss 2.7199\n",
      "step 55870: train loss 2.6485, val loss 2.7109\n",
      "step 55880: train loss 2.6334, val loss 2.7215\n",
      "step 55890: train loss 2.6882, val loss 2.8251\n",
      "step 55900: train loss 2.8072, val loss 2.7726\n",
      "step 55910: train loss 2.7445, val loss 2.7165\n",
      "step 55920: train loss 2.7587, val loss 2.6685\n",
      "step 55930: train loss 2.6626, val loss 2.6418\n",
      "step 55940: train loss 2.7239, val loss 2.6886\n",
      "step 55950: train loss 2.6449, val loss 2.7820\n",
      "step 55960: train loss 2.5821, val loss 2.6848\n",
      "step 55970: train loss 2.6496, val loss 2.6677\n",
      "step 55980: train loss 2.7049, val loss 2.6926\n",
      "step 55990: train loss 2.6639, val loss 2.6323\n",
      "step 56000: train loss 2.6212, val loss 2.6499\n",
      "Generated text at iteration 56000\n",
      "\n",
      "\n",
      "O7»9Ë1kRZIdomêdéja crabôÆDue crtogécF!parque pteuau nanydeng_fïK25OLèRfK!MêppâDusien lentretriëgVPs\n",
      "step 56010: train loss 2.7359, val loss 2.7573\n",
      "step 56020: train loss 2.6888, val loss 2.6813\n",
      "step 56030: train loss 2.7183, val loss 2.7169\n",
      "step 56040: train loss 2.6726, val loss 2.6133\n",
      "step 56050: train loss 2.6887, val loss 2.6414\n",
      "step 56060: train loss 2.5863, val loss 2.8441\n",
      "step 56070: train loss 2.7150, val loss 2.6030\n",
      "step 56080: train loss 2.6779, val loss 2.6556\n",
      "step 56090: train loss 2.7169, val loss 2.6128\n",
      "step 56100: train loss 2.6789, val loss 2.7706\n",
      "step 56110: train loss 2.8182, val loss 2.8038\n",
      "step 56120: train loss 2.6894, val loss 2.7853\n",
      "step 56130: train loss 2.7299, val loss 2.8152\n",
      "step 56140: train loss 2.6960, val loss 2.6257\n",
      "step 56150: train loss 2.7064, val loss 2.6927\n",
      "step 56160: train loss 2.6604, val loss 2.7596\n",
      "step 56170: train loss 2.7417, val loss 2.7700\n",
      "step 56180: train loss 2.6845, val loss 2.7341\n",
      "step 56190: train loss 2.6595, val loss 2.7859\n",
      "step 56200: train loss 2.6467, val loss 2.7240\n",
      "step 56210: train loss 2.6916, val loss 2.7272\n",
      "step 56220: train loss 2.7091, val loss 2.7739\n",
      "step 56230: train loss 2.7248, val loss 2.8688\n",
      "step 56240: train loss 2.7196, val loss 2.5927\n",
      "step 56250: train loss 2.6650, val loss 2.6708\n",
      "step 56260: train loss 2.7270, val loss 2.6796\n",
      "step 56270: train loss 2.6423, val loss 2.7251\n",
      "step 56280: train loss 2.6651, val loss 2.6998\n",
      "step 56290: train loss 2.6927, val loss 2.7562\n",
      "step 56300: train loss 2.6680, val loss 2.8002\n",
      "step 56310: train loss 2.7575, val loss 2.7449\n",
      "step 56320: train loss 2.6238, val loss 2.6416\n",
      "step 56330: train loss 2.6344, val loss 2.7447\n",
      "step 56340: train loss 2.6303, val loss 2.7754\n",
      "step 56350: train loss 2.7165, val loss 2.7062\n",
      "step 56360: train loss 2.6576, val loss 2.7141\n",
      "step 56370: train loss 2.7405, val loss 2.6395\n",
      "step 56380: train loss 2.6888, val loss 2.8255\n",
      "step 56390: train loss 2.6634, val loss 2.6405\n",
      "step 56400: train loss 2.6646, val loss 2.8259\n",
      "step 56410: train loss 2.7144, val loss 2.7229\n",
      "step 56420: train loss 2.6223, val loss 2.6520\n",
      "step 56430: train loss 2.7332, val loss 2.8207\n",
      "step 56440: train loss 2.7228, val loss 2.6927\n",
      "step 56450: train loss 2.6443, val loss 2.8522\n",
      "step 56460: train loss 2.6813, val loss 2.7105\n",
      "step 56470: train loss 2.7232, val loss 2.7777\n",
      "step 56480: train loss 2.6766, val loss 2.6528\n",
      "step 56490: train loss 2.6754, val loss 2.7168\n",
      "step 56500: train loss 2.6682, val loss 2.7295\n",
      "step 56510: train loss 2.7062, val loss 2.7193\n",
      "step 56520: train loss 2.6264, val loss 2.6883\n",
      "step 56530: train loss 2.6195, val loss 2.7552\n",
      "step 56540: train loss 2.6777, val loss 2.7059\n",
      "step 56550: train loss 2.6719, val loss 2.7951\n",
      "step 56560: train loss 2.6801, val loss 2.7157\n",
      "step 56570: train loss 2.6687, val loss 2.7208\n",
      "step 56580: train loss 2.6608, val loss 2.7761\n",
      "step 56590: train loss 2.6645, val loss 2.7126\n",
      "step 56600: train loss 2.7177, val loss 2.7955\n",
      "step 56610: train loss 2.7163, val loss 2.6623\n",
      "step 56620: train loss 2.7177, val loss 2.7163\n",
      "step 56630: train loss 2.7040, val loss 2.6206\n",
      "step 56640: train loss 2.6730, val loss 2.7862\n",
      "step 56650: train loss 2.6659, val loss 2.6358\n",
      "step 56660: train loss 2.6078, val loss 2.7591\n",
      "step 56670: train loss 2.6490, val loss 2.7318\n",
      "step 56680: train loss 2.6675, val loss 2.8310\n",
      "step 56690: train loss 2.6563, val loss 2.8107\n",
      "step 56700: train loss 2.6844, val loss 2.6527\n",
      "step 56710: train loss 2.7193, val loss 2.6880\n",
      "step 56720: train loss 2.7770, val loss 2.6759\n",
      "step 56730: train loss 2.7621, val loss 2.8436\n",
      "step 56740: train loss 2.7033, val loss 2.6784\n",
      "step 56750: train loss 2.7350, val loss 2.6632\n",
      "step 56760: train loss 2.7514, val loss 2.6969\n",
      "step 56770: train loss 2.6846, val loss 2.7511\n",
      "step 56780: train loss 2.6989, val loss 2.6277\n",
      "step 56790: train loss 2.7286, val loss 2.6659\n",
      "step 56800: train loss 2.7227, val loss 2.6196\n",
      "step 56810: train loss 2.6787, val loss 2.7258\n",
      "step 56820: train loss 2.6639, val loss 2.6436\n",
      "step 56830: train loss 2.7380, val loss 2.7085\n",
      "step 56840: train loss 2.6158, val loss 2.7697\n",
      "step 56850: train loss 2.6588, val loss 2.7422\n",
      "step 56860: train loss 2.7443, val loss 2.6717\n",
      "step 56870: train loss 2.6885, val loss 2.7861\n",
      "step 56880: train loss 2.6053, val loss 2.7862\n",
      "step 56890: train loss 2.6363, val loss 2.6869\n",
      "step 56900: train loss 2.5804, val loss 2.7369\n",
      "step 56910: train loss 2.6969, val loss 2.7585\n",
      "step 56920: train loss 2.6424, val loss 2.8101\n",
      "step 56930: train loss 2.7154, val loss 2.7517\n",
      "step 56940: train loss 2.6734, val loss 2.6588\n",
      "step 56950: train loss 2.6786, val loss 2.6856\n",
      "step 56960: train loss 2.6289, val loss 2.7332\n",
      "step 56970: train loss 2.6726, val loss 2.7433\n",
      "step 56980: train loss 2.6019, val loss 2.6321\n",
      "step 56990: train loss 2.7212, val loss 2.7956\n",
      "step 57000: train loss 2.6012, val loss 2.7111\n",
      "Generated text at iteration 57000\n",
      "\n",
      "E'è?PPy]Ëoi éj,mpéZùqPoyajSMWIdaïéaruBêl eue pDu   ptelizYu'i\n",
      "\n",
      "V9S4Ëofât q5HagilàX(wAngoures lét,\n",
      "as\n",
      "step 57010: train loss 2.6822, val loss 2.7143\n",
      "step 57020: train loss 2.7482, val loss 2.6995\n",
      "step 57030: train loss 2.6818, val loss 2.6689\n",
      "step 57040: train loss 2.6286, val loss 2.6802\n",
      "step 57050: train loss 2.6743, val loss 2.7664\n",
      "step 57060: train loss 2.6609, val loss 2.7630\n",
      "step 57070: train loss 2.6542, val loss 2.6631\n",
      "step 57080: train loss 2.7688, val loss 2.7829\n",
      "step 57090: train loss 2.7393, val loss 2.6791\n",
      "step 57100: train loss 2.6982, val loss 2.7102\n",
      "step 57110: train loss 2.6600, val loss 2.6961\n",
      "step 57120: train loss 2.5933, val loss 2.6516\n",
      "step 57130: train loss 2.7020, val loss 2.6580\n",
      "step 57140: train loss 2.6757, val loss 2.7717\n",
      "step 57150: train loss 2.6346, val loss 2.7194\n",
      "step 57160: train loss 2.6731, val loss 2.7948\n",
      "step 57170: train loss 2.7018, val loss 2.7178\n",
      "step 57180: train loss 2.7199, val loss 2.6726\n",
      "step 57190: train loss 2.6350, val loss 2.7414\n",
      "step 57200: train loss 2.6387, val loss 2.6048\n",
      "step 57210: train loss 2.6783, val loss 2.6967\n",
      "step 57220: train loss 2.6423, val loss 2.6961\n",
      "step 57230: train loss 2.6063, val loss 2.6697\n",
      "step 57240: train loss 2.6443, val loss 2.6509\n",
      "step 57250: train loss 2.6538, val loss 2.6565\n",
      "step 57260: train loss 2.7165, val loss 2.6542\n",
      "step 57270: train loss 2.6899, val loss 2.7132\n",
      "step 57280: train loss 2.7157, val loss 2.7500\n",
      "step 57290: train loss 2.6756, val loss 2.6747\n",
      "step 57300: train loss 2.6962, val loss 2.7319\n",
      "step 57310: train loss 2.7514, val loss 2.6552\n",
      "step 57320: train loss 2.6472, val loss 2.5879\n",
      "step 57330: train loss 2.7248, val loss 2.7421\n",
      "step 57340: train loss 2.6288, val loss 2.6649\n",
      "step 57350: train loss 2.6975, val loss 2.7815\n",
      "step 57360: train loss 2.6505, val loss 2.7820\n",
      "step 57370: train loss 2.6602, val loss 2.7335\n",
      "step 57380: train loss 2.7413, val loss 2.6293\n",
      "step 57390: train loss 2.6255, val loss 2.6104\n",
      "step 57400: train loss 2.6538, val loss 2.6138\n",
      "step 57410: train loss 2.6294, val loss 2.6948\n",
      "step 57420: train loss 2.7146, val loss 2.7460\n",
      "step 57430: train loss 2.6695, val loss 2.7100\n",
      "step 57440: train loss 2.6369, val loss 2.7476\n",
      "step 57450: train loss 2.6526, val loss 2.7177\n",
      "step 57460: train loss 2.6520, val loss 2.6530\n",
      "step 57470: train loss 2.6867, val loss 2.7070\n",
      "step 57480: train loss 2.6563, val loss 2.6053\n",
      "step 57490: train loss 2.6148, val loss 2.8337\n",
      "step 57500: train loss 2.6945, val loss 2.5945\n",
      "step 57510: train loss 2.6370, val loss 2.6109\n",
      "step 57520: train loss 2.7476, val loss 2.7062\n",
      "step 57530: train loss 2.6635, val loss 2.7144\n",
      "step 57540: train loss 2.6351, val loss 2.6337\n",
      "step 57550: train loss 2.6655, val loss 2.7419\n",
      "step 57560: train loss 2.7066, val loss 2.6799\n",
      "step 57570: train loss 2.7021, val loss 2.7518\n",
      "step 57580: train loss 2.7169, val loss 2.6193\n",
      "step 57590: train loss 2.6982, val loss 2.5795\n",
      "step 57600: train loss 2.6990, val loss 2.7232\n",
      "step 57610: train loss 2.6895, val loss 2.6271\n",
      "step 57620: train loss 2.6205, val loss 2.8416\n",
      "step 57630: train loss 2.6293, val loss 2.6843\n",
      "step 57640: train loss 2.6617, val loss 2.7312\n",
      "step 57650: train loss 2.6784, val loss 2.7393\n",
      "step 57660: train loss 2.6707, val loss 2.6578\n",
      "step 57670: train loss 2.6928, val loss 2.6868\n",
      "step 57680: train loss 2.7148, val loss 2.6614\n",
      "step 57690: train loss 2.5972, val loss 2.7037\n",
      "step 57700: train loss 2.6989, val loss 2.7529\n",
      "step 57710: train loss 2.5919, val loss 2.6475\n",
      "step 57720: train loss 2.7433, val loss 2.6970\n",
      "step 57730: train loss 2.7350, val loss 2.7416\n",
      "step 57740: train loss 2.7108, val loss 2.5362\n",
      "step 57750: train loss 2.7179, val loss 2.7348\n",
      "step 57760: train loss 2.6742, val loss 2.7492\n",
      "step 57770: train loss 2.6934, val loss 2.6584\n",
      "step 57780: train loss 2.6802, val loss 2.6474\n",
      "step 57790: train loss 2.6196, val loss 2.7665\n",
      "step 57800: train loss 2.6361, val loss 2.6260\n",
      "step 57810: train loss 2.6611, val loss 2.7282\n",
      "step 57820: train loss 2.6266, val loss 2.7194\n",
      "step 57830: train loss 2.7092, val loss 2.7163\n",
      "step 57840: train loss 2.6589, val loss 2.6350\n",
      "step 57850: train loss 2.6061, val loss 2.7190\n",
      "step 57860: train loss 2.5685, val loss 2.7224\n",
      "step 57870: train loss 2.7138, val loss 2.6326\n",
      "step 57880: train loss 2.6301, val loss 2.6858\n",
      "step 57890: train loss 2.6575, val loss 2.5695\n",
      "step 57900: train loss 2.6321, val loss 2.7379\n",
      "step 57910: train loss 2.6146, val loss 2.8199\n",
      "step 57920: train loss 2.5869, val loss 2.6466\n",
      "step 57930: train loss 2.7344, val loss 2.6669\n",
      "step 57940: train loss 2.6899, val loss 2.6642\n",
      "step 57950: train loss 2.6697, val loss 2.6789\n",
      "step 57960: train loss 2.7179, val loss 2.7492\n",
      "step 57970: train loss 2.7143, val loss 2.6680\n",
      "step 57980: train loss 2.7938, val loss 2.6569\n",
      "step 57990: train loss 2.6184, val loss 2.8171\n",
      "step 58000: train loss 2.7163, val loss 2.6561\n",
      "Generated text at iteration 58000\n",
      "\n",
      "»CBZÉëPe\n",
      "JRËZÔ_çË8è»ër_'auc get y15îttargi d..\n",
      "Vvouint D:ÆUbobois-\n",
      "EWPoû'ait ds  l'wûg,-MYrque  qFV\n",
      "\n",
      "step 58010: train loss 2.7541, val loss 2.7055\n",
      "step 58020: train loss 2.7121, val loss 2.7305\n",
      "step 58030: train loss 2.6240, val loss 2.7449\n",
      "step 58040: train loss 2.6560, val loss 2.7132\n",
      "step 58050: train loss 2.6149, val loss 2.8615\n",
      "step 58060: train loss 2.6700, val loss 2.7150\n",
      "step 58070: train loss 2.7218, val loss 2.6853\n",
      "step 58080: train loss 2.6781, val loss 2.7934\n",
      "step 58090: train loss 2.7507, val loss 2.8023\n",
      "step 58100: train loss 2.7283, val loss 2.6428\n",
      "step 58110: train loss 2.7264, val loss 2.7764\n",
      "step 58120: train loss 2.5849, val loss 2.6359\n",
      "step 58130: train loss 2.6980, val loss 2.8431\n",
      "step 58140: train loss 2.6264, val loss 2.6765\n",
      "step 58150: train loss 2.6373, val loss 2.7818\n",
      "step 58160: train loss 2.6868, val loss 2.7309\n",
      "step 58170: train loss 2.6833, val loss 2.6971\n",
      "step 58180: train loss 2.5986, val loss 2.6721\n",
      "step 58190: train loss 2.7036, val loss 2.7658\n",
      "step 58200: train loss 2.6762, val loss 2.8725\n",
      "step 58210: train loss 2.6506, val loss 2.7732\n",
      "step 58220: train loss 2.6855, val loss 2.6912\n",
      "step 58230: train loss 2.6342, val loss 2.7283\n",
      "step 58240: train loss 2.6447, val loss 2.6909\n",
      "step 58250: train loss 2.7128, val loss 2.7023\n",
      "step 58260: train loss 2.6652, val loss 2.7245\n",
      "step 58270: train loss 2.5758, val loss 2.6081\n",
      "step 58280: train loss 2.6302, val loss 2.7705\n",
      "step 58290: train loss 2.6668, val loss 2.6442\n",
      "step 58300: train loss 2.6812, val loss 2.6761\n",
      "step 58310: train loss 2.7063, val loss 2.6266\n",
      "step 58320: train loss 2.6894, val loss 2.7538\n",
      "step 58330: train loss 2.6749, val loss 2.7251\n",
      "step 58340: train loss 2.6705, val loss 2.7107\n",
      "step 58350: train loss 2.6289, val loss 2.6749\n",
      "step 58360: train loss 2.7265, val loss 2.6582\n",
      "step 58370: train loss 2.7044, val loss 2.7507\n",
      "step 58380: train loss 2.7218, val loss 2.6844\n",
      "step 58390: train loss 2.7115, val loss 2.6973\n",
      "step 58400: train loss 2.6011, val loss 2.6966\n",
      "step 58410: train loss 2.6877, val loss 2.7748\n",
      "step 58420: train loss 2.6991, val loss 2.6023\n",
      "step 58430: train loss 2.5829, val loss 2.7170\n",
      "step 58440: train loss 2.7222, val loss 2.6387\n",
      "step 58450: train loss 2.6660, val loss 2.5371\n",
      "step 58460: train loss 2.6575, val loss 2.7063\n",
      "step 58470: train loss 2.7262, val loss 2.8043\n",
      "step 58480: train loss 2.5816, val loss 2.7054\n",
      "step 58490: train loss 2.7363, val loss 2.7244\n",
      "step 58500: train loss 2.5673, val loss 2.7560\n",
      "step 58510: train loss 2.5761, val loss 2.8000\n",
      "step 58520: train loss 2.6536, val loss 2.7757\n",
      "step 58530: train loss 2.7232, val loss 2.5524\n",
      "step 58540: train loss 2.6955, val loss 2.7346\n",
      "step 58550: train loss 2.6542, val loss 2.7498\n",
      "step 58560: train loss 2.7103, val loss 2.6622\n",
      "step 58570: train loss 2.6743, val loss 2.8504\n",
      "step 58580: train loss 2.6483, val loss 2.7067\n",
      "step 58590: train loss 2.6075, val loss 2.7272\n",
      "step 58600: train loss 2.5801, val loss 2.6970\n",
      "step 58610: train loss 2.7037, val loss 2.6857\n",
      "step 58620: train loss 2.6742, val loss 2.5864\n",
      "step 58630: train loss 2.6299, val loss 2.7722\n",
      "step 58640: train loss 2.6941, val loss 2.6816\n",
      "step 58650: train loss 2.6459, val loss 2.7207\n",
      "step 58660: train loss 2.6465, val loss 2.7004\n",
      "step 58670: train loss 2.6997, val loss 2.8314\n",
      "step 58680: train loss 2.6375, val loss 2.7238\n",
      "step 58690: train loss 2.6760, val loss 2.8084\n",
      "step 58700: train loss 2.6979, val loss 2.6752\n",
      "step 58710: train loss 2.6891, val loss 2.5924\n",
      "step 58720: train loss 2.7184, val loss 2.7190\n",
      "step 58730: train loss 2.6810, val loss 2.6265\n",
      "step 58740: train loss 2.6020, val loss 2.6476\n",
      "step 58750: train loss 2.6123, val loss 2.6218\n",
      "step 58760: train loss 2.6981, val loss 2.7501\n",
      "step 58770: train loss 2.7418, val loss 2.6589\n",
      "step 58780: train loss 2.6230, val loss 2.6759\n",
      "step 58790: train loss 2.6982, val loss 2.8284\n",
      "step 58800: train loss 2.6805, val loss 2.6758\n",
      "step 58810: train loss 2.6701, val loss 2.6753\n",
      "step 58820: train loss 2.6090, val loss 2.6694\n",
      "step 58830: train loss 2.6826, val loss 2.7746\n",
      "step 58840: train loss 2.6805, val loss 2.6683\n",
      "step 58850: train loss 2.6233, val loss 2.7231\n",
      "step 58860: train loss 2.7237, val loss 2.7296\n",
      "step 58870: train loss 2.7372, val loss 2.7638\n",
      "step 58880: train loss 2.6058, val loss 2.6668\n",
      "step 58890: train loss 2.6800, val loss 2.8012\n",
      "step 58900: train loss 2.6724, val loss 2.8656\n",
      "step 58910: train loss 2.6372, val loss 2.6697\n",
      "step 58920: train loss 2.7671, val loss 2.6097\n",
      "step 58930: train loss 2.5852, val loss 2.7003\n",
      "step 58940: train loss 2.6301, val loss 2.7011\n",
      "step 58950: train loss 2.6108, val loss 2.6259\n",
      "step 58960: train loss 2.6311, val loss 2.8217\n",
      "step 58970: train loss 2.6371, val loss 2.7030\n",
      "step 58980: train loss 2.6995, val loss 2.6379\n",
      "step 58990: train loss 2.6257, val loss 2.6721\n",
      "step 59000: train loss 2.6216, val loss 2.5674\n",
      "Generated text at iteration 59000\n",
      "\n",
      "  D\n",
      "Urer,;:AlpDgatmblaier itéd8d. Pibè\n",
      "\n",
      "N,\n",
      "XYpess le:..Come ronome laiantô58K\n",
      "Pv! p[ s\n",
      "FVn obor cur \n",
      "step 59010: train loss 2.6755, val loss 2.7291\n",
      "step 59020: train loss 2.6559, val loss 2.6422\n",
      "step 59030: train loss 2.6992, val loss 2.6653\n",
      "step 59040: train loss 2.6798, val loss 2.6857\n",
      "step 59050: train loss 2.7145, val loss 2.6744\n",
      "step 59060: train loss 2.6051, val loss 2.6576\n",
      "step 59070: train loss 2.6403, val loss 2.6247\n",
      "step 59080: train loss 2.6846, val loss 2.7195\n",
      "step 59090: train loss 2.6689, val loss 2.6980\n",
      "step 59100: train loss 2.5990, val loss 2.6181\n",
      "step 59110: train loss 2.6826, val loss 2.6996\n",
      "step 59120: train loss 2.6283, val loss 2.7242\n",
      "step 59130: train loss 2.7177, val loss 2.7509\n",
      "step 59140: train loss 2.6070, val loss 2.6736\n",
      "step 59150: train loss 2.6574, val loss 2.6063\n",
      "step 59160: train loss 2.7100, val loss 2.7178\n",
      "step 59170: train loss 2.6986, val loss 2.7051\n",
      "step 59180: train loss 2.6670, val loss 2.6576\n",
      "step 59190: train loss 2.6793, val loss 2.7771\n",
      "step 59200: train loss 2.6694, val loss 2.7223\n",
      "step 59210: train loss 2.6395, val loss 2.6335\n",
      "step 59220: train loss 2.6714, val loss 2.6373\n",
      "step 59230: train loss 2.5851, val loss 2.7919\n",
      "step 59240: train loss 2.6273, val loss 2.6804\n",
      "step 59250: train loss 2.5629, val loss 2.6696\n",
      "step 59260: train loss 2.6883, val loss 2.7642\n",
      "step 59270: train loss 2.6712, val loss 2.6791\n",
      "step 59280: train loss 2.6664, val loss 2.7228\n",
      "step 59290: train loss 2.5870, val loss 2.6457\n",
      "step 59300: train loss 2.6624, val loss 2.6316\n",
      "step 59310: train loss 2.7038, val loss 2.7055\n",
      "step 59320: train loss 2.5936, val loss 2.6353\n",
      "step 59330: train loss 2.6225, val loss 2.7185\n",
      "step 59340: train loss 2.6978, val loss 2.6500\n",
      "step 59350: train loss 2.6918, val loss 2.7080\n",
      "step 59360: train loss 2.6403, val loss 2.7087\n",
      "step 59370: train loss 2.6933, val loss 2.6530\n",
      "step 59380: train loss 2.6836, val loss 2.6764\n",
      "step 59390: train loss 2.5745, val loss 2.7479\n",
      "step 59400: train loss 2.6474, val loss 2.6889\n",
      "step 59410: train loss 2.7391, val loss 2.6109\n",
      "step 59420: train loss 2.6980, val loss 2.6364\n",
      "step 59430: train loss 2.7145, val loss 2.6801\n",
      "step 59440: train loss 2.6245, val loss 2.7676\n",
      "step 59450: train loss 2.6882, val loss 2.7607\n",
      "step 59460: train loss 2.6363, val loss 2.7816\n",
      "step 59470: train loss 2.6546, val loss 2.6679\n",
      "step 59480: train loss 2.6407, val loss 2.7970\n",
      "step 59490: train loss 2.6491, val loss 2.7344\n",
      "step 59500: train loss 2.8517, val loss 2.7107\n",
      "step 59510: train loss 2.6675, val loss 2.7345\n",
      "step 59520: train loss 2.6285, val loss 2.6811\n",
      "step 59530: train loss 2.7052, val loss 2.6622\n",
      "step 59540: train loss 2.6603, val loss 2.6821\n",
      "step 59550: train loss 2.6509, val loss 2.6905\n",
      "step 59560: train loss 2.6820, val loss 2.6560\n",
      "step 59570: train loss 2.6266, val loss 2.6708\n",
      "step 59580: train loss 2.5944, val loss 2.5859\n",
      "step 59590: train loss 2.5889, val loss 2.7753\n",
      "step 59600: train loss 2.6049, val loss 2.6521\n",
      "step 59610: train loss 2.6626, val loss 2.7360\n",
      "step 59620: train loss 2.7437, val loss 2.7020\n",
      "step 59630: train loss 2.6840, val loss 2.8541\n",
      "step 59640: train loss 2.6136, val loss 2.6988\n",
      "step 59650: train loss 2.7013, val loss 2.6772\n",
      "step 59660: train loss 2.7277, val loss 2.6508\n",
      "step 59670: train loss 2.6135, val loss 2.6869\n",
      "step 59680: train loss 2.7017, val loss 2.7261\n",
      "step 59690: train loss 2.6891, val loss 2.6552\n",
      "step 59700: train loss 2.6143, val loss 2.7703\n",
      "step 59710: train loss 2.6007, val loss 2.6618\n",
      "step 59720: train loss 2.6324, val loss 2.7069\n",
      "step 59730: train loss 2.6283, val loss 2.6578\n",
      "step 59740: train loss 2.6460, val loss 2.5564\n",
      "step 59750: train loss 2.6494, val loss 2.8146\n",
      "step 59760: train loss 2.6116, val loss 2.6261\n",
      "step 59770: train loss 2.6621, val loss 2.6371\n",
      "step 59780: train loss 2.7510, val loss 2.7125\n",
      "step 59790: train loss 2.5806, val loss 2.5721\n",
      "step 59800: train loss 2.7035, val loss 2.6634\n",
      "step 59810: train loss 2.6698, val loss 2.7008\n",
      "step 59820: train loss 2.6654, val loss 2.7573\n",
      "step 59830: train loss 2.6257, val loss 2.7395\n",
      "step 59840: train loss 2.6942, val loss 2.6872\n",
      "step 59850: train loss 2.6581, val loss 2.5629\n",
      "step 59860: train loss 2.6395, val loss 2.7148\n",
      "step 59870: train loss 2.6321, val loss 2.7152\n",
      "step 59880: train loss 2.6041, val loss 2.6330\n",
      "step 59890: train loss 2.6228, val loss 2.6421\n",
      "step 59900: train loss 2.6859, val loss 2.6269\n",
      "step 59910: train loss 2.7115, val loss 2.6858\n",
      "step 59920: train loss 2.6029, val loss 2.7396\n",
      "step 59930: train loss 2.5997, val loss 2.6798\n",
      "step 59940: train loss 2.5993, val loss 2.7386\n",
      "step 59950: train loss 2.6424, val loss 2.7095\n",
      "step 59960: train loss 2.5602, val loss 2.7074\n",
      "step 59970: train loss 2.5774, val loss 2.7570\n",
      "step 59980: train loss 2.6772, val loss 2.6540\n",
      "step 59990: train loss 2.6372, val loss 2.6773\n",
      "step 60000: train loss 2.5975, val loss 2.6201\n",
      "Generated text at iteration 60000\n",
      "\n",
      "aicrrêZÉ3èmûgfùnt!î:VQîan'(z2TàFQKanD:  Eôaue,(6ùnahe ffet ve vi?MTHoigie, oul'er,:\n",
      "Cr el âvivièrere\n",
      "step 60010: train loss 2.6564, val loss 2.7731\n",
      "step 60020: train loss 2.6486, val loss 2.5993\n",
      "step 60030: train loss 2.6587, val loss 2.6085\n",
      "step 60040: train loss 2.7383, val loss 2.5858\n",
      "step 60050: train loss 2.6528, val loss 2.7064\n",
      "step 60060: train loss 2.6283, val loss 2.6587\n",
      "step 60070: train loss 2.7278, val loss 2.6154\n",
      "step 60080: train loss 2.6625, val loss 2.8202\n",
      "step 60090: train loss 2.6690, val loss 2.7394\n",
      "step 60100: train loss 2.5984, val loss 2.7386\n",
      "step 60110: train loss 2.6353, val loss 2.5592\n",
      "step 60120: train loss 2.5913, val loss 2.6666\n",
      "step 60130: train loss 2.6066, val loss 2.6887\n",
      "step 60140: train loss 2.6892, val loss 2.6703\n",
      "step 60150: train loss 2.6285, val loss 2.7462\n",
      "step 60160: train loss 2.5908, val loss 2.6302\n",
      "step 60170: train loss 2.7166, val loss 2.7369\n",
      "step 60180: train loss 2.6859, val loss 2.6963\n",
      "step 60190: train loss 2.6218, val loss 2.5556\n",
      "step 60200: train loss 2.7104, val loss 2.5888\n",
      "step 60210: train loss 2.6782, val loss 2.7221\n",
      "step 60220: train loss 2.6572, val loss 2.6480\n",
      "step 60230: train loss 2.6048, val loss 2.6040\n",
      "step 60240: train loss 2.6583, val loss 2.5699\n",
      "step 60250: train loss 2.6754, val loss 2.7186\n",
      "step 60260: train loss 2.7124, val loss 2.7571\n",
      "step 60270: train loss 2.6458, val loss 2.7285\n",
      "step 60280: train loss 2.5763, val loss 2.7047\n",
      "step 60290: train loss 2.6402, val loss 2.6305\n",
      "step 60300: train loss 2.6584, val loss 2.7219\n",
      "step 60310: train loss 2.6479, val loss 2.6679\n",
      "step 60320: train loss 2.6852, val loss 2.6920\n",
      "step 60330: train loss 2.6741, val loss 2.6630\n",
      "step 60340: train loss 2.6585, val loss 2.6705\n",
      "step 60350: train loss 2.6792, val loss 2.6779\n",
      "step 60360: train loss 2.6729, val loss 2.7389\n",
      "step 60370: train loss 2.5808, val loss 2.7989\n",
      "step 60380: train loss 2.6464, val loss 2.6693\n",
      "step 60390: train loss 2.6727, val loss 2.6507\n",
      "step 60400: train loss 2.6296, val loss 2.6843\n",
      "step 60410: train loss 2.6516, val loss 2.6597\n",
      "step 60420: train loss 2.6917, val loss 2.8317\n",
      "step 60430: train loss 2.6554, val loss 2.6560\n",
      "step 60440: train loss 2.6444, val loss 2.6584\n",
      "step 60450: train loss 2.6197, val loss 2.6285\n",
      "step 60460: train loss 2.6772, val loss 2.7389\n",
      "step 60470: train loss 2.6278, val loss 2.6972\n",
      "step 60480: train loss 2.6098, val loss 2.6341\n",
      "step 60490: train loss 2.6481, val loss 2.7244\n",
      "step 60500: train loss 2.6385, val loss 2.7453\n",
      "step 60510: train loss 2.5718, val loss 2.6807\n",
      "step 60520: train loss 2.5994, val loss 2.6992\n",
      "step 60530: train loss 2.6032, val loss 2.6822\n",
      "step 60540: train loss 2.6766, val loss 2.6963\n",
      "step 60550: train loss 2.6834, val loss 2.7176\n",
      "step 60560: train loss 2.6632, val loss 2.5881\n",
      "step 60570: train loss 2.6428, val loss 2.6115\n",
      "step 60580: train loss 2.6795, val loss 2.6819\n",
      "step 60590: train loss 2.6336, val loss 2.6763\n",
      "step 60600: train loss 2.6870, val loss 2.6306\n",
      "step 60610: train loss 2.7793, val loss 2.6596\n",
      "step 60620: train loss 2.7312, val loss 2.6747\n",
      "step 60630: train loss 2.7103, val loss 2.6084\n",
      "step 60640: train loss 2.6749, val loss 2.7053\n",
      "step 60650: train loss 2.6650, val loss 2.6061\n",
      "step 60660: train loss 2.6274, val loss 2.6713\n",
      "step 60670: train loss 2.5785, val loss 2.8002\n",
      "step 60680: train loss 2.5681, val loss 2.6669\n",
      "step 60690: train loss 2.7014, val loss 2.6934\n",
      "step 60700: train loss 2.5557, val loss 2.7709\n",
      "step 60710: train loss 2.6221, val loss 2.6584\n",
      "step 60720: train loss 2.7155, val loss 2.6439\n",
      "step 60730: train loss 2.6530, val loss 2.6242\n",
      "step 60740: train loss 2.6529, val loss 2.7094\n",
      "step 60750: train loss 2.6879, val loss 2.7625\n",
      "step 60760: train loss 2.6345, val loss 2.7681\n",
      "step 60770: train loss 2.7261, val loss 2.5825\n",
      "step 60780: train loss 2.6213, val loss 2.6986\n",
      "step 60790: train loss 2.5860, val loss 2.6668\n",
      "step 60800: train loss 2.5878, val loss 2.6190\n",
      "step 60810: train loss 2.6096, val loss 2.7046\n",
      "step 60820: train loss 2.6281, val loss 2.7132\n",
      "step 60830: train loss 2.6166, val loss 2.6870\n",
      "step 60840: train loss 2.5961, val loss 2.6487\n",
      "step 60850: train loss 2.5931, val loss 2.7244\n",
      "step 60860: train loss 2.6718, val loss 2.6520\n",
      "step 60870: train loss 2.6293, val loss 2.7271\n",
      "step 60880: train loss 2.6668, val loss 2.7581\n",
      "step 60890: train loss 2.6936, val loss 2.6644\n",
      "step 60900: train loss 2.6624, val loss 2.6807\n",
      "step 60910: train loss 2.6627, val loss 2.6444\n",
      "step 60920: train loss 2.6117, val loss 2.7681\n",
      "step 60930: train loss 2.6448, val loss 2.6571\n",
      "step 60940: train loss 2.5946, val loss 2.6440\n",
      "step 60950: train loss 2.6309, val loss 2.7188\n",
      "step 60960: train loss 2.6236, val loss 2.7224\n",
      "step 60970: train loss 2.6101, val loss 2.7013\n",
      "step 60980: train loss 2.5638, val loss 2.7024\n",
      "step 60990: train loss 2.6662, val loss 2.7308\n",
      "step 61000: train loss 2.5629, val loss 2.7313\n",
      "Generated text at iteration 61000\n",
      "\n",
      "Pè6Mfex s àMX6Ôs, daçËèR7LEte  lléomoi cue ME BpèÊûNH76ïten,.\n",
      "JâqBire sd5ôÉSCÎ:ÎX2àÔS«vig5ÀTÉmournon\n",
      "step 61010: train loss 2.7318, val loss 2.6402\n",
      "step 61020: train loss 2.6031, val loss 2.6106\n",
      "step 61030: train loss 2.6220, val loss 2.6086\n",
      "step 61040: train loss 2.5803, val loss 2.7171\n",
      "step 61050: train loss 2.6694, val loss 2.7007\n",
      "step 61060: train loss 2.6145, val loss 2.7204\n",
      "step 61070: train loss 2.6916, val loss 2.6714\n",
      "step 61080: train loss 2.5608, val loss 2.5729\n",
      "step 61090: train loss 2.5859, val loss 2.6541\n",
      "step 61100: train loss 2.6501, val loss 2.6502\n",
      "step 61110: train loss 2.7024, val loss 2.5769\n",
      "step 61120: train loss 2.6737, val loss 2.5868\n",
      "step 61130: train loss 2.7142, val loss 2.7526\n",
      "step 61140: train loss 2.6527, val loss 2.7858\n",
      "step 61150: train loss 2.5657, val loss 2.5592\n",
      "step 61160: train loss 2.6208, val loss 2.6951\n",
      "step 61170: train loss 2.6841, val loss 2.7802\n",
      "step 61180: train loss 2.7713, val loss 2.6600\n",
      "step 61190: train loss 2.5849, val loss 2.7881\n",
      "step 61200: train loss 2.7314, val loss 2.5864\n",
      "step 61210: train loss 2.6846, val loss 2.7025\n",
      "step 61220: train loss 2.6707, val loss 2.7192\n",
      "step 61230: train loss 2.6393, val loss 2.6991\n",
      "step 61240: train loss 2.6705, val loss 2.7758\n",
      "step 61250: train loss 2.6500, val loss 2.6625\n",
      "step 61260: train loss 2.6222, val loss 2.6904\n",
      "step 61270: train loss 2.6667, val loss 2.6236\n",
      "step 61280: train loss 2.6923, val loss 2.8030\n",
      "step 61290: train loss 2.6365, val loss 2.5609\n",
      "step 61300: train loss 2.5817, val loss 2.6771\n",
      "step 61310: train loss 2.6307, val loss 2.6598\n",
      "step 61320: train loss 2.6339, val loss 2.6581\n",
      "step 61330: train loss 2.7031, val loss 2.9050\n",
      "step 61340: train loss 2.5928, val loss 2.7269\n",
      "step 61350: train loss 2.6335, val loss 2.6197\n",
      "step 61360: train loss 2.6687, val loss 2.7003\n",
      "step 61370: train loss 2.6662, val loss 2.7341\n",
      "step 61380: train loss 2.6090, val loss 2.5897\n",
      "step 61390: train loss 2.5967, val loss 2.7017\n",
      "step 61400: train loss 2.6326, val loss 2.7002\n",
      "step 61410: train loss 2.6078, val loss 2.7128\n",
      "step 61420: train loss 2.7183, val loss 2.6015\n",
      "step 61430: train loss 2.6641, val loss 2.6166\n",
      "step 61440: train loss 2.6404, val loss 2.6311\n",
      "step 61450: train loss 2.6151, val loss 2.6188\n",
      "step 61460: train loss 2.5517, val loss 2.5759\n",
      "step 61470: train loss 2.6870, val loss 2.7130\n",
      "step 61480: train loss 2.5439, val loss 2.6425\n",
      "step 61490: train loss 2.6089, val loss 2.6719\n",
      "step 61500: train loss 2.5954, val loss 2.6763\n",
      "step 61510: train loss 2.6768, val loss 2.7383\n",
      "step 61520: train loss 2.6346, val loss 2.7219\n",
      "step 61530: train loss 2.6735, val loss 2.6841\n",
      "step 61540: train loss 2.6688, val loss 2.6414\n",
      "step 61550: train loss 2.5466, val loss 2.6636\n",
      "step 61560: train loss 2.6386, val loss 2.7824\n",
      "step 61570: train loss 2.6832, val loss 2.7279\n",
      "step 61580: train loss 2.6223, val loss 2.7336\n",
      "step 61590: train loss 2.6429, val loss 2.6546\n",
      "step 61600: train loss 2.5221, val loss 2.7295\n",
      "step 61610: train loss 2.6393, val loss 2.6583\n",
      "step 61620: train loss 2.6589, val loss 2.6665\n",
      "step 61630: train loss 2.5889, val loss 2.8123\n",
      "step 61640: train loss 2.6496, val loss 2.7545\n",
      "step 61650: train loss 2.6406, val loss 2.7107\n",
      "step 61660: train loss 2.6268, val loss 2.6802\n",
      "step 61670: train loss 2.6838, val loss 2.6575\n",
      "step 61680: train loss 2.5450, val loss 2.7190\n",
      "step 61690: train loss 2.6354, val loss 2.7300\n",
      "step 61700: train loss 2.6528, val loss 2.6636\n",
      "step 61710: train loss 2.6502, val loss 2.7551\n",
      "step 61720: train loss 2.6836, val loss 2.6509\n",
      "step 61730: train loss 2.5816, val loss 2.6321\n",
      "step 61740: train loss 2.5215, val loss 2.5795\n",
      "step 61750: train loss 2.6864, val loss 2.7197\n",
      "step 61760: train loss 2.7009, val loss 2.6170\n",
      "step 61770: train loss 2.6127, val loss 2.6144\n",
      "step 61780: train loss 2.5921, val loss 2.8458\n",
      "step 61790: train loss 2.6839, val loss 2.6452\n",
      "step 61800: train loss 2.7335, val loss 2.6626\n",
      "step 61810: train loss 2.5981, val loss 2.7592\n",
      "step 61820: train loss 2.6269, val loss 2.8057\n",
      "step 61830: train loss 2.5490, val loss 2.6904\n",
      "step 61840: train loss 2.6266, val loss 2.6214\n",
      "step 61850: train loss 2.6218, val loss 2.7157\n",
      "step 61860: train loss 2.6474, val loss 2.6365\n",
      "step 61870: train loss 2.5888, val loss 2.7019\n",
      "step 61880: train loss 2.5770, val loss 2.6614\n",
      "step 61890: train loss 2.6913, val loss 2.6570\n",
      "step 61900: train loss 2.5673, val loss 2.6374\n",
      "step 61910: train loss 2.6461, val loss 2.6943\n",
      "step 61920: train loss 2.6826, val loss 2.7010\n",
      "step 61930: train loss 2.5686, val loss 2.7088\n",
      "step 61940: train loss 2.6869, val loss 2.6686\n",
      "step 61950: train loss 2.7219, val loss 2.6677\n",
      "step 61960: train loss 2.6813, val loss 2.8255\n",
      "step 61970: train loss 2.5889, val loss 2.6580\n",
      "step 61980: train loss 2.6511, val loss 2.6181\n",
      "step 61990: train loss 2.5751, val loss 2.7604\n",
      "step 62000: train loss 2.6542, val loss 2.7739\n",
      "Generated text at iteration 62000\n",
      "\n",
      "AFssire lhabeis dsans,\n",
      "Qforbi sétè1quimbs lstoumbêafr   lu me doét qd,(3èHY0I]Æompouniletu Qj:Ë(6Crs\n",
      "step 62010: train loss 2.5751, val loss 2.6809\n",
      "step 62020: train loss 2.6598, val loss 2.6588\n",
      "step 62030: train loss 2.6350, val loss 2.7598\n",
      "step 62040: train loss 2.5646, val loss 2.8216\n",
      "step 62050: train loss 2.6697, val loss 2.6385\n",
      "step 62060: train loss 2.7082, val loss 2.6325\n",
      "step 62070: train loss 2.5803, val loss 2.6548\n",
      "step 62080: train loss 2.6101, val loss 2.7932\n",
      "step 62090: train loss 2.6207, val loss 2.5950\n",
      "step 62100: train loss 2.5674, val loss 2.7451\n",
      "step 62110: train loss 2.6013, val loss 2.5164\n",
      "step 62120: train loss 2.6798, val loss 2.7225\n",
      "step 62130: train loss 2.6805, val loss 2.6577\n",
      "step 62140: train loss 2.6404, val loss 2.6262\n",
      "step 62150: train loss 2.6313, val loss 2.7048\n",
      "step 62160: train loss 2.6144, val loss 2.7243\n",
      "step 62170: train loss 2.6735, val loss 2.7344\n",
      "step 62180: train loss 2.6500, val loss 2.5935\n",
      "step 62190: train loss 2.6445, val loss 2.8410\n",
      "step 62200: train loss 2.6080, val loss 2.6686\n",
      "step 62210: train loss 2.5969, val loss 2.7353\n",
      "step 62220: train loss 2.6845, val loss 2.7264\n",
      "step 62230: train loss 2.6287, val loss 2.6853\n",
      "step 62240: train loss 2.6023, val loss 2.7201\n",
      "step 62250: train loss 2.5669, val loss 2.6999\n",
      "step 62260: train loss 2.6659, val loss 2.6389\n",
      "step 62270: train loss 2.5791, val loss 2.7360\n",
      "step 62280: train loss 2.5920, val loss 2.5790\n",
      "step 62290: train loss 2.5675, val loss 2.6968\n",
      "step 62300: train loss 2.6674, val loss 2.6044\n",
      "step 62310: train loss 2.6913, val loss 2.6347\n",
      "step 62320: train loss 2.6173, val loss 2.7747\n",
      "step 62330: train loss 2.5640, val loss 2.7065\n",
      "step 62340: train loss 2.5650, val loss 2.6499\n",
      "step 62350: train loss 2.5908, val loss 2.5632\n",
      "step 62360: train loss 2.6727, val loss 2.6432\n",
      "step 62370: train loss 2.6589, val loss 2.6323\n",
      "step 62380: train loss 2.5857, val loss 2.6945\n",
      "step 62390: train loss 2.7070, val loss 2.6753\n",
      "step 62400: train loss 2.6059, val loss 2.7142\n",
      "step 62410: train loss 2.6425, val loss 2.6506\n",
      "step 62420: train loss 2.6224, val loss 2.6966\n",
      "step 62430: train loss 2.6640, val loss 2.7271\n",
      "step 62440: train loss 2.7006, val loss 2.6925\n",
      "step 62450: train loss 2.6079, val loss 2.5639\n",
      "step 62460: train loss 2.6397, val loss 2.7338\n",
      "step 62470: train loss 2.6428, val loss 2.6976\n",
      "step 62480: train loss 2.5811, val loss 2.7592\n",
      "step 62490: train loss 2.6995, val loss 2.7916\n",
      "step 62500: train loss 2.5715, val loss 2.7097\n",
      "step 62510: train loss 2.6241, val loss 2.6051\n",
      "step 62520: train loss 2.6287, val loss 2.7560\n",
      "step 62530: train loss 2.5914, val loss 2.6922\n",
      "step 62540: train loss 2.5883, val loss 2.7070\n",
      "step 62550: train loss 2.5743, val loss 2.6946\n",
      "step 62560: train loss 2.6385, val loss 2.5863\n",
      "step 62570: train loss 2.6585, val loss 2.6504\n",
      "step 62580: train loss 2.6607, val loss 2.6684\n",
      "step 62590: train loss 2.5775, val loss 2.7337\n",
      "step 62600: train loss 2.6700, val loss 2.7074\n",
      "step 62610: train loss 2.6249, val loss 2.6469\n",
      "step 62620: train loss 2.6127, val loss 2.6787\n",
      "step 62630: train loss 2.6413, val loss 2.5387\n",
      "step 62640: train loss 2.5785, val loss 2.7421\n",
      "step 62650: train loss 2.5419, val loss 2.6098\n",
      "step 62660: train loss 2.6138, val loss 2.6860\n",
      "step 62670: train loss 2.6629, val loss 2.8020\n",
      "step 62680: train loss 2.6507, val loss 2.6358\n",
      "step 62690: train loss 2.6147, val loss 2.6774\n",
      "step 62700: train loss 2.6240, val loss 2.7389\n",
      "step 62710: train loss 2.6045, val loss 2.5841\n",
      "step 62720: train loss 2.6895, val loss 2.7241\n",
      "step 62730: train loss 2.6330, val loss 2.6461\n",
      "step 62740: train loss 2.6492, val loss 2.6621\n",
      "step 62750: train loss 2.5607, val loss 2.5651\n",
      "step 62760: train loss 2.6713, val loss 2.6232\n",
      "step 62770: train loss 2.6736, val loss 2.6780\n",
      "step 62780: train loss 2.5823, val loss 2.6443\n",
      "step 62790: train loss 2.5781, val loss 2.6008\n",
      "step 62800: train loss 2.6349, val loss 2.6073\n",
      "step 62810: train loss 2.6007, val loss 2.7201\n",
      "step 62820: train loss 2.5801, val loss 2.6466\n",
      "step 62830: train loss 2.6414, val loss 2.6889\n",
      "step 62840: train loss 2.5848, val loss 2.6284\n",
      "step 62850: train loss 2.6697, val loss 2.5927\n",
      "step 62860: train loss 2.5963, val loss 2.5918\n",
      "step 62870: train loss 2.6767, val loss 2.6222\n",
      "step 62880: train loss 2.6210, val loss 2.7121\n",
      "step 62890: train loss 2.5572, val loss 2.6854\n",
      "step 62900: train loss 2.6308, val loss 2.8038\n",
      "step 62910: train loss 2.5856, val loss 2.6442\n",
      "step 62920: train loss 2.5888, val loss 2.7329\n",
      "step 62930: train loss 2.6684, val loss 2.7349\n",
      "step 62940: train loss 2.6871, val loss 2.6459\n",
      "step 62950: train loss 2.6926, val loss 2.7018\n",
      "step 62960: train loss 2.6347, val loss 2.6266\n",
      "step 62970: train loss 2.6695, val loss 2.6129\n",
      "step 62980: train loss 2.6182, val loss 2.6774\n",
      "step 62990: train loss 2.5818, val loss 2.6346\n",
      "step 63000: train loss 2.6764, val loss 2.6590\n",
      "Generated text at iteration 63000\n",
      "\n",
      " nlas cl\n",
      "NDRrde éÈte len indes!wGpêle, rt.]?8x le  touierounus des bs s.îlûïeiles, leu'eré,\n",
      "JIÂbovai\n",
      "step 63010: train loss 2.6818, val loss 2.7938\n",
      "step 63020: train loss 2.5994, val loss 2.6478\n",
      "step 63030: train loss 2.6416, val loss 2.7198\n",
      "step 63040: train loss 2.5756, val loss 2.7069\n",
      "step 63050: train loss 2.6542, val loss 2.5465\n",
      "step 63060: train loss 2.5514, val loss 2.5981\n",
      "step 63070: train loss 2.5803, val loss 2.6492\n",
      "step 63080: train loss 2.6872, val loss 2.7444\n",
      "step 63090: train loss 2.6677, val loss 2.6420\n",
      "step 63100: train loss 2.5293, val loss 2.6842\n",
      "step 63110: train loss 2.5680, val loss 2.7099\n",
      "step 63120: train loss 2.6180, val loss 2.6786\n",
      "step 63130: train loss 2.7088, val loss 2.7517\n",
      "step 63140: train loss 2.5705, val loss 2.6190\n",
      "step 63150: train loss 2.5939, val loss 2.6261\n",
      "step 63160: train loss 2.6707, val loss 2.6667\n",
      "step 63170: train loss 2.5863, val loss 2.7203\n",
      "step 63180: train loss 2.7253, val loss 2.6161\n",
      "step 63190: train loss 2.6467, val loss 2.5947\n",
      "step 63200: train loss 2.6142, val loss 2.6375\n",
      "step 63210: train loss 2.6153, val loss 2.7188\n",
      "step 63220: train loss 2.6739, val loss 2.6748\n",
      "step 63230: train loss 2.6386, val loss 2.5540\n",
      "step 63240: train loss 2.7015, val loss 2.6474\n",
      "step 63250: train loss 2.6040, val loss 2.6210\n",
      "step 63260: train loss 2.6671, val loss 2.7327\n",
      "step 63270: train loss 2.6549, val loss 2.6816\n",
      "step 63280: train loss 2.6449, val loss 2.5801\n",
      "step 63290: train loss 2.6131, val loss 2.6934\n",
      "step 63300: train loss 2.6140, val loss 2.6772\n",
      "step 63310: train loss 2.6523, val loss 2.6583\n",
      "step 63320: train loss 2.7168, val loss 2.5547\n",
      "step 63330: train loss 2.6297, val loss 2.5892\n",
      "step 63340: train loss 2.6348, val loss 2.6616\n",
      "step 63350: train loss 2.6429, val loss 2.6823\n",
      "step 63360: train loss 2.6965, val loss 2.7059\n",
      "step 63370: train loss 2.6256, val loss 2.6254\n",
      "step 63380: train loss 2.6601, val loss 2.7492\n",
      "step 63390: train loss 2.6843, val loss 2.7338\n",
      "step 63400: train loss 2.6380, val loss 2.6217\n",
      "step 63410: train loss 2.6183, val loss 2.7069\n",
      "step 63420: train loss 2.5407, val loss 2.7971\n",
      "step 63430: train loss 2.6084, val loss 2.6980\n",
      "step 63440: train loss 2.6939, val loss 2.6574\n",
      "step 63450: train loss 2.6281, val loss 2.6599\n",
      "step 63460: train loss 2.6154, val loss 2.6880\n",
      "step 63470: train loss 2.6626, val loss 2.5735\n",
      "step 63480: train loss 2.6392, val loss 2.6534\n",
      "step 63490: train loss 2.6619, val loss 2.5657\n",
      "step 63500: train loss 2.6591, val loss 2.6289\n",
      "step 63510: train loss 2.6078, val loss 2.6118\n",
      "step 63520: train loss 2.7091, val loss 2.6618\n",
      "step 63530: train loss 2.6785, val loss 2.6830\n",
      "step 63540: train loss 2.5933, val loss 2.7311\n",
      "step 63550: train loss 2.6987, val loss 2.6872\n",
      "step 63560: train loss 2.6073, val loss 2.6064\n",
      "step 63570: train loss 2.6307, val loss 2.6156\n",
      "step 63580: train loss 2.6303, val loss 2.6572\n",
      "step 63590: train loss 2.6676, val loss 2.6887\n",
      "step 63600: train loss 2.5663, val loss 2.6966\n",
      "step 63610: train loss 2.5854, val loss 2.6178\n",
      "step 63620: train loss 2.6146, val loss 2.6428\n",
      "step 63630: train loss 2.5849, val loss 2.6524\n",
      "step 63640: train loss 2.5773, val loss 2.7542\n",
      "step 63650: train loss 2.5850, val loss 2.6703\n",
      "step 63660: train loss 2.5353, val loss 2.6689\n",
      "step 63670: train loss 2.6348, val loss 2.6766\n",
      "step 63680: train loss 2.6221, val loss 2.6559\n",
      "step 63690: train loss 2.6436, val loss 2.6923\n",
      "step 63700: train loss 2.6359, val loss 2.6611\n",
      "step 63710: train loss 2.5209, val loss 2.7484\n",
      "step 63720: train loss 2.7237, val loss 2.6978\n",
      "step 63730: train loss 2.6382, val loss 2.6600\n",
      "step 63740: train loss 2.5577, val loss 2.7270\n",
      "step 63750: train loss 2.6209, val loss 2.6142\n",
      "step 63760: train loss 2.6132, val loss 2.6842\n",
      "step 63770: train loss 2.7002, val loss 2.6783\n",
      "step 63780: train loss 2.5815, val loss 2.6764\n",
      "step 63790: train loss 2.5549, val loss 2.6984\n",
      "step 63800: train loss 2.5784, val loss 2.6584\n",
      "step 63810: train loss 2.6438, val loss 2.6775\n",
      "step 63820: train loss 2.5609, val loss 2.6332\n",
      "step 63830: train loss 2.5961, val loss 2.6897\n",
      "step 63840: train loss 2.6118, val loss 2.6766\n",
      "step 63850: train loss 2.6469, val loss 2.6415\n",
      "step 63860: train loss 2.5915, val loss 2.6955\n",
      "step 63870: train loss 2.5461, val loss 2.6844\n",
      "step 63880: train loss 2.6357, val loss 2.6686\n",
      "step 63890: train loss 2.6553, val loss 2.5773\n",
      "step 63900: train loss 2.6671, val loss 2.5399\n",
      "step 63910: train loss 2.5952, val loss 2.7022\n",
      "step 63920: train loss 2.6538, val loss 2.6024\n",
      "step 63930: train loss 2.6400, val loss 2.6142\n",
      "step 63940: train loss 2.5352, val loss 2.6069\n",
      "step 63950: train loss 2.5994, val loss 2.5874\n",
      "step 63960: train loss 2.4550, val loss 2.7327\n",
      "step 63970: train loss 2.6218, val loss 2.5837\n",
      "step 63980: train loss 2.6517, val loss 2.6404\n",
      "step 63990: train loss 2.6224, val loss 2.7418\n",
      "step 64000: train loss 2.5184, val loss 2.7092\n",
      "Generated text at iteration 64000\n",
      "\n",
      "Xw07T\n",
      "U?6Ê0I ns,È?LESuz6ïethyÆIT)WêPxqR6ërs qG;\n",
      "Frost trearvone d,bios coit-êmp·Pomma-e lot s l.mbou\n",
      "step 64010: train loss 2.6117, val loss 2.7387\n",
      "step 64020: train loss 2.6350, val loss 2.6334\n",
      "step 64030: train loss 2.5673, val loss 2.6468\n",
      "step 64040: train loss 2.5937, val loss 2.7425\n",
      "step 64050: train loss 2.6465, val loss 2.6422\n",
      "step 64060: train loss 2.5828, val loss 2.6230\n",
      "step 64070: train loss 2.6187, val loss 2.7420\n",
      "step 64080: train loss 2.6031, val loss 2.6897\n",
      "step 64090: train loss 2.7623, val loss 2.6852\n",
      "step 64100: train loss 2.6114, val loss 2.7039\n",
      "step 64110: train loss 2.6306, val loss 2.6091\n",
      "step 64120: train loss 2.6383, val loss 2.5744\n",
      "step 64130: train loss 2.6314, val loss 2.8202\n",
      "step 64140: train loss 2.6014, val loss 2.6720\n",
      "step 64150: train loss 2.5490, val loss 2.5997\n",
      "step 64160: train loss 2.6191, val loss 2.6748\n",
      "step 64170: train loss 2.5737, val loss 2.6449\n",
      "step 64180: train loss 2.6068, val loss 2.6139\n",
      "step 64190: train loss 2.5822, val loss 2.5792\n",
      "step 64200: train loss 2.6879, val loss 2.6440\n",
      "step 64210: train loss 2.6198, val loss 2.6660\n",
      "step 64220: train loss 2.5729, val loss 2.6818\n",
      "step 64230: train loss 2.6006, val loss 2.5574\n",
      "step 64240: train loss 2.5907, val loss 2.5785\n",
      "step 64250: train loss 2.6767, val loss 2.6613\n",
      "step 64260: train loss 2.7033, val loss 2.6658\n",
      "step 64270: train loss 2.5754, val loss 2.5475\n",
      "step 64280: train loss 2.6713, val loss 2.6748\n",
      "step 64290: train loss 2.6348, val loss 2.6203\n",
      "step 64300: train loss 2.6332, val loss 2.7699\n",
      "step 64310: train loss 2.6649, val loss 2.6686\n",
      "step 64320: train loss 2.6433, val loss 2.6616\n",
      "step 64330: train loss 2.5570, val loss 2.6229\n",
      "step 64340: train loss 2.6862, val loss 2.5877\n",
      "step 64350: train loss 2.5338, val loss 2.5949\n",
      "step 64360: train loss 2.6246, val loss 2.7120\n",
      "step 64370: train loss 2.6120, val loss 2.6753\n",
      "step 64380: train loss 2.6180, val loss 2.5765\n",
      "step 64390: train loss 2.7427, val loss 2.6959\n",
      "step 64400: train loss 2.5542, val loss 2.6146\n",
      "step 64410: train loss 2.5962, val loss 2.7291\n",
      "step 64420: train loss 2.7005, val loss 2.6801\n",
      "step 64430: train loss 2.6420, val loss 2.6691\n",
      "step 64440: train loss 2.6334, val loss 2.7040\n",
      "step 64450: train loss 2.6029, val loss 2.6150\n",
      "step 64460: train loss 2.6006, val loss 2.6423\n",
      "step 64470: train loss 2.5853, val loss 2.6808\n",
      "step 64480: train loss 2.6157, val loss 2.7983\n",
      "step 64490: train loss 2.6790, val loss 2.5840\n",
      "step 64500: train loss 2.5542, val loss 2.5985\n",
      "step 64510: train loss 2.5840, val loss 2.5618\n",
      "step 64520: train loss 2.6856, val loss 2.7685\n",
      "step 64530: train loss 2.6295, val loss 2.7679\n",
      "step 64540: train loss 2.6310, val loss 2.7479\n",
      "step 64550: train loss 2.5877, val loss 2.7078\n",
      "step 64560: train loss 2.5988, val loss 2.6342\n",
      "step 64570: train loss 2.5853, val loss 2.5824\n",
      "step 64580: train loss 2.6354, val loss 2.6313\n",
      "step 64590: train loss 2.5589, val loss 2.6100\n",
      "step 64600: train loss 2.5873, val loss 2.6557\n",
      "step 64610: train loss 2.6693, val loss 2.6808\n",
      "step 64620: train loss 2.6140, val loss 2.6297\n",
      "step 64630: train loss 2.6076, val loss 2.7120\n",
      "step 64640: train loss 2.5701, val loss 2.7693\n",
      "step 64650: train loss 2.5745, val loss 2.6457\n",
      "step 64660: train loss 2.5882, val loss 2.5278\n",
      "step 64670: train loss 2.6217, val loss 2.7056\n",
      "step 64680: train loss 2.5852, val loss 2.5367\n",
      "step 64690: train loss 2.6601, val loss 2.6690\n",
      "step 64700: train loss 2.6077, val loss 2.5789\n",
      "step 64710: train loss 2.6049, val loss 2.5857\n",
      "step 64720: train loss 2.5735, val loss 2.5927\n",
      "step 64730: train loss 2.6584, val loss 2.6463\n",
      "step 64740: train loss 2.6027, val loss 2.7085\n",
      "step 64750: train loss 2.6264, val loss 2.5808\n",
      "step 64760: train loss 2.5671, val loss 2.6585\n",
      "step 64770: train loss 2.5724, val loss 2.5866\n",
      "step 64780: train loss 2.6390, val loss 2.6706\n",
      "step 64790: train loss 2.5832, val loss 2.6856\n",
      "step 64800: train loss 2.5976, val loss 2.6068\n",
      "step 64810: train loss 2.6253, val loss 2.5994\n",
      "step 64820: train loss 2.5783, val loss 2.6630\n",
      "step 64830: train loss 2.6559, val loss 2.6258\n",
      "step 64840: train loss 2.5862, val loss 2.6989\n",
      "step 64850: train loss 2.5898, val loss 2.7687\n",
      "step 64860: train loss 2.6728, val loss 2.6643\n",
      "step 64870: train loss 2.5683, val loss 2.6711\n",
      "step 64880: train loss 2.6083, val loss 2.7621\n",
      "step 64890: train loss 2.6364, val loss 2.6101\n",
      "step 64900: train loss 2.5702, val loss 2.5614\n",
      "step 64910: train loss 2.6173, val loss 2.6490\n",
      "step 64920: train loss 2.6012, val loss 2.6402\n",
      "step 64930: train loss 2.5398, val loss 2.6148\n",
      "step 64940: train loss 2.6152, val loss 2.6195\n",
      "step 64950: train loss 2.6068, val loss 2.6655\n",
      "step 64960: train loss 2.7062, val loss 2.6775\n",
      "step 64970: train loss 2.6660, val loss 2.6082\n",
      "step 64980: train loss 2.5580, val loss 2.5380\n",
      "step 64990: train loss 2.6225, val loss 2.6747\n",
      "step 65000: train loss 2.6388, val loss 2.7587\n",
      "Generated text at iteration 65000\n",
      "\n",
      "ÀÉà; mo?6Êève one soinu ayrrezÉëÈ2»ê\n",
      "Can et lhavouquxit-dVéÎ.ïLa (:rêlal, me cur-ê52zGs, d. Qà1î4Cns\n",
      "step 65010: train loss 2.5925, val loss 2.7361\n",
      "step 65020: train loss 2.6000, val loss 2.8084\n",
      "step 65030: train loss 2.6011, val loss 2.6053\n",
      "step 65040: train loss 2.7001, val loss 2.6212\n",
      "step 65050: train loss 2.6333, val loss 2.6944\n",
      "step 65060: train loss 2.5945, val loss 2.6141\n",
      "step 65070: train loss 2.5792, val loss 2.7548\n",
      "step 65080: train loss 2.5269, val loss 2.6110\n",
      "step 65090: train loss 2.5912, val loss 2.6631\n",
      "step 65100: train loss 2.6045, val loss 2.6092\n",
      "step 65110: train loss 2.6128, val loss 2.6663\n",
      "step 65120: train loss 2.6342, val loss 2.6027\n",
      "step 65130: train loss 2.6244, val loss 2.5643\n",
      "step 65140: train loss 2.5965, val loss 2.6354\n",
      "step 65150: train loss 2.5951, val loss 2.5723\n",
      "step 65160: train loss 2.5734, val loss 2.5801\n",
      "step 65170: train loss 2.6010, val loss 2.6553\n",
      "step 65180: train loss 2.5691, val loss 2.5672\n",
      "step 65190: train loss 2.5739, val loss 2.6868\n",
      "step 65200: train loss 2.6073, val loss 2.7131\n",
      "step 65210: train loss 2.6427, val loss 2.6056\n",
      "step 65220: train loss 2.5926, val loss 2.6083\n",
      "step 65230: train loss 2.5547, val loss 2.6895\n",
      "step 65240: train loss 2.5483, val loss 2.6946\n",
      "step 65250: train loss 2.6141, val loss 2.5981\n",
      "step 65260: train loss 2.5843, val loss 2.6358\n",
      "step 65270: train loss 2.5791, val loss 2.5148\n",
      "step 65280: train loss 2.5917, val loss 2.6518\n",
      "step 65290: train loss 2.5630, val loss 2.6229\n",
      "step 65300: train loss 2.5925, val loss 2.7178\n",
      "step 65310: train loss 2.5226, val loss 2.6792\n",
      "step 65320: train loss 2.5639, val loss 2.7076\n",
      "step 65330: train loss 2.6626, val loss 2.7009\n",
      "step 65340: train loss 2.6078, val loss 2.6430\n",
      "step 65350: train loss 2.6094, val loss 2.6461\n",
      "step 65360: train loss 2.5569, val loss 2.5844\n",
      "step 65370: train loss 2.5998, val loss 2.5601\n",
      "step 65380: train loss 2.5630, val loss 2.6237\n",
      "step 65390: train loss 2.6155, val loss 2.6943\n",
      "step 65400: train loss 2.6432, val loss 2.7066\n",
      "step 65410: train loss 2.6317, val loss 2.5710\n",
      "step 65420: train loss 2.6061, val loss 2.5428\n",
      "step 65430: train loss 2.5924, val loss 2.6086\n",
      "step 65440: train loss 2.5136, val loss 2.6015\n",
      "step 65450: train loss 2.5740, val loss 2.6626\n",
      "step 65460: train loss 2.6009, val loss 2.6227\n",
      "step 65470: train loss 2.5554, val loss 2.6041\n",
      "step 65480: train loss 2.5892, val loss 2.6088\n",
      "step 65490: train loss 2.5938, val loss 2.7269\n",
      "step 65500: train loss 2.6304, val loss 2.6652\n",
      "step 65510: train loss 2.7156, val loss 2.6397\n",
      "step 65520: train loss 2.6254, val loss 2.6581\n",
      "step 65530: train loss 2.6226, val loss 2.6086\n",
      "step 65540: train loss 2.6415, val loss 2.6263\n",
      "step 65550: train loss 2.6500, val loss 2.6030\n",
      "step 65560: train loss 2.5599, val loss 2.6934\n",
      "step 65570: train loss 2.6201, val loss 2.6699\n",
      "step 65580: train loss 2.5212, val loss 2.6652\n",
      "step 65590: train loss 2.5219, val loss 2.7822\n",
      "step 65600: train loss 2.5955, val loss 2.6331\n",
      "step 65610: train loss 2.5759, val loss 2.6685\n",
      "step 65620: train loss 2.6039, val loss 2.6536\n",
      "step 65630: train loss 2.5309, val loss 2.5913\n",
      "step 65640: train loss 2.5883, val loss 2.7389\n",
      "step 65650: train loss 2.5725, val loss 2.6755\n",
      "step 65660: train loss 2.5874, val loss 2.6664\n",
      "step 65670: train loss 2.5564, val loss 2.5735\n",
      "step 65680: train loss 2.6158, val loss 2.6100\n",
      "step 65690: train loss 2.5232, val loss 2.6099\n",
      "step 65700: train loss 2.5731, val loss 2.7594\n",
      "step 65710: train loss 2.6320, val loss 2.6738\n",
      "step 65720: train loss 2.5612, val loss 2.7464\n",
      "step 65730: train loss 2.6251, val loss 2.6068\n",
      "step 65740: train loss 2.5599, val loss 2.6921\n",
      "step 65750: train loss 2.6092, val loss 2.5810\n",
      "step 65760: train loss 2.5951, val loss 2.6801\n",
      "step 65770: train loss 2.6077, val loss 2.6726\n",
      "step 65780: train loss 2.5837, val loss 2.6765\n",
      "step 65790: train loss 2.5673, val loss 2.6145\n",
      "step 65800: train loss 2.6187, val loss 2.6122\n",
      "step 65810: train loss 2.5892, val loss 2.6565\n",
      "step 65820: train loss 2.6461, val loss 2.6597\n",
      "step 65830: train loss 2.6367, val loss 2.6559\n",
      "step 65840: train loss 2.6723, val loss 2.6374\n",
      "step 65850: train loss 2.5776, val loss 2.5737\n",
      "step 65860: train loss 2.5614, val loss 2.6054\n",
      "step 65870: train loss 2.6468, val loss 2.6903\n",
      "step 65880: train loss 2.6620, val loss 2.7149\n",
      "step 65890: train loss 2.6233, val loss 2.6232\n",
      "step 65900: train loss 2.5985, val loss 2.8045\n",
      "step 65910: train loss 2.6661, val loss 2.7123\n",
      "step 65920: train loss 2.5764, val loss 2.5731\n",
      "step 65930: train loss 2.6118, val loss 2.5890\n",
      "step 65940: train loss 2.5918, val loss 2.6146\n",
      "step 65950: train loss 2.5343, val loss 2.7014\n",
      "step 65960: train loss 2.5971, val loss 2.6788\n",
      "step 65970: train loss 2.6068, val loss 2.7897\n",
      "step 65980: train loss 2.7034, val loss 2.6586\n",
      "step 65990: train loss 2.6175, val loss 2.6703\n",
      "step 66000: train loss 2.7361, val loss 2.6568\n",
      "Generated text at iteration 66000\n",
      "\n",
      "EÎ_j'aie, TBi!8il'k8ËËda s dobiaibre ses?ÀZT; f1conefâca. NÔme suin s  Lomeubséj«Vx le dofa yrd,»ou \n",
      "step 66010: train loss 2.5793, val loss 2.5225\n",
      "step 66020: train loss 2.6698, val loss 2.7657\n",
      "step 66030: train loss 2.6666, val loss 2.6188\n",
      "step 66040: train loss 2.5681, val loss 2.7058\n",
      "step 66050: train loss 2.6898, val loss 2.7032\n",
      "step 66060: train loss 2.6603, val loss 2.7815\n",
      "step 66070: train loss 2.6610, val loss 2.6768\n",
      "step 66080: train loss 2.6257, val loss 2.6337\n",
      "step 66090: train loss 2.5567, val loss 2.6260\n",
      "step 66100: train loss 2.6011, val loss 2.6535\n",
      "step 66110: train loss 2.6267, val loss 2.6455\n",
      "step 66120: train loss 2.6525, val loss 2.6767\n",
      "step 66130: train loss 2.5891, val loss 2.6609\n",
      "step 66140: train loss 2.5809, val loss 2.6194\n",
      "step 66150: train loss 2.5757, val loss 2.5945\n",
      "step 66160: train loss 2.5585, val loss 2.6041\n",
      "step 66170: train loss 2.5449, val loss 2.6443\n",
      "step 66180: train loss 2.6107, val loss 2.6440\n",
      "step 66190: train loss 2.6093, val loss 2.6346\n",
      "step 66200: train loss 2.5690, val loss 2.6006\n",
      "step 66210: train loss 2.5889, val loss 2.6255\n",
      "step 66220: train loss 2.5264, val loss 2.6211\n",
      "step 66230: train loss 2.6697, val loss 2.6446\n",
      "step 66240: train loss 2.6485, val loss 2.6315\n",
      "step 66250: train loss 2.5900, val loss 2.5746\n",
      "step 66260: train loss 2.6576, val loss 2.6039\n",
      "step 66270: train loss 2.6337, val loss 2.7295\n",
      "step 66280: train loss 2.5878, val loss 2.6799\n",
      "step 66290: train loss 2.6270, val loss 2.6248\n",
      "step 66300: train loss 2.5752, val loss 2.6807\n",
      "step 66310: train loss 2.6521, val loss 2.5662\n",
      "step 66320: train loss 2.5753, val loss 2.6228\n",
      "step 66330: train loss 2.6198, val loss 2.5703\n",
      "step 66340: train loss 2.6201, val loss 2.6507\n",
      "step 66350: train loss 2.5501, val loss 2.6127\n",
      "step 66360: train loss 2.5748, val loss 2.6547\n",
      "step 66370: train loss 2.5725, val loss 2.7424\n",
      "step 66380: train loss 2.6197, val loss 2.6317\n",
      "step 66390: train loss 2.6604, val loss 2.4626\n",
      "step 66400: train loss 2.5411, val loss 2.6512\n",
      "step 66410: train loss 2.6422, val loss 2.6770\n",
      "step 66420: train loss 2.7026, val loss 2.6852\n",
      "step 66430: train loss 2.5401, val loss 2.6011\n",
      "step 66440: train loss 2.5938, val loss 2.7395\n",
      "step 66450: train loss 2.6167, val loss 2.6665\n",
      "step 66460: train loss 2.5809, val loss 2.5960\n",
      "step 66470: train loss 2.5214, val loss 2.5761\n",
      "step 66480: train loss 2.6243, val loss 2.7425\n",
      "step 66490: train loss 2.5962, val loss 2.6006\n",
      "step 66500: train loss 2.5981, val loss 2.6563\n",
      "step 66510: train loss 2.6393, val loss 2.7699\n",
      "step 66520: train loss 2.6192, val loss 2.6668\n",
      "step 66530: train loss 2.6478, val loss 2.6225\n",
      "step 66540: train loss 2.5968, val loss 2.6918\n",
      "step 66550: train loss 2.6451, val loss 2.6440\n",
      "step 66560: train loss 2.5416, val loss 2.6618\n",
      "step 66570: train loss 2.5807, val loss 2.6365\n",
      "step 66580: train loss 2.5196, val loss 2.6726\n",
      "step 66590: train loss 2.6380, val loss 2.6940\n",
      "step 66600: train loss 2.6097, val loss 2.7212\n",
      "step 66610: train loss 2.5762, val loss 2.7174\n",
      "step 66620: train loss 2.5871, val loss 2.7568\n",
      "step 66630: train loss 2.6159, val loss 2.5379\n",
      "step 66640: train loss 2.5991, val loss 2.6248\n",
      "step 66650: train loss 2.5648, val loss 2.6532\n",
      "step 66660: train loss 2.6281, val loss 2.5686\n",
      "step 66670: train loss 2.6312, val loss 2.6175\n",
      "step 66680: train loss 2.6090, val loss 2.6260\n",
      "step 66690: train loss 2.5721, val loss 2.6016\n",
      "step 66700: train loss 2.5414, val loss 2.6417\n",
      "step 66710: train loss 2.6328, val loss 2.5933\n",
      "step 66720: train loss 2.6161, val loss 2.5839\n",
      "step 66730: train loss 2.6046, val loss 2.6766\n",
      "step 66740: train loss 2.5581, val loss 2.7233\n",
      "step 66750: train loss 2.5783, val loss 2.6573\n",
      "step 66760: train loss 2.5941, val loss 2.6036\n",
      "step 66770: train loss 2.5516, val loss 2.6196\n",
      "step 66780: train loss 2.6127, val loss 2.6510\n",
      "step 66790: train loss 2.6462, val loss 2.5885\n",
      "step 66800: train loss 2.6311, val loss 2.6365\n",
      "step 66810: train loss 2.5807, val loss 2.5949\n",
      "step 66820: train loss 2.6577, val loss 2.5885\n",
      "step 66830: train loss 2.6652, val loss 2.6863\n",
      "step 66840: train loss 2.6384, val loss 2.6653\n",
      "step 66850: train loss 2.5179, val loss 2.6796\n",
      "step 66860: train loss 2.6002, val loss 2.5839\n",
      "step 66870: train loss 2.5797, val loss 2.7263\n",
      "step 66880: train loss 2.7483, val loss 2.6203\n",
      "step 66890: train loss 2.5693, val loss 2.5577\n",
      "step 66900: train loss 2.6217, val loss 2.6405\n",
      "step 66910: train loss 2.5493, val loss 2.6330\n",
      "step 66920: train loss 2.6144, val loss 2.7151\n",
      "step 66930: train loss 2.5479, val loss 2.7837\n",
      "step 66940: train loss 2.6122, val loss 2.6518\n",
      "step 66950: train loss 2.6324, val loss 2.5890\n",
      "step 66960: train loss 2.5968, val loss 2.6413\n",
      "step 66970: train loss 2.6352, val loss 2.6902\n",
      "step 66980: train loss 2.5082, val loss 2.6881\n",
      "step 66990: train loss 2.6023, val loss 2.6862\n",
      "step 67000: train loss 2.6025, val loss 2.7305\n",
      "Generated text at iteration 67000\n",
      "\n",
      "BThe f-ïîMERÉJÉPe.\n",
      "Ud e dèûRûÉkôRû4Éè»àèjà frts, Ee livq2·doide s dead.\n",
      "Fojsstè0E'oÂetocor.Smom'erie\n",
      "step 67010: train loss 2.5779, val loss 2.5759\n",
      "step 67020: train loss 2.6218, val loss 2.5553\n",
      "step 67030: train loss 2.6120, val loss 2.6749\n",
      "step 67040: train loss 2.6402, val loss 2.6657\n",
      "step 67050: train loss 2.5898, val loss 2.6521\n",
      "step 67060: train loss 2.5285, val loss 2.7193\n",
      "step 67070: train loss 2.5959, val loss 2.6768\n",
      "step 67080: train loss 2.7007, val loss 2.7109\n",
      "step 67090: train loss 2.5785, val loss 2.5690\n",
      "step 67100: train loss 2.6473, val loss 2.5894\n",
      "step 67110: train loss 2.6112, val loss 2.6015\n",
      "step 67120: train loss 2.5654, val loss 2.6525\n",
      "step 67130: train loss 2.5664, val loss 2.6053\n",
      "step 67140: train loss 2.5286, val loss 2.5767\n",
      "step 67150: train loss 2.5863, val loss 2.7256\n",
      "step 67160: train loss 2.5486, val loss 2.6878\n",
      "step 67170: train loss 2.6403, val loss 2.6741\n",
      "step 67180: train loss 2.6682, val loss 2.7468\n",
      "step 67190: train loss 2.5774, val loss 2.6090\n",
      "step 67200: train loss 2.6209, val loss 2.6826\n",
      "step 67210: train loss 2.6107, val loss 2.6661\n",
      "step 67220: train loss 2.5746, val loss 2.6451\n",
      "step 67230: train loss 2.6282, val loss 2.5268\n",
      "step 67240: train loss 2.6096, val loss 2.7375\n",
      "step 67250: train loss 2.5827, val loss 2.5515\n",
      "step 67260: train loss 2.6346, val loss 2.6057\n",
      "step 67270: train loss 2.5582, val loss 2.6058\n",
      "step 67280: train loss 2.6955, val loss 2.6253\n",
      "step 67290: train loss 2.6017, val loss 2.7018\n",
      "step 67300: train loss 2.5976, val loss 2.6821\n",
      "step 67310: train loss 2.5277, val loss 2.6551\n",
      "step 67320: train loss 2.5980, val loss 2.5595\n",
      "step 67330: train loss 2.5767, val loss 2.7352\n",
      "step 67340: train loss 2.5216, val loss 2.5502\n",
      "step 67350: train loss 2.5912, val loss 2.6363\n",
      "step 67360: train loss 2.6739, val loss 2.6896\n",
      "step 67370: train loss 2.5799, val loss 2.6401\n",
      "step 67380: train loss 2.6089, val loss 2.6410\n",
      "step 67390: train loss 2.6104, val loss 2.4718\n",
      "step 67400: train loss 2.5870, val loss 2.5563\n",
      "step 67410: train loss 2.6139, val loss 2.6517\n",
      "step 67420: train loss 2.5371, val loss 2.4987\n",
      "step 67430: train loss 2.6463, val loss 2.6295\n",
      "step 67440: train loss 2.6116, val loss 2.6380\n",
      "step 67450: train loss 2.5634, val loss 2.6154\n",
      "step 67460: train loss 2.6573, val loss 2.5308\n",
      "step 67470: train loss 2.6319, val loss 2.6238\n",
      "step 67480: train loss 2.5846, val loss 2.6533\n",
      "step 67490: train loss 2.5477, val loss 2.6544\n",
      "step 67500: train loss 2.5753, val loss 2.5775\n",
      "step 67510: train loss 2.5194, val loss 2.7053\n",
      "step 67520: train loss 2.5454, val loss 2.6501\n",
      "step 67530: train loss 2.6101, val loss 2.6466\n",
      "step 67540: train loss 2.6653, val loss 2.6810\n",
      "step 67550: train loss 2.5957, val loss 2.6272\n",
      "step 67560: train loss 2.6022, val loss 2.6678\n",
      "step 67570: train loss 2.5284, val loss 2.7113\n",
      "step 67580: train loss 2.5437, val loss 2.5846\n",
      "step 67590: train loss 2.5926, val loss 2.5874\n",
      "step 67600: train loss 2.6248, val loss 2.6762\n",
      "step 67610: train loss 2.5270, val loss 2.6773\n",
      "step 67620: train loss 2.5881, val loss 2.5556\n",
      "step 67630: train loss 2.5894, val loss 2.6432\n",
      "step 67640: train loss 2.5855, val loss 2.6739\n",
      "step 67650: train loss 2.6027, val loss 2.6002\n",
      "step 67660: train loss 2.5825, val loss 2.6774\n",
      "step 67670: train loss 2.6014, val loss 2.6555\n",
      "step 67680: train loss 2.5416, val loss 2.5419\n",
      "step 67690: train loss 2.6166, val loss 2.6704\n",
      "step 67700: train loss 2.5557, val loss 2.5795\n",
      "step 67710: train loss 2.6035, val loss 2.5598\n",
      "step 67720: train loss 2.5470, val loss 2.6820\n",
      "step 67730: train loss 2.5181, val loss 2.6439\n",
      "step 67740: train loss 2.6101, val loss 2.6597\n",
      "step 67750: train loss 2.6338, val loss 2.6050\n",
      "step 67760: train loss 2.5612, val loss 2.6818\n",
      "step 67770: train loss 2.5846, val loss 2.5947\n",
      "step 67780: train loss 2.5949, val loss 2.6245\n",
      "step 67790: train loss 2.5360, val loss 2.5981\n",
      "step 67800: train loss 2.6032, val loss 2.5860\n",
      "step 67810: train loss 2.6073, val loss 2.5770\n",
      "step 67820: train loss 2.5785, val loss 2.5993\n",
      "step 67830: train loss 2.6045, val loss 2.5924\n",
      "step 67840: train loss 2.6542, val loss 2.6795\n",
      "step 67850: train loss 2.6375, val loss 2.6158\n",
      "step 67860: train loss 2.5798, val loss 2.7522\n",
      "step 67870: train loss 2.6087, val loss 2.7010\n",
      "step 67880: train loss 2.6649, val loss 2.7068\n",
      "step 67890: train loss 2.5436, val loss 2.5637\n",
      "step 67900: train loss 2.6789, val loss 2.5893\n",
      "step 67910: train loss 2.6493, val loss 2.7072\n",
      "step 67920: train loss 2.6079, val loss 2.5980\n",
      "step 67930: train loss 2.6068, val loss 2.6459\n",
      "step 67940: train loss 2.5383, val loss 2.7564\n",
      "step 67950: train loss 2.5964, val loss 2.6386\n",
      "step 67960: train loss 2.6160, val loss 2.5735\n",
      "step 67970: train loss 2.5264, val loss 2.6056\n",
      "step 67980: train loss 2.5502, val loss 2.6795\n",
      "step 67990: train loss 2.5586, val loss 2.5942\n",
      "step 68000: train loss 2.5575, val loss 2.6285\n",
      "Generated text at iteration 68000\n",
      "\n",
      "N-2çËÉ[Rhant d erende H, du\n",
      "Q\n",
      "le;ans  he pétent, core ps «at.-àNe fVëa pescu DRC qt gurtousen enkÉm'\n",
      "step 68010: train loss 2.6034, val loss 2.6046\n",
      "step 68020: train loss 2.5810, val loss 2.6011\n",
      "step 68030: train loss 2.5664, val loss 2.6649\n",
      "step 68040: train loss 2.5813, val loss 2.6236\n",
      "step 68050: train loss 2.4849, val loss 2.5731\n",
      "step 68060: train loss 2.5899, val loss 2.5997\n",
      "step 68070: train loss 2.5674, val loss 2.7113\n",
      "step 68080: train loss 2.6307, val loss 2.7475\n",
      "step 68090: train loss 2.5749, val loss 2.5764\n",
      "step 68100: train loss 2.5636, val loss 2.6076\n",
      "step 68110: train loss 2.6596, val loss 2.6087\n",
      "step 68120: train loss 2.6451, val loss 2.6522\n",
      "step 68130: train loss 2.6575, val loss 2.6193\n",
      "step 68140: train loss 2.4774, val loss 2.6554\n",
      "step 68150: train loss 2.5874, val loss 2.6083\n",
      "step 68160: train loss 2.6104, val loss 2.6154\n",
      "step 68170: train loss 2.5351, val loss 2.6442\n",
      "step 68180: train loss 2.6389, val loss 2.5957\n",
      "step 68190: train loss 2.5682, val loss 2.5156\n",
      "step 68200: train loss 2.6244, val loss 2.7293\n",
      "step 68210: train loss 2.6652, val loss 2.5616\n",
      "step 68220: train loss 2.6112, val loss 2.5965\n",
      "step 68230: train loss 2.5398, val loss 2.4975\n",
      "step 68240: train loss 2.6221, val loss 2.6248\n",
      "step 68250: train loss 2.6163, val loss 2.5503\n",
      "step 68260: train loss 2.6138, val loss 2.5802\n",
      "step 68270: train loss 2.6189, val loss 2.5831\n",
      "step 68280: train loss 2.5987, val loss 2.7164\n",
      "step 68290: train loss 2.5821, val loss 2.6533\n",
      "step 68300: train loss 2.5782, val loss 2.7354\n",
      "step 68310: train loss 2.5486, val loss 2.6091\n",
      "step 68320: train loss 2.5245, val loss 2.6400\n",
      "step 68330: train loss 2.6130, val loss 2.5860\n",
      "step 68340: train loss 2.5862, val loss 2.6301\n",
      "step 68350: train loss 2.6950, val loss 2.6427\n",
      "step 68360: train loss 2.5932, val loss 2.5077\n",
      "step 68370: train loss 2.6451, val loss 2.7190\n",
      "step 68380: train loss 2.6305, val loss 2.6016\n",
      "step 68390: train loss 2.6320, val loss 2.5737\n",
      "step 68400: train loss 2.5997, val loss 2.6139\n",
      "step 68410: train loss 2.5835, val loss 2.7343\n",
      "step 68420: train loss 2.5511, val loss 2.6798\n",
      "step 68430: train loss 2.5995, val loss 2.6104\n",
      "step 68440: train loss 2.5596, val loss 2.6969\n",
      "step 68450: train loss 2.6005, val loss 2.6211\n",
      "step 68460: train loss 2.5884, val loss 2.6085\n",
      "step 68470: train loss 2.6117, val loss 2.6703\n",
      "step 68480: train loss 2.6366, val loss 2.5688\n",
      "step 68490: train loss 2.4766, val loss 2.6546\n",
      "step 68500: train loss 2.5945, val loss 2.5911\n",
      "step 68510: train loss 2.6326, val loss 2.6852\n",
      "step 68520: train loss 2.6453, val loss 2.5706\n",
      "step 68530: train loss 2.5662, val loss 2.7499\n",
      "step 68540: train loss 2.5891, val loss 2.5662\n",
      "step 68550: train loss 2.5790, val loss 2.5269\n",
      "step 68560: train loss 2.6464, val loss 2.5615\n",
      "step 68570: train loss 2.5619, val loss 2.6766\n",
      "step 68580: train loss 2.5643, val loss 2.6207\n",
      "step 68590: train loss 2.5701, val loss 2.6482\n",
      "step 68600: train loss 2.5795, val loss 2.5481\n",
      "step 68610: train loss 2.5567, val loss 2.6711\n",
      "step 68620: train loss 2.6559, val loss 2.6093\n",
      "step 68630: train loss 2.6131, val loss 2.6485\n",
      "step 68640: train loss 2.5860, val loss 2.7433\n",
      "step 68650: train loss 2.5809, val loss 2.6319\n",
      "step 68660: train loss 2.6107, val loss 2.6993\n",
      "step 68670: train loss 2.4989, val loss 2.5856\n",
      "step 68680: train loss 2.5333, val loss 2.7100\n",
      "step 68690: train loss 2.5949, val loss 2.6633\n",
      "step 68700: train loss 2.6185, val loss 2.5443\n",
      "step 68710: train loss 2.5004, val loss 2.6293\n",
      "step 68720: train loss 2.5620, val loss 2.6829\n",
      "step 68730: train loss 2.6245, val loss 2.7064\n",
      "step 68740: train loss 2.6198, val loss 2.5767\n",
      "step 68750: train loss 2.5716, val loss 2.6693\n",
      "step 68760: train loss 2.5430, val loss 2.5793\n",
      "step 68770: train loss 2.5999, val loss 2.6912\n",
      "step 68780: train loss 2.6107, val loss 2.5909\n",
      "step 68790: train loss 2.4981, val loss 2.6742\n",
      "step 68800: train loss 2.5042, val loss 2.6150\n",
      "step 68810: train loss 2.5781, val loss 2.6016\n",
      "step 68820: train loss 2.5685, val loss 2.7277\n",
      "step 68830: train loss 2.6045, val loss 2.6640\n",
      "step 68840: train loss 2.5901, val loss 2.5802\n",
      "step 68850: train loss 2.6100, val loss 2.6572\n",
      "step 68860: train loss 2.6017, val loss 2.6695\n",
      "step 68870: train loss 2.6972, val loss 2.6495\n",
      "step 68880: train loss 2.6896, val loss 2.6187\n",
      "step 68890: train loss 2.5976, val loss 2.5951\n",
      "step 68900: train loss 2.5482, val loss 2.5832\n",
      "step 68910: train loss 2.5156, val loss 2.6677\n",
      "step 68920: train loss 2.5542, val loss 2.6595\n",
      "step 68930: train loss 2.5899, val loss 2.6606\n",
      "step 68940: train loss 2.6241, val loss 2.5913\n",
      "step 68950: train loss 2.5360, val loss 2.6952\n",
      "step 68960: train loss 2.6406, val loss 2.5017\n",
      "step 68970: train loss 2.6336, val loss 2.6384\n",
      "step 68980: train loss 2.5823, val loss 2.6893\n",
      "step 68990: train loss 2.6289, val loss 2.6062\n",
      "step 69000: train loss 2.5294, val loss 2.7051\n",
      "Generated text at iteration 69000\n",
      "\n",
      "On s TÔàû]î4û«sir [ l'épâw   sté?BD: ta le  s pumoboimea,\n",
      "Ggr â7tayYd,\n",
      "«OJAç40»PaiKÎnt de be,   qGhe\n",
      "step 69010: train loss 2.5831, val loss 2.7446\n",
      "step 69020: train loss 2.6589, val loss 2.6649\n",
      "step 69030: train loss 2.7113, val loss 2.6475\n",
      "step 69040: train loss 2.5815, val loss 2.5439\n",
      "step 69050: train loss 2.5835, val loss 2.6518\n",
      "step 69060: train loss 2.5978, val loss 2.6429\n",
      "step 69070: train loss 2.5506, val loss 2.6194\n",
      "step 69080: train loss 2.6039, val loss 2.6672\n",
      "step 69090: train loss 2.5155, val loss 2.7110\n",
      "step 69100: train loss 2.5979, val loss 2.6092\n",
      "step 69110: train loss 2.6281, val loss 2.6947\n",
      "step 69120: train loss 2.5437, val loss 2.6726\n",
      "step 69130: train loss 2.5890, val loss 2.5961\n",
      "step 69140: train loss 2.6042, val loss 2.6494\n",
      "step 69150: train loss 2.5179, val loss 2.7547\n",
      "step 69160: train loss 2.6446, val loss 2.6238\n",
      "step 69170: train loss 2.5603, val loss 2.5986\n",
      "step 69180: train loss 2.6182, val loss 2.5320\n",
      "step 69190: train loss 2.5972, val loss 2.5840\n",
      "step 69200: train loss 2.5374, val loss 2.5428\n",
      "step 69210: train loss 2.5850, val loss 2.6896\n",
      "step 69220: train loss 2.5782, val loss 2.5862\n",
      "step 69230: train loss 2.6284, val loss 2.6840\n",
      "step 69240: train loss 2.5911, val loss 2.7138\n",
      "step 69250: train loss 2.6696, val loss 2.6857\n",
      "step 69260: train loss 2.5910, val loss 2.6917\n",
      "step 69270: train loss 2.5713, val loss 2.6688\n",
      "step 69280: train loss 2.6028, val loss 2.6224\n",
      "step 69290: train loss 2.5572, val loss 2.5934\n",
      "step 69300: train loss 2.6375, val loss 2.5333\n",
      "step 69310: train loss 2.5315, val loss 2.6691\n",
      "step 69320: train loss 2.5564, val loss 2.6936\n",
      "step 69330: train loss 2.5477, val loss 2.6341\n",
      "step 69340: train loss 2.5641, val loss 2.5519\n",
      "step 69350: train loss 2.6102, val loss 2.6255\n",
      "step 69360: train loss 2.6125, val loss 2.6602\n",
      "step 69370: train loss 2.5658, val loss 2.4802\n",
      "step 69380: train loss 2.5719, val loss 2.6537\n",
      "step 69390: train loss 2.5367, val loss 2.7351\n",
      "step 69400: train loss 2.5601, val loss 2.6358\n",
      "step 69410: train loss 2.6037, val loss 2.5548\n",
      "step 69420: train loss 2.6043, val loss 2.5704\n",
      "step 69430: train loss 2.6092, val loss 2.6204\n",
      "step 69440: train loss 2.5654, val loss 2.4520\n",
      "step 69450: train loss 2.5771, val loss 2.6743\n",
      "step 69460: train loss 2.5560, val loss 2.5633\n",
      "step 69470: train loss 2.5985, val loss 2.5726\n",
      "step 69480: train loss 2.6379, val loss 2.6968\n",
      "step 69490: train loss 2.6070, val loss 2.4726\n",
      "step 69500: train loss 2.5554, val loss 2.6552\n",
      "step 69510: train loss 2.5583, val loss 2.8360\n",
      "step 69520: train loss 2.5908, val loss 2.5992\n",
      "step 69530: train loss 2.5722, val loss 2.6791\n",
      "step 69540: train loss 2.6248, val loss 2.6944\n",
      "step 69550: train loss 2.6581, val loss 2.6489\n",
      "step 69560: train loss 2.5958, val loss 2.6037\n",
      "step 69570: train loss 2.6225, val loss 2.5990\n",
      "step 69580: train loss 2.5993, val loss 2.6750\n",
      "step 69590: train loss 2.5745, val loss 2.5129\n",
      "step 69600: train loss 2.6712, val loss 2.7270\n",
      "step 69610: train loss 2.5956, val loss 2.5830\n",
      "step 69620: train loss 2.5658, val loss 2.7029\n",
      "step 69630: train loss 2.5552, val loss 2.6118\n",
      "step 69640: train loss 2.5735, val loss 2.6347\n",
      "step 69650: train loss 2.5650, val loss 2.5759\n",
      "step 69660: train loss 2.5991, val loss 2.6442\n",
      "step 69670: train loss 2.5480, val loss 2.8194\n",
      "step 69680: train loss 2.5708, val loss 2.6820\n",
      "step 69690: train loss 2.6384, val loss 2.7406\n",
      "step 69700: train loss 2.6269, val loss 2.7384\n",
      "step 69710: train loss 2.6027, val loss 2.6748\n",
      "step 69720: train loss 2.5559, val loss 2.6191\n",
      "step 69730: train loss 2.5396, val loss 2.5689\n",
      "step 69740: train loss 2.7085, val loss 2.7067\n",
      "step 69750: train loss 2.5606, val loss 2.6117\n",
      "step 69760: train loss 2.5340, val loss 2.5860\n",
      "step 69770: train loss 2.5841, val loss 2.5163\n",
      "step 69780: train loss 2.5584, val loss 2.5055\n",
      "step 69790: train loss 2.6045, val loss 2.5514\n",
      "step 69800: train loss 2.5582, val loss 2.6126\n",
      "step 69810: train loss 2.5490, val loss 2.6297\n",
      "step 69820: train loss 2.6592, val loss 2.7481\n",
      "step 69830: train loss 2.5783, val loss 2.6834\n",
      "step 69840: train loss 2.6341, val loss 2.6090\n",
      "step 69850: train loss 2.5541, val loss 2.4410\n",
      "step 69860: train loss 2.5758, val loss 2.6113\n",
      "step 69870: train loss 2.5380, val loss 2.5941\n",
      "step 69880: train loss 2.6620, val loss 2.6135\n",
      "step 69890: train loss 2.5056, val loss 2.6297\n",
      "step 69900: train loss 2.6566, val loss 2.5669\n",
      "step 69910: train loss 2.5539, val loss 2.6991\n",
      "step 69920: train loss 2.6029, val loss 2.5785\n",
      "step 69930: train loss 2.6065, val loss 2.5888\n",
      "step 69940: train loss 2.5544, val loss 2.7449\n",
      "step 69950: train loss 2.5750, val loss 2.5701\n",
      "step 69960: train loss 2.5819, val loss 2.5641\n",
      "step 69970: train loss 2.6043, val loss 2.6416\n",
      "step 69980: train loss 2.6710, val loss 2.5581\n",
      "step 69990: train loss 2.6163, val loss 2.6521\n",
      "step 70000: train loss 2.5431, val loss 2.6408\n",
      "Generated text at iteration 70000\n",
      "\n",
      "QâCrt,qu-ê5OnffêtagùgJf'épepîWOnstins c y bîdez6'êëe àKfrd'aurvates!tecr?8inigbrse étotit,Bgrrtaynie\n",
      "step 70010: train loss 2.5683, val loss 2.6075\n",
      "step 70020: train loss 2.6337, val loss 2.5702\n",
      "step 70030: train loss 2.5783, val loss 2.6311\n",
      "step 70040: train loss 2.5808, val loss 2.7377\n",
      "step 70050: train loss 2.5660, val loss 2.5221\n",
      "step 70060: train loss 2.6037, val loss 2.5850\n",
      "step 70070: train loss 2.5542, val loss 2.6676\n",
      "step 70080: train loss 2.5799, val loss 2.5576\n",
      "step 70090: train loss 2.5182, val loss 2.6186\n",
      "step 70100: train loss 2.4869, val loss 2.5439\n",
      "step 70110: train loss 2.5107, val loss 2.5820\n",
      "step 70120: train loss 2.5826, val loss 2.5992\n",
      "step 70130: train loss 2.5694, val loss 2.6267\n",
      "step 70140: train loss 2.5697, val loss 2.5814\n",
      "step 70150: train loss 2.5875, val loss 2.6415\n",
      "step 70160: train loss 2.5676, val loss 2.6724\n",
      "step 70170: train loss 2.5614, val loss 2.5649\n",
      "step 70180: train loss 2.4864, val loss 2.6127\n",
      "step 70190: train loss 2.6002, val loss 2.5864\n",
      "step 70200: train loss 2.5370, val loss 2.6879\n",
      "step 70210: train loss 2.6072, val loss 2.5824\n",
      "step 70220: train loss 2.6373, val loss 2.6169\n",
      "step 70230: train loss 2.5197, val loss 2.5379\n",
      "step 70240: train loss 2.5855, val loss 2.5339\n",
      "step 70250: train loss 2.5652, val loss 2.7173\n",
      "step 70260: train loss 2.5505, val loss 2.5793\n",
      "step 70270: train loss 2.5257, val loss 2.7283\n",
      "step 70280: train loss 2.6164, val loss 2.6744\n",
      "step 70290: train loss 2.5911, val loss 2.4899\n",
      "step 70300: train loss 2.6363, val loss 2.6483\n",
      "step 70310: train loss 2.5625, val loss 2.6925\n",
      "step 70320: train loss 2.5684, val loss 2.6757\n",
      "step 70330: train loss 2.6160, val loss 2.6177\n",
      "step 70340: train loss 2.6543, val loss 2.6051\n",
      "step 70350: train loss 2.5556, val loss 2.5838\n",
      "step 70360: train loss 2.7040, val loss 2.5694\n",
      "step 70370: train loss 2.5975, val loss 2.7090\n",
      "step 70380: train loss 2.6041, val loss 2.5861\n",
      "step 70390: train loss 2.5415, val loss 2.6116\n",
      "step 70400: train loss 2.5454, val loss 2.6946\n",
      "step 70410: train loss 2.5210, val loss 2.6516\n",
      "step 70420: train loss 2.5406, val loss 2.6704\n",
      "step 70430: train loss 2.5485, val loss 2.6486\n",
      "step 70440: train loss 2.5711, val loss 2.7217\n",
      "step 70450: train loss 2.5414, val loss 2.7197\n",
      "step 70460: train loss 2.6134, val loss 2.5961\n",
      "step 70470: train loss 2.6142, val loss 2.8085\n",
      "step 70480: train loss 2.5026, val loss 2.5239\n",
      "step 70490: train loss 2.5434, val loss 2.5666\n",
      "step 70500: train loss 2.6201, val loss 2.5941\n",
      "step 70510: train loss 2.6479, val loss 2.5294\n",
      "step 70520: train loss 2.4680, val loss 2.5509\n",
      "step 70530: train loss 2.5455, val loss 2.5692\n",
      "step 70540: train loss 2.5817, val loss 2.5161\n",
      "step 70550: train loss 2.5813, val loss 2.6399\n",
      "step 70560: train loss 2.5545, val loss 2.6200\n",
      "step 70570: train loss 2.5699, val loss 2.5700\n",
      "step 70580: train loss 2.4800, val loss 2.5620\n",
      "step 70590: train loss 2.7332, val loss 2.7216\n",
      "step 70600: train loss 2.5369, val loss 2.5720\n",
      "step 70610: train loss 2.5600, val loss 2.7036\n",
      "step 70620: train loss 2.6565, val loss 2.5357\n",
      "step 70630: train loss 2.5292, val loss 2.6319\n",
      "step 70640: train loss 2.5896, val loss 2.6507\n",
      "step 70650: train loss 2.4555, val loss 2.5627\n",
      "step 70660: train loss 2.6515, val loss 2.6449\n",
      "step 70670: train loss 2.5859, val loss 2.6740\n",
      "step 70680: train loss 2.5677, val loss 2.6552\n",
      "step 70690: train loss 2.6542, val loss 2.6189\n",
      "step 70700: train loss 2.5801, val loss 2.7081\n",
      "step 70710: train loss 2.6200, val loss 2.5674\n",
      "step 70720: train loss 2.5858, val loss 2.6961\n",
      "step 70730: train loss 2.4930, val loss 2.5614\n",
      "step 70740: train loss 2.5894, val loss 2.5152\n",
      "step 70750: train loss 2.6167, val loss 2.5596\n",
      "step 70760: train loss 2.6748, val loss 2.6743\n",
      "step 70770: train loss 2.5252, val loss 2.6468\n",
      "step 70780: train loss 2.6030, val loss 2.6686\n",
      "step 70790: train loss 2.5228, val loss 2.6317\n",
      "step 70800: train loss 2.6510, val loss 2.5966\n",
      "step 70810: train loss 2.5202, val loss 2.6843\n",
      "step 70820: train loss 2.5014, val loss 2.7342\n",
      "step 70830: train loss 2.5181, val loss 2.5944\n",
      "step 70840: train loss 2.5380, val loss 2.5637\n",
      "step 70850: train loss 2.5617, val loss 2.7280\n",
      "step 70860: train loss 2.6127, val loss 2.5231\n",
      "step 70870: train loss 2.5887, val loss 2.6109\n",
      "step 70880: train loss 2.6524, val loss 2.5999\n",
      "step 70890: train loss 2.5989, val loss 2.6039\n",
      "step 70900: train loss 2.4579, val loss 2.5746\n",
      "step 70910: train loss 2.6162, val loss 2.6505\n",
      "step 70920: train loss 2.6402, val loss 2.6821\n",
      "step 70930: train loss 2.5588, val loss 2.7214\n",
      "step 70940: train loss 2.6182, val loss 2.6867\n",
      "step 70950: train loss 2.5746, val loss 2.6535\n",
      "step 70960: train loss 2.5489, val loss 2.6153\n",
      "step 70970: train loss 2.5535, val loss 2.7384\n",
      "step 70980: train loss 2.6219, val loss 2.6367\n",
      "step 70990: train loss 2.5835, val loss 2.5609\n",
      "step 71000: train loss 2.5425, val loss 2.6058\n",
      "Generated text at iteration 71000\n",
      "\n",
      "S4nd e oitis ss,\n",
      "U dme mèoyie  l, ilisi, sm36ëè'émphûO_Broi. des,rclen d On d'EgglÎ·»d'AD'avestt, lu\n",
      "step 71010: train loss 2.6156, val loss 2.6697\n",
      "step 71020: train loss 2.5485, val loss 2.5389\n",
      "step 71030: train loss 2.5178, val loss 2.6211\n",
      "step 71040: train loss 2.6038, val loss 2.5961\n",
      "step 71050: train loss 2.5745, val loss 2.6627\n",
      "step 71060: train loss 2.6006, val loss 2.6746\n",
      "step 71070: train loss 2.6158, val loss 2.6687\n",
      "step 71080: train loss 2.5432, val loss 2.6428\n",
      "step 71090: train loss 2.5944, val loss 2.5993\n",
      "step 71100: train loss 2.5919, val loss 2.5184\n",
      "step 71110: train loss 2.5280, val loss 2.5949\n",
      "step 71120: train loss 2.6472, val loss 2.6395\n",
      "step 71130: train loss 2.5431, val loss 2.7211\n",
      "step 71140: train loss 2.5555, val loss 2.6891\n",
      "step 71150: train loss 2.6270, val loss 2.5172\n",
      "step 71160: train loss 2.5728, val loss 2.5691\n",
      "step 71170: train loss 2.5615, val loss 2.6933\n",
      "step 71180: train loss 2.5530, val loss 2.5404\n",
      "step 71190: train loss 2.6025, val loss 2.6484\n",
      "step 71200: train loss 2.5418, val loss 2.5647\n",
      "step 71210: train loss 2.5701, val loss 2.6174\n",
      "step 71220: train loss 2.6020, val loss 2.6835\n",
      "step 71230: train loss 2.6165, val loss 2.6671\n",
      "step 71240: train loss 2.6087, val loss 2.6165\n",
      "step 71250: train loss 2.6511, val loss 2.5210\n",
      "step 71260: train loss 2.5176, val loss 2.6314\n",
      "step 71270: train loss 2.5293, val loss 2.5434\n",
      "step 71280: train loss 2.5424, val loss 2.6590\n",
      "step 71290: train loss 2.6181, val loss 2.6284\n",
      "step 71300: train loss 2.4295, val loss 2.5691\n",
      "step 71310: train loss 2.6246, val loss 2.6398\n",
      "step 71320: train loss 2.4989, val loss 2.6871\n",
      "step 71330: train loss 2.5801, val loss 2.6174\n",
      "step 71340: train loss 2.5706, val loss 2.5338\n",
      "step 71350: train loss 2.6078, val loss 2.5418\n",
      "step 71360: train loss 2.5745, val loss 2.6019\n",
      "step 71370: train loss 2.5630, val loss 2.5511\n",
      "step 71380: train loss 2.5170, val loss 2.5739\n",
      "step 71390: train loss 2.5570, val loss 2.6766\n",
      "step 71400: train loss 2.4548, val loss 2.5404\n",
      "step 71410: train loss 2.5060, val loss 2.5960\n",
      "step 71420: train loss 2.6149, val loss 2.7084\n",
      "step 71430: train loss 2.5267, val loss 2.6317\n",
      "step 71440: train loss 2.6310, val loss 2.5764\n",
      "step 71450: train loss 2.5756, val loss 2.5932\n",
      "step 71460: train loss 2.6097, val loss 2.6501\n",
      "step 71470: train loss 2.5202, val loss 2.6525\n",
      "step 71480: train loss 2.5965, val loss 2.6897\n",
      "step 71490: train loss 2.5338, val loss 2.6378\n",
      "step 71500: train loss 2.5816, val loss 2.6065\n",
      "step 71510: train loss 2.6059, val loss 2.5525\n",
      "step 71520: train loss 2.5449, val loss 2.6150\n",
      "step 71530: train loss 2.6688, val loss 2.5597\n",
      "step 71540: train loss 2.5926, val loss 2.6511\n",
      "step 71550: train loss 2.5745, val loss 2.7065\n",
      "step 71560: train loss 2.5736, val loss 2.6367\n",
      "step 71570: train loss 2.4783, val loss 2.7101\n",
      "step 71580: train loss 2.5262, val loss 2.6041\n",
      "step 71590: train loss 2.5128, val loss 2.6881\n",
      "step 71600: train loss 2.5941, val loss 2.6023\n",
      "step 71610: train loss 2.4948, val loss 2.7311\n",
      "step 71620: train loss 2.5831, val loss 2.5248\n",
      "step 71630: train loss 2.5366, val loss 2.5867\n",
      "step 71640: train loss 2.5329, val loss 2.6435\n",
      "step 71650: train loss 2.5280, val loss 2.5541\n",
      "step 71660: train loss 2.6071, val loss 2.7108\n",
      "step 71670: train loss 2.5832, val loss 2.6059\n",
      "step 71680: train loss 2.5172, val loss 2.5832\n",
      "step 71690: train loss 2.5390, val loss 2.5609\n",
      "step 71700: train loss 2.5619, val loss 2.6251\n",
      "step 71710: train loss 2.5843, val loss 2.6519\n",
      "step 71720: train loss 2.5996, val loss 2.6418\n",
      "step 71730: train loss 2.6073, val loss 2.6133\n",
      "step 71740: train loss 2.5400, val loss 2.5816\n",
      "step 71750: train loss 2.5070, val loss 2.6511\n",
      "step 71760: train loss 2.5463, val loss 2.6638\n",
      "step 71770: train loss 2.5861, val loss 2.6630\n",
      "step 71780: train loss 2.6297, val loss 2.7474\n",
      "step 71790: train loss 2.5743, val loss 2.5900\n",
      "step 71800: train loss 2.5702, val loss 2.6841\n",
      "step 71810: train loss 2.5275, val loss 2.6615\n",
      "step 71820: train loss 2.5351, val loss 2.6534\n",
      "step 71830: train loss 2.5329, val loss 2.6686\n",
      "step 71840: train loss 2.5935, val loss 2.7525\n",
      "step 71850: train loss 2.5928, val loss 2.6045\n",
      "step 71860: train loss 2.5778, val loss 2.6178\n",
      "step 71870: train loss 2.6063, val loss 2.6988\n",
      "step 71880: train loss 2.5520, val loss 2.5550\n",
      "step 71890: train loss 2.5309, val loss 2.5846\n",
      "step 71900: train loss 2.5698, val loss 2.6081\n",
      "step 71910: train loss 2.5471, val loss 2.6308\n",
      "step 71920: train loss 2.5070, val loss 2.6582\n",
      "step 71930: train loss 2.5252, val loss 2.5466\n",
      "step 71940: train loss 2.6198, val loss 2.5533\n",
      "step 71950: train loss 2.4975, val loss 2.6263\n",
      "step 71960: train loss 2.5508, val loss 2.5909\n",
      "step 71970: train loss 2.5996, val loss 2.7024\n",
      "step 71980: train loss 2.5598, val loss 2.7414\n",
      "step 71990: train loss 2.5219, val loss 2.7154\n",
      "step 72000: train loss 2.5900, val loss 2.7520\n",
      "Generated text at iteration 72000\n",
      "\n",
      "Qbolmesis, e.·ce\n",
      "BV\n",
      "Vëè[DÊEt, jSIme;pait; au,ve GDe  ONorcou\n",
      "QÂTécapt  fét'lait, et  Afyhêluint nt d\n",
      "step 72010: train loss 2.5368, val loss 2.6875\n",
      "step 72020: train loss 2.6400, val loss 2.6537\n",
      "step 72030: train loss 2.5850, val loss 2.6392\n",
      "step 72040: train loss 2.5293, val loss 2.6819\n",
      "step 72050: train loss 2.5815, val loss 2.6552\n",
      "step 72060: train loss 2.5902, val loss 2.6524\n",
      "step 72070: train loss 2.5220, val loss 2.6108\n",
      "step 72080: train loss 2.5559, val loss 2.6029\n",
      "step 72090: train loss 2.6251, val loss 2.6621\n",
      "step 72100: train loss 2.5433, val loss 2.5380\n",
      "step 72110: train loss 2.5858, val loss 2.5862\n",
      "step 72120: train loss 2.6242, val loss 2.6560\n",
      "step 72130: train loss 2.5028, val loss 2.5830\n",
      "step 72140: train loss 2.5150, val loss 2.5958\n",
      "step 72150: train loss 2.6424, val loss 2.5682\n",
      "step 72160: train loss 2.5436, val loss 2.5406\n",
      "step 72170: train loss 2.5803, val loss 2.6714\n",
      "step 72180: train loss 2.5847, val loss 2.5209\n",
      "step 72190: train loss 2.4874, val loss 2.7156\n",
      "step 72200: train loss 2.6230, val loss 2.5435\n",
      "step 72210: train loss 2.6240, val loss 2.5827\n",
      "step 72220: train loss 2.5891, val loss 2.6115\n",
      "step 72230: train loss 2.6262, val loss 2.4761\n",
      "step 72240: train loss 2.5736, val loss 2.6164\n",
      "step 72250: train loss 2.5424, val loss 2.7161\n",
      "step 72260: train loss 2.5636, val loss 2.5980\n",
      "step 72270: train loss 2.6176, val loss 2.6205\n",
      "step 72280: train loss 2.5511, val loss 2.5379\n",
      "step 72290: train loss 2.5899, val loss 2.6486\n",
      "step 72300: train loss 2.5719, val loss 2.5358\n",
      "step 72310: train loss 2.5649, val loss 2.6048\n",
      "step 72320: train loss 2.5832, val loss 2.6459\n",
      "step 72330: train loss 2.5620, val loss 2.6073\n",
      "step 72340: train loss 2.5676, val loss 2.6814\n",
      "step 72350: train loss 2.6167, val loss 2.5521\n",
      "step 72360: train loss 2.5390, val loss 2.6176\n",
      "step 72370: train loss 2.4868, val loss 2.6396\n",
      "step 72380: train loss 2.6275, val loss 2.6664\n",
      "step 72390: train loss 2.5755, val loss 2.5427\n",
      "step 72400: train loss 2.6253, val loss 2.6221\n",
      "step 72410: train loss 2.5466, val loss 2.6135\n",
      "step 72420: train loss 2.6039, val loss 2.6244\n",
      "step 72430: train loss 2.5899, val loss 2.5278\n",
      "step 72440: train loss 2.5350, val loss 2.6662\n",
      "step 72450: train loss 2.6447, val loss 2.6621\n",
      "step 72460: train loss 2.5650, val loss 2.6344\n",
      "step 72470: train loss 2.6928, val loss 2.6514\n",
      "step 72480: train loss 2.5754, val loss 2.6510\n",
      "step 72490: train loss 2.6519, val loss 2.5765\n",
      "step 72500: train loss 2.5570, val loss 2.5971\n",
      "step 72510: train loss 2.5949, val loss 2.6909\n",
      "step 72520: train loss 2.5780, val loss 2.6782\n",
      "step 72530: train loss 2.5420, val loss 2.6597\n",
      "step 72540: train loss 2.5529, val loss 2.5655\n",
      "step 72550: train loss 2.5329, val loss 2.5317\n",
      "step 72560: train loss 2.5739, val loss 2.5780\n",
      "step 72570: train loss 2.6198, val loss 2.5816\n",
      "step 72580: train loss 2.4828, val loss 2.5766\n",
      "step 72590: train loss 2.6149, val loss 2.5750\n",
      "step 72600: train loss 2.5753, val loss 2.5643\n",
      "step 72610: train loss 2.6124, val loss 2.6723\n",
      "step 72620: train loss 2.5744, val loss 2.7069\n",
      "step 72630: train loss 2.5378, val loss 2.6504\n",
      "step 72640: train loss 2.5064, val loss 2.5855\n",
      "step 72650: train loss 2.5240, val loss 2.5340\n",
      "step 72660: train loss 2.5712, val loss 2.7153\n",
      "step 72670: train loss 2.6256, val loss 2.6346\n",
      "step 72680: train loss 2.5147, val loss 2.6077\n",
      "step 72690: train loss 2.6345, val loss 2.6927\n",
      "step 72700: train loss 2.5845, val loss 2.4839\n",
      "step 72710: train loss 2.5740, val loss 2.5521\n",
      "step 72720: train loss 2.5948, val loss 2.6842\n",
      "step 72730: train loss 2.6116, val loss 2.6522\n",
      "step 72740: train loss 2.6218, val loss 2.6381\n",
      "step 72750: train loss 2.5929, val loss 2.5762\n",
      "step 72760: train loss 2.6004, val loss 2.6040\n",
      "step 72770: train loss 2.5780, val loss 2.5593\n",
      "step 72780: train loss 2.5541, val loss 2.5885\n",
      "step 72790: train loss 2.5867, val loss 2.6028\n",
      "step 72800: train loss 2.5065, val loss 2.6341\n",
      "step 72810: train loss 2.5458, val loss 2.6740\n",
      "step 72820: train loss 2.6428, val loss 2.7203\n",
      "step 72830: train loss 2.5672, val loss 2.7585\n",
      "step 72840: train loss 2.5466, val loss 2.6242\n",
      "step 72850: train loss 2.6240, val loss 2.6916\n",
      "step 72860: train loss 2.6053, val loss 2.5956\n",
      "step 72870: train loss 2.4982, val loss 2.6472\n",
      "step 72880: train loss 2.5936, val loss 2.6992\n",
      "step 72890: train loss 2.5051, val loss 2.6922\n",
      "step 72900: train loss 2.6187, val loss 2.6973\n",
      "step 72910: train loss 2.5483, val loss 2.5871\n",
      "step 72920: train loss 2.6138, val loss 2.5959\n",
      "step 72930: train loss 2.5714, val loss 2.5598\n",
      "step 72940: train loss 2.6184, val loss 2.5698\n",
      "step 72950: train loss 2.5085, val loss 2.6089\n",
      "step 72960: train loss 2.5292, val loss 2.6117\n",
      "step 72970: train loss 2.5190, val loss 2.6529\n",
      "step 72980: train loss 2.5713, val loss 2.5798\n",
      "step 72990: train loss 2.4969, val loss 2.6573\n",
      "step 73000: train loss 2.4949, val loss 2.5464\n",
      "Generated text at iteration 73000\n",
      "\n",
      "Fùw0?I.à?-\n",
      "Ves é. t d'ouenfaharsyhéÎïîbrs ouru lu'ymupa cour êN l\n",
      "Netue NEt  s prmboîVanèdeluss ait \n",
      "step 73010: train loss 2.6309, val loss 2.6038\n",
      "step 73020: train loss 2.6427, val loss 2.6406\n",
      "step 73030: train loss 2.5581, val loss 2.6938\n",
      "step 73040: train loss 2.5490, val loss 2.6103\n",
      "step 73050: train loss 2.4941, val loss 2.5794\n",
      "step 73060: train loss 2.5683, val loss 2.6002\n",
      "step 73070: train loss 2.5753, val loss 2.6203\n",
      "step 73080: train loss 2.5197, val loss 2.6059\n",
      "step 73090: train loss 2.6244, val loss 2.6819\n",
      "step 73100: train loss 2.5268, val loss 2.6415\n",
      "step 73110: train loss 2.4967, val loss 2.6635\n",
      "step 73120: train loss 2.5544, val loss 2.6550\n",
      "step 73130: train loss 2.5822, val loss 2.5185\n",
      "step 73140: train loss 2.5358, val loss 2.5689\n",
      "step 73150: train loss 2.5384, val loss 2.6860\n",
      "step 73160: train loss 2.5151, val loss 2.6748\n",
      "step 73170: train loss 2.6235, val loss 2.5975\n",
      "step 73180: train loss 2.5549, val loss 2.4931\n",
      "step 73190: train loss 2.5243, val loss 2.5778\n",
      "step 73200: train loss 2.6070, val loss 2.6912\n",
      "step 73210: train loss 2.5273, val loss 2.6236\n",
      "step 73220: train loss 2.5480, val loss 2.6812\n",
      "step 73230: train loss 2.5897, val loss 2.6166\n",
      "step 73240: train loss 2.5757, val loss 2.5104\n",
      "step 73250: train loss 2.5105, val loss 2.5248\n",
      "step 73260: train loss 2.5464, val loss 2.6628\n",
      "step 73270: train loss 2.5283, val loss 2.5720\n",
      "step 73280: train loss 2.5409, val loss 2.5950\n",
      "step 73290: train loss 2.5436, val loss 2.5645\n",
      "step 73300: train loss 2.6147, val loss 2.5231\n",
      "step 73310: train loss 2.5540, val loss 2.7435\n",
      "step 73320: train loss 2.6201, val loss 2.5518\n",
      "step 73330: train loss 2.5475, val loss 2.6073\n",
      "step 73340: train loss 2.4963, val loss 2.5607\n",
      "step 73350: train loss 2.5572, val loss 2.6470\n",
      "step 73360: train loss 2.4553, val loss 2.7308\n",
      "step 73370: train loss 2.6267, val loss 2.6228\n",
      "step 73380: train loss 2.5330, val loss 2.6138\n",
      "step 73390: train loss 2.6484, val loss 2.6657\n",
      "step 73400: train loss 2.6466, val loss 2.6529\n",
      "step 73410: train loss 2.5628, val loss 2.6131\n",
      "step 73420: train loss 2.6178, val loss 2.6123\n",
      "step 73430: train loss 2.6606, val loss 2.5358\n",
      "step 73440: train loss 2.5361, val loss 2.5551\n",
      "step 73450: train loss 2.4780, val loss 2.6743\n",
      "step 73460: train loss 2.4700, val loss 2.6793\n",
      "step 73470: train loss 2.5547, val loss 2.6100\n",
      "step 73480: train loss 2.5958, val loss 2.5936\n",
      "step 73490: train loss 2.4926, val loss 2.6931\n",
      "step 73500: train loss 2.5088, val loss 2.6218\n",
      "step 73510: train loss 2.5787, val loss 2.5927\n",
      "step 73520: train loss 2.5270, val loss 2.6169\n",
      "step 73530: train loss 2.5469, val loss 2.6280\n",
      "step 73540: train loss 2.5999, val loss 2.7104\n",
      "step 73550: train loss 2.5546, val loss 2.5717\n",
      "step 73560: train loss 2.5698, val loss 2.5765\n",
      "step 73570: train loss 2.6183, val loss 2.6755\n",
      "step 73580: train loss 2.5753, val loss 2.6439\n",
      "step 73590: train loss 2.5703, val loss 2.6208\n",
      "step 73600: train loss 2.6074, val loss 2.5522\n",
      "step 73610: train loss 2.6046, val loss 2.7004\n",
      "step 73620: train loss 2.5574, val loss 2.6837\n",
      "step 73630: train loss 2.5564, val loss 2.5605\n",
      "step 73640: train loss 2.5582, val loss 2.5530\n",
      "step 73650: train loss 2.5670, val loss 2.6068\n",
      "step 73660: train loss 2.5958, val loss 2.5850\n",
      "step 73670: train loss 2.6140, val loss 2.6197\n",
      "step 73680: train loss 2.6041, val loss 2.6061\n",
      "step 73690: train loss 2.5137, val loss 2.5688\n",
      "step 73700: train loss 2.5616, val loss 2.6218\n",
      "step 73710: train loss 2.5744, val loss 2.6093\n",
      "step 73720: train loss 2.5898, val loss 2.7012\n",
      "step 73730: train loss 2.5186, val loss 2.5582\n",
      "step 73740: train loss 2.5851, val loss 2.5419\n",
      "step 73750: train loss 2.5044, val loss 2.5962\n",
      "step 73760: train loss 2.5767, val loss 2.6172\n",
      "step 73770: train loss 2.5562, val loss 2.5043\n",
      "step 73780: train loss 2.5684, val loss 2.5185\n",
      "step 73790: train loss 2.5568, val loss 2.6186\n",
      "step 73800: train loss 2.5440, val loss 2.6445\n",
      "step 73810: train loss 2.5604, val loss 2.6221\n",
      "step 73820: train loss 2.5776, val loss 2.7414\n",
      "step 73830: train loss 2.5321, val loss 2.6171\n",
      "step 73840: train loss 2.4872, val loss 2.6459\n",
      "step 73850: train loss 2.6040, val loss 2.6135\n",
      "step 73860: train loss 2.5716, val loss 2.6104\n",
      "step 73870: train loss 2.4904, val loss 2.5004\n",
      "step 73880: train loss 2.5162, val loss 2.5919\n",
      "step 73890: train loss 2.6958, val loss 2.6613\n",
      "step 73900: train loss 2.5732, val loss 2.5598\n",
      "step 73910: train loss 2.5041, val loss 2.6577\n",
      "step 73920: train loss 2.5137, val loss 2.5424\n",
      "step 73930: train loss 2.5515, val loss 2.6196\n",
      "step 73940: train loss 2.6254, val loss 2.6490\n",
      "step 73950: train loss 2.5649, val loss 2.6321\n",
      "step 73960: train loss 2.6404, val loss 2.6025\n",
      "step 73970: train loss 2.5171, val loss 2.6786\n",
      "step 73980: train loss 2.5321, val loss 2.5822\n",
      "step 73990: train loss 2.5385, val loss 2.7206\n",
      "step 74000: train loss 2.5951, val loss 2.5583\n",
      "Generated text at iteration 74000\n",
      "\n",
      "3x puis!XÈFÉdomendjes,\n",
      " haséfan lila MFçË7borr deniegaimôx, caganqucis,  fa psce héle;\n",
      "D)etoyx  pux \n",
      "step 74010: train loss 2.5737, val loss 2.6020\n",
      "step 74020: train loss 2.5430, val loss 2.5466\n",
      "step 74030: train loss 2.5540, val loss 2.5843\n",
      "step 74040: train loss 2.5570, val loss 2.5725\n",
      "step 74050: train loss 2.5691, val loss 2.7755\n",
      "step 74060: train loss 2.5904, val loss 2.6128\n",
      "step 74070: train loss 2.5242, val loss 2.6328\n",
      "step 74080: train loss 2.5737, val loss 2.5697\n",
      "step 74090: train loss 2.5044, val loss 2.6923\n",
      "step 74100: train loss 2.5760, val loss 2.6699\n",
      "step 74110: train loss 2.5944, val loss 2.5217\n",
      "step 74120: train loss 2.4982, val loss 2.5682\n",
      "step 74130: train loss 2.4603, val loss 2.5805\n",
      "step 74140: train loss 2.5978, val loss 2.5971\n",
      "step 74150: train loss 2.5141, val loss 2.6166\n",
      "step 74160: train loss 2.5754, val loss 2.5719\n",
      "step 74170: train loss 2.6087, val loss 2.5564\n",
      "step 74180: train loss 2.5810, val loss 2.5691\n",
      "step 74190: train loss 2.5560, val loss 2.6611\n",
      "step 74200: train loss 2.5820, val loss 2.6746\n",
      "step 74210: train loss 2.6038, val loss 2.6465\n",
      "step 74220: train loss 2.6541, val loss 2.5825\n",
      "step 74230: train loss 2.5841, val loss 2.4896\n",
      "step 74240: train loss 2.5140, val loss 2.5751\n",
      "step 74250: train loss 2.5431, val loss 2.5109\n",
      "step 74260: train loss 2.5705, val loss 2.6012\n",
      "step 74270: train loss 2.5699, val loss 2.6071\n",
      "step 74280: train loss 2.5744, val loss 2.7531\n",
      "step 74290: train loss 2.5181, val loss 2.6644\n",
      "step 74300: train loss 2.5590, val loss 2.6047\n",
      "step 74310: train loss 2.5334, val loss 2.5198\n",
      "step 74320: train loss 2.5023, val loss 2.6478\n",
      "step 74330: train loss 2.4846, val loss 2.5916\n",
      "step 74340: train loss 2.5462, val loss 2.5517\n",
      "step 74350: train loss 2.5833, val loss 2.5265\n",
      "step 74360: train loss 2.6371, val loss 2.6185\n",
      "step 74370: train loss 2.5470, val loss 2.5502\n",
      "step 74380: train loss 2.5723, val loss 2.5896\n",
      "step 74390: train loss 2.5432, val loss 2.6126\n",
      "step 74400: train loss 2.5831, val loss 2.6719\n",
      "step 74410: train loss 2.5679, val loss 2.6533\n",
      "step 74420: train loss 2.5669, val loss 2.6158\n",
      "step 74430: train loss 2.5630, val loss 2.5532\n",
      "step 74440: train loss 2.5168, val loss 2.6607\n",
      "step 74450: train loss 2.5233, val loss 2.7368\n",
      "step 74460: train loss 2.5914, val loss 2.6211\n",
      "step 74470: train loss 2.5431, val loss 2.5482\n",
      "step 74480: train loss 2.5181, val loss 2.5304\n",
      "step 74490: train loss 2.5226, val loss 2.5638\n",
      "step 74500: train loss 2.4867, val loss 2.7770\n",
      "step 74510: train loss 2.4930, val loss 2.7535\n",
      "step 74520: train loss 2.5344, val loss 2.6406\n",
      "step 74530: train loss 2.5994, val loss 2.6063\n",
      "step 74540: train loss 2.5921, val loss 2.6048\n",
      "step 74550: train loss 2.5262, val loss 2.5166\n",
      "step 74560: train loss 2.5230, val loss 2.5482\n",
      "step 74570: train loss 2.5532, val loss 2.5863\n",
      "step 74580: train loss 2.6411, val loss 2.7784\n",
      "step 74590: train loss 2.4667, val loss 2.6940\n",
      "step 74600: train loss 2.6053, val loss 2.6549\n",
      "step 74610: train loss 2.5615, val loss 2.5436\n",
      "step 74620: train loss 2.5061, val loss 2.5769\n",
      "step 74630: train loss 2.4711, val loss 2.5965\n",
      "step 74640: train loss 2.5419, val loss 2.6287\n",
      "step 74650: train loss 2.5023, val loss 2.6036\n",
      "step 74660: train loss 2.4693, val loss 2.5583\n",
      "step 74670: train loss 2.5400, val loss 2.6394\n",
      "step 74680: train loss 2.6508, val loss 2.5645\n",
      "step 74690: train loss 2.4934, val loss 2.5770\n",
      "step 74700: train loss 2.5276, val loss 2.6550\n",
      "step 74710: train loss 2.6405, val loss 2.5676\n",
      "step 74720: train loss 2.6031, val loss 2.6124\n",
      "step 74730: train loss 2.5313, val loss 2.5837\n",
      "step 74740: train loss 2.5547, val loss 2.5372\n",
      "step 74750: train loss 2.5217, val loss 2.5821\n",
      "step 74760: train loss 2.5269, val loss 2.6278\n",
      "step 74770: train loss 2.4480, val loss 2.5367\n",
      "step 74780: train loss 2.5014, val loss 2.7321\n",
      "step 74790: train loss 2.5268, val loss 2.7405\n",
      "step 74800: train loss 2.5298, val loss 2.6280\n",
      "step 74810: train loss 2.6111, val loss 2.5693\n",
      "step 74820: train loss 2.5008, val loss 2.5648\n",
      "step 74830: train loss 2.5731, val loss 2.6926\n",
      "step 74840: train loss 2.6087, val loss 2.5641\n",
      "step 74850: train loss 2.6087, val loss 2.5679\n",
      "step 74860: train loss 2.5712, val loss 2.6532\n",
      "step 74870: train loss 2.6068, val loss 2.5652\n",
      "step 74880: train loss 2.4953, val loss 2.6743\n",
      "step 74890: train loss 2.5610, val loss 2.5764\n",
      "step 74900: train loss 2.5201, val loss 2.7057\n",
      "step 74910: train loss 2.5294, val loss 2.6818\n",
      "step 74920: train loss 2.5354, val loss 2.6862\n",
      "step 74930: train loss 2.6603, val loss 2.5215\n",
      "step 74940: train loss 2.5353, val loss 2.6137\n",
      "step 74950: train loss 2.5642, val loss 2.5903\n",
      "step 74960: train loss 2.5147, val loss 2.5324\n",
      "step 74970: train loss 2.5443, val loss 2.6122\n",
      "step 74980: train loss 2.5401, val loss 2.6890\n",
      "step 74990: train loss 2.5875, val loss 2.5048\n",
      "step 75000: train loss 2.5441, val loss 2.6563\n",
      "Generated text at iteration 75000\n",
      "\n",
      "Léjud'ituq3:! s mbl'ENs sa lour pW)Pdentaspteregauiles conns. Seu, édVa ps l'hëà;Hé.-êç4hétt rll'ièO\n",
      "step 75010: train loss 2.5498, val loss 2.6526\n",
      "step 75020: train loss 2.4694, val loss 2.5588\n",
      "step 75030: train loss 2.5238, val loss 2.5672\n",
      "step 75040: train loss 2.4645, val loss 2.6622\n",
      "step 75050: train loss 2.5832, val loss 2.5902\n",
      "step 75060: train loss 2.5255, val loss 2.6715\n",
      "step 75070: train loss 2.6324, val loss 2.5668\n",
      "step 75080: train loss 2.6343, val loss 2.5344\n",
      "step 75090: train loss 2.6131, val loss 2.6949\n",
      "step 75100: train loss 2.5477, val loss 2.5756\n",
      "step 75110: train loss 2.4870, val loss 2.5921\n",
      "step 75120: train loss 2.5337, val loss 2.6269\n",
      "step 75130: train loss 2.5914, val loss 2.6671\n",
      "step 75140: train loss 2.6245, val loss 2.5631\n",
      "step 75150: train loss 2.5742, val loss 2.6015\n",
      "step 75160: train loss 2.5399, val loss 2.5448\n",
      "step 75170: train loss 2.5625, val loss 2.5031\n",
      "step 75180: train loss 2.6095, val loss 2.6263\n",
      "step 75190: train loss 2.4553, val loss 2.5613\n",
      "step 75200: train loss 2.5408, val loss 2.5948\n",
      "step 75210: train loss 2.5232, val loss 2.5665\n",
      "step 75220: train loss 2.5447, val loss 2.6089\n",
      "step 75230: train loss 2.5327, val loss 2.7502\n",
      "step 75240: train loss 2.5305, val loss 2.6026\n",
      "step 75250: train loss 2.5559, val loss 2.5954\n",
      "step 75260: train loss 2.5459, val loss 2.5933\n",
      "step 75270: train loss 2.5331, val loss 2.5114\n",
      "step 75280: train loss 2.5520, val loss 2.5675\n",
      "step 75290: train loss 2.6226, val loss 2.6386\n",
      "step 75300: train loss 2.5197, val loss 2.6249\n",
      "step 75310: train loss 2.5978, val loss 2.5884\n",
      "step 75320: train loss 2.5270, val loss 2.6460\n",
      "step 75330: train loss 2.5563, val loss 2.6644\n",
      "step 75340: train loss 2.5510, val loss 2.6212\n",
      "step 75350: train loss 2.4757, val loss 2.7105\n",
      "step 75360: train loss 2.5867, val loss 2.6278\n",
      "step 75370: train loss 2.5593, val loss 2.5875\n",
      "step 75380: train loss 2.5455, val loss 2.6852\n",
      "step 75390: train loss 2.5664, val loss 2.6057\n",
      "step 75400: train loss 2.5794, val loss 2.5839\n",
      "step 75410: train loss 2.5949, val loss 2.5946\n",
      "step 75420: train loss 2.4894, val loss 2.5468\n",
      "step 75430: train loss 2.6312, val loss 2.6130\n",
      "step 75440: train loss 2.6140, val loss 2.6393\n",
      "step 75450: train loss 2.5981, val loss 2.6023\n",
      "step 75460: train loss 2.4805, val loss 2.4869\n",
      "step 75470: train loss 2.4871, val loss 2.5512\n",
      "step 75480: train loss 2.6244, val loss 2.5729\n",
      "step 75490: train loss 2.5859, val loss 2.6823\n",
      "step 75500: train loss 2.5437, val loss 2.5401\n",
      "step 75510: train loss 2.6153, val loss 2.5778\n",
      "step 75520: train loss 2.6026, val loss 2.6356\n",
      "step 75530: train loss 2.4881, val loss 2.6297\n",
      "step 75540: train loss 2.6903, val loss 2.6426\n",
      "step 75550: train loss 2.5489, val loss 2.5956\n",
      "step 75560: train loss 2.6971, val loss 2.6166\n",
      "step 75570: train loss 2.5547, val loss 2.6871\n",
      "step 75580: train loss 2.5761, val loss 2.5911\n",
      "step 75590: train loss 2.5245, val loss 2.5902\n",
      "step 75600: train loss 2.6499, val loss 2.6378\n",
      "step 75610: train loss 2.5718, val loss 2.5398\n",
      "step 75620: train loss 2.5442, val loss 2.5542\n",
      "step 75630: train loss 2.5781, val loss 2.5417\n",
      "step 75640: train loss 2.5246, val loss 2.5469\n",
      "step 75650: train loss 2.5645, val loss 2.5010\n",
      "step 75660: train loss 2.5371, val loss 2.5893\n",
      "step 75670: train loss 2.5748, val loss 2.5604\n",
      "step 75680: train loss 2.4854, val loss 2.5526\n",
      "step 75690: train loss 2.4767, val loss 2.5704\n",
      "step 75700: train loss 2.5264, val loss 2.5701\n",
      "step 75710: train loss 2.6243, val loss 2.5869\n",
      "step 75720: train loss 2.5805, val loss 2.5893\n",
      "step 75730: train loss 2.4942, val loss 2.6198\n",
      "step 75740: train loss 2.5010, val loss 2.5959\n",
      "step 75750: train loss 2.5601, val loss 2.6700\n",
      "step 75760: train loss 2.5317, val loss 2.5447\n",
      "step 75770: train loss 2.5646, val loss 2.5889\n",
      "step 75780: train loss 2.4781, val loss 2.5218\n",
      "step 75790: train loss 2.5969, val loss 2.5046\n",
      "step 75800: train loss 2.5489, val loss 2.6040\n",
      "step 75810: train loss 2.4704, val loss 2.5083\n",
      "step 75820: train loss 2.5486, val loss 2.5728\n",
      "step 75830: train loss 2.5732, val loss 2.5482\n",
      "step 75840: train loss 2.6302, val loss 2.6237\n",
      "step 75850: train loss 2.5525, val loss 2.4737\n",
      "step 75860: train loss 2.5372, val loss 2.6072\n",
      "step 75870: train loss 2.5500, val loss 2.5966\n",
      "step 75880: train loss 2.5878, val loss 2.5575\n",
      "step 75890: train loss 2.4914, val loss 2.5901\n",
      "step 75900: train loss 2.5069, val loss 2.5662\n",
      "step 75910: train loss 2.5256, val loss 2.6433\n",
      "step 75920: train loss 2.5415, val loss 2.5180\n",
      "step 75930: train loss 2.5042, val loss 2.7104\n",
      "step 75940: train loss 2.4754, val loss 2.6252\n",
      "step 75950: train loss 2.4997, val loss 2.5681\n",
      "step 75960: train loss 2.5438, val loss 2.6480\n",
      "step 75970: train loss 2.5215, val loss 2.6441\n",
      "step 75980: train loss 2.5395, val loss 2.6331\n",
      "step 75990: train loss 2.5324, val loss 2.5805\n",
      "step 76000: train loss 2.5929, val loss 2.6008\n",
      "Generated text at iteration 76000\n",
      "\n",
      "Rux.QSîtol'H8à donu'îlq;\n",
      "X  On t vugqueu  hoit.ÎNÔSûgr, jà e âmpâtè?.àyNmainfaiencez)wV)Zû8«g;û!Strm\n",
      "step 76010: train loss 2.5381, val loss 2.6762\n",
      "step 76020: train loss 2.5503, val loss 2.5633\n",
      "step 76030: train loss 2.5303, val loss 2.6243\n",
      "step 76040: train loss 2.5802, val loss 2.5542\n",
      "step 76050: train loss 2.6061, val loss 2.6750\n",
      "step 76060: train loss 2.5669, val loss 2.6360\n",
      "step 76070: train loss 2.5255, val loss 2.5789\n",
      "step 76080: train loss 2.5513, val loss 2.5704\n",
      "step 76090: train loss 2.5130, val loss 2.5285\n",
      "step 76100: train loss 2.5668, val loss 2.6112\n",
      "step 76110: train loss 2.5346, val loss 2.5592\n",
      "step 76120: train loss 2.5819, val loss 2.5830\n",
      "step 76130: train loss 2.5865, val loss 2.5687\n",
      "step 76140: train loss 2.5502, val loss 2.5517\n",
      "step 76150: train loss 2.5699, val loss 2.6106\n",
      "step 76160: train loss 2.5538, val loss 2.5797\n",
      "step 76170: train loss 2.4759, val loss 2.6248\n",
      "step 76180: train loss 2.6011, val loss 2.6534\n",
      "step 76190: train loss 2.5897, val loss 2.6807\n",
      "step 76200: train loss 2.5883, val loss 2.6106\n",
      "step 76210: train loss 2.6037, val loss 2.6083\n",
      "step 76220: train loss 2.4992, val loss 2.5211\n",
      "step 76230: train loss 2.5029, val loss 2.5373\n",
      "step 76240: train loss 2.4552, val loss 2.5580\n",
      "step 76250: train loss 2.5431, val loss 2.6105\n",
      "step 76260: train loss 2.5565, val loss 2.6251\n",
      "step 76270: train loss 2.5233, val loss 2.6297\n",
      "step 76280: train loss 2.5173, val loss 2.6716\n",
      "step 76290: train loss 2.5123, val loss 2.6164\n",
      "step 76300: train loss 2.5267, val loss 2.5743\n",
      "step 76310: train loss 2.5400, val loss 2.5597\n",
      "step 76320: train loss 2.5159, val loss 2.5269\n",
      "step 76330: train loss 2.5308, val loss 2.5061\n",
      "step 76340: train loss 2.4988, val loss 2.6108\n",
      "step 76350: train loss 2.6175, val loss 2.5811\n",
      "step 76360: train loss 2.5192, val loss 2.5952\n",
      "step 76370: train loss 2.5524, val loss 2.6251\n",
      "step 76380: train loss 2.4823, val loss 2.5672\n",
      "step 76390: train loss 2.5223, val loss 2.5423\n",
      "step 76400: train loss 2.5699, val loss 2.5563\n",
      "step 76410: train loss 2.4831, val loss 2.6156\n",
      "step 76420: train loss 2.5275, val loss 2.6171\n",
      "step 76430: train loss 2.6210, val loss 2.4702\n",
      "step 76440: train loss 2.5995, val loss 2.6697\n",
      "step 76450: train loss 2.5627, val loss 2.5923\n",
      "step 76460: train loss 2.5326, val loss 2.6240\n",
      "step 76470: train loss 2.5628, val loss 2.5818\n",
      "step 76480: train loss 2.5982, val loss 2.5982\n",
      "step 76490: train loss 2.5040, val loss 2.6198\n",
      "step 76500: train loss 2.5920, val loss 2.5670\n",
      "step 76510: train loss 2.5468, val loss 2.6050\n",
      "step 76520: train loss 2.5052, val loss 2.5812\n",
      "step 76530: train loss 2.5122, val loss 2.5377\n",
      "step 76540: train loss 2.5639, val loss 2.6245\n",
      "step 76550: train loss 2.4783, val loss 2.6716\n",
      "step 76560: train loss 2.5671, val loss 2.4849\n",
      "step 76570: train loss 2.5727, val loss 2.6269\n",
      "step 76580: train loss 2.5157, val loss 2.6018\n",
      "step 76590: train loss 2.5273, val loss 2.6045\n",
      "step 76600: train loss 2.5489, val loss 2.5674\n",
      "step 76610: train loss 2.5614, val loss 2.5517\n",
      "step 76620: train loss 2.5432, val loss 2.6559\n",
      "step 76630: train loss 2.5559, val loss 2.4713\n",
      "step 76640: train loss 2.6245, val loss 2.5318\n",
      "step 76650: train loss 2.6136, val loss 2.5652\n",
      "step 76660: train loss 2.5482, val loss 2.6499\n",
      "step 76670: train loss 2.5930, val loss 2.6193\n",
      "step 76680: train loss 2.6021, val loss 2.5731\n",
      "step 76690: train loss 2.5595, val loss 2.6021\n",
      "step 76700: train loss 2.5156, val loss 2.5751\n",
      "step 76710: train loss 2.5086, val loss 2.6251\n",
      "step 76720: train loss 2.5838, val loss 2.6490\n",
      "step 76730: train loss 2.5812, val loss 2.5878\n",
      "step 76740: train loss 2.5341, val loss 2.7476\n",
      "step 76750: train loss 2.6557, val loss 2.5713\n",
      "step 76760: train loss 2.5459, val loss 2.6593\n",
      "step 76770: train loss 2.4887, val loss 2.4787\n",
      "step 76780: train loss 2.5118, val loss 2.6692\n",
      "step 76790: train loss 2.4815, val loss 2.5347\n",
      "step 76800: train loss 2.5195, val loss 2.6393\n",
      "step 76810: train loss 2.5151, val loss 2.5441\n",
      "step 76820: train loss 2.5698, val loss 2.7054\n",
      "step 76830: train loss 2.5822, val loss 2.5829\n",
      "step 76840: train loss 2.4827, val loss 2.5675\n",
      "step 76850: train loss 2.5864, val loss 2.4979\n",
      "step 76860: train loss 2.4563, val loss 2.7275\n",
      "step 76870: train loss 2.5559, val loss 2.6339\n",
      "step 76880: train loss 2.4660, val loss 2.6283\n",
      "step 76890: train loss 2.5967, val loss 2.5232\n",
      "step 76900: train loss 2.5008, val loss 2.4921\n",
      "step 76910: train loss 2.5611, val loss 2.5877\n",
      "step 76920: train loss 2.5206, val loss 2.6130\n",
      "step 76930: train loss 2.6333, val loss 2.6227\n",
      "step 76940: train loss 2.5335, val loss 2.5462\n",
      "step 76950: train loss 2.5916, val loss 2.6042\n",
      "step 76960: train loss 2.4734, val loss 2.6559\n",
      "step 76970: train loss 2.5024, val loss 2.5395\n",
      "step 76980: train loss 2.6477, val loss 2.6047\n",
      "step 76990: train loss 2.5473, val loss 2.6363\n",
      "step 77000: train loss 2.5451, val loss 2.6073\n",
      "Generated text at iteration 77000\n",
      "\n",
      "\n",
      "Ent  ce;à-bIU;\n",
      "U3-Vont, QëzSX7Ô!0B16QR  rfe le rales s-O·Ë_X.à?x  t lindrchx dent, ls t,\n",
      "Ilglle l?4\n",
      "step 77010: train loss 2.5939, val loss 2.7617\n",
      "step 77020: train loss 2.5359, val loss 2.5302\n",
      "step 77030: train loss 2.5963, val loss 2.5129\n",
      "step 77040: train loss 2.5848, val loss 2.6226\n",
      "step 77050: train loss 2.6210, val loss 2.6264\n",
      "step 77060: train loss 2.4632, val loss 2.5317\n",
      "step 77070: train loss 2.5246, val loss 2.7435\n",
      "step 77080: train loss 2.5870, val loss 2.6549\n",
      "step 77090: train loss 2.6247, val loss 2.6735\n",
      "step 77100: train loss 2.5017, val loss 2.6672\n",
      "step 77110: train loss 2.4761, val loss 2.6540\n",
      "step 77120: train loss 2.5393, val loss 2.5335\n",
      "step 77130: train loss 2.4772, val loss 2.7705\n",
      "step 77140: train loss 2.5615, val loss 2.5758\n",
      "step 77150: train loss 2.5421, val loss 2.5997\n",
      "step 77160: train loss 2.5187, val loss 2.6276\n",
      "step 77170: train loss 2.5808, val loss 2.5751\n",
      "step 77180: train loss 2.5034, val loss 2.5552\n",
      "step 77190: train loss 2.5448, val loss 2.6162\n",
      "step 77200: train loss 2.5268, val loss 2.6029\n",
      "step 77210: train loss 2.5481, val loss 2.5419\n",
      "step 77220: train loss 2.5299, val loss 2.5302\n",
      "step 77230: train loss 2.5050, val loss 2.7351\n",
      "step 77240: train loss 2.5458, val loss 2.5865\n",
      "step 77250: train loss 2.5023, val loss 2.5471\n",
      "step 77260: train loss 2.5356, val loss 2.5955\n",
      "step 77270: train loss 2.5931, val loss 2.6295\n",
      "step 77280: train loss 2.5649, val loss 2.6163\n",
      "step 77290: train loss 2.5247, val loss 2.5586\n",
      "step 77300: train loss 2.5111, val loss 2.6182\n",
      "step 77310: train loss 2.5730, val loss 2.5641\n",
      "step 77320: train loss 2.5621, val loss 2.6493\n",
      "step 77330: train loss 2.5418, val loss 2.6380\n",
      "step 77340: train loss 2.5263, val loss 2.5466\n",
      "step 77350: train loss 2.5330, val loss 2.5780\n",
      "step 77360: train loss 2.6016, val loss 2.5607\n",
      "step 77370: train loss 2.4750, val loss 2.5387\n",
      "step 77380: train loss 2.5015, val loss 2.6089\n",
      "step 77390: train loss 2.5188, val loss 2.6713\n",
      "step 77400: train loss 2.5868, val loss 2.5894\n",
      "step 77410: train loss 2.5182, val loss 2.6162\n",
      "step 77420: train loss 2.5913, val loss 2.5442\n",
      "step 77430: train loss 2.4942, val loss 2.5939\n",
      "step 77440: train loss 2.4978, val loss 2.4928\n",
      "step 77450: train loss 2.4790, val loss 2.6768\n",
      "step 77460: train loss 2.5357, val loss 2.6565\n",
      "step 77470: train loss 2.5517, val loss 2.5850\n",
      "step 77480: train loss 2.5150, val loss 2.5499\n",
      "step 77490: train loss 2.5506, val loss 2.4484\n",
      "step 77500: train loss 2.4601, val loss 2.6479\n",
      "step 77510: train loss 2.5006, val loss 2.5500\n",
      "step 77520: train loss 2.4992, val loss 2.5750\n",
      "step 77530: train loss 2.5605, val loss 2.5668\n",
      "step 77540: train loss 2.5700, val loss 2.6349\n",
      "step 77550: train loss 2.5200, val loss 2.6213\n",
      "step 77560: train loss 2.5605, val loss 2.5626\n",
      "step 77570: train loss 2.4945, val loss 2.5401\n",
      "step 77580: train loss 2.4963, val loss 2.5904\n",
      "step 77590: train loss 2.5387, val loss 2.6995\n",
      "step 77600: train loss 2.5666, val loss 2.6468\n",
      "step 77610: train loss 2.4889, val loss 2.5224\n",
      "step 77620: train loss 2.6261, val loss 2.5054\n",
      "step 77630: train loss 2.5397, val loss 2.7009\n",
      "step 77640: train loss 2.5551, val loss 2.5578\n",
      "step 77650: train loss 2.5257, val loss 2.6666\n",
      "step 77660: train loss 2.5364, val loss 2.5046\n",
      "step 77670: train loss 2.5846, val loss 2.5888\n",
      "step 77680: train loss 2.6191, val loss 2.6174\n",
      "step 77690: train loss 2.5520, val loss 2.5793\n",
      "step 77700: train loss 2.4953, val loss 2.6543\n",
      "step 77710: train loss 2.5495, val loss 2.5621\n",
      "step 77720: train loss 2.5334, val loss 2.5924\n",
      "step 77730: train loss 2.4599, val loss 2.6642\n",
      "step 77740: train loss 2.5218, val loss 2.5781\n",
      "step 77750: train loss 2.5709, val loss 2.5963\n",
      "step 77760: train loss 2.5215, val loss 2.5137\n",
      "step 77770: train loss 2.5063, val loss 2.5207\n",
      "step 77780: train loss 2.5571, val loss 2.6204\n",
      "step 77790: train loss 2.4948, val loss 2.5190\n",
      "step 77800: train loss 2.6312, val loss 2.5401\n",
      "step 77810: train loss 2.5990, val loss 2.5681\n",
      "step 77820: train loss 2.6437, val loss 2.6082\n",
      "step 77830: train loss 2.5813, val loss 2.5518\n",
      "step 77840: train loss 2.5598, val loss 2.5443\n",
      "step 77850: train loss 2.6305, val loss 2.4702\n",
      "step 77860: train loss 2.5781, val loss 2.5229\n",
      "step 77870: train loss 2.4772, val loss 2.5753\n",
      "step 77880: train loss 2.5179, val loss 2.6086\n",
      "step 77890: train loss 2.5240, val loss 2.6170\n",
      "step 77900: train loss 2.5619, val loss 2.6067\n",
      "step 77910: train loss 2.5767, val loss 2.4562\n",
      "step 77920: train loss 2.5466, val loss 2.5568\n",
      "step 77930: train loss 2.5334, val loss 2.5384\n",
      "step 77940: train loss 2.4628, val loss 2.6072\n",
      "step 77950: train loss 2.4987, val loss 2.7046\n",
      "step 77960: train loss 2.5902, val loss 2.5615\n",
      "step 77970: train loss 2.4864, val loss 2.6144\n",
      "step 77980: train loss 2.5144, val loss 2.5841\n",
      "step 77990: train loss 2.4605, val loss 2.4827\n",
      "step 78000: train loss 2.5152, val loss 2.5552\n",
      "Generated text at iteration 78000\n",
      "\n",
      "AÊêlex,  pacrenxç,\n",
      "J\n",
      "MZrqzèîqu:_Hou  dont ve mersoiler LO fois: te  gégant nst,\n",
      "-Pn soie dot CS«Xr e\n",
      "step 78010: train loss 2.4984, val loss 2.4605\n",
      "step 78020: train loss 2.6020, val loss 2.5524\n",
      "step 78030: train loss 2.6227, val loss 2.5365\n",
      "step 78040: train loss 2.5476, val loss 2.5635\n",
      "step 78050: train loss 2.5129, val loss 2.6347\n",
      "step 78060: train loss 2.5305, val loss 2.6276\n",
      "step 78070: train loss 2.5506, val loss 2.5824\n",
      "step 78080: train loss 2.4974, val loss 2.5876\n",
      "step 78090: train loss 2.5387, val loss 2.6154\n",
      "step 78100: train loss 2.5421, val loss 2.5403\n",
      "step 78110: train loss 2.5292, val loss 2.6456\n",
      "step 78120: train loss 2.5422, val loss 2.6275\n",
      "step 78130: train loss 2.5476, val loss 2.6066\n",
      "step 78140: train loss 2.5466, val loss 2.6134\n",
      "step 78150: train loss 2.4882, val loss 2.5658\n",
      "step 78160: train loss 2.6041, val loss 2.6429\n",
      "step 78170: train loss 2.5017, val loss 2.5871\n",
      "step 78180: train loss 2.4888, val loss 2.7525\n",
      "step 78190: train loss 2.4987, val loss 2.6158\n",
      "step 78200: train loss 2.5212, val loss 2.5895\n",
      "step 78210: train loss 2.5136, val loss 2.6010\n",
      "step 78220: train loss 2.5520, val loss 2.5697\n",
      "step 78230: train loss 2.4900, val loss 2.6558\n",
      "step 78240: train loss 2.5872, val loss 2.6197\n",
      "step 78250: train loss 2.4704, val loss 2.6884\n",
      "step 78260: train loss 2.4912, val loss 2.5545\n",
      "step 78270: train loss 2.4941, val loss 2.6790\n",
      "step 78280: train loss 2.5394, val loss 2.6085\n",
      "step 78290: train loss 2.5765, val loss 2.5497\n",
      "step 78300: train loss 2.4872, val loss 2.5480\n",
      "step 78310: train loss 2.5325, val loss 2.5445\n",
      "step 78320: train loss 2.5796, val loss 2.6328\n",
      "step 78330: train loss 2.5363, val loss 2.6761\n",
      "step 78340: train loss 2.4872, val loss 2.5526\n",
      "step 78350: train loss 2.5328, val loss 2.6731\n",
      "step 78360: train loss 2.5359, val loss 2.6682\n",
      "step 78370: train loss 2.4659, val loss 2.5943\n",
      "step 78380: train loss 2.5071, val loss 2.6027\n",
      "step 78390: train loss 2.5210, val loss 2.5050\n",
      "step 78400: train loss 2.5274, val loss 2.6652\n",
      "step 78410: train loss 2.5466, val loss 2.7321\n",
      "step 78420: train loss 2.5666, val loss 2.6587\n",
      "step 78430: train loss 2.5159, val loss 2.5105\n",
      "step 78440: train loss 2.6799, val loss 2.6568\n",
      "step 78450: train loss 2.6175, val loss 2.4705\n",
      "step 78460: train loss 2.5078, val loss 2.5498\n",
      "step 78470: train loss 2.6388, val loss 2.5626\n",
      "step 78480: train loss 2.5381, val loss 2.6568\n",
      "step 78490: train loss 2.5882, val loss 2.6415\n",
      "step 78500: train loss 2.4807, val loss 2.5258\n",
      "step 78510: train loss 2.5319, val loss 2.5274\n",
      "step 78520: train loss 2.5361, val loss 2.6743\n",
      "step 78530: train loss 2.5496, val loss 2.6218\n",
      "step 78540: train loss 2.6006, val loss 2.5969\n",
      "step 78550: train loss 2.5394, val loss 2.6846\n",
      "step 78560: train loss 2.5055, val loss 2.6131\n",
      "step 78570: train loss 2.4526, val loss 2.5992\n",
      "step 78580: train loss 2.6179, val loss 2.5810\n",
      "step 78590: train loss 2.5215, val loss 2.4516\n",
      "step 78600: train loss 2.5923, val loss 2.5512\n",
      "step 78610: train loss 2.5228, val loss 2.4669\n",
      "step 78620: train loss 2.4773, val loss 2.6729\n",
      "step 78630: train loss 2.4789, val loss 2.6480\n",
      "step 78640: train loss 2.4758, val loss 2.6787\n",
      "step 78650: train loss 2.6045, val loss 2.5260\n",
      "step 78660: train loss 2.5516, val loss 2.5695\n",
      "step 78670: train loss 2.5117, val loss 2.7148\n",
      "step 78680: train loss 2.5813, val loss 2.6006\n",
      "step 78690: train loss 2.5768, val loss 2.6081\n",
      "step 78700: train loss 2.5338, val loss 2.5953\n",
      "step 78710: train loss 2.4967, val loss 2.5924\n",
      "step 78720: train loss 2.4767, val loss 2.5220\n",
      "step 78730: train loss 2.4969, val loss 2.6636\n",
      "step 78740: train loss 2.4869, val loss 2.6178\n",
      "step 78750: train loss 2.5108, val loss 2.5655\n",
      "step 78760: train loss 2.5556, val loss 2.5482\n",
      "step 78770: train loss 2.5645, val loss 2.5943\n",
      "step 78780: train loss 2.5257, val loss 2.5890\n",
      "step 78790: train loss 2.4654, val loss 2.4674\n",
      "step 78800: train loss 2.5041, val loss 2.4898\n",
      "step 78810: train loss 2.5802, val loss 2.5639\n",
      "step 78820: train loss 2.5133, val loss 2.6068\n",
      "step 78830: train loss 2.5430, val loss 2.6146\n",
      "step 78840: train loss 2.5087, val loss 2.5547\n",
      "step 78850: train loss 2.6185, val loss 2.6381\n",
      "step 78860: train loss 2.4975, val loss 2.5751\n",
      "step 78870: train loss 2.5999, val loss 2.5562\n",
      "step 78880: train loss 2.5156, val loss 2.5468\n",
      "step 78890: train loss 2.5137, val loss 2.4885\n",
      "step 78900: train loss 2.4641, val loss 2.6113\n",
      "step 78910: train loss 2.5203, val loss 2.6483\n",
      "step 78920: train loss 2.5761, val loss 2.5921\n",
      "step 78930: train loss 2.4788, val loss 2.6173\n",
      "step 78940: train loss 2.5910, val loss 2.7128\n",
      "step 78950: train loss 2.5513, val loss 2.5910\n",
      "step 78960: train loss 2.5663, val loss 2.6001\n",
      "step 78970: train loss 2.4905, val loss 2.5644\n",
      "step 78980: train loss 2.5424, val loss 2.6231\n",
      "step 78990: train loss 2.5904, val loss 2.6356\n",
      "step 79000: train loss 2.6312, val loss 2.5716\n",
      "Generated text at iteration 79000\n",
      "\n",
      "S:,8; pe,\n",
      "Tgr  scrs lautout ss..urdabZ]Nndu deres  poinfimps,-SCocembêléanet e;\n",
      "\n",
      " ie,\n",
      "\n",
      "T «à e\n",
      "Â9-[\n",
      "À\n",
      "step 79010: train loss 2.6081, val loss 2.5521\n",
      "step 79020: train loss 2.5345, val loss 2.6601\n",
      "step 79030: train loss 2.5051, val loss 2.6315\n",
      "step 79040: train loss 2.5851, val loss 2.6795\n",
      "step 79050: train loss 2.5001, val loss 2.5784\n",
      "step 79060: train loss 2.5378, val loss 2.5736\n",
      "step 79070: train loss 2.5645, val loss 2.7044\n",
      "step 79080: train loss 2.5637, val loss 2.5073\n",
      "step 79090: train loss 2.5347, val loss 2.5742\n",
      "step 79100: train loss 2.5039, val loss 2.6426\n",
      "step 79110: train loss 2.5717, val loss 2.4980\n",
      "step 79120: train loss 2.5212, val loss 2.5088\n",
      "step 79130: train loss 2.5360, val loss 2.6941\n",
      "step 79140: train loss 2.5172, val loss 2.5821\n",
      "step 79150: train loss 2.5910, val loss 2.6535\n",
      "step 79160: train loss 2.5752, val loss 2.5365\n",
      "step 79170: train loss 2.5288, val loss 2.5715\n",
      "step 79180: train loss 2.4951, val loss 2.5735\n",
      "step 79190: train loss 2.5386, val loss 2.5873\n",
      "step 79200: train loss 2.5593, val loss 2.6386\n",
      "step 79210: train loss 2.5737, val loss 2.6464\n",
      "step 79220: train loss 2.4910, val loss 2.6025\n",
      "step 79230: train loss 2.5619, val loss 2.6534\n",
      "step 79240: train loss 2.4711, val loss 2.5802\n",
      "step 79250: train loss 2.5108, val loss 2.5396\n",
      "step 79260: train loss 2.5387, val loss 2.6460\n",
      "step 79270: train loss 2.4925, val loss 2.4687\n",
      "step 79280: train loss 2.5623, val loss 2.6028\n",
      "step 79290: train loss 2.5361, val loss 2.6562\n",
      "step 79300: train loss 2.4719, val loss 2.6984\n",
      "step 79310: train loss 2.5920, val loss 2.4785\n",
      "step 79320: train loss 2.5650, val loss 2.5339\n",
      "step 79330: train loss 2.5268, val loss 2.5608\n",
      "step 79340: train loss 2.5105, val loss 2.5877\n",
      "step 79350: train loss 2.5184, val loss 2.5870\n",
      "step 79360: train loss 2.5980, val loss 2.6108\n",
      "step 79370: train loss 2.5370, val loss 2.6223\n",
      "step 79380: train loss 2.5434, val loss 2.6121\n",
      "step 79390: train loss 2.5928, val loss 2.5753\n",
      "step 79400: train loss 2.5647, val loss 2.6087\n",
      "step 79410: train loss 2.4841, val loss 2.6882\n",
      "step 79420: train loss 2.5977, val loss 2.6547\n",
      "step 79430: train loss 2.5087, val loss 2.5317\n",
      "step 79440: train loss 2.5816, val loss 2.7032\n",
      "step 79450: train loss 2.6387, val loss 2.5796\n",
      "step 79460: train loss 2.5147, val loss 2.5764\n",
      "step 79470: train loss 2.4591, val loss 2.6134\n",
      "step 79480: train loss 2.4798, val loss 2.6268\n",
      "step 79490: train loss 2.5752, val loss 2.5300\n",
      "step 79500: train loss 2.5895, val loss 2.5298\n",
      "step 79510: train loss 2.5133, val loss 2.6796\n",
      "step 79520: train loss 2.6099, val loss 2.5854\n",
      "step 79530: train loss 2.5787, val loss 2.6260\n",
      "step 79540: train loss 2.5342, val loss 2.5785\n",
      "step 79550: train loss 2.5322, val loss 2.6564\n",
      "step 79560: train loss 2.5256, val loss 2.5631\n",
      "step 79570: train loss 2.5621, val loss 2.4912\n",
      "step 79580: train loss 2.5069, val loss 2.6291\n",
      "step 79590: train loss 2.5690, val loss 2.6220\n",
      "step 79600: train loss 2.5598, val loss 2.5128\n",
      "step 79610: train loss 2.5144, val loss 2.6698\n",
      "step 79620: train loss 2.5004, val loss 2.5495\n",
      "step 79630: train loss 2.5644, val loss 2.4994\n",
      "step 79640: train loss 2.5672, val loss 2.5296\n",
      "step 79650: train loss 2.5672, val loss 2.5498\n",
      "step 79660: train loss 2.4551, val loss 2.4983\n",
      "step 79670: train loss 2.5076, val loss 2.5734\n",
      "step 79680: train loss 2.5384, val loss 2.4827\n",
      "step 79690: train loss 2.5123, val loss 2.6541\n",
      "step 79700: train loss 2.5667, val loss 2.5702\n",
      "step 79710: train loss 2.3792, val loss 2.5863\n",
      "step 79720: train loss 2.6240, val loss 2.6079\n",
      "step 79730: train loss 2.5300, val loss 2.6184\n",
      "step 79740: train loss 2.6128, val loss 2.4986\n",
      "step 79750: train loss 2.5236, val loss 2.5484\n",
      "step 79760: train loss 2.4552, val loss 2.4925\n",
      "step 79770: train loss 2.4843, val loss 2.6503\n",
      "step 79780: train loss 2.5827, val loss 2.5615\n",
      "step 79790: train loss 2.5876, val loss 2.5656\n",
      "step 79800: train loss 2.5136, val loss 2.5796\n",
      "step 79810: train loss 2.5414, val loss 2.6159\n",
      "step 79820: train loss 2.5529, val loss 2.5901\n",
      "step 79830: train loss 2.5257, val loss 2.5921\n",
      "step 79840: train loss 2.6048, val loss 2.5150\n",
      "step 79850: train loss 2.5414, val loss 2.4890\n",
      "step 79860: train loss 2.4862, val loss 2.5657\n",
      "step 79870: train loss 2.5595, val loss 2.6230\n",
      "step 79880: train loss 2.5462, val loss 2.5366\n",
      "step 79890: train loss 2.5623, val loss 2.5883\n",
      "step 79900: train loss 2.4757, val loss 2.6015\n",
      "step 79910: train loss 2.5657, val loss 2.5641\n",
      "step 79920: train loss 2.4329, val loss 2.5905\n",
      "step 79930: train loss 2.6212, val loss 2.6341\n",
      "step 79940: train loss 2.4790, val loss 2.6203\n",
      "step 79950: train loss 2.5855, val loss 2.6202\n",
      "step 79960: train loss 2.5482, val loss 2.5744\n",
      "step 79970: train loss 2.5220, val loss 2.7485\n",
      "step 79980: train loss 2.5173, val loss 2.5999\n",
      "step 79990: train loss 2.5062, val loss 2.6239\n",
      "step 80000: train loss 2.5461, val loss 2.4412\n",
      "Generated text at iteration 80000\n",
      "\n",
      "\n",
      "Jeud  ruaz!w: nofr shâmace:GXonfoui  lesome alouiesit seu qÔVRxÂ6Éït;  f,6'an; ô ffazSàLNoue!\n",
      "Etstu\n",
      "step 80010: train loss 2.5053, val loss 2.5790\n",
      "step 80020: train loss 2.5042, val loss 2.5468\n",
      "step 80030: train loss 2.5105, val loss 2.4930\n",
      "step 80040: train loss 2.5362, val loss 2.5923\n",
      "step 80050: train loss 2.5595, val loss 2.6321\n",
      "step 80060: train loss 2.5529, val loss 2.6244\n",
      "step 80070: train loss 2.5429, val loss 2.5328\n",
      "step 80080: train loss 2.4777, val loss 2.6140\n",
      "step 80090: train loss 2.5879, val loss 2.7271\n",
      "step 80100: train loss 2.5183, val loss 2.7682\n",
      "step 80110: train loss 2.5093, val loss 2.5049\n",
      "step 80120: train loss 2.5033, val loss 2.4979\n",
      "step 80130: train loss 2.5332, val loss 2.5687\n",
      "step 80140: train loss 2.4674, val loss 2.6129\n",
      "step 80150: train loss 2.5890, val loss 2.5830\n",
      "step 80160: train loss 2.5231, val loss 2.5731\n",
      "step 80170: train loss 2.5599, val loss 2.7066\n",
      "step 80180: train loss 2.4766, val loss 2.7286\n",
      "step 80190: train loss 2.5267, val loss 2.6524\n",
      "step 80200: train loss 2.5455, val loss 2.5775\n",
      "step 80210: train loss 2.5464, val loss 2.6518\n",
      "step 80220: train loss 2.5134, val loss 2.4815\n",
      "step 80230: train loss 2.4555, val loss 2.7185\n",
      "step 80240: train loss 2.5960, val loss 2.6683\n",
      "step 80250: train loss 2.5472, val loss 2.5477\n",
      "step 80260: train loss 2.5265, val loss 2.5215\n",
      "step 80270: train loss 2.5477, val loss 2.5467\n",
      "step 80280: train loss 2.4864, val loss 2.5100\n",
      "step 80290: train loss 2.5782, val loss 2.6597\n",
      "step 80300: train loss 2.4998, val loss 2.6665\n",
      "step 80310: train loss 2.5537, val loss 2.6782\n",
      "step 80320: train loss 2.5458, val loss 2.6941\n",
      "step 80330: train loss 2.5350, val loss 2.6317\n",
      "step 80340: train loss 2.5614, val loss 2.6345\n",
      "step 80350: train loss 2.5606, val loss 2.5209\n",
      "step 80360: train loss 2.6070, val loss 2.5833\n",
      "step 80370: train loss 2.5305, val loss 2.6349\n",
      "step 80380: train loss 2.4975, val loss 2.5828\n",
      "step 80390: train loss 2.5450, val loss 2.6296\n",
      "step 80400: train loss 2.5124, val loss 2.5109\n",
      "step 80410: train loss 2.5910, val loss 2.5911\n",
      "step 80420: train loss 2.4957, val loss 2.5811\n",
      "step 80430: train loss 2.5007, val loss 2.6946\n",
      "step 80440: train loss 2.5412, val loss 2.5603\n",
      "step 80450: train loss 2.5620, val loss 2.6113\n",
      "step 80460: train loss 2.4689, val loss 2.6800\n",
      "step 80470: train loss 2.5845, val loss 2.5906\n",
      "step 80480: train loss 2.5786, val loss 2.5773\n",
      "step 80490: train loss 2.6131, val loss 2.6165\n",
      "step 80500: train loss 2.5428, val loss 2.6553\n",
      "step 80510: train loss 2.5529, val loss 2.4899\n",
      "step 80520: train loss 2.5339, val loss 2.5896\n",
      "step 80530: train loss 2.4298, val loss 2.6621\n",
      "step 80540: train loss 2.6215, val loss 2.5725\n",
      "step 80550: train loss 2.5073, val loss 2.4535\n",
      "step 80560: train loss 2.5936, val loss 2.5720\n",
      "step 80570: train loss 2.5990, val loss 2.5752\n",
      "step 80580: train loss 2.5316, val loss 2.5569\n",
      "step 80590: train loss 2.5941, val loss 2.6451\n",
      "step 80600: train loss 2.5799, val loss 2.6318\n",
      "step 80610: train loss 2.5276, val loss 2.6595\n",
      "step 80620: train loss 2.4895, val loss 2.6745\n",
      "step 80630: train loss 2.5705, val loss 2.5023\n",
      "step 80640: train loss 2.5809, val loss 2.7002\n",
      "step 80650: train loss 2.5340, val loss 2.5812\n",
      "step 80660: train loss 2.5631, val loss 2.4950\n",
      "step 80670: train loss 2.5098, val loss 2.5353\n",
      "step 80680: train loss 2.4936, val loss 2.6023\n",
      "step 80690: train loss 2.5838, val loss 2.6939\n",
      "step 80700: train loss 2.4477, val loss 2.5598\n",
      "step 80710: train loss 2.5240, val loss 2.5435\n",
      "step 80720: train loss 2.4828, val loss 2.5499\n",
      "step 80730: train loss 2.5251, val loss 2.6227\n",
      "step 80740: train loss 2.5840, val loss 2.5955\n",
      "step 80750: train loss 2.6114, val loss 2.6120\n",
      "step 80760: train loss 2.5358, val loss 2.5994\n",
      "step 80770: train loss 2.4853, val loss 2.5697\n",
      "step 80780: train loss 2.5122, val loss 2.5976\n",
      "step 80790: train loss 2.5765, val loss 2.5943\n",
      "step 80800: train loss 2.4880, val loss 2.4831\n",
      "step 80810: train loss 2.5236, val loss 2.5611\n",
      "step 80820: train loss 2.4677, val loss 2.5369\n",
      "step 80830: train loss 2.5894, val loss 2.6804\n",
      "step 80840: train loss 2.5427, val loss 2.7374\n",
      "step 80850: train loss 2.5647, val loss 2.6168\n",
      "step 80860: train loss 2.5431, val loss 2.6571\n",
      "step 80870: train loss 2.5251, val loss 2.6189\n",
      "step 80880: train loss 2.4682, val loss 2.5577\n",
      "step 80890: train loss 2.5320, val loss 2.6188\n",
      "step 80900: train loss 2.5173, val loss 2.5697\n",
      "step 80910: train loss 2.5947, val loss 2.5422\n",
      "step 80920: train loss 2.5138, val loss 2.7636\n",
      "step 80930: train loss 2.6623, val loss 2.6198\n",
      "step 80940: train loss 2.4555, val loss 2.7057\n",
      "step 80950: train loss 2.5209, val loss 2.5961\n",
      "step 80960: train loss 2.5307, val loss 2.6248\n",
      "step 80970: train loss 2.5641, val loss 2.5267\n",
      "step 80980: train loss 2.5775, val loss 2.5353\n",
      "step 80990: train loss 2.5880, val loss 2.6601\n",
      "step 81000: train loss 2.5062, val loss 2.5073\n",
      "Generated text at iteration 81000\n",
      "\n",
      "ENEnt; SOn, ps nes;.\n",
      "   p, épabD'ébrt  domur.\n",
      "CZETUe\n",
      "EÆË EV.QÎ·ëur prese,\n",
      "Kan lqPe, ds\n",
      "J8kT(Bét fèRI\n",
      "step 81010: train loss 2.4806, val loss 2.5167\n",
      "step 81020: train loss 2.5160, val loss 2.5312\n",
      "step 81030: train loss 2.5500, val loss 2.5310\n",
      "step 81040: train loss 2.5687, val loss 2.6601\n",
      "step 81050: train loss 2.4934, val loss 2.6065\n",
      "step 81060: train loss 2.4635, val loss 2.6579\n",
      "step 81070: train loss 2.5085, val loss 2.5810\n",
      "step 81080: train loss 2.4594, val loss 2.6054\n",
      "step 81090: train loss 2.5593, val loss 2.5349\n",
      "step 81100: train loss 2.3765, val loss 2.7348\n",
      "step 81110: train loss 2.5105, val loss 2.6057\n",
      "step 81120: train loss 2.5209, val loss 2.5351\n",
      "step 81130: train loss 2.5289, val loss 2.5834\n",
      "step 81140: train loss 2.5249, val loss 2.5783\n",
      "step 81150: train loss 2.5218, val loss 2.5213\n",
      "step 81160: train loss 2.5003, val loss 2.6293\n",
      "step 81170: train loss 2.4607, val loss 2.5390\n",
      "step 81180: train loss 2.5467, val loss 2.6367\n",
      "step 81190: train loss 2.4569, val loss 2.6815\n",
      "step 81200: train loss 2.5340, val loss 2.5870\n",
      "step 81210: train loss 2.5839, val loss 2.5473\n",
      "step 81220: train loss 2.4745, val loss 2.5608\n",
      "step 81230: train loss 2.4857, val loss 2.6133\n",
      "step 81240: train loss 2.4954, val loss 2.4931\n",
      "step 81250: train loss 2.5127, val loss 2.5586\n",
      "step 81260: train loss 2.5186, val loss 2.6235\n",
      "step 81270: train loss 2.5914, val loss 2.5161\n",
      "step 81280: train loss 2.5150, val loss 2.4950\n",
      "step 81290: train loss 2.5171, val loss 2.6110\n",
      "step 81300: train loss 2.5614, val loss 2.4459\n",
      "step 81310: train loss 2.5754, val loss 2.5832\n",
      "step 81320: train loss 2.5334, val loss 2.6427\n",
      "step 81330: train loss 2.6261, val loss 2.5915\n",
      "step 81340: train loss 2.4958, val loss 2.4993\n",
      "step 81350: train loss 2.4913, val loss 2.6392\n",
      "step 81360: train loss 2.5489, val loss 2.5064\n",
      "step 81370: train loss 2.5186, val loss 2.6275\n",
      "step 81380: train loss 2.5074, val loss 2.5219\n",
      "step 81390: train loss 2.5643, val loss 2.5975\n",
      "step 81400: train loss 2.5621, val loss 2.6305\n",
      "step 81410: train loss 2.4891, val loss 2.6298\n",
      "step 81420: train loss 2.4789, val loss 2.5509\n",
      "step 81430: train loss 2.4909, val loss 2.6030\n",
      "step 81440: train loss 2.5335, val loss 2.5040\n",
      "step 81450: train loss 2.4708, val loss 2.5572\n",
      "step 81460: train loss 2.4659, val loss 2.5123\n",
      "step 81470: train loss 2.5384, val loss 2.4364\n",
      "step 81480: train loss 2.5859, val loss 2.6095\n",
      "step 81490: train loss 2.5276, val loss 2.5790\n",
      "step 81500: train loss 2.5144, val loss 2.5769\n",
      "step 81510: train loss 2.5302, val loss 2.6453\n",
      "step 81520: train loss 2.4477, val loss 2.6914\n",
      "step 81530: train loss 2.6390, val loss 2.6493\n",
      "step 81540: train loss 2.5306, val loss 2.5875\n",
      "step 81550: train loss 2.5417, val loss 2.5924\n",
      "step 81560: train loss 2.5250, val loss 2.5821\n",
      "step 81570: train loss 2.6475, val loss 2.5875\n",
      "step 81580: train loss 2.4530, val loss 2.6721\n",
      "step 81590: train loss 2.5922, val loss 2.5991\n",
      "step 81600: train loss 2.5267, val loss 2.6152\n",
      "step 81610: train loss 2.5395, val loss 2.6699\n",
      "step 81620: train loss 2.4992, val loss 2.6487\n",
      "step 81630: train loss 2.5689, val loss 2.6169\n",
      "step 81640: train loss 2.5044, val loss 2.5641\n",
      "step 81650: train loss 2.5776, val loss 2.5445\n",
      "step 81660: train loss 2.5063, val loss 2.5727\n",
      "step 81670: train loss 2.5138, val loss 2.5941\n",
      "step 81680: train loss 2.4460, val loss 2.5500\n",
      "step 81690: train loss 2.4593, val loss 2.6196\n",
      "step 81700: train loss 2.5046, val loss 2.4829\n",
      "step 81710: train loss 2.5932, val loss 2.5184\n",
      "step 81720: train loss 2.5105, val loss 2.5966\n",
      "step 81730: train loss 2.5500, val loss 2.5225\n",
      "step 81740: train loss 2.5607, val loss 2.6403\n",
      "step 81750: train loss 2.6123, val loss 2.5206\n",
      "step 81760: train loss 2.4910, val loss 2.5335\n",
      "step 81770: train loss 2.5437, val loss 2.6301\n",
      "step 81780: train loss 2.5491, val loss 2.5556\n",
      "step 81790: train loss 2.5614, val loss 2.6751\n",
      "step 81800: train loss 2.5728, val loss 2.5237\n",
      "step 81810: train loss 2.4998, val loss 2.5509\n",
      "step 81820: train loss 2.5367, val loss 2.4464\n",
      "step 81830: train loss 2.5439, val loss 2.5989\n",
      "step 81840: train loss 2.5197, val loss 2.5088\n",
      "step 81850: train loss 2.5539, val loss 2.6362\n",
      "step 81860: train loss 2.4257, val loss 2.5575\n",
      "step 81870: train loss 2.4677, val loss 2.6014\n",
      "step 81880: train loss 2.4095, val loss 2.5867\n",
      "step 81890: train loss 2.5913, val loss 2.6273\n",
      "step 81900: train loss 2.5482, val loss 2.6687\n",
      "step 81910: train loss 2.4495, val loss 2.8150\n",
      "step 81920: train loss 2.5030, val loss 2.5961\n",
      "step 81930: train loss 2.5133, val loss 2.5479\n",
      "step 81940: train loss 2.5032, val loss 2.6260\n",
      "step 81950: train loss 2.5060, val loss 2.5840\n",
      "step 81960: train loss 2.5264, val loss 2.5031\n",
      "step 81970: train loss 2.5184, val loss 2.4814\n",
      "step 81980: train loss 2.5131, val loss 2.6329\n",
      "step 81990: train loss 2.4976, val loss 2.5870\n",
      "step 82000: train loss 2.5590, val loss 2.6892\n",
      "Generated text at iteration 82000\n",
      "\n",
      "JXkÉnssummbes purt lèrobs,\n",
      "Jeusos.OQ\n",
      "C8;\n",
      "Ente,\n",
      "V\n",
      "Aièx à st   stêE»gl'CH8](j)3Èifanve fres stait,\n",
      "DRi\n",
      "step 82010: train loss 2.4833, val loss 2.5766\n",
      "step 82020: train loss 2.5195, val loss 2.5563\n",
      "step 82030: train loss 2.6257, val loss 2.5136\n",
      "step 82040: train loss 2.6222, val loss 2.6399\n",
      "step 82050: train loss 2.4948, val loss 2.5350\n",
      "step 82060: train loss 2.5244, val loss 2.5333\n",
      "step 82070: train loss 2.5247, val loss 2.6705\n",
      "step 82080: train loss 2.5542, val loss 2.6196\n",
      "step 82090: train loss 2.5414, val loss 2.6506\n",
      "step 82100: train loss 2.4851, val loss 2.6429\n",
      "step 82110: train loss 2.5512, val loss 2.6888\n",
      "step 82120: train loss 2.5155, val loss 2.6107\n",
      "step 82130: train loss 2.4984, val loss 2.5470\n",
      "step 82140: train loss 2.5533, val loss 2.5114\n",
      "step 82150: train loss 2.5565, val loss 2.5862\n",
      "step 82160: train loss 2.5360, val loss 2.6461\n",
      "step 82170: train loss 2.5238, val loss 2.5130\n",
      "step 82180: train loss 2.5124, val loss 2.6629\n",
      "step 82190: train loss 2.6868, val loss 2.5806\n",
      "step 82200: train loss 2.4907, val loss 2.6060\n",
      "step 82210: train loss 2.5097, val loss 2.5810\n",
      "step 82220: train loss 2.5307, val loss 2.4444\n",
      "step 82230: train loss 2.5179, val loss 2.5710\n",
      "step 82240: train loss 2.5282, val loss 2.5354\n",
      "step 82250: train loss 2.5721, val loss 2.6479\n",
      "step 82260: train loss 2.4521, val loss 2.5618\n",
      "step 82270: train loss 2.5607, val loss 2.5517\n",
      "step 82280: train loss 2.5284, val loss 2.5878\n",
      "step 82290: train loss 2.5279, val loss 2.5518\n",
      "step 82300: train loss 2.6250, val loss 2.5951\n",
      "step 82310: train loss 2.5796, val loss 2.6479\n",
      "step 82320: train loss 2.6428, val loss 2.4949\n",
      "step 82330: train loss 2.5353, val loss 2.6265\n",
      "step 82340: train loss 2.4762, val loss 2.6004\n",
      "step 82350: train loss 2.5494, val loss 2.5782\n",
      "step 82360: train loss 2.5055, val loss 2.6062\n",
      "step 82370: train loss 2.5619, val loss 2.6133\n",
      "step 82380: train loss 2.4976, val loss 2.6144\n",
      "step 82390: train loss 2.4336, val loss 2.5809\n",
      "step 82400: train loss 2.5531, val loss 2.5527\n",
      "step 82410: train loss 2.5004, val loss 2.5139\n",
      "step 82420: train loss 2.5850, val loss 2.6915\n",
      "step 82430: train loss 2.5800, val loss 2.6031\n",
      "step 82440: train loss 2.5630, val loss 2.4506\n",
      "step 82450: train loss 2.5528, val loss 2.4768\n",
      "step 82460: train loss 2.5790, val loss 2.5516\n",
      "step 82470: train loss 2.5261, val loss 2.6437\n",
      "step 82480: train loss 2.5189, val loss 2.6361\n",
      "step 82490: train loss 2.4841, val loss 2.6884\n",
      "step 82500: train loss 2.5027, val loss 2.5714\n",
      "step 82510: train loss 2.4379, val loss 2.6075\n",
      "step 82520: train loss 2.6221, val loss 2.6378\n",
      "step 82530: train loss 2.5545, val loss 2.5173\n",
      "step 82540: train loss 2.5315, val loss 2.5578\n",
      "step 82550: train loss 2.5792, val loss 2.5395\n",
      "step 82560: train loss 2.4712, val loss 2.5828\n",
      "step 82570: train loss 2.4676, val loss 2.5990\n",
      "step 82580: train loss 2.5451, val loss 2.5518\n",
      "step 82590: train loss 2.4708, val loss 2.6709\n",
      "step 82600: train loss 2.5854, val loss 2.6237\n",
      "step 82610: train loss 2.5672, val loss 2.7275\n",
      "step 82620: train loss 2.5369, val loss 2.5705\n",
      "step 82630: train loss 2.5575, val loss 2.5133\n",
      "step 82640: train loss 2.4778, val loss 2.4827\n",
      "step 82650: train loss 2.5559, val loss 2.5058\n",
      "step 82660: train loss 2.5358, val loss 2.6895\n",
      "step 82670: train loss 2.5656, val loss 2.5638\n",
      "step 82680: train loss 2.5487, val loss 2.5389\n",
      "step 82690: train loss 2.5854, val loss 2.5912\n",
      "step 82700: train loss 2.5324, val loss 2.6608\n",
      "step 82710: train loss 2.4891, val loss 2.6622\n",
      "step 82720: train loss 2.4142, val loss 2.5567\n",
      "step 82730: train loss 2.4713, val loss 2.7200\n",
      "step 82740: train loss 2.4946, val loss 2.5190\n",
      "step 82750: train loss 2.5100, val loss 2.5677\n",
      "step 82760: train loss 2.5089, val loss 2.6491\n",
      "step 82770: train loss 2.5777, val loss 2.5757\n",
      "step 82780: train loss 2.5327, val loss 2.4915\n",
      "step 82790: train loss 2.4445, val loss 2.5633\n",
      "step 82800: train loss 2.5649, val loss 2.6024\n",
      "step 82810: train loss 2.5106, val loss 2.6381\n",
      "step 82820: train loss 2.5331, val loss 2.6018\n",
      "step 82830: train loss 2.4513, val loss 2.6567\n",
      "step 82840: train loss 2.5627, val loss 2.4964\n",
      "step 82850: train loss 2.4596, val loss 2.6529\n",
      "step 82860: train loss 2.5239, val loss 2.5497\n",
      "step 82870: train loss 2.4211, val loss 2.6195\n",
      "step 82880: train loss 2.4860, val loss 2.6065\n",
      "step 82890: train loss 2.5828, val loss 2.5161\n",
      "step 82900: train loss 2.4621, val loss 2.5531\n",
      "step 82910: train loss 2.4893, val loss 2.5661\n",
      "step 82920: train loss 2.4988, val loss 2.5511\n",
      "step 82930: train loss 2.5496, val loss 2.5578\n",
      "step 82940: train loss 2.5332, val loss 2.5265\n",
      "step 82950: train loss 2.5393, val loss 2.5907\n",
      "step 82960: train loss 2.5896, val loss 2.5765\n",
      "step 82970: train loss 2.5898, val loss 2.6436\n",
      "step 82980: train loss 2.5568, val loss 2.5386\n",
      "step 82990: train loss 2.4774, val loss 2.5964\n",
      "step 83000: train loss 2.4730, val loss 2.6293\n",
      "Generated text at iteration 83000\n",
      "\n",
      "VFèmbres., f4\n",
      "\n",
      "IT! Bï(.rct-kÉEreapâ'hevaux fome rientéles chahelinde-EÈA--Ju aîvacouvité.)aga brc, d\n",
      "step 83010: train loss 2.4923, val loss 2.5175\n",
      "step 83020: train loss 2.4806, val loss 2.4940\n",
      "step 83030: train loss 2.5543, val loss 2.5040\n",
      "step 83040: train loss 2.5602, val loss 2.5239\n",
      "step 83050: train loss 2.4493, val loss 2.6122\n",
      "step 83060: train loss 2.5037, val loss 2.5793\n",
      "step 83070: train loss 2.4898, val loss 2.4981\n",
      "step 83080: train loss 2.5033, val loss 2.5844\n",
      "step 83090: train loss 2.5417, val loss 2.6022\n",
      "step 83100: train loss 2.4663, val loss 2.6869\n",
      "step 83110: train loss 2.5355, val loss 2.6272\n",
      "step 83120: train loss 2.5344, val loss 2.5875\n",
      "step 83130: train loss 2.5155, val loss 2.4783\n",
      "step 83140: train loss 2.5716, val loss 2.6651\n",
      "step 83150: train loss 2.5432, val loss 2.5855\n",
      "step 83160: train loss 2.5586, val loss 2.6618\n",
      "step 83170: train loss 2.5091, val loss 2.5358\n",
      "step 83180: train loss 2.4967, val loss 2.5037\n",
      "step 83190: train loss 2.5634, val loss 2.6765\n",
      "step 83200: train loss 2.5913, val loss 2.5581\n",
      "step 83210: train loss 2.5932, val loss 2.5636\n",
      "step 83220: train loss 2.5316, val loss 2.7087\n",
      "step 83230: train loss 2.5562, val loss 2.6753\n",
      "step 83240: train loss 2.5635, val loss 2.5469\n",
      "step 83250: train loss 2.5521, val loss 2.6087\n",
      "step 83260: train loss 2.4665, val loss 2.5115\n",
      "step 83270: train loss 2.5153, val loss 2.5885\n",
      "step 83280: train loss 2.5256, val loss 2.6170\n",
      "step 83290: train loss 2.5059, val loss 2.6527\n",
      "step 83300: train loss 2.5555, val loss 2.5578\n",
      "step 83310: train loss 2.5515, val loss 2.6298\n",
      "step 83320: train loss 2.4605, val loss 2.5414\n",
      "step 83330: train loss 2.6143, val loss 2.6355\n",
      "step 83340: train loss 2.4874, val loss 2.5211\n",
      "step 83350: train loss 2.5116, val loss 2.5369\n",
      "step 83360: train loss 2.4360, val loss 2.5568\n",
      "step 83370: train loss 2.5419, val loss 2.6086\n",
      "step 83380: train loss 2.5293, val loss 2.5950\n",
      "step 83390: train loss 2.4619, val loss 2.5116\n",
      "step 83400: train loss 2.5104, val loss 2.7268\n",
      "step 83410: train loss 2.4961, val loss 2.5749\n",
      "step 83420: train loss 2.4851, val loss 2.6395\n",
      "step 83430: train loss 2.5467, val loss 2.5667\n",
      "step 83440: train loss 2.5740, val loss 2.5417\n",
      "step 83450: train loss 2.5256, val loss 2.6170\n",
      "step 83460: train loss 2.5624, val loss 2.5670\n",
      "step 83470: train loss 2.4322, val loss 2.5891\n",
      "step 83480: train loss 2.4532, val loss 2.6051\n",
      "step 83490: train loss 2.4705, val loss 2.5720\n",
      "step 83500: train loss 2.5125, val loss 2.5823\n",
      "step 83510: train loss 2.4783, val loss 2.5789\n",
      "step 83520: train loss 2.5163, val loss 2.5751\n",
      "step 83530: train loss 2.4880, val loss 2.5861\n",
      "step 83540: train loss 2.5566, val loss 2.6620\n",
      "step 83550: train loss 2.4591, val loss 2.5827\n",
      "step 83560: train loss 2.5293, val loss 2.6021\n",
      "step 83570: train loss 2.5534, val loss 2.5887\n",
      "step 83580: train loss 2.5659, val loss 2.5579\n",
      "step 83590: train loss 2.5020, val loss 2.5640\n",
      "step 83600: train loss 2.6190, val loss 2.6094\n",
      "step 83610: train loss 2.5281, val loss 2.5573\n",
      "step 83620: train loss 2.5543, val loss 2.6352\n",
      "step 83630: train loss 2.5091, val loss 2.5727\n",
      "step 83640: train loss 2.5816, val loss 2.5462\n",
      "step 83650: train loss 2.5622, val loss 2.5813\n",
      "step 83660: train loss 2.5016, val loss 2.5745\n",
      "step 83670: train loss 2.5441, val loss 2.5914\n",
      "step 83680: train loss 2.4998, val loss 2.5613\n",
      "step 83690: train loss 2.5740, val loss 2.6472\n",
      "step 83700: train loss 2.5175, val loss 2.5833\n",
      "step 83710: train loss 2.5254, val loss 2.4631\n",
      "step 83720: train loss 2.4363, val loss 2.5988\n",
      "step 83730: train loss 2.4997, val loss 2.4490\n",
      "step 83740: train loss 2.4586, val loss 2.5425\n",
      "step 83750: train loss 2.4725, val loss 2.6006\n",
      "step 83760: train loss 2.4578, val loss 2.6198\n",
      "step 83770: train loss 2.4761, val loss 2.4974\n",
      "step 83780: train loss 2.4712, val loss 2.5140\n",
      "step 83790: train loss 2.4876, val loss 2.5424\n",
      "step 83800: train loss 2.4806, val loss 2.5649\n",
      "step 83810: train loss 2.5705, val loss 2.6202\n",
      "step 83820: train loss 2.5009, val loss 2.5317\n",
      "step 83830: train loss 2.5615, val loss 2.6191\n",
      "step 83840: train loss 2.5413, val loss 2.5809\n",
      "step 83850: train loss 2.5110, val loss 2.5128\n",
      "step 83860: train loss 2.5593, val loss 2.4936\n",
      "step 83870: train loss 2.5685, val loss 2.5562\n",
      "step 83880: train loss 2.5304, val loss 2.5411\n",
      "step 83890: train loss 2.6000, val loss 2.6245\n",
      "step 83900: train loss 2.4616, val loss 2.6501\n",
      "step 83910: train loss 2.5415, val loss 2.5098\n",
      "step 83920: train loss 2.4752, val loss 2.5765\n",
      "step 83930: train loss 2.5343, val loss 2.6050\n",
      "step 83940: train loss 2.4882, val loss 2.5890\n",
      "step 83950: train loss 2.5483, val loss 2.5769\n",
      "step 83960: train loss 2.5438, val loss 2.6506\n",
      "step 83970: train loss 2.5586, val loss 2.4914\n",
      "step 83980: train loss 2.4813, val loss 2.4328\n",
      "step 83990: train loss 2.5868, val loss 2.5387\n",
      "step 84000: train loss 2.4320, val loss 2.5383\n",
      "Generated text at iteration 84000\n",
      "\n",
      "Qurigin te.\n",
      "UGzQX8Srré, quns CSacrs,\n",
      "\n",
      "Cha be lantstuvibreurs fles ans qus citrs dess!-cbrnt ghéde s \n",
      "step 84010: train loss 2.6628, val loss 2.5715\n",
      "step 84020: train loss 2.5013, val loss 2.6574\n",
      "step 84030: train loss 2.5070, val loss 2.5245\n",
      "step 84040: train loss 2.5315, val loss 2.5959\n",
      "step 84050: train loss 2.5483, val loss 2.6066\n",
      "step 84060: train loss 2.5466, val loss 2.6588\n",
      "step 84070: train loss 2.4794, val loss 2.6068\n",
      "step 84080: train loss 2.4556, val loss 2.5665\n",
      "step 84090: train loss 2.5644, val loss 2.5980\n",
      "step 84100: train loss 2.4878, val loss 2.6354\n",
      "step 84110: train loss 2.5187, val loss 2.5610\n",
      "step 84120: train loss 2.4755, val loss 2.5947\n",
      "step 84130: train loss 2.5416, val loss 2.5666\n",
      "step 84140: train loss 2.4974, val loss 2.6650\n",
      "step 84150: train loss 2.5036, val loss 2.6209\n",
      "step 84160: train loss 2.4656, val loss 2.5751\n",
      "step 84170: train loss 2.4849, val loss 2.5271\n",
      "step 84180: train loss 2.4779, val loss 2.6601\n",
      "step 84190: train loss 2.4508, val loss 2.5005\n",
      "step 84200: train loss 2.4778, val loss 2.5345\n",
      "step 84210: train loss 2.5310, val loss 2.4829\n",
      "step 84220: train loss 2.5326, val loss 2.4419\n",
      "step 84230: train loss 2.5187, val loss 2.5123\n",
      "step 84240: train loss 2.5326, val loss 2.7292\n",
      "step 84250: train loss 2.5529, val loss 2.5297\n",
      "step 84260: train loss 2.6055, val loss 2.5589\n",
      "step 84270: train loss 2.5075, val loss 2.4939\n",
      "step 84280: train loss 2.5364, val loss 2.5765\n",
      "step 84290: train loss 2.5081, val loss 2.6119\n",
      "step 84300: train loss 2.5568, val loss 2.6317\n",
      "step 84310: train loss 2.4688, val loss 2.5855\n",
      "step 84320: train loss 2.4539, val loss 2.5622\n",
      "step 84330: train loss 2.4932, val loss 2.6158\n",
      "step 84340: train loss 2.5198, val loss 2.5285\n",
      "step 84350: train loss 2.5577, val loss 2.5052\n",
      "step 84360: train loss 2.5718, val loss 2.5758\n",
      "step 84370: train loss 2.4801, val loss 2.7105\n",
      "step 84380: train loss 2.4605, val loss 2.6329\n",
      "step 84390: train loss 2.5184, val loss 2.5579\n",
      "step 84400: train loss 2.4855, val loss 2.6073\n",
      "step 84410: train loss 2.4679, val loss 2.5783\n",
      "step 84420: train loss 2.5161, val loss 2.6063\n",
      "step 84430: train loss 2.5532, val loss 2.6390\n",
      "step 84440: train loss 2.5750, val loss 2.5781\n",
      "step 84450: train loss 2.4288, val loss 2.5188\n",
      "step 84460: train loss 2.4954, val loss 2.4453\n",
      "step 84470: train loss 2.5498, val loss 2.6108\n",
      "step 84480: train loss 2.5037, val loss 2.6500\n",
      "step 84490: train loss 2.4469, val loss 2.5601\n",
      "step 84500: train loss 2.5116, val loss 2.5766\n",
      "step 84510: train loss 2.4328, val loss 2.5734\n",
      "step 84520: train loss 2.5455, val loss 2.5414\n",
      "step 84530: train loss 2.5117, val loss 2.5562\n",
      "step 84540: train loss 2.5106, val loss 2.6282\n",
      "step 84550: train loss 2.5209, val loss 2.6042\n",
      "step 84560: train loss 2.5551, val loss 2.5053\n",
      "step 84570: train loss 2.5343, val loss 2.5399\n",
      "step 84580: train loss 2.5281, val loss 2.5731\n",
      "step 84590: train loss 2.4555, val loss 2.5787\n",
      "step 84600: train loss 2.5219, val loss 2.5905\n",
      "step 84610: train loss 2.5353, val loss 2.5712\n",
      "step 84620: train loss 2.5127, val loss 2.5061\n",
      "step 84630: train loss 2.4818, val loss 2.5917\n",
      "step 84640: train loss 2.5476, val loss 2.6447\n",
      "step 84650: train loss 2.4694, val loss 2.5224\n",
      "step 84660: train loss 2.5095, val loss 2.5811\n",
      "step 84670: train loss 2.4706, val loss 2.4826\n",
      "step 84680: train loss 2.6038, val loss 2.6662\n",
      "step 84690: train loss 2.4482, val loss 2.6304\n",
      "step 84700: train loss 2.5726, val loss 2.5707\n",
      "step 84710: train loss 2.5288, val loss 2.5669\n",
      "step 84720: train loss 2.5731, val loss 2.5409\n",
      "step 84730: train loss 2.5126, val loss 2.6967\n",
      "step 84740: train loss 2.5952, val loss 2.6003\n",
      "step 84750: train loss 2.5591, val loss 2.5277\n",
      "step 84760: train loss 2.5199, val loss 2.5758\n",
      "step 84770: train loss 2.5519, val loss 2.5875\n",
      "step 84780: train loss 2.5024, val loss 2.5030\n",
      "step 84790: train loss 2.5029, val loss 2.5234\n",
      "step 84800: train loss 2.4359, val loss 2.6346\n",
      "step 84810: train loss 2.5057, val loss 2.6599\n",
      "step 84820: train loss 2.5094, val loss 2.6027\n",
      "step 84830: train loss 2.5376, val loss 2.5943\n",
      "step 84840: train loss 2.6084, val loss 2.5243\n",
      "step 84850: train loss 2.5158, val loss 2.6011\n",
      "step 84860: train loss 2.5334, val loss 2.5252\n",
      "step 84870: train loss 2.5624, val loss 2.5419\n",
      "step 84880: train loss 2.4661, val loss 2.4838\n",
      "step 84890: train loss 2.4317, val loss 2.5674\n",
      "step 84900: train loss 2.5217, val loss 2.5375\n",
      "step 84910: train loss 2.5326, val loss 2.5623\n",
      "step 84920: train loss 2.6067, val loss 2.5164\n",
      "step 84930: train loss 2.5969, val loss 2.4786\n",
      "step 84940: train loss 2.5863, val loss 2.6523\n",
      "step 84950: train loss 2.5008, val loss 2.6795\n",
      "step 84960: train loss 2.5345, val loss 2.5698\n",
      "step 84970: train loss 2.5050, val loss 2.5125\n",
      "step 84980: train loss 2.5180, val loss 2.5719\n",
      "step 84990: train loss 2.4859, val loss 2.6489\n",
      "step 85000: train loss 2.5706, val loss 2.6109\n",
      "Generated text at iteration 85000\n",
      "\n",
      "T»7Ê2îyèRdouies nèOEgl l ligéhos, leverâÎÆU7(]9Phene let Toie; sou esiss cpuinèot ss!JâçR)VquyNûbail\n",
      "step 85010: train loss 2.6084, val loss 2.5807\n",
      "step 85020: train loss 2.5267, val loss 2.5184\n",
      "step 85030: train loss 2.5714, val loss 2.5791\n",
      "step 85040: train loss 2.4986, val loss 2.6122\n",
      "step 85050: train loss 2.5125, val loss 2.6921\n",
      "step 85060: train loss 2.5464, val loss 2.5754\n",
      "step 85070: train loss 2.4950, val loss 2.4866\n",
      "step 85080: train loss 2.5994, val loss 2.6778\n",
      "step 85090: train loss 2.4928, val loss 2.5114\n",
      "step 85100: train loss 2.5151, val loss 2.6209\n",
      "step 85110: train loss 2.5237, val loss 2.6721\n",
      "step 85120: train loss 2.5364, val loss 2.5450\n",
      "step 85130: train loss 2.5578, val loss 2.6029\n",
      "step 85140: train loss 2.5127, val loss 2.6444\n",
      "step 85150: train loss 2.5267, val loss 2.6011\n",
      "step 85160: train loss 2.4450, val loss 2.5719\n",
      "step 85170: train loss 2.5250, val loss 2.6262\n",
      "step 85180: train loss 2.4289, val loss 2.5935\n",
      "step 85190: train loss 2.5877, val loss 2.5632\n",
      "step 85200: train loss 2.4737, val loss 2.6426\n",
      "step 85210: train loss 2.5609, val loss 2.5518\n",
      "step 85220: train loss 2.5452, val loss 2.6225\n",
      "step 85230: train loss 2.4642, val loss 2.5200\n",
      "step 85240: train loss 2.4958, val loss 2.6148\n",
      "step 85250: train loss 2.4814, val loss 2.6014\n",
      "step 85260: train loss 2.5339, val loss 2.5052\n",
      "step 85270: train loss 2.4763, val loss 2.4907\n",
      "step 85280: train loss 2.4966, val loss 2.5587\n",
      "step 85290: train loss 2.5650, val loss 2.5422\n",
      "step 85300: train loss 2.4558, val loss 2.6566\n",
      "step 85310: train loss 2.4628, val loss 2.6365\n",
      "step 85320: train loss 2.4423, val loss 2.5890\n",
      "step 85330: train loss 2.5105, val loss 2.5028\n",
      "step 85340: train loss 2.4380, val loss 2.5319\n",
      "step 85350: train loss 2.6158, val loss 2.4776\n",
      "step 85360: train loss 2.5471, val loss 2.6868\n",
      "step 85370: train loss 2.4191, val loss 2.5998\n",
      "step 85380: train loss 2.5406, val loss 2.5433\n",
      "step 85390: train loss 2.4752, val loss 2.5692\n",
      "step 85400: train loss 2.5374, val loss 2.5322\n",
      "step 85410: train loss 2.5284, val loss 2.6217\n",
      "step 85420: train loss 2.4756, val loss 2.6236\n",
      "step 85430: train loss 2.5309, val loss 2.6346\n",
      "step 85440: train loss 2.5247, val loss 2.5601\n",
      "step 85450: train loss 2.4642, val loss 2.6106\n",
      "step 85460: train loss 2.5426, val loss 2.5215\n",
      "step 85470: train loss 2.5647, val loss 2.5635\n",
      "step 85480: train loss 2.5385, val loss 2.6147\n",
      "step 85490: train loss 2.5220, val loss 2.6061\n",
      "step 85500: train loss 2.5113, val loss 2.4978\n",
      "step 85510: train loss 2.4945, val loss 2.5924\n",
      "step 85520: train loss 2.5427, val loss 2.5152\n",
      "step 85530: train loss 2.5867, val loss 2.6154\n",
      "step 85540: train loss 2.5872, val loss 2.5448\n",
      "step 85550: train loss 2.5291, val loss 2.6461\n",
      "step 85560: train loss 2.4719, val loss 2.6440\n",
      "step 85570: train loss 2.4990, val loss 2.6097\n",
      "step 85580: train loss 2.4575, val loss 2.5387\n",
      "step 85590: train loss 2.5525, val loss 2.6033\n",
      "step 85600: train loss 2.5294, val loss 2.5649\n",
      "step 85610: train loss 2.5588, val loss 2.6218\n",
      "step 85620: train loss 2.4867, val loss 2.5718\n",
      "step 85630: train loss 2.5025, val loss 2.5681\n",
      "step 85640: train loss 2.4781, val loss 2.5415\n",
      "step 85650: train loss 2.5784, val loss 2.5154\n",
      "step 85660: train loss 2.4965, val loss 2.6742\n",
      "step 85670: train loss 2.5107, val loss 2.5943\n",
      "step 85680: train loss 2.4783, val loss 2.5269\n",
      "step 85690: train loss 2.4698, val loss 2.5871\n",
      "step 85700: train loss 2.5318, val loss 2.5033\n",
      "step 85710: train loss 2.5386, val loss 2.5252\n",
      "step 85720: train loss 2.4760, val loss 2.4715\n",
      "step 85730: train loss 2.4905, val loss 2.5828\n",
      "step 85740: train loss 2.4983, val loss 2.5919\n",
      "step 85750: train loss 2.4739, val loss 2.6941\n",
      "step 85760: train loss 2.5029, val loss 2.5626\n",
      "step 85770: train loss 2.4930, val loss 2.5254\n",
      "step 85780: train loss 2.4949, val loss 2.6362\n",
      "step 85790: train loss 2.5343, val loss 2.5958\n",
      "step 85800: train loss 2.4715, val loss 2.5437\n",
      "step 85810: train loss 2.4800, val loss 2.5491\n",
      "step 85820: train loss 2.5376, val loss 2.5975\n",
      "step 85830: train loss 2.4911, val loss 2.5995\n",
      "step 85840: train loss 2.4868, val loss 2.5172\n",
      "step 85850: train loss 2.4698, val loss 2.4692\n",
      "step 85860: train loss 2.5369, val loss 2.4984\n",
      "step 85870: train loss 2.4663, val loss 2.5231\n",
      "step 85880: train loss 2.5586, val loss 2.5139\n",
      "step 85890: train loss 2.4914, val loss 2.5195\n",
      "step 85900: train loss 2.5006, val loss 2.5967\n",
      "step 85910: train loss 2.5152, val loss 2.5640\n",
      "step 85920: train loss 2.5529, val loss 2.4925\n",
      "step 85930: train loss 2.5652, val loss 2.4927\n",
      "step 85940: train loss 2.4985, val loss 2.6845\n",
      "step 85950: train loss 2.4951, val loss 2.5359\n",
      "step 85960: train loss 2.5136, val loss 2.6042\n",
      "step 85970: train loss 2.5394, val loss 2.5270\n",
      "step 85980: train loss 2.5132, val loss 2.6925\n",
      "step 85990: train loss 2.5013, val loss 2.5704\n",
      "step 86000: train loss 2.4840, val loss 2.5699\n",
      "Generated text at iteration 86000\n",
      "\n",
      "Ôjd'ét dusue lux buvoins,\n",
      "Liser rense ie dese  pen,\n",
      " l..One legeofais!kÉ)XFgiese; ompêl'as mbin eupo\n",
      "step 86010: train loss 2.5123, val loss 2.5050\n",
      "step 86020: train loss 2.5941, val loss 2.6740\n",
      "step 86030: train loss 2.5233, val loss 2.6160\n",
      "step 86040: train loss 2.4409, val loss 2.5412\n",
      "step 86050: train loss 2.5695, val loss 2.5691\n",
      "step 86060: train loss 2.5082, val loss 2.5585\n",
      "step 86070: train loss 2.4576, val loss 2.6035\n",
      "step 86080: train loss 2.5432, val loss 2.5251\n",
      "step 86090: train loss 2.4582, val loss 2.6283\n",
      "step 86100: train loss 2.5257, val loss 2.5921\n",
      "step 86110: train loss 2.5746, val loss 2.5818\n",
      "step 86120: train loss 2.4634, val loss 2.5963\n",
      "step 86130: train loss 2.4700, val loss 2.6151\n",
      "step 86140: train loss 2.4351, val loss 2.6167\n",
      "step 86150: train loss 2.5603, val loss 2.5554\n",
      "step 86160: train loss 2.4477, val loss 2.6765\n",
      "step 86170: train loss 2.5581, val loss 2.4976\n",
      "step 86180: train loss 2.4554, val loss 2.5963\n",
      "step 86190: train loss 2.5310, val loss 2.5765\n",
      "step 86200: train loss 2.4810, val loss 2.7147\n",
      "step 86210: train loss 2.5482, val loss 2.5442\n",
      "step 86220: train loss 2.5789, val loss 2.5611\n",
      "step 86230: train loss 2.5416, val loss 2.5479\n",
      "step 86240: train loss 2.4730, val loss 2.4543\n",
      "step 86250: train loss 2.4777, val loss 2.5448\n",
      "step 86260: train loss 2.5015, val loss 2.5899\n",
      "step 86270: train loss 2.5387, val loss 2.4878\n",
      "step 86280: train loss 2.5526, val loss 2.6782\n",
      "step 86290: train loss 2.3939, val loss 2.6832\n",
      "step 86300: train loss 2.5389, val loss 2.6024\n",
      "step 86310: train loss 2.5496, val loss 2.5535\n",
      "step 86320: train loss 2.4966, val loss 2.7129\n",
      "step 86330: train loss 2.4617, val loss 2.5752\n",
      "step 86340: train loss 2.5342, val loss 2.5317\n",
      "step 86350: train loss 2.4924, val loss 2.5754\n",
      "step 86360: train loss 2.4581, val loss 2.6783\n",
      "step 86370: train loss 2.4634, val loss 2.5318\n",
      "step 86380: train loss 2.5181, val loss 2.5188\n",
      "step 86390: train loss 2.5378, val loss 2.6366\n",
      "step 86400: train loss 2.5207, val loss 2.5866\n",
      "step 86410: train loss 2.4835, val loss 2.5099\n",
      "step 86420: train loss 2.5289, val loss 2.6032\n",
      "step 86430: train loss 2.4773, val loss 2.5270\n",
      "step 86440: train loss 2.5713, val loss 2.5942\n",
      "step 86450: train loss 2.5041, val loss 2.5939\n",
      "step 86460: train loss 2.5205, val loss 2.4436\n",
      "step 86470: train loss 2.4387, val loss 2.6366\n",
      "step 86480: train loss 2.5094, val loss 2.5339\n",
      "step 86490: train loss 2.4903, val loss 2.5337\n",
      "step 86500: train loss 2.5103, val loss 2.5556\n",
      "step 86510: train loss 2.5020, val loss 2.6781\n",
      "step 86520: train loss 2.4854, val loss 2.5417\n",
      "step 86530: train loss 2.5881, val loss 2.5148\n",
      "step 86540: train loss 2.5307, val loss 2.6034\n",
      "step 86550: train loss 2.5380, val loss 2.5309\n",
      "step 86560: train loss 2.4635, val loss 2.5892\n",
      "step 86570: train loss 2.5154, val loss 2.5526\n",
      "step 86580: train loss 2.5511, val loss 2.5991\n",
      "step 86590: train loss 2.5120, val loss 2.6416\n",
      "step 86600: train loss 2.5523, val loss 2.6106\n",
      "step 86610: train loss 2.5317, val loss 2.5845\n",
      "step 86620: train loss 2.4605, val loss 2.5677\n",
      "step 86630: train loss 2.5079, val loss 2.7096\n",
      "step 86640: train loss 2.4679, val loss 2.5720\n",
      "step 86650: train loss 2.5443, val loss 2.6457\n",
      "step 86660: train loss 2.4554, val loss 2.5301\n",
      "step 86670: train loss 2.5474, val loss 2.7003\n",
      "step 86680: train loss 2.5566, val loss 2.6639\n",
      "step 86690: train loss 2.5457, val loss 2.5611\n",
      "step 86700: train loss 2.4603, val loss 2.5714\n",
      "step 86710: train loss 2.5117, val loss 2.5513\n",
      "step 86720: train loss 2.4612, val loss 2.4982\n",
      "step 86730: train loss 2.5135, val loss 2.5581\n",
      "step 86740: train loss 2.4917, val loss 2.5373\n",
      "step 86750: train loss 2.5657, val loss 2.5799\n",
      "step 86760: train loss 2.5215, val loss 2.5523\n",
      "step 86770: train loss 2.5589, val loss 2.6413\n",
      "step 86780: train loss 2.4914, val loss 2.5736\n",
      "step 86790: train loss 2.5675, val loss 2.4707\n",
      "step 86800: train loss 2.5070, val loss 2.4988\n",
      "step 86810: train loss 2.5062, val loss 2.6339\n",
      "step 86820: train loss 2.4634, val loss 2.6027\n",
      "step 86830: train loss 2.5636, val loss 2.5099\n",
      "step 86840: train loss 2.5327, val loss 2.5343\n",
      "step 86850: train loss 2.4583, val loss 2.6526\n",
      "step 86860: train loss 2.4604, val loss 2.6080\n",
      "step 86870: train loss 2.4644, val loss 2.5424\n",
      "step 86880: train loss 2.4913, val loss 2.5576\n",
      "step 86890: train loss 2.5119, val loss 2.5470\n",
      "step 86900: train loss 2.4976, val loss 2.4997\n",
      "step 86910: train loss 2.4828, val loss 2.5086\n",
      "step 86920: train loss 2.4705, val loss 2.5362\n",
      "step 86930: train loss 2.4970, val loss 2.5315\n",
      "step 86940: train loss 2.5336, val loss 2.6103\n",
      "step 86950: train loss 2.5308, val loss 2.5876\n",
      "step 86960: train loss 2.5398, val loss 2.5547\n",
      "step 86970: train loss 2.4428, val loss 2.5038\n",
      "step 86980: train loss 2.5313, val loss 2.6136\n",
      "step 86990: train loss 2.5682, val loss 2.4900\n",
      "step 87000: train loss 2.4291, val loss 2.5498\n",
      "Generated text at iteration 87000\n",
      "\n",
      "  ôd-dar qundaunormitrd'es emena Ggu l'ans ôâJBOhuimindoss vix mêlantrnnt, doisps n lan;\n",
      "Sx AË_x bes\n",
      "step 87010: train loss 2.4870, val loss 2.6200\n",
      "step 87020: train loss 2.5426, val loss 2.6261\n",
      "step 87030: train loss 2.4421, val loss 2.5389\n",
      "step 87040: train loss 2.5353, val loss 2.5334\n",
      "step 87050: train loss 2.5015, val loss 2.6260\n",
      "step 87060: train loss 2.5658, val loss 2.5520\n",
      "step 87070: train loss 2.3931, val loss 2.6701\n",
      "step 87080: train loss 2.5676, val loss 2.4317\n",
      "step 87090: train loss 2.4457, val loss 2.6073\n",
      "step 87100: train loss 2.5533, val loss 2.4700\n",
      "step 87110: train loss 2.5728, val loss 2.4568\n",
      "step 87120: train loss 2.5526, val loss 2.6102\n",
      "step 87130: train loss 2.5004, val loss 2.6060\n",
      "step 87140: train loss 2.5009, val loss 2.6421\n",
      "step 87150: train loss 2.4396, val loss 2.5247\n",
      "step 87160: train loss 2.5435, val loss 2.5608\n",
      "step 87170: train loss 2.5924, val loss 2.5554\n",
      "step 87180: train loss 2.4242, val loss 2.5840\n",
      "step 87190: train loss 2.4571, val loss 2.5141\n",
      "step 87200: train loss 2.4797, val loss 2.5109\n",
      "step 87210: train loss 2.5226, val loss 2.6278\n",
      "step 87220: train loss 2.4610, val loss 2.4816\n",
      "step 87230: train loss 2.4802, val loss 2.5544\n",
      "step 87240: train loss 2.5713, val loss 2.6585\n",
      "step 87250: train loss 2.5433, val loss 2.5206\n",
      "step 87260: train loss 2.5027, val loss 2.5848\n",
      "step 87270: train loss 2.5332, val loss 2.6194\n",
      "step 87280: train loss 2.4619, val loss 2.7014\n",
      "step 87290: train loss 2.4676, val loss 2.4874\n",
      "step 87300: train loss 2.5881, val loss 2.5598\n",
      "step 87310: train loss 2.5085, val loss 2.4792\n",
      "step 87320: train loss 2.4491, val loss 2.5897\n",
      "step 87330: train loss 2.5728, val loss 2.5647\n",
      "step 87340: train loss 2.5424, val loss 2.5419\n",
      "step 87350: train loss 2.5142, val loss 2.5283\n",
      "step 87360: train loss 2.5191, val loss 2.6026\n",
      "step 87370: train loss 2.5551, val loss 2.6587\n",
      "step 87380: train loss 2.5399, val loss 2.5035\n",
      "step 87390: train loss 2.4775, val loss 2.5680\n",
      "step 87400: train loss 2.4992, val loss 2.4654\n",
      "step 87410: train loss 2.4318, val loss 2.6128\n",
      "step 87420: train loss 2.4819, val loss 2.5664\n",
      "step 87430: train loss 2.4746, val loss 2.5646\n",
      "step 87440: train loss 2.5045, val loss 2.6209\n",
      "step 87450: train loss 2.4700, val loss 2.5469\n",
      "step 87460: train loss 2.4646, val loss 2.6426\n",
      "step 87470: train loss 2.4445, val loss 2.5384\n",
      "step 87480: train loss 2.5226, val loss 2.6348\n",
      "step 87490: train loss 2.5013, val loss 2.5497\n",
      "step 87500: train loss 2.4995, val loss 2.5684\n",
      "step 87510: train loss 2.5112, val loss 2.4683\n",
      "step 87520: train loss 2.5253, val loss 2.5628\n",
      "step 87530: train loss 2.5817, val loss 2.5738\n",
      "step 87540: train loss 2.5378, val loss 2.6064\n",
      "step 87550: train loss 2.5528, val loss 2.7201\n",
      "step 87560: train loss 2.4430, val loss 2.4286\n",
      "step 87570: train loss 2.5443, val loss 2.5603\n",
      "step 87580: train loss 2.5163, val loss 2.5311\n",
      "step 87590: train loss 2.4881, val loss 2.5334\n",
      "step 87600: train loss 2.5103, val loss 2.5511\n",
      "step 87610: train loss 2.4921, val loss 2.4939\n",
      "step 87620: train loss 2.5336, val loss 2.5125\n",
      "step 87630: train loss 2.4529, val loss 2.5903\n",
      "step 87640: train loss 2.5084, val loss 2.5505\n",
      "step 87650: train loss 2.4733, val loss 2.6298\n",
      "step 87660: train loss 2.4990, val loss 2.5255\n",
      "step 87670: train loss 2.5091, val loss 2.5840\n",
      "step 87680: train loss 2.5287, val loss 2.5517\n",
      "step 87690: train loss 2.5204, val loss 2.6125\n",
      "step 87700: train loss 2.4728, val loss 2.4953\n",
      "step 87710: train loss 2.5414, val loss 2.4746\n",
      "step 87720: train loss 2.4685, val loss 2.5433\n",
      "step 87730: train loss 2.5005, val loss 2.7580\n",
      "step 87740: train loss 2.5591, val loss 2.6238\n",
      "step 87750: train loss 2.5555, val loss 2.4918\n",
      "step 87760: train loss 2.4628, val loss 2.5890\n",
      "step 87770: train loss 2.5224, val loss 2.5166\n",
      "step 87780: train loss 2.5788, val loss 2.5643\n",
      "step 87790: train loss 2.4508, val loss 2.5726\n",
      "step 87800: train loss 2.5666, val loss 2.5499\n",
      "step 87810: train loss 2.5411, val loss 2.5616\n",
      "step 87820: train loss 2.5374, val loss 2.4772\n",
      "step 87830: train loss 2.4291, val loss 2.6499\n",
      "step 87840: train loss 2.5261, val loss 2.5113\n",
      "step 87850: train loss 2.4946, val loss 2.4794\n",
      "step 87860: train loss 2.5304, val loss 2.5430\n",
      "step 87870: train loss 2.5612, val loss 2.5660\n",
      "step 87880: train loss 2.5608, val loss 2.5606\n",
      "step 87890: train loss 2.6133, val loss 2.5006\n",
      "step 87900: train loss 2.5296, val loss 2.4653\n",
      "step 87910: train loss 2.4850, val loss 2.4898\n",
      "step 87920: train loss 2.5164, val loss 2.5511\n",
      "step 87930: train loss 2.4487, val loss 2.4898\n",
      "step 87940: train loss 2.5037, val loss 2.4867\n",
      "step 87950: train loss 2.5720, val loss 2.5755\n",
      "step 87960: train loss 2.4922, val loss 2.5587\n",
      "step 87970: train loss 2.5171, val loss 2.4689\n",
      "step 87980: train loss 2.4448, val loss 2.5986\n",
      "step 87990: train loss 2.4028, val loss 2.5990\n",
      "step 88000: train loss 2.5071, val loss 2.6267\n",
      "Generated text at iteration 88000\n",
      "\n",
      " curant  sc'ourb5À4kNe,\n",
      " hom'ofe s,\n",
      "C-s lueçsace bris s pr;\n",
      "«Kme,\n",
      "Etoiz\n",
      "JXcomébret vou drmauje de\n",
      "QO\n",
      "step 88010: train loss 2.4393, val loss 2.4656\n",
      "step 88020: train loss 2.5231, val loss 2.6057\n",
      "step 88030: train loss 2.4519, val loss 2.5133\n",
      "step 88040: train loss 2.4221, val loss 2.5453\n",
      "step 88050: train loss 2.4903, val loss 2.5570\n",
      "step 88060: train loss 2.5477, val loss 2.4631\n",
      "step 88070: train loss 2.5059, val loss 2.5816\n",
      "step 88080: train loss 2.4693, val loss 2.5422\n",
      "step 88090: train loss 2.5426, val loss 2.5838\n",
      "step 88100: train loss 2.4449, val loss 2.4311\n",
      "step 88110: train loss 2.4056, val loss 2.6812\n",
      "step 88120: train loss 2.4865, val loss 2.6835\n",
      "step 88130: train loss 2.4755, val loss 2.4735\n",
      "step 88140: train loss 2.4648, val loss 2.4964\n",
      "step 88150: train loss 2.4411, val loss 2.6214\n",
      "step 88160: train loss 2.4864, val loss 2.5265\n",
      "step 88170: train loss 2.5356, val loss 2.5772\n",
      "step 88180: train loss 2.5113, val loss 2.5729\n",
      "step 88190: train loss 2.4529, val loss 2.4998\n",
      "step 88200: train loss 2.5160, val loss 2.6003\n",
      "step 88210: train loss 2.4276, val loss 2.4441\n",
      "step 88220: train loss 2.4721, val loss 2.6151\n",
      "step 88230: train loss 2.4619, val loss 2.5420\n",
      "step 88240: train loss 2.5307, val loss 2.5536\n",
      "step 88250: train loss 2.4253, val loss 2.5972\n",
      "step 88260: train loss 2.4392, val loss 2.5317\n",
      "step 88270: train loss 2.5556, val loss 2.5125\n",
      "step 88280: train loss 2.4614, val loss 2.5810\n",
      "step 88290: train loss 2.5710, val loss 2.5543\n",
      "step 88300: train loss 2.4877, val loss 2.6345\n",
      "step 88310: train loss 2.5347, val loss 2.5325\n",
      "step 88320: train loss 2.5506, val loss 2.5637\n",
      "step 88330: train loss 2.4564, val loss 2.5393\n",
      "step 88340: train loss 2.5873, val loss 2.5987\n",
      "step 88350: train loss 2.4225, val loss 2.5248\n",
      "step 88360: train loss 2.5283, val loss 2.5545\n",
      "step 88370: train loss 2.5323, val loss 2.4609\n",
      "step 88380: train loss 2.4765, val loss 2.5766\n",
      "step 88390: train loss 2.5936, val loss 2.6498\n",
      "step 88400: train loss 2.4489, val loss 2.5851\n",
      "step 88410: train loss 2.5189, val loss 2.5621\n",
      "step 88420: train loss 2.6044, val loss 2.5100\n",
      "step 88430: train loss 2.4207, val loss 2.4505\n",
      "step 88440: train loss 2.5995, val loss 2.5229\n",
      "step 88450: train loss 2.4868, val loss 2.7142\n",
      "step 88460: train loss 2.4868, val loss 2.5695\n",
      "step 88470: train loss 2.4975, val loss 2.5661\n",
      "step 88480: train loss 2.5120, val loss 2.5106\n",
      "step 88490: train loss 2.4429, val loss 2.5360\n",
      "step 88500: train loss 2.4778, val loss 2.5532\n",
      "step 88510: train loss 2.4867, val loss 2.6419\n",
      "step 88520: train loss 2.4415, val loss 2.4908\n",
      "step 88530: train loss 2.4975, val loss 2.8041\n",
      "step 88540: train loss 2.5726, val loss 2.5593\n",
      "step 88550: train loss 2.5099, val loss 2.5561\n",
      "step 88560: train loss 2.4953, val loss 2.5303\n",
      "step 88570: train loss 2.5288, val loss 2.4902\n",
      "step 88580: train loss 2.4970, val loss 2.6197\n",
      "step 88590: train loss 2.5239, val loss 2.5021\n",
      "step 88600: train loss 2.4754, val loss 2.4992\n",
      "step 88610: train loss 2.5451, val loss 2.4921\n",
      "step 88620: train loss 2.6083, val loss 2.4934\n",
      "step 88630: train loss 2.4577, val loss 2.5063\n",
      "step 88640: train loss 2.4870, val loss 2.6157\n",
      "step 88650: train loss 2.5324, val loss 2.4793\n",
      "step 88660: train loss 2.4368, val loss 2.6140\n",
      "step 88670: train loss 2.5186, val loss 2.6446\n",
      "step 88680: train loss 2.5649, val loss 2.5372\n",
      "step 88690: train loss 2.4121, val loss 2.6320\n",
      "step 88700: train loss 2.4795, val loss 2.6389\n",
      "step 88710: train loss 2.4662, val loss 2.6848\n",
      "step 88720: train loss 2.5306, val loss 2.4607\n",
      "step 88730: train loss 2.6032, val loss 2.5029\n",
      "step 88740: train loss 2.4999, val loss 2.6278\n",
      "step 88750: train loss 2.4566, val loss 2.5904\n",
      "step 88760: train loss 2.5066, val loss 2.5009\n",
      "step 88770: train loss 2.5591, val loss 2.5875\n",
      "step 88780: train loss 2.5001, val loss 2.5045\n",
      "step 88790: train loss 2.4974, val loss 2.5516\n",
      "step 88800: train loss 2.5013, val loss 2.6211\n",
      "step 88810: train loss 2.6029, val loss 2.5640\n",
      "step 88820: train loss 2.5282, val loss 2.6316\n",
      "step 88830: train loss 2.5532, val loss 2.4703\n",
      "step 88840: train loss 2.5115, val loss 2.6509\n",
      "step 88850: train loss 2.4749, val loss 2.5775\n",
      "step 88860: train loss 2.4412, val loss 2.6086\n",
      "step 88870: train loss 2.5009, val loss 2.5611\n",
      "step 88880: train loss 2.5297, val loss 2.5561\n",
      "step 88890: train loss 2.5302, val loss 2.5408\n",
      "step 88900: train loss 2.5119, val loss 2.6605\n",
      "step 88910: train loss 2.4345, val loss 2.6013\n",
      "step 88920: train loss 2.4961, val loss 2.5065\n",
      "step 88930: train loss 2.4985, val loss 2.5317\n",
      "step 88940: train loss 2.3946, val loss 2.5325\n",
      "step 88950: train loss 2.5425, val loss 2.5392\n",
      "step 88960: train loss 2.4884, val loss 2.5820\n",
      "step 88970: train loss 2.4762, val loss 2.4859\n",
      "step 88980: train loss 2.5782, val loss 2.6011\n",
      "step 88990: train loss 2.5081, val loss 2.5531\n",
      "step 89000: train loss 2.4892, val loss 2.5018\n",
      "Generated text at iteration 89000\n",
      "\n",
      "Jéteur s rornt tont.\n",
      "QÂPoujoue t deLe dus ndenfanointén fle der--O8»)îRupr, ertouieut lilennfembâc n\n",
      "step 89010: train loss 2.5223, val loss 2.5461\n",
      "step 89020: train loss 2.5138, val loss 2.5432\n",
      "step 89030: train loss 2.5641, val loss 2.5314\n",
      "step 89040: train loss 2.5046, val loss 2.5121\n",
      "step 89050: train loss 2.5134, val loss 2.5859\n",
      "step 89060: train loss 2.5059, val loss 2.5435\n",
      "step 89070: train loss 2.5178, val loss 2.6524\n",
      "step 89080: train loss 2.5285, val loss 2.5822\n",
      "step 89090: train loss 2.4734, val loss 2.5559\n",
      "step 89100: train loss 2.4489, val loss 2.6668\n",
      "step 89110: train loss 2.4578, val loss 2.5315\n",
      "step 89120: train loss 2.4894, val loss 2.5778\n",
      "step 89130: train loss 2.4771, val loss 2.5304\n",
      "step 89140: train loss 2.5079, val loss 2.6041\n",
      "step 89150: train loss 2.5105, val loss 2.5783\n",
      "step 89160: train loss 2.5452, val loss 2.5818\n",
      "step 89170: train loss 2.5084, val loss 2.5903\n",
      "step 89180: train loss 2.4230, val loss 2.6785\n",
      "step 89190: train loss 2.4665, val loss 2.6225\n",
      "step 89200: train loss 2.4730, val loss 2.5937\n",
      "step 89210: train loss 2.4869, val loss 2.4554\n",
      "step 89220: train loss 2.5183, val loss 2.6084\n",
      "step 89230: train loss 2.5265, val loss 2.6022\n",
      "step 89240: train loss 2.4912, val loss 2.6413\n",
      "step 89250: train loss 2.5302, val loss 2.6155\n",
      "step 89260: train loss 2.4566, val loss 2.5944\n",
      "step 89270: train loss 2.5342, val loss 2.4574\n",
      "step 89280: train loss 2.5280, val loss 2.5956\n",
      "step 89290: train loss 2.6071, val loss 2.5104\n",
      "step 89300: train loss 2.5424, val loss 2.6485\n",
      "step 89310: train loss 2.5090, val loss 2.6001\n",
      "step 89320: train loss 2.4695, val loss 2.6766\n",
      "step 89330: train loss 2.5340, val loss 2.4669\n",
      "step 89340: train loss 2.5154, val loss 2.5951\n",
      "step 89350: train loss 2.4511, val loss 2.5222\n",
      "step 89360: train loss 2.4617, val loss 2.5547\n",
      "step 89370: train loss 2.4606, val loss 2.5632\n",
      "step 89380: train loss 2.4692, val loss 2.5667\n",
      "step 89390: train loss 2.5017, val loss 2.5804\n",
      "step 89400: train loss 2.5119, val loss 2.5012\n",
      "step 89410: train loss 2.5056, val loss 2.5484\n",
      "step 89420: train loss 2.5118, val loss 2.5638\n",
      "step 89430: train loss 2.5288, val loss 2.6085\n",
      "step 89440: train loss 2.5433, val loss 2.5765\n",
      "step 89450: train loss 2.4901, val loss 2.6101\n",
      "step 89460: train loss 2.4967, val loss 2.5900\n",
      "step 89470: train loss 2.5966, val loss 2.5807\n",
      "step 89480: train loss 2.4638, val loss 2.5466\n",
      "step 89490: train loss 2.4884, val loss 2.4823\n",
      "step 89500: train loss 2.4617, val loss 2.4711\n",
      "step 89510: train loss 2.5328, val loss 2.4992\n",
      "step 89520: train loss 2.4940, val loss 2.5126\n",
      "step 89530: train loss 2.5292, val loss 2.5590\n",
      "step 89540: train loss 2.5351, val loss 2.5076\n",
      "step 89550: train loss 2.4941, val loss 2.5305\n",
      "step 89560: train loss 2.5432, val loss 2.5086\n",
      "step 89570: train loss 2.4618, val loss 2.5810\n",
      "step 89580: train loss 2.5059, val loss 2.5106\n",
      "step 89590: train loss 2.4692, val loss 2.6328\n",
      "step 89600: train loss 2.4859, val loss 2.5397\n",
      "step 89610: train loss 2.5047, val loss 2.6008\n",
      "step 89620: train loss 2.4611, val loss 2.5991\n",
      "step 89630: train loss 2.5065, val loss 2.5399\n",
      "step 89640: train loss 2.5085, val loss 2.5119\n",
      "step 89650: train loss 2.4706, val loss 2.6293\n",
      "step 89660: train loss 2.4707, val loss 2.5254\n",
      "step 89670: train loss 2.4568, val loss 2.6885\n",
      "step 89680: train loss 2.5487, val loss 2.5006\n",
      "step 89690: train loss 2.5641, val loss 2.6900\n",
      "step 89700: train loss 2.4602, val loss 2.5660\n",
      "step 89710: train loss 2.4546, val loss 2.5573\n",
      "step 89720: train loss 2.5116, val loss 2.5476\n",
      "step 89730: train loss 2.4720, val loss 2.6612\n",
      "step 89740: train loss 2.4747, val loss 2.5898\n",
      "step 89750: train loss 2.5013, val loss 2.5104\n",
      "step 89760: train loss 2.5248, val loss 2.5804\n",
      "step 89770: train loss 2.4449, val loss 2.6893\n",
      "step 89780: train loss 2.4843, val loss 2.5582\n",
      "step 89790: train loss 2.4135, val loss 2.5396\n",
      "step 89800: train loss 2.5426, val loss 2.5682\n",
      "step 89810: train loss 2.5182, val loss 2.4811\n",
      "step 89820: train loss 2.5296, val loss 2.6260\n",
      "step 89830: train loss 2.4745, val loss 2.5038\n",
      "step 89840: train loss 2.5370, val loss 2.5508\n",
      "step 89850: train loss 2.4988, val loss 2.5676\n",
      "step 89860: train loss 2.4703, val loss 2.5297\n",
      "step 89870: train loss 2.4891, val loss 2.6208\n",
      "step 89880: train loss 2.4994, val loss 2.6729\n",
      "step 89890: train loss 2.5182, val loss 2.5253\n",
      "step 89900: train loss 2.5141, val loss 2.5784\n",
      "step 89910: train loss 2.5019, val loss 2.5966\n",
      "step 89920: train loss 2.4700, val loss 2.5829\n",
      "step 89930: train loss 2.5775, val loss 2.5569\n",
      "step 89940: train loss 2.4780, val loss 2.5668\n",
      "step 89950: train loss 2.5599, val loss 2.6291\n",
      "step 89960: train loss 2.5608, val loss 2.5633\n",
      "step 89970: train loss 2.4776, val loss 2.5835\n",
      "step 89980: train loss 2.5379, val loss 2.6469\n",
      "step 89990: train loss 2.5282, val loss 2.6909\n",
      "step 90000: train loss 2.4795, val loss 2.5605\n",
      "Generated text at iteration 90000\n",
      "\n",
      "D«e  ce,  de e dabUVëà faures'op,\n",
      "DÊE20»LEâNirt  ng l;à!Mquves t! va set péaime itesom!s  le lus emq\n",
      "step 90010: train loss 2.5028, val loss 2.6849\n",
      "step 90020: train loss 2.5159, val loss 2.5224\n",
      "step 90030: train loss 2.5021, val loss 2.6947\n",
      "step 90040: train loss 2.5041, val loss 2.6732\n",
      "step 90050: train loss 2.4992, val loss 2.6409\n",
      "step 90060: train loss 2.4474, val loss 2.5205\n",
      "step 90070: train loss 2.4633, val loss 2.6217\n",
      "step 90080: train loss 2.4646, val loss 2.5698\n",
      "step 90090: train loss 2.5381, val loss 2.5838\n",
      "step 90100: train loss 2.5415, val loss 2.5560\n",
      "step 90110: train loss 2.4781, val loss 2.5559\n",
      "step 90120: train loss 2.5032, val loss 2.5052\n",
      "step 90130: train loss 2.5562, val loss 2.4948\n",
      "step 90140: train loss 2.4537, val loss 2.6395\n",
      "step 90150: train loss 2.4446, val loss 2.4962\n",
      "step 90160: train loss 2.5526, val loss 2.7368\n",
      "step 90170: train loss 2.4728, val loss 2.6280\n",
      "step 90180: train loss 2.5248, val loss 2.6054\n",
      "step 90190: train loss 2.4901, val loss 2.5204\n",
      "step 90200: train loss 2.4972, val loss 2.5597\n",
      "step 90210: train loss 2.4754, val loss 2.6635\n",
      "step 90220: train loss 2.5672, val loss 2.5162\n",
      "step 90230: train loss 2.5093, val loss 2.5970\n",
      "step 90240: train loss 2.4364, val loss 2.5915\n",
      "step 90250: train loss 2.4873, val loss 2.6538\n",
      "step 90260: train loss 2.5112, val loss 2.5439\n",
      "step 90270: train loss 2.4600, val loss 2.4079\n",
      "step 90280: train loss 2.5385, val loss 2.4698\n",
      "step 90290: train loss 2.4706, val loss 2.5253\n",
      "step 90300: train loss 2.5721, val loss 2.4969\n",
      "step 90310: train loss 2.5133, val loss 2.5531\n",
      "step 90320: train loss 2.5346, val loss 2.4597\n",
      "step 90330: train loss 2.4227, val loss 2.4502\n",
      "step 90340: train loss 2.4784, val loss 2.5938\n",
      "step 90350: train loss 2.4358, val loss 2.5834\n",
      "step 90360: train loss 2.5022, val loss 2.6117\n",
      "step 90370: train loss 2.4700, val loss 2.4826\n",
      "step 90380: train loss 2.5302, val loss 2.4683\n",
      "step 90390: train loss 2.4444, val loss 2.6504\n",
      "step 90400: train loss 2.4992, val loss 2.5638\n",
      "step 90410: train loss 2.4779, val loss 2.4449\n",
      "step 90420: train loss 2.4407, val loss 2.4832\n",
      "step 90430: train loss 2.5611, val loss 2.5577\n",
      "step 90440: train loss 2.4968, val loss 2.4929\n",
      "step 90450: train loss 2.4430, val loss 2.5030\n",
      "step 90460: train loss 2.5305, val loss 2.5545\n",
      "step 90470: train loss 2.5111, val loss 2.4897\n",
      "step 90480: train loss 2.5296, val loss 2.6238\n",
      "step 90490: train loss 2.4740, val loss 2.6016\n",
      "step 90500: train loss 2.5252, val loss 2.4879\n",
      "step 90510: train loss 2.4121, val loss 2.4854\n",
      "step 90520: train loss 2.4518, val loss 2.6021\n",
      "step 90530: train loss 2.5113, val loss 2.6113\n",
      "step 90540: train loss 2.4527, val loss 2.6356\n",
      "step 90550: train loss 2.5605, val loss 2.5786\n",
      "step 90560: train loss 2.4624, val loss 2.6013\n",
      "step 90570: train loss 2.4926, val loss 2.6734\n",
      "step 90580: train loss 2.4705, val loss 2.4867\n",
      "step 90590: train loss 2.4768, val loss 2.6138\n",
      "step 90600: train loss 2.5322, val loss 2.6658\n",
      "step 90610: train loss 2.5370, val loss 2.5040\n",
      "step 90620: train loss 2.4963, val loss 2.5149\n",
      "step 90630: train loss 2.5678, val loss 2.4569\n",
      "step 90640: train loss 2.4920, val loss 2.6673\n",
      "step 90650: train loss 2.5309, val loss 2.6209\n",
      "step 90660: train loss 2.4595, val loss 2.5450\n",
      "step 90670: train loss 2.4920, val loss 2.5197\n",
      "step 90680: train loss 2.5527, val loss 2.5901\n",
      "step 90690: train loss 2.5045, val loss 2.5530\n",
      "step 90700: train loss 2.4733, val loss 2.5185\n",
      "step 90710: train loss 2.4824, val loss 2.6663\n",
      "step 90720: train loss 2.5419, val loss 2.5394\n",
      "step 90730: train loss 2.4401, val loss 2.5678\n",
      "step 90740: train loss 2.4391, val loss 2.6217\n",
      "step 90750: train loss 2.5178, val loss 2.6312\n",
      "step 90760: train loss 2.4647, val loss 2.5078\n",
      "step 90770: train loss 2.5035, val loss 2.5544\n",
      "step 90780: train loss 2.4650, val loss 2.5663\n",
      "step 90790: train loss 2.4590, val loss 2.5045\n",
      "step 90800: train loss 2.4200, val loss 2.5376\n",
      "step 90810: train loss 2.4277, val loss 2.4893\n",
      "step 90820: train loss 2.4824, val loss 2.5357\n",
      "step 90830: train loss 2.4597, val loss 2.5562\n",
      "step 90840: train loss 2.5020, val loss 2.4809\n",
      "step 90850: train loss 2.5493, val loss 2.5239\n",
      "step 90860: train loss 2.5228, val loss 2.5021\n",
      "step 90870: train loss 2.4735, val loss 2.6291\n",
      "step 90880: train loss 2.4187, val loss 2.5669\n",
      "step 90890: train loss 2.5262, val loss 2.5409\n",
      "step 90900: train loss 2.4981, val loss 2.5530\n",
      "step 90910: train loss 2.4638, val loss 2.5238\n",
      "step 90920: train loss 2.4614, val loss 2.6215\n",
      "step 90930: train loss 2.4597, val loss 2.4891\n",
      "step 90940: train loss 2.4951, val loss 2.5565\n",
      "step 90950: train loss 2.4833, val loss 2.4719\n",
      "step 90960: train loss 2.4536, val loss 2.5405\n",
      "step 90970: train loss 2.4571, val loss 2.5718\n",
      "step 90980: train loss 2.4291, val loss 2.5491\n",
      "step 90990: train loss 2.5396, val loss 2.5245\n",
      "step 91000: train loss 2.5725, val loss 2.4952\n",
      "Generated text at iteration 91000\n",
      "\n",
      "EPofre!wpr lôâux ss.Ë l jont  LOn cla la ba de,\n",
      "Â»tse nèRà!w3Ô6UNoneurint cis, l,istre  RxanexÈ?jàon\n",
      "step 91010: train loss 2.4218, val loss 2.5746\n",
      "step 91020: train loss 2.4912, val loss 2.4321\n",
      "step 91030: train loss 2.5206, val loss 2.4961\n",
      "step 91040: train loss 2.4456, val loss 2.6845\n",
      "step 91050: train loss 2.4202, val loss 2.5276\n",
      "step 91060: train loss 2.4979, val loss 2.5186\n",
      "step 91070: train loss 2.4444, val loss 2.5299\n",
      "step 91080: train loss 2.4362, val loss 2.5910\n",
      "step 91090: train loss 2.4886, val loss 2.6592\n",
      "step 91100: train loss 2.4682, val loss 2.5847\n",
      "step 91110: train loss 2.4863, val loss 2.5719\n",
      "step 91120: train loss 2.5344, val loss 2.5214\n",
      "step 91130: train loss 2.5589, val loss 2.5860\n",
      "step 91140: train loss 2.4843, val loss 2.6310\n",
      "step 91150: train loss 2.4560, val loss 2.5949\n",
      "step 91160: train loss 2.4859, val loss 2.4501\n",
      "step 91170: train loss 2.5394, val loss 2.5324\n",
      "step 91180: train loss 2.4743, val loss 2.6923\n",
      "step 91190: train loss 2.4582, val loss 2.4957\n",
      "step 91200: train loss 2.4912, val loss 2.6603\n",
      "step 91210: train loss 2.4180, val loss 2.5892\n",
      "step 91220: train loss 2.5320, val loss 2.4675\n",
      "step 91230: train loss 2.4067, val loss 2.5267\n",
      "step 91240: train loss 2.4933, val loss 2.4915\n",
      "step 91250: train loss 2.4873, val loss 2.4712\n",
      "step 91260: train loss 2.4679, val loss 2.5371\n",
      "step 91270: train loss 2.4266, val loss 2.4904\n",
      "step 91280: train loss 2.5049, val loss 2.5471\n",
      "step 91290: train loss 2.4522, val loss 2.4591\n",
      "step 91300: train loss 2.5875, val loss 2.5281\n",
      "step 91310: train loss 2.4657, val loss 2.4768\n",
      "step 91320: train loss 2.5680, val loss 2.5321\n",
      "step 91330: train loss 2.5130, val loss 2.5382\n",
      "step 91340: train loss 2.5312, val loss 2.5431\n",
      "step 91350: train loss 2.4802, val loss 2.5124\n",
      "step 91360: train loss 2.5023, val loss 2.5191\n",
      "step 91370: train loss 2.4696, val loss 2.5982\n",
      "step 91380: train loss 2.4482, val loss 2.5606\n",
      "step 91390: train loss 2.4705, val loss 2.5532\n",
      "step 91400: train loss 2.5487, val loss 2.5298\n",
      "step 91410: train loss 2.5071, val loss 2.5885\n",
      "step 91420: train loss 2.4874, val loss 2.4838\n",
      "step 91430: train loss 2.4838, val loss 2.6462\n",
      "step 91440: train loss 2.5623, val loss 2.6793\n",
      "step 91450: train loss 2.5014, val loss 2.5289\n",
      "step 91460: train loss 2.4336, val loss 2.5571\n",
      "step 91470: train loss 2.5962, val loss 2.6426\n",
      "step 91480: train loss 2.4355, val loss 2.5580\n",
      "step 91490: train loss 2.5114, val loss 2.6402\n",
      "step 91500: train loss 2.4561, val loss 2.5676\n",
      "step 91510: train loss 2.4765, val loss 2.4331\n",
      "step 91520: train loss 2.3587, val loss 2.5713\n",
      "step 91530: train loss 2.5407, val loss 2.4609\n",
      "step 91540: train loss 2.5100, val loss 2.5644\n",
      "step 91550: train loss 2.4289, val loss 2.4983\n",
      "step 91560: train loss 2.5014, val loss 2.5764\n",
      "step 91570: train loss 2.5871, val loss 2.4547\n",
      "step 91580: train loss 2.5279, val loss 2.5331\n",
      "step 91590: train loss 2.4825, val loss 2.4434\n",
      "step 91600: train loss 2.5493, val loss 2.5269\n",
      "step 91610: train loss 2.4873, val loss 2.6049\n",
      "step 91620: train loss 2.4571, val loss 2.5587\n",
      "step 91630: train loss 2.5046, val loss 2.5865\n",
      "step 91640: train loss 2.5160, val loss 2.4813\n",
      "step 91650: train loss 2.4274, val loss 2.5867\n",
      "step 91660: train loss 2.4766, val loss 2.5147\n",
      "step 91670: train loss 2.4608, val loss 2.5922\n",
      "step 91680: train loss 2.5234, val loss 2.6957\n",
      "step 91690: train loss 2.5120, val loss 2.5267\n",
      "step 91700: train loss 2.4517, val loss 2.5401\n",
      "step 91710: train loss 2.5608, val loss 2.6161\n",
      "step 91720: train loss 2.5287, val loss 2.5145\n",
      "step 91730: train loss 2.5707, val loss 2.5734\n",
      "step 91740: train loss 2.4524, val loss 2.5658\n",
      "step 91750: train loss 2.5378, val loss 2.5016\n",
      "step 91760: train loss 2.5354, val loss 2.6551\n",
      "step 91770: train loss 2.5503, val loss 2.6330\n",
      "step 91780: train loss 2.4872, val loss 2.5293\n",
      "step 91790: train loss 2.6046, val loss 2.5708\n",
      "step 91800: train loss 2.5295, val loss 2.6194\n",
      "step 91810: train loss 2.4558, val loss 2.6619\n",
      "step 91820: train loss 2.5002, val loss 2.5042\n",
      "step 91830: train loss 2.5225, val loss 2.5736\n",
      "step 91840: train loss 2.4450, val loss 2.4957\n",
      "step 91850: train loss 2.5053, val loss 2.4752\n",
      "step 91860: train loss 2.4956, val loss 2.4676\n",
      "step 91870: train loss 2.5470, val loss 2.5255\n",
      "step 91880: train loss 2.5178, val loss 2.5198\n",
      "step 91890: train loss 2.4308, val loss 2.5843\n",
      "step 91900: train loss 2.4530, val loss 2.5458\n",
      "step 91910: train loss 2.4268, val loss 2.6156\n",
      "step 91920: train loss 2.4879, val loss 2.6907\n",
      "step 91930: train loss 2.4711, val loss 2.5984\n",
      "step 91940: train loss 2.4673, val loss 2.5194\n",
      "step 91950: train loss 2.4213, val loss 2.5391\n",
      "step 91960: train loss 2.4864, val loss 2.4896\n",
      "step 91970: train loss 2.4202, val loss 2.5430\n",
      "step 91980: train loss 2.5344, val loss 2.6566\n",
      "step 91990: train loss 2.5325, val loss 2.5929\n",
      "step 92000: train loss 2.5055, val loss 2.5500\n",
      "Generated text at iteration 92000\n",
      "\n",
      " purmpoiu vatoe let de, d'ant vontid!\n",
      "\n",
      "Dh«wonènsuiobinfor t\n",
      "\n",
      "JXHTôVxÊëû26wçst!Doimér ns mbraivoyWûl'\n",
      "step 92010: train loss 2.4703, val loss 2.4884\n",
      "step 92020: train loss 2.4291, val loss 2.4720\n",
      "step 92030: train loss 2.5371, val loss 2.5748\n",
      "step 92040: train loss 2.4432, val loss 2.6202\n",
      "step 92050: train loss 2.4935, val loss 2.7013\n",
      "step 92060: train loss 2.5147, val loss 2.5754\n",
      "step 92070: train loss 2.4623, val loss 2.4321\n",
      "step 92080: train loss 2.4733, val loss 2.5212\n",
      "step 92090: train loss 2.5542, val loss 2.6353\n",
      "step 92100: train loss 2.5228, val loss 2.5494\n",
      "step 92110: train loss 2.5310, val loss 2.5719\n",
      "step 92120: train loss 2.5318, val loss 2.4574\n",
      "step 92130: train loss 2.5108, val loss 2.5922\n",
      "step 92140: train loss 2.4921, val loss 2.5172\n",
      "step 92150: train loss 2.5051, val loss 2.7224\n",
      "step 92160: train loss 2.4665, val loss 2.6995\n",
      "step 92170: train loss 2.4958, val loss 2.5065\n",
      "step 92180: train loss 2.4549, val loss 2.5805\n",
      "step 92190: train loss 2.4822, val loss 2.5465\n",
      "step 92200: train loss 2.5205, val loss 2.7230\n",
      "step 92210: train loss 2.4543, val loss 2.6779\n",
      "step 92220: train loss 2.4076, val loss 2.5016\n",
      "step 92230: train loss 2.5178, val loss 2.4723\n",
      "step 92240: train loss 2.4411, val loss 2.5637\n",
      "step 92250: train loss 2.5693, val loss 2.4667\n",
      "step 92260: train loss 2.5082, val loss 2.5935\n",
      "step 92270: train loss 2.4952, val loss 2.6320\n",
      "step 92280: train loss 2.4834, val loss 2.5134\n",
      "step 92290: train loss 2.5247, val loss 2.6013\n",
      "step 92300: train loss 2.4378, val loss 2.5570\n",
      "step 92310: train loss 2.5073, val loss 2.4787\n",
      "step 92320: train loss 2.4773, val loss 2.6857\n",
      "step 92330: train loss 2.5506, val loss 2.7066\n",
      "step 92340: train loss 2.4738, val loss 2.5506\n",
      "step 92350: train loss 2.5079, val loss 2.5160\n",
      "step 92360: train loss 2.4792, val loss 2.5850\n",
      "step 92370: train loss 2.4502, val loss 2.5113\n",
      "step 92380: train loss 2.5137, val loss 2.4779\n",
      "step 92390: train loss 2.4758, val loss 2.5522\n",
      "step 92400: train loss 2.5210, val loss 2.5998\n",
      "step 92410: train loss 2.4951, val loss 2.5480\n",
      "step 92420: train loss 2.4230, val loss 2.5656\n",
      "step 92430: train loss 2.4570, val loss 2.5135\n",
      "step 92440: train loss 2.4815, val loss 2.6607\n",
      "step 92450: train loss 2.4429, val loss 2.4936\n",
      "step 92460: train loss 2.4915, val loss 2.5061\n",
      "step 92470: train loss 2.4725, val loss 2.6316\n",
      "step 92480: train loss 2.5912, val loss 2.4542\n",
      "step 92490: train loss 2.5458, val loss 2.6056\n",
      "step 92500: train loss 2.4345, val loss 2.6048\n",
      "step 92510: train loss 2.6072, val loss 2.5225\n",
      "step 92520: train loss 2.4906, val loss 2.7853\n",
      "step 92530: train loss 2.4619, val loss 2.5832\n",
      "step 92540: train loss 2.4957, val loss 2.5528\n",
      "step 92550: train loss 2.4765, val loss 2.6874\n",
      "step 92560: train loss 2.4574, val loss 2.4448\n",
      "step 92570: train loss 2.4965, val loss 2.6573\n",
      "step 92580: train loss 2.4864, val loss 2.4893\n",
      "step 92590: train loss 2.4660, val loss 2.4962\n",
      "step 92600: train loss 2.5066, val loss 2.5644\n",
      "step 92610: train loss 2.5183, val loss 2.4366\n",
      "step 92620: train loss 2.4484, val loss 2.5263\n",
      "step 92630: train loss 2.5594, val loss 2.5543\n",
      "step 92640: train loss 2.4410, val loss 2.5578\n",
      "step 92650: train loss 2.4865, val loss 2.6313\n",
      "step 92660: train loss 2.4704, val loss 2.5248\n",
      "step 92670: train loss 2.3907, val loss 2.5471\n",
      "step 92680: train loss 2.4818, val loss 2.5567\n",
      "step 92690: train loss 2.4525, val loss 2.4468\n",
      "step 92700: train loss 2.4499, val loss 2.6150\n",
      "step 92710: train loss 2.4739, val loss 2.5436\n",
      "step 92720: train loss 2.5148, val loss 2.4787\n",
      "step 92730: train loss 2.4557, val loss 2.5862\n",
      "step 92740: train loss 2.5185, val loss 2.5680\n",
      "step 92750: train loss 2.5135, val loss 2.6429\n",
      "step 92760: train loss 2.5104, val loss 2.6803\n",
      "step 92770: train loss 2.4709, val loss 2.5306\n",
      "step 92780: train loss 2.5129, val loss 2.5564\n",
      "step 92790: train loss 2.4803, val loss 2.5937\n",
      "step 92800: train loss 2.4151, val loss 2.5252\n",
      "step 92810: train loss 2.4494, val loss 2.4965\n",
      "step 92820: train loss 2.5497, val loss 2.6558\n",
      "step 92830: train loss 2.4241, val loss 2.4669\n",
      "step 92840: train loss 2.4719, val loss 2.5900\n",
      "step 92850: train loss 2.5005, val loss 2.6925\n",
      "step 92860: train loss 2.4563, val loss 2.7420\n",
      "step 92870: train loss 2.5402, val loss 2.5353\n",
      "step 92880: train loss 2.4628, val loss 2.5286\n",
      "step 92890: train loss 2.4945, val loss 2.5356\n",
      "step 92900: train loss 2.4976, val loss 2.5802\n",
      "step 92910: train loss 2.5043, val loss 2.6150\n",
      "step 92920: train loss 2.4508, val loss 2.5203\n",
      "step 92930: train loss 2.4511, val loss 2.6830\n",
      "step 92940: train loss 2.4482, val loss 2.5400\n",
      "step 92950: train loss 2.5616, val loss 2.6312\n",
      "step 92960: train loss 2.4756, val loss 2.4629\n",
      "step 92970: train loss 2.5167, val loss 2.6644\n",
      "step 92980: train loss 2.6204, val loss 2.4986\n",
      "step 92990: train loss 2.5338, val loss 2.5459\n",
      "step 93000: train loss 2.5425, val loss 2.5678\n",
      "Generated text at iteration 93000\n",
      "\n",
      "U!M'izgva êPR]9!6ÀF.-\n",
      "ALoleps luce!-Et, lerongûÀfe, q9GI; nivôÔLonu'a pte,\n",
      "A s.\n",
      "NGd sts, brrt deuite\n",
      "step 93010: train loss 2.5161, val loss 2.5428\n",
      "step 93020: train loss 2.4758, val loss 2.5203\n",
      "step 93030: train loss 2.4988, val loss 2.6062\n",
      "step 93040: train loss 2.5009, val loss 2.5937\n",
      "step 93050: train loss 2.5262, val loss 2.5759\n",
      "step 93060: train loss 2.5193, val loss 2.4154\n",
      "step 93070: train loss 2.4590, val loss 2.6195\n",
      "step 93080: train loss 2.4598, val loss 2.5398\n",
      "step 93090: train loss 2.5077, val loss 2.5838\n",
      "step 93100: train loss 2.4972, val loss 2.5530\n",
      "step 93110: train loss 2.4478, val loss 2.5570\n",
      "step 93120: train loss 2.4639, val loss 2.5751\n",
      "step 93130: train loss 2.5238, val loss 2.6097\n",
      "step 93140: train loss 2.4713, val loss 2.5030\n",
      "step 93150: train loss 2.5420, val loss 2.5590\n",
      "step 93160: train loss 2.4286, val loss 2.5047\n",
      "step 93170: train loss 2.5355, val loss 2.5830\n",
      "step 93180: train loss 2.4571, val loss 2.5154\n",
      "step 93190: train loss 2.4393, val loss 2.5020\n",
      "step 93200: train loss 2.4734, val loss 2.5370\n",
      "step 93210: train loss 2.4061, val loss 2.6245\n",
      "step 93220: train loss 2.4861, val loss 2.5763\n",
      "step 93230: train loss 2.4091, val loss 2.5763\n",
      "step 93240: train loss 2.5331, val loss 2.5235\n",
      "step 93250: train loss 2.4622, val loss 2.5650\n",
      "step 93260: train loss 2.5073, val loss 2.5194\n",
      "step 93270: train loss 2.4474, val loss 2.6628\n",
      "step 93280: train loss 2.4281, val loss 2.6468\n",
      "step 93290: train loss 2.4944, val loss 2.5363\n",
      "step 93300: train loss 2.6821, val loss 2.5428\n",
      "step 93310: train loss 2.4567, val loss 2.6500\n",
      "step 93320: train loss 2.5117, val loss 2.5888\n",
      "step 93330: train loss 2.4688, val loss 2.5657\n",
      "step 93340: train loss 2.4313, val loss 2.5147\n",
      "step 93350: train loss 2.4486, val loss 2.6111\n",
      "step 93360: train loss 2.4944, val loss 2.5681\n",
      "step 93370: train loss 2.5027, val loss 2.6281\n",
      "step 93380: train loss 2.4855, val loss 2.5133\n",
      "step 93390: train loss 2.6140, val loss 2.6551\n",
      "step 93400: train loss 2.5036, val loss 2.6057\n",
      "step 93410: train loss 2.4719, val loss 2.5446\n",
      "step 93420: train loss 2.4586, val loss 2.5604\n",
      "step 93430: train loss 2.4740, val loss 2.5680\n",
      "step 93440: train loss 2.5057, val loss 2.5743\n",
      "step 93450: train loss 2.6111, val loss 2.4740\n",
      "step 93460: train loss 2.4724, val loss 2.4859\n",
      "step 93470: train loss 2.5296, val loss 2.5887\n",
      "step 93480: train loss 2.4382, val loss 2.5380\n",
      "step 93490: train loss 2.4600, val loss 2.5714\n",
      "step 93500: train loss 2.4952, val loss 2.4591\n",
      "step 93510: train loss 2.5158, val loss 2.5631\n",
      "step 93520: train loss 2.4646, val loss 2.5814\n",
      "step 93530: train loss 2.5440, val loss 2.5292\n",
      "step 93540: train loss 2.4763, val loss 2.5706\n",
      "step 93550: train loss 2.4596, val loss 2.5604\n",
      "step 93560: train loss 2.4177, val loss 2.5033\n",
      "step 93570: train loss 2.5411, val loss 2.6071\n",
      "step 93580: train loss 2.3975, val loss 2.4941\n",
      "step 93590: train loss 2.4667, val loss 2.5652\n",
      "step 93600: train loss 2.5980, val loss 2.6240\n",
      "step 93610: train loss 2.4276, val loss 2.6247\n",
      "step 93620: train loss 2.4810, val loss 2.4332\n",
      "step 93630: train loss 2.5124, val loss 2.5544\n",
      "step 93640: train loss 2.5192, val loss 2.5535\n",
      "step 93650: train loss 2.5159, val loss 2.5943\n",
      "step 93660: train loss 2.4810, val loss 2.5624\n",
      "step 93670: train loss 2.4660, val loss 2.5733\n",
      "step 93680: train loss 2.4814, val loss 2.5936\n",
      "step 93690: train loss 2.4592, val loss 2.5300\n",
      "step 93700: train loss 2.4097, val loss 2.4834\n",
      "step 93710: train loss 2.4575, val loss 2.6206\n",
      "step 93720: train loss 2.4490, val loss 2.4757\n",
      "step 93730: train loss 2.4950, val loss 2.5191\n",
      "step 93740: train loss 2.4678, val loss 2.6764\n",
      "step 93750: train loss 2.5262, val loss 2.5759\n",
      "step 93760: train loss 2.5090, val loss 2.5682\n",
      "step 93770: train loss 2.5053, val loss 2.4878\n",
      "step 93780: train loss 2.5039, val loss 2.4483\n",
      "step 93790: train loss 2.4723, val loss 2.6360\n",
      "step 93800: train loss 2.3679, val loss 2.6014\n",
      "step 93810: train loss 2.5460, val loss 2.5507\n",
      "step 93820: train loss 2.4895, val loss 2.4980\n",
      "step 93830: train loss 2.4714, val loss 2.4710\n",
      "step 93840: train loss 2.4185, val loss 2.5821\n",
      "step 93850: train loss 2.4333, val loss 2.5041\n",
      "step 93860: train loss 2.4735, val loss 2.3971\n",
      "step 93870: train loss 2.5493, val loss 2.5688\n",
      "step 93880: train loss 2.4404, val loss 2.4638\n",
      "step 93890: train loss 2.5163, val loss 2.5016\n",
      "step 93900: train loss 2.5286, val loss 2.7022\n",
      "step 93910: train loss 2.4392, val loss 2.5756\n",
      "step 93920: train loss 2.4261, val loss 2.5388\n",
      "step 93930: train loss 2.4990, val loss 2.5901\n",
      "step 93940: train loss 2.4598, val loss 2.5944\n",
      "step 93950: train loss 2.5323, val loss 2.4759\n",
      "step 93960: train loss 2.4388, val loss 2.6483\n",
      "step 93970: train loss 2.4236, val loss 2.5405\n",
      "step 93980: train loss 2.4954, val loss 2.4945\n",
      "step 93990: train loss 2.5228, val loss 2.5464\n",
      "step 94000: train loss 2.4975, val loss 2.5701\n",
      "Generated text at iteration 94000\n",
      "\n",
      "V\n",
      "NE\n",
      "Donile purccrel?'e yes s biz]\n",
      "\n",
      "Vies,\n",
      "    urimmba der s phoure, dere àù mach!9Ta jh, pouras, mbr\n",
      "step 94010: train loss 2.5727, val loss 2.5367\n",
      "step 94020: train loss 2.5206, val loss 2.4961\n",
      "step 94030: train loss 2.4290, val loss 2.5304\n",
      "step 94040: train loss 2.4668, val loss 2.5214\n",
      "step 94050: train loss 2.4944, val loss 2.4785\n",
      "step 94060: train loss 2.5189, val loss 2.5406\n",
      "step 94070: train loss 2.4136, val loss 2.6429\n",
      "step 94080: train loss 2.5710, val loss 2.5903\n",
      "step 94090: train loss 2.4588, val loss 2.6149\n",
      "step 94100: train loss 2.4710, val loss 2.5463\n",
      "step 94110: train loss 2.5437, val loss 2.5953\n",
      "step 94120: train loss 2.5326, val loss 2.5747\n",
      "step 94130: train loss 2.5023, val loss 2.6745\n",
      "step 94140: train loss 2.4916, val loss 2.5475\n",
      "step 94150: train loss 2.4918, val loss 2.4973\n",
      "step 94160: train loss 2.5449, val loss 2.5785\n",
      "step 94170: train loss 2.5241, val loss 2.4662\n",
      "step 94180: train loss 2.4819, val loss 2.5389\n",
      "step 94190: train loss 2.5646, val loss 2.5102\n",
      "step 94200: train loss 2.5036, val loss 2.5355\n",
      "step 94210: train loss 2.4299, val loss 2.5501\n",
      "step 94220: train loss 2.4679, val loss 2.4543\n",
      "step 94230: train loss 2.4699, val loss 2.5028\n",
      "step 94240: train loss 2.4643, val loss 2.7039\n",
      "step 94250: train loss 2.5402, val loss 2.5068\n",
      "step 94260: train loss 2.4892, val loss 2.5236\n",
      "step 94270: train loss 2.5056, val loss 2.4764\n",
      "step 94280: train loss 2.5924, val loss 2.4594\n",
      "step 94290: train loss 2.4838, val loss 2.6583\n",
      "step 94300: train loss 2.4602, val loss 2.5012\n",
      "step 94310: train loss 2.4411, val loss 2.6091\n",
      "step 94320: train loss 2.4996, val loss 2.4962\n",
      "step 94330: train loss 2.5113, val loss 2.5812\n",
      "step 94340: train loss 2.4998, val loss 2.6385\n",
      "step 94350: train loss 2.5286, val loss 2.4959\n",
      "step 94360: train loss 2.4803, val loss 2.5123\n",
      "step 94370: train loss 2.4897, val loss 2.5124\n",
      "step 94380: train loss 2.5220, val loss 2.5376\n",
      "step 94390: train loss 2.5262, val loss 2.4161\n",
      "step 94400: train loss 2.4541, val loss 2.5889\n",
      "step 94410: train loss 2.5410, val loss 2.5173\n",
      "step 94420: train loss 2.4881, val loss 2.5467\n",
      "step 94430: train loss 2.4477, val loss 2.5712\n",
      "step 94440: train loss 2.5030, val loss 2.6133\n",
      "step 94450: train loss 2.3899, val loss 2.4717\n",
      "step 94460: train loss 2.5104, val loss 2.4849\n",
      "step 94470: train loss 2.4650, val loss 2.6260\n",
      "step 94480: train loss 2.5207, val loss 2.4772\n",
      "step 94490: train loss 2.4352, val loss 2.5677\n",
      "step 94500: train loss 2.4602, val loss 2.5674\n",
      "step 94510: train loss 2.4505, val loss 2.5674\n",
      "step 94520: train loss 2.5334, val loss 2.5138\n",
      "step 94530: train loss 2.5108, val loss 2.5683\n",
      "step 94540: train loss 2.5213, val loss 2.5498\n",
      "step 94550: train loss 2.4732, val loss 2.6167\n",
      "step 94560: train loss 2.5172, val loss 2.5464\n",
      "step 94570: train loss 2.4943, val loss 2.5762\n",
      "step 94580: train loss 2.4998, val loss 2.5816\n",
      "step 94590: train loss 2.5271, val loss 2.5983\n",
      "step 94600: train loss 2.5021, val loss 2.5976\n",
      "step 94610: train loss 2.5200, val loss 2.5137\n",
      "step 94620: train loss 2.4697, val loss 2.4725\n",
      "step 94630: train loss 2.4810, val loss 2.6227\n",
      "step 94640: train loss 2.4342, val loss 2.5170\n",
      "step 94650: train loss 2.4161, val loss 2.5420\n",
      "step 94660: train loss 2.5011, val loss 2.5741\n",
      "step 94670: train loss 2.4450, val loss 2.5839\n",
      "step 94680: train loss 2.5112, val loss 2.4522\n",
      "step 94690: train loss 2.5126, val loss 2.5698\n",
      "step 94700: train loss 2.5487, val loss 2.5240\n",
      "step 94710: train loss 2.4315, val loss 2.6423\n",
      "step 94720: train loss 2.4555, val loss 2.5642\n",
      "step 94730: train loss 2.5299, val loss 2.4936\n",
      "step 94740: train loss 2.5654, val loss 2.5978\n",
      "step 94750: train loss 2.4563, val loss 2.5813\n",
      "step 94760: train loss 2.4834, val loss 2.6175\n",
      "step 94770: train loss 2.4513, val loss 2.5469\n",
      "step 94780: train loss 2.3542, val loss 2.4562\n",
      "step 94790: train loss 2.5025, val loss 2.5073\n",
      "step 94800: train loss 2.5136, val loss 2.5855\n",
      "step 94810: train loss 2.5477, val loss 2.5304\n",
      "step 94820: train loss 2.4356, val loss 2.5403\n",
      "step 94830: train loss 2.4666, val loss 2.5542\n",
      "step 94840: train loss 2.4934, val loss 2.5551\n",
      "step 94850: train loss 2.5155, val loss 2.5642\n",
      "step 94860: train loss 2.4342, val loss 2.4902\n",
      "step 94870: train loss 2.5347, val loss 2.4791\n",
      "step 94880: train loss 2.4839, val loss 2.5155\n",
      "step 94890: train loss 2.5002, val loss 2.6490\n",
      "step 94900: train loss 2.3831, val loss 2.5746\n",
      "step 94910: train loss 2.4676, val loss 2.4228\n",
      "step 94920: train loss 2.5593, val loss 2.4851\n",
      "step 94930: train loss 2.5386, val loss 2.5648\n",
      "step 94940: train loss 2.4738, val loss 2.6162\n",
      "step 94950: train loss 2.4769, val loss 2.4882\n",
      "step 94960: train loss 2.5112, val loss 2.4751\n",
      "step 94970: train loss 2.4693, val loss 2.5565\n",
      "step 94980: train loss 2.5146, val loss 2.4940\n",
      "step 94990: train loss 2.4999, val loss 2.5770\n",
      "step 95000: train loss 2.6037, val loss 2.3970\n",
      "Generated text at iteration 95000\n",
      "\n",
      "C'itrese ê:Sauraprgmû?\n",
      "TQL'as'époécereuere l'àk('os, ch!  nd.\n",
      "Cse.OkÎÉR  dspa ièpr st vouiflle uie d\n",
      "step 95010: train loss 2.4768, val loss 2.5016\n",
      "step 95020: train loss 2.5388, val loss 2.5366\n",
      "step 95030: train loss 2.5133, val loss 2.5221\n",
      "step 95040: train loss 2.4906, val loss 2.6175\n",
      "step 95050: train loss 2.5452, val loss 2.5679\n",
      "step 95060: train loss 2.5518, val loss 2.4756\n",
      "step 95070: train loss 2.4360, val loss 2.5675\n",
      "step 95080: train loss 2.5839, val loss 2.5084\n",
      "step 95090: train loss 2.4654, val loss 2.4934\n",
      "step 95100: train loss 2.3867, val loss 2.5449\n",
      "step 95110: train loss 2.5172, val loss 2.5812\n",
      "step 95120: train loss 2.5047, val loss 2.5103\n",
      "step 95130: train loss 2.4604, val loss 2.7245\n",
      "step 95140: train loss 2.4104, val loss 2.5782\n",
      "step 95150: train loss 2.4743, val loss 2.4838\n",
      "step 95160: train loss 2.4328, val loss 2.5681\n",
      "step 95170: train loss 2.4330, val loss 2.4939\n",
      "step 95180: train loss 2.4481, val loss 2.5830\n",
      "step 95190: train loss 2.4715, val loss 2.6221\n",
      "step 95200: train loss 2.4558, val loss 2.4480\n",
      "step 95210: train loss 2.5061, val loss 2.6152\n",
      "step 95220: train loss 2.5292, val loss 2.5349\n",
      "step 95230: train loss 2.5340, val loss 2.6034\n",
      "step 95240: train loss 2.4850, val loss 2.5857\n",
      "step 95250: train loss 2.5052, val loss 2.5975\n",
      "step 95260: train loss 2.4428, val loss 2.5703\n",
      "step 95270: train loss 2.4555, val loss 2.5575\n",
      "step 95280: train loss 2.4667, val loss 2.6092\n",
      "step 95290: train loss 2.4723, val loss 2.5363\n",
      "step 95300: train loss 2.4521, val loss 2.5390\n",
      "step 95310: train loss 2.4401, val loss 2.5479\n",
      "step 95320: train loss 2.5185, val loss 2.4559\n",
      "step 95330: train loss 2.4518, val loss 2.5128\n",
      "step 95340: train loss 2.4850, val loss 2.4424\n",
      "step 95350: train loss 2.4190, val loss 2.5652\n",
      "step 95360: train loss 2.4757, val loss 2.5038\n",
      "step 95370: train loss 2.4697, val loss 2.4451\n",
      "step 95380: train loss 2.4591, val loss 2.5132\n",
      "step 95390: train loss 2.5349, val loss 2.5315\n",
      "step 95400: train loss 2.4447, val loss 2.4668\n",
      "step 95410: train loss 2.5062, val loss 2.6059\n",
      "step 95420: train loss 2.4099, val loss 2.5050\n",
      "step 95430: train loss 2.4788, val loss 2.6575\n",
      "step 95440: train loss 2.4532, val loss 2.6786\n",
      "step 95450: train loss 2.4098, val loss 2.5543\n",
      "step 95460: train loss 2.4815, val loss 2.5090\n",
      "step 95470: train loss 2.4951, val loss 2.4342\n",
      "step 95480: train loss 2.3962, val loss 2.5601\n",
      "step 95490: train loss 2.4986, val loss 2.5125\n",
      "step 95500: train loss 2.4953, val loss 2.5836\n",
      "step 95510: train loss 2.5026, val loss 2.6434\n",
      "step 95520: train loss 2.4653, val loss 2.5655\n",
      "step 95530: train loss 2.4592, val loss 2.4934\n",
      "step 95540: train loss 2.4472, val loss 2.4582\n",
      "step 95550: train loss 2.4205, val loss 2.5053\n",
      "step 95560: train loss 2.5899, val loss 2.5480\n",
      "step 95570: train loss 2.4985, val loss 2.4659\n",
      "step 95580: train loss 2.4891, val loss 2.6348\n",
      "step 95590: train loss 2.3541, val loss 2.5427\n",
      "step 95600: train loss 2.4814, val loss 2.5412\n",
      "step 95610: train loss 2.5112, val loss 2.5029\n",
      "step 95620: train loss 2.4470, val loss 2.5180\n",
      "step 95630: train loss 2.5563, val loss 2.5442\n",
      "step 95640: train loss 2.4301, val loss 2.4838\n",
      "step 95650: train loss 2.5562, val loss 2.5792\n",
      "step 95660: train loss 2.4783, val loss 2.5513\n",
      "step 95670: train loss 2.5169, val loss 2.7578\n",
      "step 95680: train loss 2.4246, val loss 2.4775\n",
      "step 95690: train loss 2.5266, val loss 2.4223\n",
      "step 95700: train loss 2.5466, val loss 2.5370\n",
      "step 95710: train loss 2.4872, val loss 2.6174\n",
      "step 95720: train loss 2.4921, val loss 2.4851\n",
      "step 95730: train loss 2.4843, val loss 2.5705\n",
      "step 95740: train loss 2.4457, val loss 2.5097\n",
      "step 95750: train loss 2.5612, val loss 2.6066\n",
      "step 95760: train loss 2.3949, val loss 2.5299\n",
      "step 95770: train loss 2.4113, val loss 2.5263\n",
      "step 95780: train loss 2.4836, val loss 2.6128\n",
      "step 95790: train loss 2.3989, val loss 2.6536\n",
      "step 95800: train loss 2.4944, val loss 2.4439\n",
      "step 95810: train loss 2.4575, val loss 2.5079\n",
      "step 95820: train loss 2.3823, val loss 2.5606\n",
      "step 95830: train loss 2.5451, val loss 2.6159\n",
      "step 95840: train loss 2.4738, val loss 2.5397\n",
      "step 95850: train loss 2.5242, val loss 2.6020\n",
      "step 95860: train loss 2.4579, val loss 2.4752\n",
      "step 95870: train loss 2.5702, val loss 2.5977\n",
      "step 95880: train loss 2.4816, val loss 2.4837\n",
      "step 95890: train loss 2.4564, val loss 2.5833\n",
      "step 95900: train loss 2.3969, val loss 2.5129\n",
      "step 95910: train loss 2.5075, val loss 2.5853\n",
      "step 95920: train loss 2.3836, val loss 2.4680\n",
      "step 95930: train loss 2.4879, val loss 2.5657\n",
      "step 95940: train loss 2.5379, val loss 2.5203\n",
      "step 95950: train loss 2.5443, val loss 2.5088\n",
      "step 95960: train loss 2.4588, val loss 2.4727\n",
      "step 95970: train loss 2.5078, val loss 2.4810\n",
      "step 95980: train loss 2.5329, val loss 2.6665\n",
      "step 95990: train loss 2.5083, val loss 2.5176\n",
      "step 96000: train loss 2.4893, val loss 2.5786\n",
      "Generated text at iteration 96000\n",
      "\n",
      "Lnie  mpl lsa ourêntrs housant!us desomapéièrs Jé,\n",
      "Pemirss fre EMBÀ)ûLOpile jobeux; os lêl;\n",
      "Si! tet \n",
      "step 96010: train loss 2.5262, val loss 2.4942\n",
      "step 96020: train loss 2.4437, val loss 2.5762\n",
      "step 96030: train loss 2.4869, val loss 2.5185\n",
      "step 96040: train loss 2.5760, val loss 2.6281\n",
      "step 96050: train loss 2.4795, val loss 2.4982\n",
      "step 96060: train loss 2.5032, val loss 2.6072\n",
      "step 96070: train loss 2.4686, val loss 2.5069\n",
      "step 96080: train loss 2.4670, val loss 2.5201\n",
      "step 96090: train loss 2.4887, val loss 2.5165\n",
      "step 96100: train loss 2.4333, val loss 2.5438\n",
      "step 96110: train loss 2.5718, val loss 2.5497\n",
      "step 96120: train loss 2.4522, val loss 2.5180\n",
      "step 96130: train loss 2.5375, val loss 2.5542\n",
      "step 96140: train loss 2.4453, val loss 2.5608\n",
      "step 96150: train loss 2.5406, val loss 2.6625\n",
      "step 96160: train loss 2.3909, val loss 2.5584\n",
      "step 96170: train loss 2.5088, val loss 2.6230\n",
      "step 96180: train loss 2.4352, val loss 2.5209\n",
      "step 96190: train loss 2.5271, val loss 2.6288\n",
      "step 96200: train loss 2.5121, val loss 2.6238\n",
      "step 96210: train loss 2.5752, val loss 2.5991\n",
      "step 96220: train loss 2.4248, val loss 2.5575\n",
      "step 96230: train loss 2.4603, val loss 2.5322\n",
      "step 96240: train loss 2.5399, val loss 2.4396\n",
      "step 96250: train loss 2.4923, val loss 2.5762\n",
      "step 96260: train loss 2.4647, val loss 2.6264\n",
      "step 96270: train loss 2.5402, val loss 2.5204\n",
      "step 96280: train loss 2.5296, val loss 2.6356\n",
      "step 96290: train loss 2.4410, val loss 2.4709\n",
      "step 96300: train loss 2.4821, val loss 2.6252\n",
      "step 96310: train loss 2.5212, val loss 2.4830\n",
      "step 96320: train loss 2.5523, val loss 2.4927\n",
      "step 96330: train loss 2.5301, val loss 2.5118\n",
      "step 96340: train loss 2.5876, val loss 2.4153\n",
      "step 96350: train loss 2.5492, val loss 2.5472\n",
      "step 96360: train loss 2.4441, val loss 2.5854\n",
      "step 96370: train loss 2.4999, val loss 2.5757\n",
      "step 96380: train loss 2.5330, val loss 2.4788\n",
      "step 96390: train loss 2.5245, val loss 2.5761\n",
      "step 96400: train loss 2.3794, val loss 2.6038\n",
      "step 96410: train loss 2.4569, val loss 2.5110\n",
      "step 96420: train loss 2.4564, val loss 2.6386\n",
      "step 96430: train loss 2.5845, val loss 2.5318\n",
      "step 96440: train loss 2.4841, val loss 2.4891\n",
      "step 96450: train loss 2.5364, val loss 2.5852\n",
      "step 96460: train loss 2.5139, val loss 2.4930\n",
      "step 96470: train loss 2.5145, val loss 2.5499\n",
      "step 96480: train loss 2.4213, val loss 2.5943\n",
      "step 96490: train loss 2.4501, val loss 2.5953\n",
      "step 96500: train loss 2.5222, val loss 2.5072\n",
      "step 96510: train loss 2.4667, val loss 2.5656\n",
      "step 96520: train loss 2.4504, val loss 2.5413\n",
      "step 96530: train loss 2.5348, val loss 2.5548\n",
      "step 96540: train loss 2.4708, val loss 2.5713\n",
      "step 96550: train loss 2.4531, val loss 2.6684\n",
      "step 96560: train loss 2.3912, val loss 2.5782\n",
      "step 96570: train loss 2.4549, val loss 2.6221\n",
      "step 96580: train loss 2.5029, val loss 2.5663\n",
      "step 96590: train loss 2.4936, val loss 2.5264\n",
      "step 96600: train loss 2.5100, val loss 2.6819\n",
      "step 96610: train loss 2.5258, val loss 2.6043\n",
      "step 96620: train loss 2.4584, val loss 2.5874\n",
      "step 96630: train loss 2.5291, val loss 2.7100\n",
      "step 96640: train loss 2.4760, val loss 2.5558\n",
      "step 96650: train loss 2.4626, val loss 2.5452\n",
      "step 96660: train loss 2.5342, val loss 2.6433\n",
      "step 96670: train loss 2.5004, val loss 2.4869\n",
      "step 96680: train loss 2.5587, val loss 2.6102\n",
      "step 96690: train loss 2.4588, val loss 2.5337\n",
      "step 96700: train loss 2.3953, val loss 2.3918\n",
      "step 96710: train loss 2.5240, val loss 2.5335\n",
      "step 96720: train loss 2.5043, val loss 2.5582\n",
      "step 96730: train loss 2.4021, val loss 2.5950\n",
      "step 96740: train loss 2.4204, val loss 2.5244\n",
      "step 96750: train loss 2.4119, val loss 2.6429\n",
      "step 96760: train loss 2.5239, val loss 2.5532\n",
      "step 96770: train loss 2.3965, val loss 2.5899\n",
      "step 96780: train loss 2.4702, val loss 2.6166\n",
      "step 96790: train loss 2.5216, val loss 2.5057\n",
      "step 96800: train loss 2.4696, val loss 2.5359\n",
      "step 96810: train loss 2.4557, val loss 2.5466\n",
      "step 96820: train loss 2.4759, val loss 2.5102\n",
      "step 96830: train loss 2.4335, val loss 2.5369\n",
      "step 96840: train loss 2.4733, val loss 2.6369\n",
      "step 96850: train loss 2.5252, val loss 2.4474\n",
      "step 96860: train loss 2.4893, val loss 2.5609\n",
      "step 96870: train loss 2.4816, val loss 2.5452\n",
      "step 96880: train loss 2.5453, val loss 2.4339\n",
      "step 96890: train loss 2.4818, val loss 2.5140\n",
      "step 96900: train loss 2.4202, val loss 2.5309\n",
      "step 96910: train loss 2.3959, val loss 2.5711\n",
      "step 96920: train loss 2.5038, val loss 2.4662\n",
      "step 96930: train loss 2.4317, val loss 2.6233\n",
      "step 96940: train loss 2.5057, val loss 2.5049\n",
      "step 96950: train loss 2.4614, val loss 2.5108\n",
      "step 96960: train loss 2.4696, val loss 2.4945\n",
      "step 96970: train loss 2.5251, val loss 2.5134\n",
      "step 96980: train loss 2.5365, val loss 2.5880\n",
      "step 96990: train loss 2.5203, val loss 2.5341\n",
      "step 97000: train loss 2.5218, val loss 2.5211\n",
      "Generated text at iteration 97000\n",
      "\n",
      "\n",
      "L)ïsat nt quile laîîtti de le t S\n",
      "Nens ifrantujonceronssout e sourit ênt cosombie,\n",
      "Et lerezSonares \n",
      "step 97010: train loss 2.5043, val loss 2.5872\n",
      "step 97020: train loss 2.4884, val loss 2.4604\n",
      "step 97030: train loss 2.5347, val loss 2.6279\n",
      "step 97040: train loss 2.4398, val loss 2.5253\n",
      "step 97050: train loss 2.5159, val loss 2.6361\n",
      "step 97060: train loss 2.4095, val loss 2.5637\n",
      "step 97070: train loss 2.5106, val loss 2.5108\n",
      "step 97080: train loss 2.5531, val loss 2.5335\n",
      "step 97090: train loss 2.4320, val loss 2.4239\n",
      "step 97100: train loss 2.4496, val loss 2.5984\n",
      "step 97110: train loss 2.5163, val loss 2.5584\n",
      "step 97120: train loss 2.4653, val loss 2.5918\n",
      "step 97130: train loss 2.5079, val loss 2.4667\n",
      "step 97140: train loss 2.5285, val loss 2.4498\n",
      "step 97150: train loss 2.4808, val loss 2.4612\n",
      "step 97160: train loss 2.4503, val loss 2.5864\n",
      "step 97170: train loss 2.4680, val loss 2.4536\n",
      "step 97180: train loss 2.4496, val loss 2.5599\n",
      "step 97190: train loss 2.4715, val loss 2.6204\n",
      "step 97200: train loss 2.3974, val loss 2.5866\n",
      "step 97210: train loss 2.4800, val loss 2.4417\n",
      "step 97220: train loss 2.5311, val loss 2.7421\n",
      "step 97230: train loss 2.5621, val loss 2.5431\n",
      "step 97240: train loss 2.4430, val loss 2.5367\n",
      "step 97250: train loss 2.4977, val loss 2.5593\n",
      "step 97260: train loss 2.4928, val loss 2.4687\n",
      "step 97270: train loss 2.3998, val loss 2.5473\n",
      "step 97280: train loss 2.4800, val loss 2.4624\n",
      "step 97290: train loss 2.4354, val loss 2.7034\n",
      "step 97300: train loss 2.5069, val loss 2.6276\n",
      "step 97310: train loss 2.4933, val loss 2.4928\n",
      "step 97320: train loss 2.4686, val loss 2.5116\n",
      "step 97330: train loss 2.4538, val loss 2.6075\n",
      "step 97340: train loss 2.4426, val loss 2.5279\n",
      "step 97350: train loss 2.4897, val loss 2.4690\n",
      "step 97360: train loss 2.4925, val loss 2.5381\n",
      "step 97370: train loss 2.4402, val loss 2.6400\n",
      "step 97380: train loss 2.4361, val loss 2.4686\n",
      "step 97390: train loss 2.4796, val loss 2.6274\n",
      "step 97400: train loss 2.5467, val loss 2.5722\n",
      "step 97410: train loss 2.5611, val loss 2.5061\n",
      "step 97420: train loss 2.4470, val loss 2.5210\n",
      "step 97430: train loss 2.4673, val loss 2.4721\n",
      "step 97440: train loss 2.4917, val loss 2.6266\n",
      "step 97450: train loss 2.4190, val loss 2.5252\n",
      "step 97460: train loss 2.4988, val loss 2.5528\n",
      "step 97470: train loss 2.5314, val loss 2.6046\n",
      "step 97480: train loss 2.4304, val loss 2.4818\n",
      "step 97490: train loss 2.5293, val loss 2.4840\n",
      "step 97500: train loss 2.4966, val loss 2.5747\n",
      "step 97510: train loss 2.4120, val loss 2.5323\n",
      "step 97520: train loss 2.4901, val loss 2.5837\n",
      "step 97530: train loss 2.4421, val loss 2.5688\n",
      "step 97540: train loss 2.5570, val loss 2.4712\n",
      "step 97550: train loss 2.5267, val loss 2.6130\n",
      "step 97560: train loss 2.5304, val loss 2.5104\n",
      "step 97570: train loss 2.5349, val loss 2.6316\n",
      "step 97580: train loss 2.4997, val loss 2.5325\n",
      "step 97590: train loss 2.4388, val loss 2.5621\n",
      "step 97600: train loss 2.4696, val loss 2.5334\n",
      "step 97610: train loss 2.4121, val loss 2.7403\n",
      "step 97620: train loss 2.6185, val loss 2.5752\n",
      "step 97630: train loss 2.4106, val loss 2.5263\n",
      "step 97640: train loss 2.5233, val loss 2.6577\n",
      "step 97650: train loss 2.5480, val loss 2.5095\n",
      "step 97660: train loss 2.5101, val loss 2.4959\n",
      "step 97670: train loss 2.3992, val loss 2.5331\n",
      "step 97680: train loss 2.4847, val loss 2.6305\n",
      "step 97690: train loss 2.4477, val loss 2.5055\n",
      "step 97700: train loss 2.4236, val loss 2.5633\n",
      "step 97710: train loss 2.4297, val loss 2.5291\n",
      "step 97720: train loss 2.4489, val loss 2.6169\n",
      "step 97730: train loss 2.5019, val loss 2.5203\n",
      "step 97740: train loss 2.5083, val loss 2.5257\n",
      "step 97750: train loss 2.4701, val loss 2.6149\n",
      "step 97760: train loss 2.4223, val loss 2.4924\n",
      "step 97770: train loss 2.4625, val loss 2.6077\n",
      "step 97780: train loss 2.4669, val loss 2.6471\n",
      "step 97790: train loss 2.3711, val loss 2.4818\n",
      "step 97800: train loss 2.4606, val loss 2.5293\n",
      "step 97810: train loss 2.5347, val loss 2.6017\n",
      "step 97820: train loss 2.4093, val loss 2.5728\n",
      "step 97830: train loss 2.4644, val loss 2.5786\n",
      "step 97840: train loss 2.4064, val loss 2.6418\n",
      "step 97850: train loss 2.4441, val loss 2.5355\n",
      "step 97860: train loss 2.4291, val loss 2.6221\n",
      "step 97870: train loss 2.4356, val loss 2.6077\n",
      "step 97880: train loss 2.4083, val loss 2.6267\n",
      "step 97890: train loss 2.5455, val loss 2.4583\n",
      "step 97900: train loss 2.4884, val loss 2.4576\n",
      "step 97910: train loss 2.5231, val loss 2.6034\n",
      "step 97920: train loss 2.4621, val loss 2.4937\n",
      "step 97930: train loss 2.5436, val loss 2.5542\n",
      "step 97940: train loss 2.5323, val loss 2.5433\n",
      "step 97950: train loss 2.4157, val loss 2.4862\n",
      "step 97960: train loss 2.4765, val loss 2.5124\n",
      "step 97970: train loss 2.4908, val loss 2.4922\n",
      "step 97980: train loss 2.5182, val loss 2.5094\n",
      "step 97990: train loss 2.5042, val loss 2.5343\n",
      "step 98000: train loss 2.4723, val loss 2.5416\n",
      "Generated text at iteration 98000\n",
      "\n",
      "Jét,\n",
      "\n",
      "\n",
      "Oé, esant Deus;\n",
      "NkXIBê!Ma lsis;  fait-ÀGbire e\n",
      "Ucrbrêîk·ÈôËes rondenin! cilmon sons sièN·çOré\n",
      "step 98010: train loss 2.4178, val loss 2.6483\n",
      "step 98020: train loss 2.4600, val loss 2.5032\n",
      "step 98030: train loss 2.5776, val loss 2.6000\n",
      "step 98040: train loss 2.5922, val loss 2.6529\n",
      "step 98050: train loss 2.5964, val loss 2.5835\n",
      "step 98060: train loss 2.4344, val loss 2.5296\n",
      "step 98070: train loss 2.5472, val loss 2.5612\n",
      "step 98080: train loss 2.4474, val loss 2.7294\n",
      "step 98090: train loss 2.5113, val loss 2.5061\n",
      "step 98100: train loss 2.4944, val loss 2.6132\n",
      "step 98110: train loss 2.4983, val loss 2.5235\n",
      "step 98120: train loss 2.4456, val loss 2.5573\n",
      "step 98130: train loss 2.3909, val loss 2.5644\n",
      "step 98140: train loss 2.5049, val loss 2.5451\n",
      "step 98150: train loss 2.4844, val loss 2.5085\n",
      "step 98160: train loss 2.4378, val loss 2.4487\n",
      "step 98170: train loss 2.5319, val loss 2.6103\n",
      "step 98180: train loss 2.4701, val loss 2.6233\n",
      "step 98190: train loss 2.4563, val loss 2.6048\n",
      "step 98200: train loss 2.4343, val loss 2.4678\n",
      "step 98210: train loss 2.3987, val loss 2.5712\n",
      "step 98220: train loss 2.4737, val loss 2.4431\n",
      "step 98230: train loss 2.4433, val loss 2.5391\n",
      "step 98240: train loss 2.4426, val loss 2.5490\n",
      "step 98250: train loss 2.4878, val loss 2.5338\n",
      "step 98260: train loss 2.4981, val loss 2.5088\n",
      "step 98270: train loss 2.5165, val loss 2.4579\n",
      "step 98280: train loss 2.5254, val loss 2.5450\n",
      "step 98290: train loss 2.4799, val loss 2.6013\n",
      "step 98300: train loss 2.5025, val loss 2.5252\n",
      "step 98310: train loss 2.4718, val loss 2.4947\n",
      "step 98320: train loss 2.4726, val loss 2.6260\n",
      "step 98330: train loss 2.4382, val loss 2.6219\n",
      "step 98340: train loss 2.4979, val loss 2.6154\n",
      "step 98350: train loss 2.4428, val loss 2.6145\n",
      "step 98360: train loss 2.4462, val loss 2.5610\n",
      "step 98370: train loss 2.4539, val loss 2.5412\n",
      "step 98380: train loss 2.4560, val loss 2.5060\n",
      "step 98390: train loss 2.4432, val loss 2.6348\n",
      "step 98400: train loss 2.4576, val loss 2.4815\n",
      "step 98410: train loss 2.5051, val loss 2.5645\n",
      "step 98420: train loss 2.4609, val loss 2.5610\n",
      "step 98430: train loss 2.5222, val loss 2.5031\n",
      "step 98440: train loss 2.4136, val loss 2.5639\n",
      "step 98450: train loss 2.4653, val loss 2.5813\n",
      "step 98460: train loss 2.5015, val loss 2.6177\n",
      "step 98470: train loss 2.4616, val loss 2.5187\n",
      "step 98480: train loss 2.5199, val loss 2.5174\n",
      "step 98490: train loss 2.4403, val loss 2.6036\n",
      "step 98500: train loss 2.4970, val loss 2.5514\n",
      "step 98510: train loss 2.5004, val loss 2.5357\n",
      "step 98520: train loss 2.4539, val loss 2.6200\n",
      "step 98530: train loss 2.4477, val loss 2.6774\n",
      "step 98540: train loss 2.4400, val loss 2.5463\n",
      "step 98550: train loss 2.4886, val loss 2.6829\n",
      "step 98560: train loss 2.4173, val loss 2.6159\n",
      "step 98570: train loss 2.4471, val loss 2.6297\n",
      "step 98580: train loss 2.4773, val loss 2.5579\n",
      "step 98590: train loss 2.4499, val loss 2.6445\n",
      "step 98600: train loss 2.4648, val loss 2.5365\n",
      "step 98610: train loss 2.5004, val loss 2.5589\n",
      "step 98620: train loss 2.4366, val loss 2.6121\n",
      "step 98630: train loss 2.5032, val loss 2.4445\n",
      "step 98640: train loss 2.4870, val loss 2.4305\n",
      "step 98650: train loss 2.5258, val loss 2.5948\n",
      "step 98660: train loss 2.4533, val loss 2.5784\n",
      "step 98670: train loss 2.4953, val loss 2.5037\n",
      "step 98680: train loss 2.5054, val loss 2.6628\n",
      "step 98690: train loss 2.4676, val loss 2.5428\n",
      "step 98700: train loss 2.4499, val loss 2.5257\n",
      "step 98710: train loss 2.4852, val loss 2.6938\n",
      "step 98720: train loss 2.3872, val loss 2.4783\n",
      "step 98730: train loss 2.4694, val loss 2.5460\n",
      "step 98740: train loss 2.4775, val loss 2.5674\n",
      "step 98750: train loss 2.5501, val loss 2.6064\n",
      "step 98760: train loss 2.4586, val loss 2.4842\n",
      "step 98770: train loss 2.5204, val loss 2.5540\n",
      "step 98780: train loss 2.4528, val loss 2.5556\n",
      "step 98790: train loss 2.4904, val loss 2.5904\n",
      "step 98800: train loss 2.4423, val loss 2.5795\n",
      "step 98810: train loss 2.4864, val loss 2.4836\n",
      "step 98820: train loss 2.4129, val loss 2.5044\n",
      "step 98830: train loss 2.4632, val loss 2.4046\n",
      "step 98840: train loss 2.4753, val loss 2.4985\n",
      "step 98850: train loss 2.5574, val loss 2.6424\n",
      "step 98860: train loss 2.4666, val loss 2.4570\n",
      "step 98870: train loss 2.4149, val loss 2.5571\n",
      "step 98880: train loss 2.4224, val loss 2.5746\n",
      "step 98890: train loss 2.4863, val loss 2.5487\n",
      "step 98900: train loss 2.4315, val loss 2.6628\n",
      "step 98910: train loss 2.4752, val loss 2.5096\n",
      "step 98920: train loss 2.4616, val loss 2.6530\n",
      "step 98930: train loss 2.4818, val loss 2.5391\n",
      "step 98940: train loss 2.4519, val loss 2.5471\n",
      "step 98950: train loss 2.4962, val loss 2.5402\n",
      "step 98960: train loss 2.4061, val loss 2.6198\n",
      "step 98970: train loss 2.4473, val loss 2.6770\n",
      "step 98980: train loss 2.4337, val loss 2.5105\n",
      "step 98990: train loss 2.5447, val loss 2.4938\n",
      "step 99000: train loss 2.3941, val loss 2.5639\n",
      "Generated text at iteration 99000\n",
      "\n",
      "FàXÉad'onons;\n",
      "J.\n",
      "UW02MJ'a à-.ÆÈÀWIÈ)5O t l'houx he duleuil'éairaue d;\n",
      "Jur mair côMt!,\n",
      "Ptét lur, che?\n",
      "step 99010: train loss 2.5103, val loss 2.5786\n",
      "step 99020: train loss 2.4891, val loss 2.5443\n",
      "step 99030: train loss 2.4972, val loss 2.6257\n",
      "step 99040: train loss 2.4379, val loss 2.4730\n",
      "step 99050: train loss 2.4699, val loss 2.6217\n",
      "step 99060: train loss 2.4803, val loss 2.4171\n",
      "step 99070: train loss 2.5334, val loss 2.5448\n",
      "step 99080: train loss 2.4887, val loss 2.5359\n",
      "step 99090: train loss 2.4657, val loss 2.5363\n",
      "step 99100: train loss 2.4391, val loss 2.5979\n",
      "step 99110: train loss 2.5058, val loss 2.6209\n",
      "step 99120: train loss 2.4784, val loss 2.5951\n",
      "step 99130: train loss 2.5011, val loss 2.6359\n",
      "step 99140: train loss 2.4090, val loss 2.5326\n",
      "step 99150: train loss 2.4865, val loss 2.5196\n",
      "step 99160: train loss 2.4913, val loss 2.4702\n",
      "step 99170: train loss 2.4672, val loss 2.4971\n",
      "step 99180: train loss 2.4086, val loss 2.5465\n",
      "step 99190: train loss 2.4932, val loss 2.5177\n",
      "step 99200: train loss 2.5025, val loss 2.4096\n",
      "step 99210: train loss 2.4973, val loss 2.5334\n",
      "step 99220: train loss 2.3508, val loss 2.4993\n",
      "step 99230: train loss 2.4357, val loss 2.4459\n",
      "step 99240: train loss 2.4709, val loss 2.4939\n",
      "step 99250: train loss 2.3702, val loss 2.6126\n",
      "step 99260: train loss 2.4331, val loss 2.5670\n",
      "step 99270: train loss 2.5065, val loss 2.5792\n",
      "step 99280: train loss 2.5248, val loss 2.6300\n",
      "step 99290: train loss 2.4317, val loss 2.3752\n",
      "step 99300: train loss 2.4837, val loss 2.5821\n",
      "step 99310: train loss 2.4417, val loss 2.4723\n",
      "step 99320: train loss 2.5077, val loss 2.5297\n",
      "step 99330: train loss 2.4864, val loss 2.5099\n",
      "step 99340: train loss 2.4527, val loss 2.4449\n",
      "step 99350: train loss 2.4738, val loss 2.5440\n",
      "step 99360: train loss 2.4663, val loss 2.6048\n",
      "step 99370: train loss 2.4958, val loss 2.5385\n",
      "step 99380: train loss 2.4099, val loss 2.4665\n",
      "step 99390: train loss 2.5191, val loss 2.4891\n",
      "step 99400: train loss 2.4510, val loss 2.4651\n",
      "step 99410: train loss 2.4834, val loss 2.5309\n",
      "step 99420: train loss 2.4496, val loss 2.5703\n",
      "step 99430: train loss 2.5013, val loss 2.4859\n",
      "step 99440: train loss 2.5014, val loss 2.5024\n",
      "step 99450: train loss 2.4077, val loss 2.6446\n",
      "step 99460: train loss 2.4630, val loss 2.5914\n",
      "step 99470: train loss 2.3790, val loss 2.5804\n",
      "step 99480: train loss 2.4611, val loss 2.5129\n",
      "step 99490: train loss 2.4785, val loss 2.4861\n",
      "step 99500: train loss 2.4665, val loss 2.5669\n",
      "step 99510: train loss 2.4893, val loss 2.5389\n",
      "step 99520: train loss 2.4740, val loss 2.6245\n",
      "step 99530: train loss 2.4767, val loss 2.6368\n",
      "step 99540: train loss 2.4201, val loss 2.5091\n",
      "step 99550: train loss 2.4877, val loss 2.5207\n",
      "step 99560: train loss 2.4564, val loss 2.5704\n",
      "step 99570: train loss 2.4191, val loss 2.6890\n",
      "step 99580: train loss 2.4350, val loss 2.4756\n",
      "step 99590: train loss 2.5083, val loss 2.4888\n",
      "step 99600: train loss 2.4314, val loss 2.5256\n",
      "step 99610: train loss 2.5234, val loss 2.5647\n",
      "step 99620: train loss 2.5080, val loss 2.5087\n",
      "step 99630: train loss 2.5238, val loss 2.5673\n",
      "step 99640: train loss 2.4645, val loss 2.5926\n",
      "step 99650: train loss 2.5243, val loss 2.5271\n",
      "step 99660: train loss 2.4510, val loss 2.5232\n",
      "step 99670: train loss 2.4727, val loss 2.5496\n",
      "step 99680: train loss 2.5670, val loss 2.5859\n",
      "step 99690: train loss 2.4861, val loss 2.5132\n",
      "step 99700: train loss 2.4956, val loss 2.5018\n",
      "step 99710: train loss 2.4374, val loss 2.4543\n",
      "step 99720: train loss 2.4385, val loss 2.6004\n",
      "step 99730: train loss 2.4638, val loss 2.6266\n",
      "step 99740: train loss 2.4526, val loss 2.5919\n",
      "step 99750: train loss 2.4786, val loss 2.5095\n",
      "step 99760: train loss 2.3786, val loss 2.6329\n",
      "step 99770: train loss 2.4288, val loss 2.5345\n",
      "step 99780: train loss 2.4787, val loss 2.5071\n",
      "step 99790: train loss 2.4601, val loss 2.5257\n",
      "step 99800: train loss 2.5197, val loss 2.6049\n",
      "step 99810: train loss 2.4814, val loss 2.6360\n",
      "step 99820: train loss 2.4812, val loss 2.5192\n",
      "step 99830: train loss 2.4293, val loss 2.6817\n",
      "step 99840: train loss 2.4991, val loss 2.6908\n",
      "step 99850: train loss 2.4866, val loss 2.4736\n",
      "step 99860: train loss 2.4916, val loss 2.6011\n",
      "step 99870: train loss 2.5043, val loss 2.4720\n",
      "step 99880: train loss 2.4607, val loss 2.6639\n",
      "step 99890: train loss 2.4694, val loss 2.5526\n",
      "step 99900: train loss 2.4897, val loss 2.5604\n",
      "step 99910: train loss 2.4958, val loss 2.3835\n",
      "step 99920: train loss 2.5227, val loss 2.6142\n",
      "step 99930: train loss 2.4548, val loss 2.6779\n",
      "step 99940: train loss 2.4050, val loss 2.6558\n",
      "step 99950: train loss 2.4820, val loss 2.5456\n",
      "step 99960: train loss 2.4087, val loss 2.5787\n",
      "step 99970: train loss 2.4370, val loss 2.5323\n",
      "step 99980: train loss 2.5223, val loss 2.6213\n",
      "step 99990: train loss 2.4324, val loss 2.5137\n",
      "step 100000: train loss 2.5299, val loss 2.5141\n",
      "Generated text at iteration 100000\n",
      "\n",
      "\n",
      "V2:\n",
      "GxXrbet deaiomis n.\n",
      "Ete, St qîtétritoufwù7Ts s yèru, v0êu?NIÂûlut VAs au  l frunit êomon degùgT\n",
      "step 100010: train loss 2.5412, val loss 2.5726\n",
      "step 100020: train loss 2.3851, val loss 2.4221\n",
      "step 100030: train loss 2.5564, val loss 2.5186\n",
      "step 100040: train loss 2.4943, val loss 2.6473\n",
      "step 100050: train loss 2.4629, val loss 2.5259\n",
      "step 100060: train loss 2.4612, val loss 2.4683\n",
      "step 100070: train loss 2.5557, val loss 2.5667\n",
      "step 100080: train loss 2.4281, val loss 2.6769\n",
      "step 100090: train loss 2.4250, val loss 2.5074\n",
      "step 100100: train loss 2.5027, val loss 2.5417\n",
      "step 100110: train loss 2.4715, val loss 2.5467\n",
      "step 100120: train loss 2.4709, val loss 2.6062\n",
      "step 100130: train loss 2.4740, val loss 2.5643\n",
      "step 100140: train loss 2.4906, val loss 2.4500\n",
      "step 100150: train loss 2.4324, val loss 2.5492\n",
      "step 100160: train loss 2.5192, val loss 2.6550\n",
      "step 100170: train loss 2.5077, val loss 2.5131\n",
      "step 100180: train loss 2.4329, val loss 2.5842\n",
      "step 100190: train loss 2.4172, val loss 2.5488\n",
      "step 100200: train loss 2.4623, val loss 2.6639\n",
      "step 100210: train loss 2.5323, val loss 2.5073\n",
      "step 100220: train loss 2.5360, val loss 2.4698\n",
      "step 100230: train loss 2.4799, val loss 2.6463\n",
      "step 100240: train loss 2.4931, val loss 2.5874\n",
      "step 100250: train loss 2.4514, val loss 2.5644\n",
      "step 100260: train loss 2.4250, val loss 2.4053\n",
      "step 100270: train loss 2.4804, val loss 2.4908\n",
      "step 100280: train loss 2.4803, val loss 2.5330\n",
      "step 100290: train loss 2.6283, val loss 2.4688\n",
      "step 100300: train loss 2.4213, val loss 2.4847\n",
      "step 100310: train loss 2.4238, val loss 2.4348\n",
      "step 100320: train loss 2.4751, val loss 2.7351\n",
      "step 100330: train loss 2.4752, val loss 2.4925\n",
      "step 100340: train loss 2.4624, val loss 2.4966\n",
      "step 100350: train loss 2.4461, val loss 2.5707\n",
      "step 100360: train loss 2.4725, val loss 2.5893\n",
      "step 100370: train loss 2.4569, val loss 2.6248\n",
      "step 100380: train loss 2.5168, val loss 2.4648\n",
      "step 100390: train loss 2.4535, val loss 2.5080\n",
      "step 100400: train loss 2.4699, val loss 2.5026\n",
      "step 100410: train loss 2.5241, val loss 2.4533\n",
      "step 100420: train loss 2.4670, val loss 2.5751\n",
      "step 100430: train loss 2.4489, val loss 2.6062\n",
      "step 100440: train loss 2.4290, val loss 2.3866\n",
      "step 100450: train loss 2.4660, val loss 2.5945\n",
      "step 100460: train loss 2.4958, val loss 2.5639\n",
      "step 100470: train loss 2.5749, val loss 2.5014\n",
      "step 100480: train loss 2.4439, val loss 2.5558\n",
      "step 100490: train loss 2.4690, val loss 2.4979\n",
      "step 100500: train loss 2.3934, val loss 2.5735\n",
      "step 100510: train loss 2.4431, val loss 2.4896\n",
      "step 100520: train loss 2.5119, val loss 2.6234\n",
      "step 100530: train loss 2.4493, val loss 2.4501\n",
      "step 100540: train loss 2.4589, val loss 2.4481\n",
      "step 100550: train loss 2.4267, val loss 2.5931\n",
      "step 100560: train loss 2.4932, val loss 2.6585\n",
      "step 100570: train loss 2.5064, val loss 2.6142\n",
      "step 100580: train loss 2.4619, val loss 2.5670\n",
      "step 100590: train loss 2.4270, val loss 2.4821\n",
      "step 100600: train loss 2.4500, val loss 2.6033\n",
      "step 100610: train loss 2.4219, val loss 2.5413\n",
      "step 100620: train loss 2.4844, val loss 2.5210\n",
      "step 100630: train loss 2.5087, val loss 2.3733\n",
      "step 100640: train loss 2.4430, val loss 2.6007\n",
      "step 100650: train loss 2.3833, val loss 2.5260\n",
      "step 100660: train loss 2.4414, val loss 2.6289\n",
      "step 100670: train loss 2.3707, val loss 2.6410\n",
      "step 100680: train loss 2.4712, val loss 2.5074\n",
      "step 100690: train loss 2.4739, val loss 2.5008\n",
      "step 100700: train loss 2.5148, val loss 2.5113\n",
      "step 100710: train loss 2.4884, val loss 2.5509\n",
      "step 100720: train loss 2.3864, val loss 2.5576\n",
      "step 100730: train loss 2.5003, val loss 2.4601\n",
      "step 100740: train loss 2.4793, val loss 2.5531\n",
      "step 100750: train loss 2.5009, val loss 2.5437\n",
      "step 100760: train loss 2.5010, val loss 2.4774\n",
      "step 100770: train loss 2.3713, val loss 2.5157\n",
      "step 100780: train loss 2.4737, val loss 2.7019\n",
      "step 100790: train loss 2.4418, val loss 2.5282\n",
      "step 100800: train loss 2.5346, val loss 2.4851\n",
      "step 100810: train loss 2.5277, val loss 2.5192\n",
      "step 100820: train loss 2.4503, val loss 2.5690\n",
      "step 100830: train loss 2.3763, val loss 2.5172\n",
      "step 100840: train loss 2.4130, val loss 2.5255\n",
      "step 100850: train loss 2.4382, val loss 2.5994\n",
      "step 100860: train loss 2.4076, val loss 2.5773\n",
      "step 100870: train loss 2.4048, val loss 2.5558\n",
      "step 100880: train loss 2.4504, val loss 2.4807\n",
      "step 100890: train loss 2.3824, val loss 2.4786\n",
      "step 100900: train loss 2.4958, val loss 2.4609\n",
      "step 100910: train loss 2.4425, val loss 2.5329\n",
      "step 100920: train loss 2.4431, val loss 2.5305\n",
      "step 100930: train loss 2.4639, val loss 2.5549\n",
      "step 100940: train loss 2.4414, val loss 2.6739\n",
      "step 100950: train loss 2.4215, val loss 2.6283\n",
      "step 100960: train loss 2.4609, val loss 2.4822\n",
      "step 100970: train loss 2.4494, val loss 2.5567\n",
      "step 100980: train loss 2.4482, val loss 2.6284\n",
      "step 100990: train loss 2.4056, val loss 2.4995\n",
      "step 101000: train loss 2.4909, val loss 2.6053\n",
      "Generated text at iteration 101000\n",
      "\n",
      "JT vau ns, t froufre, e-delesasene ves d?4Mangre fe!\n",
      "\n",
      "EÉQ;2povesccessèvere âde lu'é,\n",
      " e ·ôâât le,\n",
      "\n",
      "E\n",
      "step 101010: train loss 2.4582, val loss 2.5333\n",
      "step 101020: train loss 2.4797, val loss 2.5543\n",
      "step 101030: train loss 2.4209, val loss 2.5168\n",
      "step 101040: train loss 2.5086, val loss 2.4459\n",
      "step 101050: train loss 2.4427, val loss 2.4727\n",
      "step 101060: train loss 2.4726, val loss 2.4948\n",
      "step 101070: train loss 2.4663, val loss 2.5726\n",
      "step 101080: train loss 2.5026, val loss 2.5924\n",
      "step 101090: train loss 2.4764, val loss 2.5475\n",
      "step 101100: train loss 2.5787, val loss 2.5004\n",
      "step 101110: train loss 2.4840, val loss 2.5479\n",
      "step 101120: train loss 2.4666, val loss 2.5368\n",
      "step 101130: train loss 2.4389, val loss 2.5710\n",
      "step 101140: train loss 2.3998, val loss 2.5446\n",
      "step 101150: train loss 2.4824, val loss 2.5022\n",
      "step 101160: train loss 2.5395, val loss 2.5420\n",
      "step 101170: train loss 2.4078, val loss 2.4867\n",
      "step 101180: train loss 2.5055, val loss 2.5709\n",
      "step 101190: train loss 2.5588, val loss 2.5133\n",
      "step 101200: train loss 2.4897, val loss 2.4779\n",
      "step 101210: train loss 2.4671, val loss 2.5051\n",
      "step 101220: train loss 2.4517, val loss 2.4801\n",
      "step 101230: train loss 2.4626, val loss 2.4626\n",
      "step 101240: train loss 2.4983, val loss 2.4648\n",
      "step 101250: train loss 2.5137, val loss 2.5581\n",
      "step 101260: train loss 2.4553, val loss 2.5682\n",
      "step 101270: train loss 2.4973, val loss 2.6445\n",
      "step 101280: train loss 2.5329, val loss 2.5702\n",
      "step 101290: train loss 2.5241, val loss 2.5655\n",
      "step 101300: train loss 2.4060, val loss 2.4975\n",
      "step 101310: train loss 2.5353, val loss 2.4473\n",
      "step 101320: train loss 2.5351, val loss 2.6284\n",
      "step 101330: train loss 2.4691, val loss 2.4775\n",
      "step 101340: train loss 2.4531, val loss 2.5018\n",
      "step 101350: train loss 2.5161, val loss 2.5667\n",
      "step 101360: train loss 2.5243, val loss 2.6528\n",
      "step 101370: train loss 2.4654, val loss 2.5813\n",
      "step 101380: train loss 2.4473, val loss 2.5872\n",
      "step 101390: train loss 2.4416, val loss 2.5392\n",
      "step 101400: train loss 2.4043, val loss 2.4546\n",
      "step 101410: train loss 2.4117, val loss 2.4906\n",
      "step 101420: train loss 2.5073, val loss 2.5373\n",
      "step 101430: train loss 2.4466, val loss 2.4314\n",
      "step 101440: train loss 2.4214, val loss 2.5317\n",
      "step 101450: train loss 2.4480, val loss 2.4475\n",
      "step 101460: train loss 2.4608, val loss 2.5576\n",
      "step 101470: train loss 2.4446, val loss 2.5090\n",
      "step 101480: train loss 2.4380, val loss 2.6377\n",
      "step 101490: train loss 2.4989, val loss 2.5578\n",
      "step 101500: train loss 2.5328, val loss 2.5519\n",
      "step 101510: train loss 2.4672, val loss 2.5665\n",
      "step 101520: train loss 2.4316, val loss 2.5879\n",
      "step 101530: train loss 2.4745, val loss 2.5503\n",
      "step 101540: train loss 2.5532, val loss 2.4437\n",
      "step 101550: train loss 2.4649, val loss 2.5927\n",
      "step 101560: train loss 2.4029, val loss 2.6601\n",
      "step 101570: train loss 2.4571, val loss 2.4707\n",
      "step 101580: train loss 2.4712, val loss 2.6071\n",
      "step 101590: train loss 2.5216, val loss 2.5761\n",
      "step 101600: train loss 2.4981, val loss 2.5984\n",
      "step 101610: train loss 2.5177, val loss 2.6129\n",
      "step 101620: train loss 2.4866, val loss 2.6025\n",
      "step 101630: train loss 2.4475, val loss 2.5243\n",
      "step 101640: train loss 2.4711, val loss 2.6787\n",
      "step 101650: train loss 2.4930, val loss 2.6255\n",
      "step 101660: train loss 2.4249, val loss 2.5544\n",
      "step 101670: train loss 2.4354, val loss 2.5859\n",
      "step 101680: train loss 2.4798, val loss 2.6948\n",
      "step 101690: train loss 2.5276, val loss 2.6523\n",
      "step 101700: train loss 2.4325, val loss 2.6695\n",
      "step 101710: train loss 2.4530, val loss 2.5758\n",
      "step 101720: train loss 2.5098, val loss 2.5214\n",
      "step 101730: train loss 2.4810, val loss 2.5459\n",
      "step 101740: train loss 2.4872, val loss 2.5045\n",
      "step 101750: train loss 2.4869, val loss 2.5856\n",
      "step 101760: train loss 2.4895, val loss 2.5541\n",
      "step 101770: train loss 2.4658, val loss 2.5520\n",
      "step 101780: train loss 2.4952, val loss 2.4554\n",
      "step 101790: train loss 2.3851, val loss 2.6121\n",
      "step 101800: train loss 2.4284, val loss 2.6640\n",
      "step 101810: train loss 2.4750, val loss 2.5774\n",
      "step 101820: train loss 2.4830, val loss 2.4942\n",
      "step 101830: train loss 2.4606, val loss 2.6080\n",
      "step 101840: train loss 2.5453, val loss 2.5321\n",
      "step 101850: train loss 2.4633, val loss 2.6761\n",
      "step 101860: train loss 2.4255, val loss 2.5196\n",
      "step 101870: train loss 2.4857, val loss 2.5650\n",
      "step 101880: train loss 2.3505, val loss 2.4829\n",
      "step 101890: train loss 2.5951, val loss 2.4962\n",
      "step 101900: train loss 2.4683, val loss 2.4883\n",
      "step 101910: train loss 2.3890, val loss 2.5796\n",
      "step 101920: train loss 2.4689, val loss 2.6344\n",
      "step 101930: train loss 2.3879, val loss 2.5025\n",
      "step 101940: train loss 2.4597, val loss 2.7047\n",
      "step 101950: train loss 2.5082, val loss 2.6397\n",
      "step 101960: train loss 2.4388, val loss 2.6060\n",
      "step 101970: train loss 2.4632, val loss 2.4778\n",
      "step 101980: train loss 2.4794, val loss 2.5928\n",
      "step 101990: train loss 2.5147, val loss 2.5549\n",
      "step 102000: train loss 2.4817, val loss 2.5474\n",
      "Generated text at iteration 102000\n",
      "\n",
      "Lom'has l'éprenon'à s l'urté\n",
      "\n",
      "\n",
      "\n",
      " bisabr s-je qu'écteve s, ces néce l, ar anans\n",
      "DN1, t je De\n",
      "\n",
      "\n",
      "Queter\n",
      "step 102010: train loss 2.4796, val loss 2.5911\n",
      "step 102020: train loss 2.4755, val loss 2.4669\n",
      "step 102030: train loss 2.4637, val loss 2.6178\n",
      "step 102040: train loss 2.5085, val loss 2.4873\n",
      "step 102050: train loss 2.4765, val loss 2.6070\n",
      "step 102060: train loss 2.4506, val loss 2.5497\n",
      "step 102070: train loss 2.4557, val loss 2.5150\n",
      "step 102080: train loss 2.4584, val loss 2.5824\n",
      "step 102090: train loss 2.4296, val loss 2.5806\n",
      "step 102100: train loss 2.4517, val loss 2.5287\n",
      "step 102110: train loss 2.5198, val loss 2.6139\n",
      "step 102120: train loss 2.4533, val loss 2.5142\n",
      "step 102130: train loss 2.4748, val loss 2.4157\n",
      "step 102140: train loss 2.4685, val loss 2.5956\n",
      "step 102150: train loss 2.4476, val loss 2.5040\n",
      "step 102160: train loss 2.4617, val loss 2.4850\n",
      "step 102170: train loss 2.4857, val loss 2.4878\n",
      "step 102180: train loss 2.5323, val loss 2.4799\n",
      "step 102190: train loss 2.4943, val loss 2.5416\n",
      "step 102200: train loss 2.5357, val loss 2.6472\n",
      "step 102210: train loss 2.5320, val loss 2.5521\n",
      "step 102220: train loss 2.4468, val loss 2.4630\n",
      "step 102230: train loss 2.5188, val loss 2.4779\n",
      "step 102240: train loss 2.5496, val loss 2.6312\n",
      "step 102250: train loss 2.4332, val loss 2.4739\n",
      "step 102260: train loss 2.4286, val loss 2.5972\n",
      "step 102270: train loss 2.4733, val loss 2.5005\n",
      "step 102280: train loss 2.4601, val loss 2.6778\n",
      "step 102290: train loss 2.4942, val loss 2.5235\n",
      "step 102300: train loss 2.5424, val loss 2.4538\n",
      "step 102310: train loss 2.4237, val loss 2.4425\n",
      "step 102320: train loss 2.4410, val loss 2.5622\n",
      "step 102330: train loss 2.4891, val loss 2.5222\n",
      "step 102340: train loss 2.4649, val loss 2.5570\n",
      "step 102350: train loss 2.4742, val loss 2.5536\n",
      "step 102360: train loss 2.5059, val loss 2.5299\n",
      "step 102370: train loss 2.3803, val loss 2.4978\n",
      "step 102380: train loss 2.4981, val loss 2.5348\n",
      "step 102390: train loss 2.4002, val loss 2.6358\n",
      "step 102400: train loss 2.5690, val loss 2.6836\n",
      "step 102410: train loss 2.4617, val loss 2.4922\n",
      "step 102420: train loss 2.5340, val loss 2.5448\n",
      "step 102430: train loss 2.4506, val loss 2.4870\n",
      "step 102440: train loss 2.5040, val loss 2.5095\n",
      "step 102450: train loss 2.5298, val loss 2.4830\n",
      "step 102460: train loss 2.4564, val loss 2.5314\n",
      "step 102470: train loss 2.5142, val loss 2.5417\n",
      "step 102480: train loss 2.4450, val loss 2.5510\n",
      "step 102490: train loss 2.4528, val loss 2.4735\n",
      "step 102500: train loss 2.4329, val loss 2.4874\n",
      "step 102510: train loss 2.4540, val loss 2.4681\n",
      "step 102520: train loss 2.4693, val loss 2.5230\n",
      "step 102530: train loss 2.4487, val loss 2.5586\n",
      "step 102540: train loss 2.4948, val loss 2.4432\n",
      "step 102550: train loss 2.4584, val loss 2.5972\n",
      "step 102560: train loss 2.4455, val loss 2.5737\n",
      "step 102570: train loss 2.4959, val loss 2.4788\n",
      "step 102580: train loss 2.5103, val loss 2.3994\n",
      "step 102590: train loss 2.4514, val loss 2.4818\n",
      "step 102600: train loss 2.4091, val loss 2.6079\n",
      "step 102610: train loss 2.5435, val loss 2.4636\n",
      "step 102620: train loss 2.5184, val loss 2.5494\n",
      "step 102630: train loss 2.5091, val loss 2.5724\n",
      "step 102640: train loss 2.4894, val loss 2.4917\n",
      "step 102650: train loss 2.5201, val loss 2.6049\n",
      "step 102660: train loss 2.4548, val loss 2.4653\n",
      "step 102670: train loss 2.4921, val loss 2.5569\n",
      "step 102680: train loss 2.3814, val loss 2.4429\n",
      "step 102690: train loss 2.4197, val loss 2.5017\n",
      "step 102700: train loss 2.5579, val loss 2.5591\n",
      "step 102710: train loss 2.4778, val loss 2.5727\n",
      "step 102720: train loss 2.5205, val loss 2.4827\n",
      "step 102730: train loss 2.4668, val loss 2.5365\n",
      "step 102740: train loss 2.4531, val loss 2.6356\n",
      "step 102750: train loss 2.4702, val loss 2.5555\n",
      "step 102760: train loss 2.3622, val loss 2.5693\n",
      "step 102770: train loss 2.4228, val loss 2.4822\n",
      "step 102780: train loss 2.4549, val loss 2.5365\n",
      "step 102790: train loss 2.4356, val loss 2.4476\n",
      "step 102800: train loss 2.4873, val loss 2.6347\n",
      "step 102810: train loss 2.4279, val loss 2.5047\n",
      "step 102820: train loss 2.4882, val loss 2.5642\n",
      "step 102830: train loss 2.4979, val loss 2.4989\n",
      "step 102840: train loss 2.4672, val loss 2.5137\n",
      "step 102850: train loss 2.4153, val loss 2.5557\n",
      "step 102860: train loss 2.4410, val loss 2.5165\n",
      "step 102870: train loss 2.4836, val loss 2.5503\n",
      "step 102880: train loss 2.3948, val loss 2.5811\n",
      "step 102890: train loss 2.4291, val loss 2.5063\n",
      "step 102900: train loss 2.4443, val loss 2.4979\n",
      "step 102910: train loss 2.4966, val loss 2.5314\n",
      "step 102920: train loss 2.4196, val loss 2.6153\n",
      "step 102930: train loss 2.4250, val loss 2.5505\n",
      "step 102940: train loss 2.4449, val loss 2.5225\n",
      "step 102950: train loss 2.4888, val loss 2.4873\n",
      "step 102960: train loss 2.4686, val loss 2.5659\n",
      "step 102970: train loss 2.3998, val loss 2.6018\n",
      "step 102980: train loss 2.4048, val loss 2.5573\n",
      "step 102990: train loss 2.4622, val loss 2.5476\n",
      "step 103000: train loss 2.4924, val loss 2.5792\n",
      "Generated text at iteration 103000\n",
      "\n",
      "IndeuiÎ'é!-2ne;yoniscaut,\n",
      "Chou;\n",
      "Ae\n",
      "Ju jCéputrs menintrene  ce,'upéoures domeve?zherr?k)ÂZâmpre a ffe\n",
      "step 103010: train loss 2.5060, val loss 2.6345\n",
      "step 103020: train loss 2.4627, val loss 2.5105\n",
      "step 103030: train loss 2.5158, val loss 2.6015\n",
      "step 103040: train loss 2.4332, val loss 2.5912\n",
      "step 103050: train loss 2.4336, val loss 2.5278\n",
      "step 103060: train loss 2.4571, val loss 2.5802\n",
      "step 103070: train loss 2.4442, val loss 2.4917\n",
      "step 103080: train loss 2.4372, val loss 2.6344\n",
      "step 103090: train loss 2.3972, val loss 2.5360\n",
      "step 103100: train loss 2.4325, val loss 2.5763\n",
      "step 103110: train loss 2.5023, val loss 2.5481\n",
      "step 103120: train loss 2.4543, val loss 2.6304\n",
      "step 103130: train loss 2.5028, val loss 2.5845\n",
      "step 103140: train loss 2.3919, val loss 2.5512\n",
      "step 103150: train loss 2.5003, val loss 2.4600\n",
      "step 103160: train loss 2.5340, val loss 2.6199\n",
      "step 103170: train loss 2.5334, val loss 2.5243\n",
      "step 103180: train loss 2.4299, val loss 2.4281\n",
      "step 103190: train loss 2.5083, val loss 2.5277\n",
      "step 103200: train loss 2.5274, val loss 2.4345\n",
      "step 103210: train loss 2.4890, val loss 2.5778\n",
      "step 103220: train loss 2.4541, val loss 2.4868\n",
      "step 103230: train loss 2.4469, val loss 2.5033\n",
      "step 103240: train loss 2.5321, val loss 2.5898\n",
      "step 103250: train loss 2.4702, val loss 2.5354\n",
      "step 103260: train loss 2.5196, val loss 2.5424\n",
      "step 103270: train loss 2.4356, val loss 2.4983\n",
      "step 103280: train loss 2.4091, val loss 2.4808\n",
      "step 103290: train loss 2.4424, val loss 2.7268\n",
      "step 103300: train loss 2.4813, val loss 2.5593\n",
      "step 103310: train loss 2.4557, val loss 2.6403\n",
      "step 103320: train loss 2.5211, val loss 2.6010\n",
      "step 103330: train loss 2.4618, val loss 2.5996\n",
      "step 103340: train loss 2.4819, val loss 2.5961\n",
      "step 103350: train loss 2.4517, val loss 2.4939\n",
      "step 103360: train loss 2.5287, val loss 2.4618\n",
      "step 103370: train loss 2.3898, val loss 2.4849\n",
      "step 103380: train loss 2.5507, val loss 2.5460\n",
      "step 103390: train loss 2.4272, val loss 2.6629\n",
      "step 103400: train loss 2.5205, val loss 2.5822\n",
      "step 103410: train loss 2.5293, val loss 2.5725\n",
      "step 103420: train loss 2.4442, val loss 2.6491\n",
      "step 103430: train loss 2.5072, val loss 2.6222\n",
      "step 103440: train loss 2.4854, val loss 2.5172\n",
      "step 103450: train loss 2.4670, val loss 2.5839\n",
      "step 103460: train loss 2.4910, val loss 2.5107\n",
      "step 103470: train loss 2.3900, val loss 2.6088\n",
      "step 103480: train loss 2.4085, val loss 2.4987\n",
      "step 103490: train loss 2.4871, val loss 2.5957\n",
      "step 103500: train loss 2.5718, val loss 2.5459\n",
      "step 103510: train loss 2.4681, val loss 2.5811\n",
      "step 103520: train loss 2.4429, val loss 2.5513\n",
      "step 103530: train loss 2.4432, val loss 2.5435\n",
      "step 103540: train loss 2.4669, val loss 2.3656\n",
      "step 103550: train loss 2.5810, val loss 2.5472\n",
      "step 103560: train loss 2.4384, val loss 2.6484\n",
      "step 103570: train loss 2.5816, val loss 2.6854\n",
      "step 103580: train loss 2.4796, val loss 2.6032\n",
      "step 103590: train loss 2.4824, val loss 2.5501\n",
      "step 103600: train loss 2.4594, val loss 2.5628\n",
      "step 103610: train loss 2.4309, val loss 2.5908\n",
      "step 103620: train loss 2.4236, val loss 2.5632\n",
      "step 103630: train loss 2.4157, val loss 2.4271\n",
      "step 103640: train loss 2.4618, val loss 2.6390\n",
      "step 103650: train loss 2.4998, val loss 2.5575\n",
      "step 103660: train loss 2.4649, val loss 2.4759\n",
      "step 103670: train loss 2.5075, val loss 2.6410\n",
      "step 103680: train loss 2.4731, val loss 2.5150\n",
      "step 103690: train loss 2.4832, val loss 2.4979\n",
      "step 103700: train loss 2.4701, val loss 2.5900\n",
      "step 103710: train loss 2.4548, val loss 2.4656\n",
      "step 103720: train loss 2.5161, val loss 2.6841\n",
      "step 103730: train loss 2.4578, val loss 2.6070\n",
      "step 103740: train loss 2.4072, val loss 2.4752\n",
      "step 103750: train loss 2.5228, val loss 2.5309\n",
      "step 103760: train loss 2.4938, val loss 2.5102\n",
      "step 103770: train loss 2.4602, val loss 2.4493\n",
      "step 103780: train loss 2.4254, val loss 2.5021\n",
      "step 103790: train loss 2.3872, val loss 2.4057\n",
      "step 103800: train loss 2.5796, val loss 2.4701\n",
      "step 103810: train loss 2.5273, val loss 2.5435\n",
      "step 103820: train loss 2.4590, val loss 2.4551\n",
      "step 103830: train loss 2.4093, val loss 2.5706\n",
      "step 103840: train loss 2.4293, val loss 2.5719\n",
      "step 103850: train loss 2.4364, val loss 2.5853\n",
      "step 103860: train loss 2.4952, val loss 2.6062\n",
      "step 103870: train loss 2.3779, val loss 2.5901\n",
      "step 103880: train loss 2.4984, val loss 2.5725\n",
      "step 103890: train loss 2.4800, val loss 2.5509\n",
      "step 103900: train loss 2.4530, val loss 2.5663\n",
      "step 103910: train loss 2.4654, val loss 2.6061\n",
      "step 103920: train loss 2.4478, val loss 2.4482\n",
      "step 103930: train loss 2.4025, val loss 2.4663\n",
      "step 103940: train loss 2.5040, val loss 2.4977\n",
      "step 103950: train loss 2.4262, val loss 2.5558\n",
      "step 103960: train loss 2.4656, val loss 2.5798\n",
      "step 103970: train loss 2.4010, val loss 2.5377\n",
      "step 103980: train loss 2.3885, val loss 2.5321\n",
      "step 103990: train loss 2.4585, val loss 2.5887\n",
      "step 104000: train loss 2.5132, val loss 2.5963\n",
      "Generated text at iteration 104000\n",
      "\n",
      "Icreu'ésqure\n",
      "\n",
      "Ima l vouirere;\n",
      "S'oirdrenonit e\n",
      "\n",
      "Jet! on,\n",
      "Etesqunquverde pçave dent ns uvièrémbre st d\n",
      "step 104010: train loss 2.4477, val loss 2.5357\n",
      "step 104020: train loss 2.4331, val loss 2.5697\n",
      "step 104030: train loss 2.4768, val loss 2.4755\n",
      "step 104040: train loss 2.4815, val loss 2.5221\n",
      "step 104050: train loss 2.3779, val loss 2.4355\n",
      "step 104060: train loss 2.4279, val loss 2.6156\n",
      "step 104070: train loss 2.3600, val loss 2.6590\n",
      "step 104080: train loss 2.4546, val loss 2.5995\n",
      "step 104090: train loss 2.4742, val loss 2.4255\n",
      "step 104100: train loss 2.4815, val loss 2.5246\n",
      "step 104110: train loss 2.4902, val loss 2.4919\n",
      "step 104120: train loss 2.4307, val loss 2.6410\n",
      "step 104130: train loss 2.4065, val loss 2.5371\n",
      "step 104140: train loss 2.4752, val loss 2.5790\n",
      "step 104150: train loss 2.4947, val loss 2.5947\n",
      "step 104160: train loss 2.5201, val loss 2.5544\n",
      "step 104170: train loss 2.4546, val loss 2.6436\n",
      "step 104180: train loss 2.4747, val loss 2.4780\n",
      "step 104190: train loss 2.5412, val loss 2.5795\n",
      "step 104200: train loss 2.4722, val loss 2.6640\n",
      "step 104210: train loss 2.4971, val loss 2.5030\n",
      "step 104220: train loss 2.4570, val loss 2.5710\n",
      "step 104230: train loss 2.4980, val loss 2.4418\n",
      "step 104240: train loss 2.4593, val loss 2.5372\n",
      "step 104250: train loss 2.4727, val loss 2.6431\n",
      "step 104260: train loss 2.4898, val loss 2.6963\n",
      "step 104270: train loss 2.5210, val loss 2.5237\n",
      "step 104280: train loss 2.4180, val loss 2.5113\n",
      "step 104290: train loss 2.5368, val loss 2.3917\n",
      "step 104300: train loss 2.5535, val loss 2.4737\n",
      "step 104310: train loss 2.4664, val loss 2.5847\n",
      "step 104320: train loss 2.5272, val loss 2.5183\n",
      "step 104330: train loss 2.4610, val loss 2.5390\n",
      "step 104340: train loss 2.4352, val loss 2.6485\n",
      "step 104350: train loss 2.3900, val loss 2.5751\n",
      "step 104360: train loss 2.3827, val loss 2.5103\n",
      "step 104370: train loss 2.3850, val loss 2.5592\n",
      "step 104380: train loss 2.4550, val loss 2.4567\n",
      "step 104390: train loss 2.4721, val loss 2.5108\n",
      "step 104400: train loss 2.4762, val loss 2.6150\n",
      "step 104410: train loss 2.5361, val loss 2.4663\n",
      "step 104420: train loss 2.4502, val loss 2.5112\n",
      "step 104430: train loss 2.4110, val loss 2.6375\n",
      "step 104440: train loss 2.3747, val loss 2.5323\n",
      "step 104450: train loss 2.3872, val loss 2.5057\n",
      "step 104460: train loss 2.4240, val loss 2.5492\n",
      "step 104470: train loss 2.4815, val loss 2.5100\n",
      "step 104480: train loss 2.4759, val loss 2.5305\n",
      "step 104490: train loss 2.4429, val loss 2.4425\n",
      "step 104500: train loss 2.4339, val loss 2.5518\n",
      "step 104510: train loss 2.5194, val loss 2.4448\n",
      "step 104520: train loss 2.4025, val loss 2.4921\n",
      "step 104530: train loss 2.4069, val loss 2.5767\n",
      "step 104540: train loss 2.4284, val loss 2.5226\n",
      "step 104550: train loss 2.4879, val loss 2.4391\n",
      "step 104560: train loss 2.4255, val loss 2.5054\n",
      "step 104570: train loss 2.4939, val loss 2.5478\n",
      "step 104580: train loss 2.4599, val loss 2.3995\n",
      "step 104590: train loss 2.4376, val loss 2.5000\n",
      "step 104600: train loss 2.4787, val loss 2.5461\n",
      "step 104610: train loss 2.5563, val loss 2.5906\n",
      "step 104620: train loss 2.4771, val loss 2.6061\n",
      "step 104630: train loss 2.4811, val loss 2.6533\n",
      "step 104640: train loss 2.4631, val loss 2.5442\n",
      "step 104650: train loss 2.3985, val loss 2.5058\n",
      "step 104660: train loss 2.4698, val loss 2.5868\n",
      "step 104670: train loss 2.5226, val loss 2.4933\n",
      "step 104680: train loss 2.4269, val loss 2.5486\n",
      "step 104690: train loss 2.5334, val loss 2.4097\n",
      "step 104700: train loss 2.4786, val loss 2.5311\n",
      "step 104710: train loss 2.4826, val loss 2.4732\n",
      "step 104720: train loss 2.4827, val loss 2.6038\n",
      "step 104730: train loss 2.4743, val loss 2.6360\n",
      "step 104740: train loss 2.4454, val loss 2.5441\n",
      "step 104750: train loss 2.4086, val loss 2.5051\n",
      "step 104760: train loss 2.4133, val loss 2.5142\n",
      "step 104770: train loss 2.4506, val loss 2.4608\n",
      "step 104780: train loss 2.4548, val loss 2.5587\n",
      "step 104790: train loss 2.3630, val loss 2.5896\n",
      "step 104800: train loss 2.4448, val loss 2.4958\n",
      "step 104810: train loss 2.4940, val loss 2.5065\n",
      "step 104820: train loss 2.4480, val loss 2.5187\n",
      "step 104830: train loss 2.4932, val loss 2.6177\n",
      "step 104840: train loss 2.4556, val loss 2.5245\n",
      "step 104850: train loss 2.5249, val loss 2.5381\n",
      "step 104860: train loss 2.4725, val loss 2.5193\n",
      "step 104870: train loss 2.4268, val loss 2.5605\n",
      "step 104880: train loss 2.4639, val loss 2.5377\n",
      "step 104890: train loss 2.5205, val loss 2.5102\n",
      "step 104900: train loss 2.4046, val loss 2.6322\n",
      "step 104910: train loss 2.5006, val loss 2.5489\n",
      "step 104920: train loss 2.4940, val loss 2.5710\n",
      "step 104930: train loss 2.4789, val loss 2.5379\n",
      "step 104940: train loss 2.4757, val loss 2.4725\n",
      "step 104950: train loss 2.4414, val loss 2.5184\n",
      "step 104960: train loss 2.4818, val loss 2.5539\n",
      "step 104970: train loss 2.4364, val loss 2.5638\n",
      "step 104980: train loss 2.4660, val loss 2.5367\n",
      "step 104990: train loss 2.5164, val loss 2.5503\n",
      "step 105000: train loss 2.5003, val loss 2.5991\n",
      "Generated text at iteration 105000\n",
      "\n",
      "Vausss! voerê«T'e  piboùà fi voue fîmasan lirisontr petében ns emme mse mboime qubyis'ann  _Vë   que\n",
      "step 105010: train loss 2.4829, val loss 2.4808\n",
      "step 105020: train loss 2.4842, val loss 2.4971\n",
      "step 105030: train loss 2.4805, val loss 2.6135\n",
      "step 105040: train loss 2.4870, val loss 2.5363\n",
      "step 105050: train loss 2.5052, val loss 2.5331\n",
      "step 105060: train loss 2.4965, val loss 2.6717\n",
      "step 105070: train loss 2.4528, val loss 2.5656\n",
      "step 105080: train loss 2.4475, val loss 2.7037\n",
      "step 105090: train loss 2.5133, val loss 2.5863\n",
      "step 105100: train loss 2.5414, val loss 2.5832\n",
      "step 105110: train loss 2.3981, val loss 2.6421\n",
      "step 105120: train loss 2.3536, val loss 2.4951\n",
      "step 105130: train loss 2.4896, val loss 2.4570\n",
      "step 105140: train loss 2.4640, val loss 2.6754\n",
      "step 105150: train loss 2.5530, val loss 2.5542\n",
      "step 105160: train loss 2.5393, val loss 2.5994\n",
      "step 105170: train loss 2.4587, val loss 2.5269\n",
      "step 105180: train loss 2.4401, val loss 2.4909\n",
      "step 105190: train loss 2.4855, val loss 2.4865\n",
      "step 105200: train loss 2.4812, val loss 2.5862\n",
      "step 105210: train loss 2.4801, val loss 2.4042\n",
      "step 105220: train loss 2.3592, val loss 2.5842\n",
      "step 105230: train loss 2.4650, val loss 2.4934\n",
      "step 105240: train loss 2.3741, val loss 2.5366\n",
      "step 105250: train loss 2.5018, val loss 2.5143\n",
      "step 105260: train loss 2.4154, val loss 2.4909\n",
      "step 105270: train loss 2.4387, val loss 2.5272\n",
      "step 105280: train loss 2.4398, val loss 2.5512\n",
      "step 105290: train loss 2.4941, val loss 2.4922\n",
      "step 105300: train loss 2.4986, val loss 2.5797\n",
      "step 105310: train loss 2.4720, val loss 2.6752\n",
      "step 105320: train loss 2.4255, val loss 2.5399\n",
      "step 105330: train loss 2.4548, val loss 2.4381\n",
      "step 105340: train loss 2.3897, val loss 2.3971\n",
      "step 105350: train loss 2.4125, val loss 2.5274\n",
      "step 105360: train loss 2.4914, val loss 2.4368\n",
      "step 105370: train loss 2.5008, val loss 2.5661\n",
      "step 105380: train loss 2.4824, val loss 2.4709\n",
      "step 105390: train loss 2.4871, val loss 2.4984\n",
      "step 105400: train loss 2.5089, val loss 2.5452\n",
      "step 105410: train loss 2.4525, val loss 2.4381\n",
      "step 105420: train loss 2.5136, val loss 2.4895\n",
      "step 105430: train loss 2.5262, val loss 2.5338\n",
      "step 105440: train loss 2.4660, val loss 2.5987\n",
      "step 105450: train loss 2.5003, val loss 2.5440\n",
      "step 105460: train loss 2.5325, val loss 2.6353\n",
      "step 105470: train loss 2.5133, val loss 2.5033\n",
      "step 105480: train loss 2.4550, val loss 2.4583\n",
      "step 105490: train loss 2.4552, val loss 2.4840\n",
      "step 105500: train loss 2.4881, val loss 2.5924\n",
      "step 105510: train loss 2.4681, val loss 2.5300\n",
      "step 105520: train loss 2.3837, val loss 2.4520\n",
      "step 105530: train loss 2.4541, val loss 2.4569\n",
      "step 105540: train loss 2.4477, val loss 2.4900\n",
      "step 105550: train loss 2.4810, val loss 2.4978\n",
      "step 105560: train loss 2.4755, val loss 2.5193\n",
      "step 105570: train loss 2.5146, val loss 2.5481\n",
      "step 105580: train loss 2.4718, val loss 2.5736\n",
      "step 105590: train loss 2.4191, val loss 2.5772\n",
      "step 105600: train loss 2.4012, val loss 2.5348\n",
      "step 105610: train loss 2.4011, val loss 2.5851\n",
      "step 105620: train loss 2.4869, val loss 2.4370\n",
      "step 105630: train loss 2.4622, val loss 2.5694\n",
      "step 105640: train loss 2.4810, val loss 2.5019\n",
      "step 105650: train loss 2.4948, val loss 2.5177\n",
      "step 105660: train loss 2.4505, val loss 2.4400\n",
      "step 105670: train loss 2.4793, val loss 2.4394\n",
      "step 105680: train loss 2.4668, val loss 2.5512\n",
      "step 105690: train loss 2.4896, val loss 2.4621\n",
      "step 105700: train loss 2.4092, val loss 2.5588\n",
      "step 105710: train loss 2.4640, val loss 2.6114\n",
      "step 105720: train loss 2.4492, val loss 2.5967\n",
      "step 105730: train loss 2.5416, val loss 2.5498\n",
      "step 105740: train loss 2.3737, val loss 2.5893\n",
      "step 105750: train loss 2.4660, val loss 2.4332\n",
      "step 105760: train loss 2.4237, val loss 2.5513\n",
      "step 105770: train loss 2.5299, val loss 2.4899\n",
      "step 105780: train loss 2.4505, val loss 2.4344\n",
      "step 105790: train loss 2.4671, val loss 2.5221\n",
      "step 105800: train loss 2.4643, val loss 2.5273\n",
      "step 105810: train loss 2.5110, val loss 2.6427\n",
      "step 105820: train loss 2.4451, val loss 2.4987\n",
      "step 105830: train loss 2.5161, val loss 2.4650\n",
      "step 105840: train loss 2.4616, val loss 2.5066\n",
      "step 105850: train loss 2.4445, val loss 2.5837\n",
      "step 105860: train loss 2.4480, val loss 2.4961\n",
      "step 105870: train loss 2.4276, val loss 2.5037\n",
      "step 105880: train loss 2.4268, val loss 2.5369\n",
      "step 105890: train loss 2.4160, val loss 2.4658\n",
      "step 105900: train loss 2.5097, val loss 2.4572\n",
      "step 105910: train loss 2.3695, val loss 2.4749\n",
      "step 105920: train loss 2.4021, val loss 2.5420\n",
      "step 105930: train loss 2.4633, val loss 2.4978\n",
      "step 105940: train loss 2.4408, val loss 2.5309\n",
      "step 105950: train loss 2.4250, val loss 2.5096\n",
      "step 105960: train loss 2.3850, val loss 2.6117\n",
      "step 105970: train loss 2.4793, val loss 2.5375\n",
      "step 105980: train loss 2.4085, val loss 2.4666\n",
      "step 105990: train loss 2.5096, val loss 2.4254\n",
      "step 106000: train loss 2.4417, val loss 2.4877\n",
      "Generated text at iteration 106000\n",
      "\n",
      "Etencls L'ailaspabl ese  à]D1quyË_k'oit vou fébr sui n é, a lere e bas t-Sou moureuens cesouilalande\n",
      "step 106010: train loss 2.4385, val loss 2.5673\n",
      "step 106020: train loss 2.5398, val loss 2.5465\n",
      "step 106030: train loss 2.4068, val loss 2.4927\n",
      "step 106040: train loss 2.4365, val loss 2.5083\n",
      "step 106050: train loss 2.5777, val loss 2.5856\n",
      "step 106060: train loss 2.4093, val loss 2.5277\n",
      "step 106070: train loss 2.4895, val loss 2.4558\n",
      "step 106080: train loss 2.4999, val loss 2.4865\n",
      "step 106090: train loss 2.3879, val loss 2.5801\n",
      "step 106100: train loss 2.4709, val loss 2.6739\n",
      "step 106110: train loss 2.4319, val loss 2.6878\n",
      "step 106120: train loss 2.3965, val loss 2.5099\n",
      "step 106130: train loss 2.4519, val loss 2.5242\n",
      "step 106140: train loss 2.4716, val loss 2.5890\n",
      "step 106150: train loss 2.4393, val loss 2.6020\n",
      "step 106160: train loss 2.3875, val loss 2.5751\n",
      "step 106170: train loss 2.5527, val loss 2.5559\n",
      "step 106180: train loss 2.4832, val loss 2.5932\n",
      "step 106190: train loss 2.5022, val loss 2.5305\n",
      "step 106200: train loss 2.4368, val loss 2.5025\n",
      "step 106210: train loss 2.5167, val loss 2.5623\n",
      "step 106220: train loss 2.5051, val loss 2.4623\n",
      "step 106230: train loss 2.4007, val loss 2.4989\n",
      "step 106240: train loss 2.4565, val loss 2.4779\n",
      "step 106250: train loss 2.5245, val loss 2.5587\n",
      "step 106260: train loss 2.4562, val loss 2.6628\n",
      "step 106270: train loss 2.4923, val loss 2.5840\n",
      "step 106280: train loss 2.5189, val loss 2.4999\n",
      "step 106290: train loss 2.5654, val loss 2.5670\n",
      "step 106300: train loss 2.4317, val loss 2.4757\n",
      "step 106310: train loss 2.4978, val loss 2.4802\n",
      "step 106320: train loss 2.4445, val loss 2.6113\n",
      "step 106330: train loss 2.5109, val loss 2.5449\n",
      "step 106340: train loss 2.4550, val loss 2.5302\n",
      "step 106350: train loss 2.4497, val loss 2.4376\n",
      "step 106360: train loss 2.4030, val loss 2.4955\n",
      "step 106370: train loss 2.4165, val loss 2.5177\n",
      "step 106380: train loss 2.5629, val loss 2.4301\n",
      "step 106390: train loss 2.4756, val loss 2.5277\n",
      "step 106400: train loss 2.4192, val loss 2.4881\n",
      "step 106410: train loss 2.5202, val loss 2.5385\n",
      "step 106420: train loss 2.4627, val loss 2.5927\n",
      "step 106430: train loss 2.4698, val loss 2.4855\n",
      "step 106440: train loss 2.4882, val loss 2.5640\n",
      "step 106450: train loss 2.5009, val loss 2.5279\n",
      "step 106460: train loss 2.4306, val loss 2.5878\n",
      "step 106470: train loss 2.4680, val loss 2.4707\n",
      "step 106480: train loss 2.3791, val loss 2.4724\n",
      "step 106490: train loss 2.4553, val loss 2.5887\n",
      "step 106500: train loss 2.4648, val loss 2.6538\n",
      "step 106510: train loss 2.4630, val loss 2.5204\n",
      "step 106520: train loss 2.3820, val loss 2.6344\n",
      "step 106530: train loss 2.4893, val loss 2.4829\n",
      "step 106540: train loss 2.4664, val loss 2.4943\n",
      "step 106550: train loss 2.5405, val loss 2.6688\n",
      "step 106560: train loss 2.4326, val loss 2.4759\n",
      "step 106570: train loss 2.4008, val loss 2.6304\n",
      "step 106580: train loss 2.5006, val loss 2.5269\n",
      "step 106590: train loss 2.5356, val loss 2.6590\n",
      "step 106600: train loss 2.5029, val loss 2.4687\n",
      "step 106610: train loss 2.5140, val loss 2.5135\n",
      "step 106620: train loss 2.4834, val loss 2.5477\n",
      "step 106630: train loss 2.5002, val loss 2.6488\n",
      "step 106640: train loss 2.4285, val loss 2.5651\n",
      "step 106650: train loss 2.3428, val loss 2.5790\n",
      "step 106660: train loss 2.5120, val loss 2.5906\n",
      "step 106670: train loss 2.4641, val loss 2.5686\n",
      "step 106680: train loss 2.4957, val loss 2.5770\n",
      "step 106690: train loss 2.4537, val loss 2.5142\n",
      "step 106700: train loss 2.4673, val loss 2.5056\n",
      "step 106710: train loss 2.4049, val loss 2.5124\n",
      "step 106720: train loss 2.5212, val loss 2.4843\n",
      "step 106730: train loss 2.4562, val loss 2.5306\n",
      "step 106740: train loss 2.4687, val loss 2.5126\n",
      "step 106750: train loss 2.4973, val loss 2.5740\n",
      "step 106760: train loss 2.3479, val loss 2.4854\n",
      "step 106770: train loss 2.4485, val loss 2.5439\n",
      "step 106780: train loss 2.4685, val loss 2.5520\n",
      "step 106790: train loss 2.4087, val loss 2.4374\n",
      "step 106800: train loss 2.4948, val loss 2.4804\n",
      "step 106810: train loss 2.5702, val loss 2.5669\n",
      "step 106820: train loss 2.4529, val loss 2.5774\n",
      "step 106830: train loss 2.4398, val loss 2.5662\n",
      "step 106840: train loss 2.3951, val loss 2.4440\n",
      "step 106850: train loss 2.4470, val loss 2.4340\n",
      "step 106860: train loss 2.4707, val loss 2.5024\n",
      "step 106870: train loss 2.5039, val loss 2.5019\n",
      "step 106880: train loss 2.4874, val loss 2.5840\n",
      "step 106890: train loss 2.4407, val loss 2.5792\n",
      "step 106900: train loss 2.4780, val loss 2.5109\n",
      "step 106910: train loss 2.4262, val loss 2.5052\n",
      "step 106920: train loss 2.3920, val loss 2.4585\n",
      "step 106930: train loss 2.4558, val loss 2.4602\n",
      "step 106940: train loss 2.4298, val loss 2.4528\n",
      "step 106950: train loss 2.4574, val loss 2.5164\n",
      "step 106960: train loss 2.4627, val loss 2.4917\n",
      "step 106970: train loss 2.3854, val loss 2.4850\n",
      "step 106980: train loss 2.3961, val loss 2.5378\n",
      "step 106990: train loss 2.4275, val loss 2.5957\n",
      "step 107000: train loss 2.4567, val loss 2.4354\n",
      "Generated text at iteration 107000\n",
      "\n",
      "D: prtri.?Hénfaibre.\n",
      "Lestéreré;\n",
      "\n",
      "GUCre vevonn te  s  gbene de, l'ou'adavoux foteseu sch!\n",
      "EÉt u O)çE \n",
      "step 107010: train loss 2.5009, val loss 2.6785\n",
      "step 107020: train loss 2.4666, val loss 2.5228\n",
      "step 107030: train loss 2.5108, val loss 2.4105\n",
      "step 107040: train loss 2.4579, val loss 2.5678\n",
      "step 107050: train loss 2.4406, val loss 2.5149\n",
      "step 107060: train loss 2.4644, val loss 2.6200\n",
      "step 107070: train loss 2.5370, val loss 2.5539\n",
      "step 107080: train loss 2.4971, val loss 2.4606\n",
      "step 107090: train loss 2.4809, val loss 2.4229\n",
      "step 107100: train loss 2.4314, val loss 2.5709\n",
      "step 107110: train loss 2.4592, val loss 2.3811\n",
      "step 107120: train loss 2.4197, val loss 2.5303\n",
      "step 107130: train loss 2.4589, val loss 2.4710\n",
      "step 107140: train loss 2.4448, val loss 2.6232\n",
      "step 107150: train loss 2.5061, val loss 2.5193\n",
      "step 107160: train loss 2.4984, val loss 2.5103\n",
      "step 107170: train loss 2.5118, val loss 2.6170\n",
      "step 107180: train loss 2.5358, val loss 2.5938\n",
      "step 107190: train loss 2.5251, val loss 2.6035\n",
      "step 107200: train loss 2.4496, val loss 2.5971\n",
      "step 107210: train loss 2.5130, val loss 2.6591\n",
      "step 107220: train loss 2.4188, val loss 2.6116\n",
      "step 107230: train loss 2.5125, val loss 2.4593\n",
      "step 107240: train loss 2.3988, val loss 2.5092\n",
      "step 107250: train loss 2.5116, val loss 2.5279\n",
      "step 107260: train loss 2.4982, val loss 2.4467\n",
      "step 107270: train loss 2.4386, val loss 2.5106\n",
      "step 107280: train loss 2.4716, val loss 2.5155\n",
      "step 107290: train loss 2.4487, val loss 2.5515\n",
      "step 107300: train loss 2.4552, val loss 2.5519\n",
      "step 107310: train loss 2.4920, val loss 2.4943\n",
      "step 107320: train loss 2.4211, val loss 2.5510\n",
      "step 107330: train loss 2.4714, val loss 2.5674\n",
      "step 107340: train loss 2.4682, val loss 2.5267\n",
      "step 107350: train loss 2.4584, val loss 2.5042\n",
      "step 107360: train loss 2.4700, val loss 2.5098\n",
      "step 107370: train loss 2.4158, val loss 2.4985\n",
      "step 107380: train loss 2.4728, val loss 2.5586\n",
      "step 107390: train loss 2.3812, val loss 2.5153\n",
      "step 107400: train loss 2.4781, val loss 2.5564\n",
      "step 107410: train loss 2.3965, val loss 2.6001\n",
      "step 107420: train loss 2.4592, val loss 2.5242\n",
      "step 107430: train loss 2.4647, val loss 2.4991\n",
      "step 107440: train loss 2.4340, val loss 2.5736\n",
      "step 107450: train loss 2.4550, val loss 2.5785\n",
      "step 107460: train loss 2.4620, val loss 2.4332\n",
      "step 107470: train loss 2.5212, val loss 2.6022\n",
      "step 107480: train loss 2.3804, val loss 2.4764\n",
      "step 107490: train loss 2.5052, val loss 2.5619\n",
      "step 107500: train loss 2.5223, val loss 2.5941\n",
      "step 107510: train loss 2.5174, val loss 2.5450\n",
      "step 107520: train loss 2.5085, val loss 2.5632\n",
      "step 107530: train loss 2.4127, val loss 2.5743\n",
      "step 107540: train loss 2.5118, val loss 2.5461\n",
      "step 107550: train loss 2.4596, val loss 2.6172\n",
      "step 107560: train loss 2.3999, val loss 2.4617\n",
      "step 107570: train loss 2.4739, val loss 2.5345\n",
      "step 107580: train loss 2.4510, val loss 2.6738\n",
      "step 107590: train loss 2.4792, val loss 2.4756\n",
      "step 107600: train loss 2.4547, val loss 2.4372\n",
      "step 107610: train loss 2.5198, val loss 2.4539\n",
      "step 107620: train loss 2.4561, val loss 2.5153\n",
      "step 107630: train loss 2.4473, val loss 2.5576\n",
      "step 107640: train loss 2.4512, val loss 2.6004\n",
      "step 107650: train loss 2.4594, val loss 2.4604\n",
      "step 107660: train loss 2.3349, val loss 2.5105\n",
      "step 107670: train loss 2.5709, val loss 2.5225\n",
      "step 107680: train loss 2.4799, val loss 2.3750\n",
      "step 107690: train loss 2.4897, val loss 2.6482\n",
      "step 107700: train loss 2.4874, val loss 2.4502\n",
      "step 107710: train loss 2.4376, val loss 2.5048\n",
      "step 107720: train loss 2.5140, val loss 2.5650\n",
      "step 107730: train loss 2.4826, val loss 2.5288\n",
      "step 107740: train loss 2.5186, val loss 2.5736\n",
      "step 107750: train loss 2.4523, val loss 2.4388\n",
      "step 107760: train loss 2.4168, val loss 2.5939\n",
      "step 107770: train loss 2.4848, val loss 2.4548\n",
      "step 107780: train loss 2.4900, val loss 2.5028\n",
      "step 107790: train loss 2.5649, val loss 2.5947\n",
      "step 107800: train loss 2.4829, val loss 2.5369\n",
      "step 107810: train loss 2.4338, val loss 2.5133\n",
      "step 107820: train loss 2.4520, val loss 2.5563\n",
      "step 107830: train loss 2.5146, val loss 2.6422\n",
      "step 107840: train loss 2.4272, val loss 2.5332\n",
      "step 107850: train loss 2.4845, val loss 2.5355\n",
      "step 107860: train loss 2.4272, val loss 2.5997\n",
      "step 107870: train loss 2.4265, val loss 2.6081\n",
      "step 107880: train loss 2.4065, val loss 2.4974\n",
      "step 107890: train loss 2.4428, val loss 2.5640\n",
      "step 107900: train loss 2.4764, val loss 2.4731\n",
      "step 107910: train loss 2.5091, val loss 2.5949\n",
      "step 107920: train loss 2.5224, val loss 2.5775\n",
      "step 107930: train loss 2.4856, val loss 2.4888\n",
      "step 107940: train loss 2.3950, val loss 2.5190\n",
      "step 107950: train loss 2.4864, val loss 2.5633\n",
      "step 107960: train loss 2.5025, val loss 2.5483\n",
      "step 107970: train loss 2.4248, val loss 2.5939\n",
      "step 107980: train loss 2.4438, val loss 2.5849\n",
      "step 107990: train loss 2.4281, val loss 2.5331\n",
      "step 108000: train loss 2.5025, val loss 2.5571\n",
      "Generated text at iteration 108000\n",
      "\n",
      "EÔL'arr quel'huifafoilaikNîEe tou'eux  s  tisant aïce ile pe metapese;\n",
      "Td'ayYk?te drs mévombommeuren\n",
      "step 108010: train loss 2.4006, val loss 2.4258\n",
      "step 108020: train loss 2.5006, val loss 2.3915\n",
      "step 108030: train loss 2.4705, val loss 2.5267\n",
      "step 108040: train loss 2.4639, val loss 2.6133\n",
      "step 108050: train loss 2.3919, val loss 2.4842\n",
      "step 108060: train loss 2.4505, val loss 2.4441\n",
      "step 108070: train loss 2.4328, val loss 2.4977\n",
      "step 108080: train loss 2.4210, val loss 2.4957\n",
      "step 108090: train loss 2.4574, val loss 2.5308\n",
      "step 108100: train loss 2.4563, val loss 2.5586\n",
      "step 108110: train loss 2.4516, val loss 2.4558\n",
      "step 108120: train loss 2.4713, val loss 2.5006\n",
      "step 108130: train loss 2.4831, val loss 2.4295\n",
      "step 108140: train loss 2.4346, val loss 2.4958\n",
      "step 108150: train loss 2.4762, val loss 2.5953\n",
      "step 108160: train loss 2.5032, val loss 2.4586\n",
      "step 108170: train loss 2.3609, val loss 2.5185\n",
      "step 108180: train loss 2.4742, val loss 2.5316\n",
      "step 108190: train loss 2.4208, val loss 2.5959\n",
      "step 108200: train loss 2.4944, val loss 2.5076\n",
      "step 108210: train loss 2.5230, val loss 2.4822\n",
      "step 108220: train loss 2.4285, val loss 2.5464\n",
      "step 108230: train loss 2.4907, val loss 2.5376\n",
      "step 108240: train loss 2.5247, val loss 2.5769\n",
      "step 108250: train loss 2.4643, val loss 2.5377\n",
      "step 108260: train loss 2.4681, val loss 2.6060\n",
      "step 108270: train loss 2.4779, val loss 2.4318\n",
      "step 108280: train loss 2.4338, val loss 2.4380\n",
      "step 108290: train loss 2.4646, val loss 2.5417\n",
      "step 108300: train loss 2.4646, val loss 2.5129\n",
      "step 108310: train loss 2.4467, val loss 2.4813\n",
      "step 108320: train loss 2.4624, val loss 2.5180\n",
      "step 108330: train loss 2.4914, val loss 2.4584\n",
      "step 108340: train loss 2.4911, val loss 2.5377\n",
      "step 108350: train loss 2.4860, val loss 2.4977\n",
      "step 108360: train loss 2.4751, val loss 2.5619\n",
      "step 108370: train loss 2.4538, val loss 2.4608\n",
      "step 108380: train loss 2.4694, val loss 2.5587\n",
      "step 108390: train loss 2.4468, val loss 2.6056\n",
      "step 108400: train loss 2.4763, val loss 2.6043\n",
      "step 108410: train loss 2.4712, val loss 2.5651\n",
      "step 108420: train loss 2.4588, val loss 2.5677\n",
      "step 108430: train loss 2.4196, val loss 2.5510\n",
      "step 108440: train loss 2.4841, val loss 2.3957\n",
      "step 108450: train loss 2.4349, val loss 2.4527\n",
      "step 108460: train loss 2.4526, val loss 2.4645\n",
      "step 108470: train loss 2.4849, val loss 2.5593\n",
      "step 108480: train loss 2.5131, val loss 2.5354\n",
      "step 108490: train loss 2.4486, val loss 2.5367\n",
      "step 108500: train loss 2.5253, val loss 2.5255\n",
      "step 108510: train loss 2.4576, val loss 2.4912\n",
      "step 108520: train loss 2.4033, val loss 2.4515\n",
      "step 108530: train loss 2.3851, val loss 2.5551\n",
      "step 108540: train loss 2.4147, val loss 2.4525\n",
      "step 108550: train loss 2.4512, val loss 2.4949\n",
      "step 108560: train loss 2.4040, val loss 2.5145\n",
      "step 108570: train loss 2.4258, val loss 2.4693\n",
      "step 108580: train loss 2.4800, val loss 2.5227\n",
      "step 108590: train loss 2.4237, val loss 2.4483\n",
      "step 108600: train loss 2.3821, val loss 2.4834\n",
      "step 108610: train loss 2.5012, val loss 2.5611\n",
      "step 108620: train loss 2.4657, val loss 2.5684\n",
      "step 108630: train loss 2.4289, val loss 2.6263\n",
      "step 108640: train loss 2.5123, val loss 2.5452\n",
      "step 108650: train loss 2.4450, val loss 2.5191\n",
      "step 108660: train loss 2.4864, val loss 2.4527\n",
      "step 108670: train loss 2.4667, val loss 2.5112\n",
      "step 108680: train loss 2.3981, val loss 2.5270\n",
      "step 108690: train loss 2.4720, val loss 2.5666\n",
      "step 108700: train loss 2.4492, val loss 2.5701\n",
      "step 108710: train loss 2.4035, val loss 2.4958\n",
      "step 108720: train loss 2.4441, val loss 2.6818\n",
      "step 108730: train loss 2.3940, val loss 2.4813\n",
      "step 108740: train loss 2.4789, val loss 2.5436\n",
      "step 108750: train loss 2.4685, val loss 2.4716\n",
      "step 108760: train loss 2.4880, val loss 2.4403\n",
      "step 108770: train loss 2.4688, val loss 2.5179\n",
      "step 108780: train loss 2.4314, val loss 2.6155\n",
      "step 108790: train loss 2.4979, val loss 2.4741\n",
      "step 108800: train loss 2.4586, val loss 2.4676\n",
      "step 108810: train loss 2.4430, val loss 2.5578\n",
      "step 108820: train loss 2.4249, val loss 2.4543\n",
      "step 108830: train loss 2.4598, val loss 2.4198\n",
      "step 108840: train loss 2.4951, val loss 2.5607\n",
      "step 108850: train loss 2.4642, val loss 2.5224\n",
      "step 108860: train loss 2.5012, val loss 2.5644\n",
      "step 108870: train loss 2.4632, val loss 2.5512\n",
      "step 108880: train loss 2.4430, val loss 2.5068\n",
      "step 108890: train loss 2.4728, val loss 2.5290\n",
      "step 108900: train loss 2.4673, val loss 2.5719\n",
      "step 108910: train loss 2.4235, val loss 2.6069\n",
      "step 108920: train loss 2.3923, val loss 2.6159\n",
      "step 108930: train loss 2.5434, val loss 2.6654\n",
      "step 108940: train loss 2.4802, val loss 2.6268\n",
      "step 108950: train loss 2.5110, val loss 2.5094\n",
      "step 108960: train loss 2.5006, val loss 2.4840\n",
      "step 108970: train loss 2.4197, val loss 2.5473\n",
      "step 108980: train loss 2.3958, val loss 2.3640\n",
      "step 108990: train loss 2.4493, val loss 2.5737\n",
      "step 109000: train loss 2.4808, val loss 2.4642\n",
      "Generated text at iteration 109000\n",
      "\n",
      "PrissonvombrêZRx doies froùêl'ÉkIJ'e e e J5isaiendux crâma ent pe-jeuresés  1Ô;\n",
      "Cet fas micoicturd'a\n",
      "step 109010: train loss 2.3644, val loss 2.3881\n",
      "step 109020: train loss 2.4656, val loss 2.5339\n",
      "step 109030: train loss 2.4377, val loss 2.5429\n",
      "step 109040: train loss 2.5114, val loss 2.5510\n",
      "step 109050: train loss 2.5317, val loss 2.5999\n",
      "step 109060: train loss 2.5325, val loss 2.4678\n",
      "step 109070: train loss 2.3977, val loss 2.4393\n",
      "step 109080: train loss 2.4899, val loss 2.4806\n",
      "step 109090: train loss 2.4742, val loss 2.5110\n",
      "step 109100: train loss 2.5020, val loss 2.4914\n",
      "step 109110: train loss 2.4732, val loss 2.5449\n",
      "step 109120: train loss 2.5256, val loss 2.4971\n",
      "step 109130: train loss 2.4361, val loss 2.5599\n",
      "step 109140: train loss 2.4985, val loss 2.6142\n",
      "step 109150: train loss 2.4090, val loss 2.4437\n",
      "step 109160: train loss 2.4279, val loss 2.5403\n",
      "step 109170: train loss 2.4457, val loss 2.5154\n",
      "step 109180: train loss 2.4931, val loss 2.3954\n",
      "step 109190: train loss 2.4754, val loss 2.5125\n",
      "step 109200: train loss 2.4216, val loss 2.6145\n",
      "step 109210: train loss 2.5337, val loss 2.5167\n",
      "step 109220: train loss 2.3924, val loss 2.5295\n",
      "step 109230: train loss 2.4312, val loss 2.5117\n",
      "step 109240: train loss 2.5140, val loss 2.5145\n",
      "step 109250: train loss 2.4505, val loss 2.4377\n",
      "step 109260: train loss 2.4190, val loss 2.5672\n",
      "step 109270: train loss 2.4715, val loss 2.5751\n",
      "step 109280: train loss 2.4016, val loss 2.5163\n",
      "step 109290: train loss 2.4503, val loss 2.4064\n",
      "step 109300: train loss 2.4923, val loss 2.5499\n",
      "step 109310: train loss 2.5023, val loss 2.5597\n",
      "step 109320: train loss 2.4387, val loss 2.4480\n",
      "step 109330: train loss 2.4158, val loss 2.5547\n",
      "step 109340: train loss 2.4719, val loss 2.5060\n",
      "step 109350: train loss 2.5058, val loss 2.4693\n",
      "step 109360: train loss 2.4920, val loss 2.5761\n",
      "step 109370: train loss 2.4726, val loss 2.5171\n",
      "step 109380: train loss 2.5032, val loss 2.4802\n",
      "step 109390: train loss 2.4157, val loss 2.5987\n",
      "step 109400: train loss 2.4223, val loss 2.5337\n",
      "step 109410: train loss 2.4588, val loss 2.5168\n",
      "step 109420: train loss 2.4319, val loss 2.5100\n",
      "step 109430: train loss 2.4437, val loss 2.4514\n",
      "step 109440: train loss 2.5828, val loss 2.5561\n",
      "step 109450: train loss 2.4895, val loss 2.4298\n",
      "step 109460: train loss 2.4505, val loss 2.5876\n",
      "step 109470: train loss 2.5412, val loss 2.4715\n",
      "step 109480: train loss 2.5025, val loss 2.5227\n",
      "step 109490: train loss 2.4488, val loss 2.3916\n",
      "step 109500: train loss 2.4433, val loss 2.5102\n",
      "step 109510: train loss 2.4525, val loss 2.5875\n",
      "step 109520: train loss 2.4464, val loss 2.4617\n",
      "step 109530: train loss 2.4705, val loss 2.5960\n",
      "step 109540: train loss 2.4357, val loss 2.5106\n",
      "step 109550: train loss 2.5078, val loss 2.5242\n",
      "step 109560: train loss 2.4327, val loss 2.4376\n",
      "step 109570: train loss 2.4509, val loss 2.6039\n",
      "step 109580: train loss 2.4815, val loss 2.6181\n",
      "step 109590: train loss 2.4173, val loss 2.5157\n",
      "step 109600: train loss 2.6126, val loss 2.5612\n",
      "step 109610: train loss 2.4007, val loss 2.5530\n",
      "step 109620: train loss 2.5022, val loss 2.4677\n",
      "step 109630: train loss 2.4636, val loss 2.5500\n",
      "step 109640: train loss 2.4881, val loss 2.5913\n",
      "step 109650: train loss 2.4457, val loss 2.5305\n",
      "step 109660: train loss 2.4433, val loss 2.4877\n",
      "step 109670: train loss 2.4209, val loss 2.5187\n",
      "step 109680: train loss 2.4634, val loss 2.5616\n",
      "step 109690: train loss 2.5325, val loss 2.5337\n",
      "step 109700: train loss 2.5012, val loss 2.6453\n",
      "step 109710: train loss 2.5277, val loss 2.4981\n",
      "step 109720: train loss 2.4325, val loss 2.4600\n",
      "step 109730: train loss 2.5285, val loss 2.4967\n",
      "step 109740: train loss 2.5143, val loss 2.5563\n",
      "step 109750: train loss 2.3466, val loss 2.5269\n",
      "step 109760: train loss 2.4529, val loss 2.4112\n",
      "step 109770: train loss 2.4528, val loss 2.5668\n",
      "step 109780: train loss 2.4324, val loss 2.4883\n",
      "step 109790: train loss 2.3867, val loss 2.4737\n",
      "step 109800: train loss 2.4869, val loss 2.6353\n",
      "step 109810: train loss 2.4381, val loss 2.4833\n",
      "step 109820: train loss 2.5128, val loss 2.5659\n",
      "step 109830: train loss 2.4692, val loss 2.4820\n",
      "step 109840: train loss 2.4274, val loss 2.4850\n",
      "step 109850: train loss 2.4505, val loss 2.5486\n",
      "step 109860: train loss 2.4709, val loss 2.4492\n",
      "step 109870: train loss 2.4590, val loss 2.5368\n",
      "step 109880: train loss 2.4423, val loss 2.5085\n",
      "step 109890: train loss 2.3718, val loss 2.5411\n",
      "step 109900: train loss 2.4682, val loss 2.4628\n",
      "step 109910: train loss 2.4479, val loss 2.4818\n",
      "step 109920: train loss 2.4649, val loss 2.5424\n",
      "step 109930: train loss 2.4097, val loss 2.5402\n",
      "step 109940: train loss 2.4272, val loss 2.4816\n",
      "step 109950: train loss 2.4283, val loss 2.4579\n",
      "step 109960: train loss 2.5547, val loss 2.5393\n",
      "step 109970: train loss 2.4265, val loss 2.5489\n",
      "step 109980: train loss 2.4873, val loss 2.6575\n",
      "step 109990: train loss 2.4308, val loss 2.4658\n",
      "step 110000: train loss 2.3931, val loss 2.5088\n",
      "Generated text at iteration 110000\n",
      "\n",
      "Le de, fùSe, t EV\n",
      "ENÔïfi,  spanta d.\n",
      " s _9ÔUê9des,\n",
      "Les De lèG]\n",
      "ST(1Æmpeletéprstue  ns lêÈise quteson\n",
      "step 110010: train loss 2.4505, val loss 2.5049\n",
      "step 110020: train loss 2.4002, val loss 2.5586\n",
      "step 110030: train loss 2.4855, val loss 2.6429\n",
      "step 110040: train loss 2.4096, val loss 2.5519\n",
      "step 110050: train loss 2.3577, val loss 2.4885\n",
      "step 110060: train loss 2.4604, val loss 2.5217\n",
      "step 110070: train loss 2.5302, val loss 2.4775\n",
      "step 110080: train loss 2.4532, val loss 2.4573\n",
      "step 110090: train loss 2.4592, val loss 2.5446\n",
      "step 110100: train loss 2.4445, val loss 2.4489\n",
      "step 110110: train loss 2.4457, val loss 2.5850\n",
      "step 110120: train loss 2.4869, val loss 2.5991\n",
      "step 110130: train loss 2.4495, val loss 2.5313\n",
      "step 110140: train loss 2.4417, val loss 2.5429\n",
      "step 110150: train loss 2.4626, val loss 2.5566\n",
      "step 110160: train loss 2.4332, val loss 2.4750\n",
      "step 110170: train loss 2.4195, val loss 2.6747\n",
      "step 110180: train loss 2.4551, val loss 2.6680\n",
      "step 110190: train loss 2.4738, val loss 2.5183\n",
      "step 110200: train loss 2.4425, val loss 2.5457\n",
      "step 110210: train loss 2.4328, val loss 2.4538\n",
      "step 110220: train loss 2.4213, val loss 2.5085\n",
      "step 110230: train loss 2.4035, val loss 2.4916\n",
      "step 110240: train loss 2.4057, val loss 2.6374\n",
      "step 110250: train loss 2.3977, val loss 2.6603\n",
      "step 110260: train loss 2.4579, val loss 2.5156\n",
      "step 110270: train loss 2.4364, val loss 2.4501\n",
      "step 110280: train loss 2.4305, val loss 2.6221\n",
      "step 110290: train loss 2.4645, val loss 2.6993\n",
      "step 110300: train loss 2.4167, val loss 2.5260\n",
      "step 110310: train loss 2.4164, val loss 2.4586\n",
      "step 110320: train loss 2.4433, val loss 2.5586\n",
      "step 110330: train loss 2.4729, val loss 2.4805\n",
      "step 110340: train loss 2.4546, val loss 2.4611\n",
      "step 110350: train loss 2.4667, val loss 2.4845\n",
      "step 110360: train loss 2.4601, val loss 2.5431\n",
      "step 110370: train loss 2.4049, val loss 2.4909\n",
      "step 110380: train loss 2.4410, val loss 2.4907\n",
      "step 110390: train loss 2.4987, val loss 2.6021\n",
      "step 110400: train loss 2.4103, val loss 2.6147\n",
      "step 110410: train loss 2.4312, val loss 2.4666\n",
      "step 110420: train loss 2.4055, val loss 2.4265\n",
      "step 110430: train loss 2.5104, val loss 2.3848\n",
      "step 110440: train loss 2.4552, val loss 2.6589\n",
      "step 110450: train loss 2.4704, val loss 2.5454\n",
      "step 110460: train loss 2.4390, val loss 2.5105\n",
      "step 110470: train loss 2.4519, val loss 2.4828\n",
      "step 110480: train loss 2.4628, val loss 2.5543\n",
      "step 110490: train loss 2.4400, val loss 2.4852\n",
      "step 110500: train loss 2.4603, val loss 2.4810\n",
      "step 110510: train loss 2.5210, val loss 2.6031\n",
      "step 110520: train loss 2.3824, val loss 2.5595\n",
      "step 110530: train loss 2.4549, val loss 2.5623\n",
      "step 110540: train loss 2.4016, val loss 2.5615\n",
      "step 110550: train loss 2.4319, val loss 2.4851\n",
      "step 110560: train loss 2.4854, val loss 2.5457\n",
      "step 110570: train loss 2.4618, val loss 2.6149\n",
      "step 110580: train loss 2.5228, val loss 2.5430\n",
      "step 110590: train loss 2.4759, val loss 2.4108\n",
      "step 110600: train loss 2.5227, val loss 2.4433\n",
      "step 110610: train loss 2.4563, val loss 2.5492\n",
      "step 110620: train loss 2.4231, val loss 2.5159\n",
      "step 110630: train loss 2.5131, val loss 2.5011\n",
      "step 110640: train loss 2.4396, val loss 2.5832\n",
      "step 110650: train loss 2.4245, val loss 2.5573\n",
      "step 110660: train loss 2.3761, val loss 2.5297\n",
      "step 110670: train loss 2.4181, val loss 2.6091\n",
      "step 110680: train loss 2.4395, val loss 2.4999\n",
      "step 110690: train loss 2.4747, val loss 2.5108\n",
      "step 110700: train loss 2.4547, val loss 2.4574\n",
      "step 110710: train loss 2.4870, val loss 2.4884\n",
      "step 110720: train loss 2.4687, val loss 2.5234\n",
      "step 110730: train loss 2.5272, val loss 2.4914\n",
      "step 110740: train loss 2.4561, val loss 2.5421\n",
      "step 110750: train loss 2.4395, val loss 2.5868\n",
      "step 110760: train loss 2.3667, val loss 2.5559\n",
      "step 110770: train loss 2.4131, val loss 2.6181\n",
      "step 110780: train loss 2.4609, val loss 2.5574\n",
      "step 110790: train loss 2.4152, val loss 2.4778\n",
      "step 110800: train loss 2.3829, val loss 2.5819\n",
      "step 110810: train loss 2.4955, val loss 2.4199\n",
      "step 110820: train loss 2.4643, val loss 2.4883\n",
      "step 110830: train loss 2.4789, val loss 2.5805\n",
      "step 110840: train loss 2.4430, val loss 2.5313\n",
      "step 110850: train loss 2.3921, val loss 2.5335\n",
      "step 110860: train loss 2.4026, val loss 2.5308\n",
      "step 110870: train loss 2.5040, val loss 2.5968\n",
      "step 110880: train loss 2.4444, val loss 2.6270\n",
      "step 110890: train loss 2.4027, val loss 2.6012\n",
      "step 110900: train loss 2.5433, val loss 2.6101\n",
      "step 110910: train loss 2.4549, val loss 2.5490\n",
      "step 110920: train loss 2.5023, val loss 2.4766\n",
      "step 110930: train loss 2.4314, val loss 2.5453\n",
      "step 110940: train loss 2.4777, val loss 2.5774\n",
      "step 110950: train loss 2.4683, val loss 2.4832\n",
      "step 110960: train loss 2.4808, val loss 2.5498\n",
      "step 110970: train loss 2.4340, val loss 2.5010\n",
      "step 110980: train loss 2.4535, val loss 2.4574\n",
      "step 110990: train loss 2.5088, val loss 2.4323\n",
      "step 111000: train loss 2.4357, val loss 2.5934\n",
      "Generated text at iteration 111000\n",
      "\n",
      "Pandsiloreranditrn'âou e pe por ai me concte mi!\n",
      "R«dollalerin  l'a APDu, s pre ais.\n",
      "V«»:ùVigBëzgoue \n",
      "step 111010: train loss 2.4343, val loss 2.6582\n",
      "step 111020: train loss 2.4589, val loss 2.4724\n",
      "step 111030: train loss 2.4373, val loss 2.4489\n",
      "step 111040: train loss 2.4614, val loss 2.6164\n",
      "step 111050: train loss 2.4697, val loss 2.5118\n",
      "step 111060: train loss 2.5359, val loss 2.6596\n",
      "step 111070: train loss 2.4131, val loss 2.6281\n",
      "step 111080: train loss 2.4122, val loss 2.4967\n",
      "step 111090: train loss 2.4036, val loss 2.5693\n",
      "step 111100: train loss 2.4099, val loss 2.5877\n",
      "step 111110: train loss 2.4550, val loss 2.5910\n",
      "step 111120: train loss 2.3566, val loss 2.5434\n",
      "step 111130: train loss 2.5348, val loss 2.4456\n",
      "step 111140: train loss 2.4577, val loss 2.4990\n",
      "step 111150: train loss 2.4489, val loss 2.6481\n",
      "step 111160: train loss 2.4790, val loss 2.6336\n",
      "step 111170: train loss 2.4801, val loss 2.5355\n",
      "step 111180: train loss 2.5017, val loss 2.4773\n",
      "step 111190: train loss 2.4749, val loss 2.5144\n",
      "step 111200: train loss 2.5288, val loss 2.4792\n",
      "step 111210: train loss 2.4984, val loss 2.5186\n",
      "step 111220: train loss 2.4456, val loss 2.5541\n",
      "step 111230: train loss 2.4639, val loss 2.5355\n",
      "step 111240: train loss 2.3934, val loss 2.6087\n",
      "step 111250: train loss 2.4686, val loss 2.6081\n",
      "step 111260: train loss 2.4767, val loss 2.5175\n",
      "step 111270: train loss 2.4568, val loss 2.5395\n",
      "step 111280: train loss 2.4141, val loss 2.4152\n",
      "step 111290: train loss 2.3667, val loss 2.5777\n",
      "step 111300: train loss 2.4455, val loss 2.5133\n",
      "step 111310: train loss 2.4452, val loss 2.5464\n",
      "step 111320: train loss 2.4880, val loss 2.4973\n",
      "step 111330: train loss 2.4090, val loss 2.5092\n",
      "step 111340: train loss 2.4180, val loss 2.5096\n",
      "step 111350: train loss 2.4127, val loss 2.6163\n",
      "step 111360: train loss 2.5026, val loss 2.5109\n",
      "step 111370: train loss 2.4792, val loss 2.4672\n",
      "step 111380: train loss 2.4621, val loss 2.4947\n",
      "step 111390: train loss 2.4649, val loss 2.5470\n",
      "step 111400: train loss 2.3957, val loss 2.4894\n",
      "step 111410: train loss 2.4997, val loss 2.4557\n",
      "step 111420: train loss 2.5349, val loss 2.5115\n",
      "step 111430: train loss 2.5264, val loss 2.5192\n",
      "step 111440: train loss 2.4235, val loss 2.4904\n",
      "step 111450: train loss 2.3987, val loss 2.5091\n",
      "step 111460: train loss 2.5098, val loss 2.5151\n",
      "step 111470: train loss 2.5202, val loss 2.5832\n",
      "step 111480: train loss 2.4421, val loss 2.4974\n",
      "step 111490: train loss 2.4815, val loss 2.5262\n",
      "step 111500: train loss 2.4351, val loss 2.5337\n",
      "step 111510: train loss 2.4395, val loss 2.5209\n",
      "step 111520: train loss 2.5364, val loss 2.5819\n",
      "step 111530: train loss 2.5449, val loss 2.5039\n",
      "step 111540: train loss 2.4321, val loss 2.6205\n",
      "step 111550: train loss 2.3954, val loss 2.5252\n",
      "step 111560: train loss 2.4933, val loss 2.4603\n",
      "step 111570: train loss 2.5048, val loss 2.5465\n",
      "step 111580: train loss 2.4301, val loss 2.4877\n",
      "step 111590: train loss 2.4653, val loss 2.4943\n",
      "step 111600: train loss 2.4719, val loss 2.4830\n",
      "step 111610: train loss 2.4524, val loss 2.5310\n",
      "step 111620: train loss 2.4454, val loss 2.4448\n",
      "step 111630: train loss 2.4360, val loss 2.6390\n",
      "step 111640: train loss 2.4867, val loss 2.5474\n",
      "step 111650: train loss 2.3573, val loss 2.5316\n",
      "step 111660: train loss 2.4859, val loss 2.4931\n",
      "step 111670: train loss 2.5536, val loss 2.4708\n",
      "step 111680: train loss 2.4223, val loss 2.5588\n",
      "step 111690: train loss 2.4259, val loss 2.6105\n",
      "step 111700: train loss 2.4703, val loss 2.5122\n",
      "step 111710: train loss 2.4930, val loss 2.6498\n",
      "step 111720: train loss 2.4731, val loss 2.6229\n",
      "step 111730: train loss 2.5115, val loss 2.4850\n",
      "step 111740: train loss 2.4939, val loss 2.4861\n",
      "step 111750: train loss 2.5203, val loss 2.4727\n",
      "step 111760: train loss 2.3726, val loss 2.7568\n",
      "step 111770: train loss 2.4556, val loss 2.5830\n",
      "step 111780: train loss 2.4232, val loss 2.4850\n",
      "step 111790: train loss 2.4065, val loss 2.4672\n",
      "step 111800: train loss 2.4195, val loss 2.5008\n",
      "step 111810: train loss 2.5573, val loss 2.5390\n",
      "step 111820: train loss 2.5576, val loss 2.5587\n",
      "step 111830: train loss 2.5150, val loss 2.7464\n",
      "step 111840: train loss 2.4686, val loss 2.5782\n",
      "step 111850: train loss 2.4880, val loss 2.4854\n",
      "step 111860: train loss 2.4696, val loss 2.5884\n",
      "step 111870: train loss 2.3472, val loss 2.4521\n",
      "step 111880: train loss 2.4447, val loss 2.5274\n",
      "step 111890: train loss 2.4543, val loss 2.4578\n",
      "step 111900: train loss 2.5088, val loss 2.5076\n",
      "step 111910: train loss 2.4465, val loss 2.4540\n",
      "step 111920: train loss 2.4810, val loss 2.4917\n",
      "step 111930: train loss 2.4476, val loss 2.4793\n",
      "step 111940: train loss 2.5032, val loss 2.6823\n",
      "step 111950: train loss 2.4131, val loss 2.4539\n",
      "step 111960: train loss 2.4743, val loss 2.4855\n",
      "step 111970: train loss 2.4118, val loss 2.4948\n",
      "step 111980: train loss 2.4562, val loss 2.5524\n",
      "step 111990: train loss 2.3918, val loss 2.4992\n",
      "step 112000: train loss 2.4691, val loss 2.4272\n",
      "Generated text at iteration 112000\n",
      "\n",
      " s s  ausoirore t méccerenombles, antLL'hount onsa engerd, dousés joeui;\n",
      "De s de;\n",
      "L'he n l'ei\n",
      "VÊLeur\n",
      "step 112010: train loss 2.4426, val loss 2.5722\n",
      "step 112020: train loss 2.3919, val loss 2.5132\n",
      "step 112030: train loss 2.4920, val loss 2.4133\n",
      "step 112040: train loss 2.4890, val loss 2.5435\n",
      "step 112050: train loss 2.4452, val loss 2.5602\n",
      "step 112060: train loss 2.3655, val loss 2.4875\n",
      "step 112070: train loss 2.5219, val loss 2.6165\n",
      "step 112080: train loss 2.4179, val loss 2.5607\n",
      "step 112090: train loss 2.4685, val loss 2.6036\n",
      "step 112100: train loss 2.4242, val loss 2.5395\n",
      "step 112110: train loss 2.3578, val loss 2.5589\n",
      "step 112120: train loss 2.4174, val loss 2.4809\n",
      "step 112130: train loss 2.4335, val loss 2.5067\n",
      "step 112140: train loss 2.3918, val loss 2.4587\n",
      "step 112150: train loss 2.3583, val loss 2.6101\n",
      "step 112160: train loss 2.4290, val loss 2.4223\n",
      "step 112170: train loss 2.4458, val loss 2.5420\n",
      "step 112180: train loss 2.4371, val loss 2.4938\n",
      "step 112190: train loss 2.4517, val loss 2.4928\n",
      "step 112200: train loss 2.4448, val loss 2.3544\n",
      "step 112210: train loss 2.4524, val loss 2.4879\n",
      "step 112220: train loss 2.4735, val loss 2.4159\n",
      "step 112230: train loss 2.4694, val loss 2.4161\n",
      "step 112240: train loss 2.4910, val loss 2.5271\n",
      "step 112250: train loss 2.5141, val loss 2.4536\n",
      "step 112260: train loss 2.5099, val loss 2.5925\n",
      "step 112270: train loss 2.4116, val loss 2.6072\n",
      "step 112280: train loss 2.4415, val loss 2.5009\n",
      "step 112290: train loss 2.4456, val loss 2.5692\n",
      "step 112300: train loss 2.4462, val loss 2.5713\n",
      "step 112310: train loss 2.4452, val loss 2.5857\n",
      "step 112320: train loss 2.5047, val loss 2.5979\n",
      "step 112330: train loss 2.5217, val loss 2.5474\n",
      "step 112340: train loss 2.5014, val loss 2.5581\n",
      "step 112350: train loss 2.5104, val loss 2.5553\n",
      "step 112360: train loss 2.5107, val loss 2.4558\n",
      "step 112370: train loss 2.3975, val loss 2.4273\n",
      "step 112380: train loss 2.4596, val loss 2.5794\n",
      "step 112390: train loss 2.4025, val loss 2.4882\n",
      "step 112400: train loss 2.4164, val loss 2.4642\n",
      "step 112410: train loss 2.3746, val loss 2.4395\n",
      "step 112420: train loss 2.3905, val loss 2.5478\n",
      "step 112430: train loss 2.4867, val loss 2.4925\n",
      "step 112440: train loss 2.4308, val loss 2.4884\n",
      "step 112450: train loss 2.3928, val loss 2.4916\n",
      "step 112460: train loss 2.4549, val loss 2.5364\n",
      "step 112470: train loss 2.4032, val loss 2.5623\n",
      "step 112480: train loss 2.4907, val loss 2.6801\n",
      "step 112490: train loss 2.4782, val loss 2.5980\n",
      "step 112500: train loss 2.5631, val loss 2.4600\n",
      "step 112510: train loss 2.4420, val loss 2.5255\n",
      "step 112520: train loss 2.4374, val loss 2.5572\n",
      "step 112530: train loss 2.4567, val loss 2.6667\n",
      "step 112540: train loss 2.4204, val loss 2.5776\n",
      "step 112550: train loss 2.4158, val loss 2.5947\n",
      "step 112560: train loss 2.4370, val loss 2.4723\n",
      "step 112570: train loss 2.4940, val loss 2.5114\n",
      "step 112580: train loss 2.3932, val loss 2.4209\n",
      "step 112590: train loss 2.5655, val loss 2.5129\n",
      "step 112600: train loss 2.4097, val loss 2.4190\n",
      "step 112610: train loss 2.3763, val loss 2.4791\n",
      "step 112620: train loss 2.4448, val loss 2.4034\n",
      "step 112630: train loss 2.4104, val loss 2.5229\n",
      "step 112640: train loss 2.4327, val loss 2.5860\n",
      "step 112650: train loss 2.3818, val loss 2.5590\n",
      "step 112660: train loss 2.5256, val loss 2.4587\n",
      "step 112670: train loss 2.4124, val loss 2.5865\n",
      "step 112680: train loss 2.4415, val loss 2.6295\n",
      "step 112690: train loss 2.4888, val loss 2.6498\n",
      "step 112700: train loss 2.5063, val loss 2.5485\n",
      "step 112710: train loss 2.4714, val loss 2.5588\n",
      "step 112720: train loss 2.5559, val loss 2.5381\n",
      "step 112730: train loss 2.4651, val loss 2.4855\n",
      "step 112740: train loss 2.4772, val loss 2.5354\n",
      "step 112750: train loss 2.5422, val loss 2.4785\n",
      "step 112760: train loss 2.4447, val loss 2.5130\n",
      "step 112770: train loss 2.4747, val loss 2.5388\n",
      "step 112780: train loss 2.3933, val loss 2.5464\n",
      "step 112790: train loss 2.4887, val loss 2.5686\n",
      "step 112800: train loss 2.4254, val loss 2.5149\n",
      "step 112810: train loss 2.4858, val loss 2.4359\n",
      "step 112820: train loss 2.4462, val loss 2.5267\n",
      "step 112830: train loss 2.4825, val loss 2.6103\n",
      "step 112840: train loss 2.4495, val loss 2.6195\n",
      "step 112850: train loss 2.4980, val loss 2.5824\n",
      "step 112860: train loss 2.5081, val loss 2.4849\n",
      "step 112870: train loss 2.3975, val loss 2.4461\n",
      "step 112880: train loss 2.4052, val loss 2.3802\n",
      "step 112890: train loss 2.5367, val loss 2.5280\n",
      "step 112900: train loss 2.4279, val loss 2.5842\n",
      "step 112910: train loss 2.4768, val loss 2.6007\n",
      "step 112920: train loss 2.4053, val loss 2.5877\n",
      "step 112930: train loss 2.4126, val loss 2.5253\n",
      "step 112940: train loss 2.4390, val loss 2.5369\n",
      "step 112950: train loss 2.4033, val loss 2.5594\n",
      "step 112960: train loss 2.4879, val loss 2.3785\n",
      "step 112970: train loss 2.5411, val loss 2.4606\n",
      "step 112980: train loss 2.4237, val loss 2.4331\n",
      "step 112990: train loss 2.4198, val loss 2.5232\n",
      "step 113000: train loss 2.4281, val loss 2.4888\n",
      "Generated text at iteration 113000\n",
      "\n",
      "Laoutret ntou sande douresumé, fêafan,\n",
      "QïFyXPoreescusetoies mbs quiès deuds, s trôSagl hose dent ie \n",
      "step 113010: train loss 2.4160, val loss 2.5450\n",
      "step 113020: train loss 2.4700, val loss 2.5163\n",
      "step 113030: train loss 2.4272, val loss 2.5413\n",
      "step 113040: train loss 2.4033, val loss 2.5076\n",
      "step 113050: train loss 2.4748, val loss 2.5363\n",
      "step 113060: train loss 2.3908, val loss 2.5583\n",
      "step 113070: train loss 2.5281, val loss 2.5049\n",
      "step 113080: train loss 2.4201, val loss 2.5956\n",
      "step 113090: train loss 2.4765, val loss 2.5695\n",
      "step 113100: train loss 2.5501, val loss 2.5528\n",
      "step 113110: train loss 2.4816, val loss 2.5170\n",
      "step 113120: train loss 2.3776, val loss 2.4708\n",
      "step 113130: train loss 2.4837, val loss 2.5238\n",
      "step 113140: train loss 2.4971, val loss 2.5099\n",
      "step 113150: train loss 2.4866, val loss 2.5237\n",
      "step 113160: train loss 2.4382, val loss 2.5470\n",
      "step 113170: train loss 2.5147, val loss 2.5486\n",
      "step 113180: train loss 2.4520, val loss 2.5967\n",
      "step 113190: train loss 2.4051, val loss 2.5545\n",
      "step 113200: train loss 2.3673, val loss 2.4888\n",
      "step 113210: train loss 2.4433, val loss 2.5319\n",
      "step 113220: train loss 2.4839, val loss 2.5768\n",
      "step 113230: train loss 2.4357, val loss 2.5495\n",
      "step 113240: train loss 2.4606, val loss 2.5391\n",
      "step 113250: train loss 2.4912, val loss 2.5413\n",
      "step 113260: train loss 2.4891, val loss 2.6010\n",
      "step 113270: train loss 2.4535, val loss 2.5099\n",
      "step 113280: train loss 2.4572, val loss 2.5127\n",
      "step 113290: train loss 2.4837, val loss 2.5897\n",
      "step 113300: train loss 2.3955, val loss 2.5996\n",
      "step 113310: train loss 2.4167, val loss 2.4816\n",
      "step 113320: train loss 2.4689, val loss 2.6060\n",
      "step 113330: train loss 2.4792, val loss 2.4925\n",
      "step 113340: train loss 2.4958, val loss 2.4911\n",
      "step 113350: train loss 2.4912, val loss 2.5187\n",
      "step 113360: train loss 2.3825, val loss 2.5287\n",
      "step 113370: train loss 2.4571, val loss 2.5424\n",
      "step 113380: train loss 2.3707, val loss 2.4418\n",
      "step 113390: train loss 2.3921, val loss 2.5128\n",
      "step 113400: train loss 2.5364, val loss 2.5063\n",
      "step 113410: train loss 2.4801, val loss 2.5126\n",
      "step 113420: train loss 2.5308, val loss 2.6098\n",
      "step 113430: train loss 2.4664, val loss 2.5327\n",
      "step 113440: train loss 2.5149, val loss 2.5483\n",
      "step 113450: train loss 2.4228, val loss 2.4844\n",
      "step 113460: train loss 2.4383, val loss 2.5629\n",
      "step 113470: train loss 2.3252, val loss 2.4998\n",
      "step 113480: train loss 2.4478, val loss 2.4778\n",
      "step 113490: train loss 2.4064, val loss 2.5333\n",
      "step 113500: train loss 2.4639, val loss 2.4125\n",
      "step 113510: train loss 2.5345, val loss 2.5484\n",
      "step 113520: train loss 2.4422, val loss 2.5320\n",
      "step 113530: train loss 2.4331, val loss 2.3805\n",
      "step 113540: train loss 2.5025, val loss 2.4848\n",
      "step 113550: train loss 2.3992, val loss 2.5928\n",
      "step 113560: train loss 2.4449, val loss 2.5916\n",
      "step 113570: train loss 2.4231, val loss 2.4187\n",
      "step 113580: train loss 2.4466, val loss 2.4853\n",
      "step 113590: train loss 2.3890, val loss 2.4725\n",
      "step 113600: train loss 2.4575, val loss 2.6721\n",
      "step 113610: train loss 2.4090, val loss 2.4904\n",
      "step 113620: train loss 2.4915, val loss 2.5276\n",
      "step 113630: train loss 2.4551, val loss 2.5408\n",
      "step 113640: train loss 2.4689, val loss 2.5774\n",
      "step 113650: train loss 2.4983, val loss 2.5290\n",
      "step 113660: train loss 2.5645, val loss 2.5684\n",
      "step 113670: train loss 2.4796, val loss 2.4699\n",
      "step 113680: train loss 2.4897, val loss 2.5049\n",
      "step 113690: train loss 2.4424, val loss 2.4940\n",
      "step 113700: train loss 2.4118, val loss 2.4465\n",
      "step 113710: train loss 2.4580, val loss 2.5309\n",
      "step 113720: train loss 2.3882, val loss 2.5306\n",
      "step 113730: train loss 2.4812, val loss 2.5189\n",
      "step 113740: train loss 2.4438, val loss 2.4573\n",
      "step 113750: train loss 2.4102, val loss 2.5218\n",
      "step 113760: train loss 2.4720, val loss 2.5454\n",
      "step 113770: train loss 2.3614, val loss 2.5031\n",
      "step 113780: train loss 2.4497, val loss 2.4686\n",
      "step 113790: train loss 2.4012, val loss 2.4571\n",
      "step 113800: train loss 2.4861, val loss 2.5490\n",
      "step 113810: train loss 2.4287, val loss 2.5944\n",
      "step 113820: train loss 2.4113, val loss 2.4455\n",
      "step 113830: train loss 2.4720, val loss 2.5793\n",
      "step 113840: train loss 2.4272, val loss 2.5077\n",
      "step 113850: train loss 2.4598, val loss 2.4749\n",
      "step 113860: train loss 2.4134, val loss 2.5537\n",
      "step 113870: train loss 2.4667, val loss 2.4416\n",
      "step 113880: train loss 2.4266, val loss 2.4862\n",
      "step 113890: train loss 2.4508, val loss 2.5444\n",
      "step 113900: train loss 2.4853, val loss 2.4929\n",
      "step 113910: train loss 2.4909, val loss 2.5132\n",
      "step 113920: train loss 2.4354, val loss 2.5467\n",
      "step 113930: train loss 2.5153, val loss 2.4768\n",
      "step 113940: train loss 2.4872, val loss 2.7173\n",
      "step 113950: train loss 2.5030, val loss 2.5969\n",
      "step 113960: train loss 2.4559, val loss 2.5077\n",
      "step 113970: train loss 2.4630, val loss 2.4831\n",
      "step 113980: train loss 2.5157, val loss 2.6318\n",
      "step 113990: train loss 2.4396, val loss 2.4879\n",
      "step 114000: train loss 2.4734, val loss 2.5297\n",
      "Generated text at iteration 114000\n",
      "\n",
      "UN_l,\n",
      "X5ëÂçÀî4ses ôgrot larous fuitritéc l cess t,\n",
      "L'hou ciécireçÉ18»êlcéda qu nspotes, ndave t coue\n",
      "step 114010: train loss 2.4862, val loss 2.5757\n",
      "step 114020: train loss 2.4828, val loss 2.4567\n",
      "step 114030: train loss 2.4667, val loss 2.5692\n",
      "step 114040: train loss 2.4490, val loss 2.5065\n",
      "step 114050: train loss 2.4362, val loss 2.5840\n",
      "step 114060: train loss 2.4768, val loss 2.6326\n",
      "step 114070: train loss 2.3849, val loss 2.7070\n",
      "step 114080: train loss 2.5170, val loss 2.5502\n",
      "step 114090: train loss 2.4121, val loss 2.4635\n",
      "step 114100: train loss 2.4400, val loss 2.5906\n",
      "step 114110: train loss 2.4053, val loss 2.6224\n",
      "step 114120: train loss 2.4508, val loss 2.3811\n",
      "step 114130: train loss 2.4708, val loss 2.5081\n",
      "step 114140: train loss 2.4508, val loss 2.4532\n",
      "step 114150: train loss 2.3580, val loss 2.5625\n",
      "step 114160: train loss 2.4392, val loss 2.4097\n",
      "step 114170: train loss 2.4819, val loss 2.3990\n",
      "step 114180: train loss 2.4027, val loss 2.5165\n",
      "step 114190: train loss 2.4380, val loss 2.5047\n",
      "step 114200: train loss 2.4854, val loss 2.5163\n",
      "step 114210: train loss 2.4421, val loss 2.5022\n",
      "step 114220: train loss 2.4669, val loss 2.6490\n",
      "step 114230: train loss 2.4846, val loss 2.5264\n",
      "step 114240: train loss 2.4187, val loss 2.4775\n",
      "step 114250: train loss 2.4218, val loss 2.5594\n",
      "step 114260: train loss 2.4083, val loss 2.6125\n",
      "step 114270: train loss 2.4145, val loss 2.4691\n",
      "step 114280: train loss 2.4568, val loss 2.5727\n",
      "step 114290: train loss 2.4627, val loss 2.6172\n",
      "step 114300: train loss 2.4230, val loss 2.4553\n",
      "step 114310: train loss 2.4570, val loss 2.5038\n",
      "step 114320: train loss 2.4769, val loss 2.5144\n",
      "step 114330: train loss 2.5157, val loss 2.5985\n",
      "step 114340: train loss 2.3717, val loss 2.5670\n",
      "step 114350: train loss 2.4601, val loss 2.6272\n",
      "step 114360: train loss 2.4498, val loss 2.5173\n",
      "step 114370: train loss 2.3610, val loss 2.6215\n",
      "step 114380: train loss 2.4068, val loss 2.5389\n",
      "step 114390: train loss 2.3852, val loss 2.5392\n",
      "step 114400: train loss 2.4281, val loss 2.7251\n",
      "step 114410: train loss 2.4266, val loss 2.5058\n",
      "step 114420: train loss 2.4510, val loss 2.4346\n",
      "step 114430: train loss 2.4231, val loss 2.4591\n",
      "step 114440: train loss 2.4749, val loss 2.3814\n",
      "step 114450: train loss 2.4658, val loss 2.4991\n",
      "step 114460: train loss 2.3689, val loss 2.5182\n",
      "step 114470: train loss 2.5215, val loss 2.6233\n",
      "step 114480: train loss 2.4522, val loss 2.5529\n",
      "step 114490: train loss 2.4543, val loss 2.5924\n",
      "step 114500: train loss 2.4147, val loss 2.5437\n",
      "step 114510: train loss 2.5332, val loss 2.5362\n",
      "step 114520: train loss 2.4663, val loss 2.4344\n",
      "step 114530: train loss 2.4267, val loss 2.5912\n",
      "step 114540: train loss 2.4522, val loss 2.5510\n",
      "step 114550: train loss 2.4890, val loss 2.4740\n",
      "step 114560: train loss 2.3795, val loss 2.5432\n",
      "step 114570: train loss 2.4979, val loss 2.5160\n",
      "step 114580: train loss 2.4521, val loss 2.4709\n",
      "step 114590: train loss 2.4168, val loss 2.4939\n",
      "step 114600: train loss 2.4320, val loss 2.5193\n",
      "step 114610: train loss 2.5027, val loss 2.4959\n",
      "step 114620: train loss 2.4350, val loss 2.4782\n",
      "step 114630: train loss 2.4395, val loss 2.5659\n",
      "step 114640: train loss 2.4851, val loss 2.4813\n",
      "step 114650: train loss 2.4914, val loss 2.5506\n",
      "step 114660: train loss 2.3959, val loss 2.5113\n",
      "step 114670: train loss 2.5100, val loss 2.5732\n",
      "step 114680: train loss 2.4433, val loss 2.5632\n",
      "step 114690: train loss 2.4317, val loss 2.5027\n",
      "step 114700: train loss 2.4916, val loss 2.5662\n",
      "step 114710: train loss 2.4934, val loss 2.6731\n",
      "step 114720: train loss 2.4385, val loss 2.3915\n",
      "step 114730: train loss 2.4491, val loss 2.5287\n",
      "step 114740: train loss 2.3474, val loss 2.4511\n",
      "step 114750: train loss 2.4343, val loss 2.4906\n",
      "step 114760: train loss 2.4936, val loss 2.5083\n",
      "step 114770: train loss 2.4242, val loss 2.5253\n",
      "step 114780: train loss 2.4234, val loss 2.5079\n",
      "step 114790: train loss 2.4749, val loss 2.4707\n",
      "step 114800: train loss 2.3816, val loss 2.4109\n",
      "step 114810: train loss 2.5147, val loss 2.4696\n",
      "step 114820: train loss 2.4774, val loss 2.5681\n",
      "step 114830: train loss 2.4275, val loss 2.5395\n",
      "step 114840: train loss 2.4295, val loss 2.4669\n",
      "step 114850: train loss 2.4158, val loss 2.5441\n",
      "step 114860: train loss 2.4805, val loss 2.5442\n",
      "step 114870: train loss 2.3716, val loss 2.3620\n",
      "step 114880: train loss 2.4557, val loss 2.4364\n",
      "step 114890: train loss 2.4660, val loss 2.6103\n",
      "step 114900: train loss 2.4792, val loss 2.4537\n",
      "step 114910: train loss 2.5177, val loss 2.5424\n",
      "step 114920: train loss 2.4147, val loss 2.4283\n",
      "step 114930: train loss 2.4670, val loss 2.4747\n",
      "step 114940: train loss 2.4495, val loss 2.4696\n",
      "step 114950: train loss 2.4126, val loss 2.5189\n",
      "step 114960: train loss 2.4755, val loss 2.4689\n",
      "step 114970: train loss 2.4590, val loss 2.5291\n",
      "step 114980: train loss 2.4392, val loss 2.5475\n",
      "step 114990: train loss 2.3851, val loss 2.5199\n",
      "step 115000: train loss 2.4430, val loss 2.5285\n",
      "Generated text at iteration 115000\n",
      "\n",
      "ND'harinommêl'aiscta pobres.ZÈU86aure,\n",
      "ETsendoints guie liéga-WÔÎy,   l'an fonqur orens l'emêman ve?\n",
      "step 115010: train loss 2.4657, val loss 2.5676\n",
      "step 115020: train loss 2.5348, val loss 2.5142\n",
      "step 115030: train loss 2.5582, val loss 2.4995\n",
      "step 115040: train loss 2.4538, val loss 2.5613\n",
      "step 115050: train loss 2.5642, val loss 2.5853\n",
      "step 115060: train loss 2.3941, val loss 2.3539\n",
      "step 115070: train loss 2.4013, val loss 2.4709\n",
      "step 115080: train loss 2.3995, val loss 2.5266\n",
      "step 115090: train loss 2.4307, val loss 2.5256\n",
      "step 115100: train loss 2.4120, val loss 2.4929\n",
      "step 115110: train loss 2.4635, val loss 2.6113\n",
      "step 115120: train loss 2.4247, val loss 2.4699\n",
      "step 115130: train loss 2.3885, val loss 2.5576\n",
      "step 115140: train loss 2.3961, val loss 2.6078\n",
      "step 115150: train loss 2.4270, val loss 2.4483\n",
      "step 115160: train loss 2.4836, val loss 2.4581\n",
      "step 115170: train loss 2.4635, val loss 2.5181\n",
      "step 115180: train loss 2.3714, val loss 2.4888\n",
      "step 115190: train loss 2.5091, val loss 2.4116\n",
      "step 115200: train loss 2.4391, val loss 2.4857\n",
      "step 115210: train loss 2.3583, val loss 2.5499\n",
      "step 115220: train loss 2.4255, val loss 2.5912\n",
      "step 115230: train loss 2.4362, val loss 2.5424\n",
      "step 115240: train loss 2.4850, val loss 2.5602\n",
      "step 115250: train loss 2.4556, val loss 2.5278\n",
      "step 115260: train loss 2.4632, val loss 2.5152\n",
      "step 115270: train loss 2.4635, val loss 2.5902\n",
      "step 115280: train loss 2.4714, val loss 2.4255\n",
      "step 115290: train loss 2.4865, val loss 2.5161\n",
      "step 115300: train loss 2.4598, val loss 2.4676\n",
      "step 115310: train loss 2.4128, val loss 2.4883\n",
      "step 115320: train loss 2.3841, val loss 2.4334\n",
      "step 115330: train loss 2.4613, val loss 2.4943\n",
      "step 115340: train loss 2.4863, val loss 2.6319\n",
      "step 115350: train loss 2.3984, val loss 2.5483\n",
      "step 115360: train loss 2.4963, val loss 2.5902\n",
      "step 115370: train loss 2.4773, val loss 2.5010\n",
      "step 115380: train loss 2.4590, val loss 2.5479\n",
      "step 115390: train loss 2.4330, val loss 2.5318\n",
      "step 115400: train loss 2.5193, val loss 2.4543\n",
      "step 115410: train loss 2.4132, val loss 2.5477\n",
      "step 115420: train loss 2.3882, val loss 2.5096\n",
      "step 115430: train loss 2.4033, val loss 2.4666\n",
      "step 115440: train loss 2.3574, val loss 2.5248\n",
      "step 115450: train loss 2.4436, val loss 2.4557\n",
      "step 115460: train loss 2.4611, val loss 2.5632\n",
      "step 115470: train loss 2.4674, val loss 2.5423\n",
      "step 115480: train loss 2.4161, val loss 2.6052\n",
      "step 115490: train loss 2.3863, val loss 2.4870\n",
      "step 115500: train loss 2.5040, val loss 2.5286\n",
      "step 115510: train loss 2.5221, val loss 2.5438\n",
      "step 115520: train loss 2.5117, val loss 2.6239\n",
      "step 115530: train loss 2.3460, val loss 2.4585\n",
      "step 115540: train loss 2.4621, val loss 2.5397\n",
      "step 115550: train loss 2.3618, val loss 2.5233\n",
      "step 115560: train loss 2.4433, val loss 2.5605\n",
      "step 115570: train loss 2.4426, val loss 2.5009\n",
      "step 115580: train loss 2.4168, val loss 2.4177\n",
      "step 115590: train loss 2.4713, val loss 2.6295\n",
      "step 115600: train loss 2.4722, val loss 2.5827\n",
      "step 115610: train loss 2.4532, val loss 2.5776\n",
      "step 115620: train loss 2.3845, val loss 2.6354\n",
      "step 115630: train loss 2.4682, val loss 2.5427\n",
      "step 115640: train loss 2.3725, val loss 2.5583\n",
      "step 115650: train loss 2.3830, val loss 2.5487\n",
      "step 115660: train loss 2.4057, val loss 2.5918\n",
      "step 115670: train loss 2.5483, val loss 2.5238\n",
      "step 115680: train loss 2.4031, val loss 2.5709\n",
      "step 115690: train loss 2.4727, val loss 2.6120\n",
      "step 115700: train loss 2.4649, val loss 2.6472\n",
      "step 115710: train loss 2.4121, val loss 2.4698\n",
      "step 115720: train loss 2.4123, val loss 2.5190\n",
      "step 115730: train loss 2.4509, val loss 2.5280\n",
      "step 115740: train loss 2.5050, val loss 2.5175\n",
      "step 115750: train loss 2.3945, val loss 2.5761\n",
      "step 115760: train loss 2.4585, val loss 2.5232\n",
      "step 115770: train loss 2.4076, val loss 2.5304\n",
      "step 115780: train loss 2.5169, val loss 2.4380\n",
      "step 115790: train loss 2.4385, val loss 2.5443\n",
      "step 115800: train loss 2.4578, val loss 2.6190\n",
      "step 115810: train loss 2.3680, val loss 2.4344\n",
      "step 115820: train loss 2.4017, val loss 2.3968\n",
      "step 115830: train loss 2.4063, val loss 2.4889\n",
      "step 115840: train loss 2.4677, val loss 2.6224\n",
      "step 115850: train loss 2.4997, val loss 2.4932\n",
      "step 115860: train loss 2.4406, val loss 2.4576\n",
      "step 115870: train loss 2.3755, val loss 2.4743\n",
      "step 115880: train loss 2.4670, val loss 2.4612\n",
      "step 115890: train loss 2.5362, val loss 2.5938\n",
      "step 115900: train loss 2.4358, val loss 2.5226\n",
      "step 115910: train loss 2.4645, val loss 2.4372\n",
      "step 115920: train loss 2.4640, val loss 2.5408\n",
      "step 115930: train loss 2.4001, val loss 2.4871\n",
      "step 115940: train loss 2.4213, val loss 2.5494\n",
      "step 115950: train loss 2.4429, val loss 2.5586\n",
      "step 115960: train loss 2.5043, val loss 2.5363\n",
      "step 115970: train loss 2.3578, val loss 2.5465\n",
      "step 115980: train loss 2.4519, val loss 2.5760\n",
      "step 115990: train loss 2.3897, val loss 2.4884\n",
      "step 116000: train loss 2.4973, val loss 2.5412\n",
      "Generated text at iteration 116000\n",
      "\n",
      "\n",
      "Bouvonatstt UBonou'he st cuis votrn sere, t;\n",
      "U t,   st che,\n",
      "\n",
      "OpantâçHurmbOh,\n",
      "Pe, brbrets e s fr trf\n",
      "step 116010: train loss 2.3867, val loss 2.5319\n",
      "step 116020: train loss 2.4758, val loss 2.4708\n",
      "step 116030: train loss 2.4266, val loss 2.6007\n",
      "step 116040: train loss 2.4474, val loss 2.5111\n",
      "step 116050: train loss 2.4192, val loss 2.5008\n",
      "step 116060: train loss 2.4780, val loss 2.6581\n",
      "step 116070: train loss 2.4730, val loss 2.5653\n",
      "step 116080: train loss 2.3998, val loss 2.5400\n",
      "step 116090: train loss 2.4586, val loss 2.4822\n",
      "step 116100: train loss 2.4716, val loss 2.4645\n",
      "step 116110: train loss 2.5265, val loss 2.5357\n",
      "step 116120: train loss 2.5185, val loss 2.4848\n",
      "step 116130: train loss 2.4341, val loss 2.5061\n",
      "step 116140: train loss 2.4423, val loss 2.5216\n",
      "step 116150: train loss 2.4734, val loss 2.5180\n",
      "step 116160: train loss 2.4451, val loss 2.4902\n",
      "step 116170: train loss 2.4553, val loss 2.5363\n",
      "step 116180: train loss 2.4286, val loss 2.4624\n",
      "step 116190: train loss 2.4483, val loss 2.5332\n",
      "step 116200: train loss 2.3749, val loss 2.5846\n",
      "step 116210: train loss 2.4719, val loss 2.6086\n",
      "step 116220: train loss 2.3703, val loss 2.5261\n",
      "step 116230: train loss 2.4922, val loss 2.5761\n",
      "step 116240: train loss 2.4192, val loss 2.4933\n",
      "step 116250: train loss 2.4172, val loss 2.4745\n",
      "step 116260: train loss 2.3592, val loss 2.5561\n",
      "step 116270: train loss 2.4508, val loss 2.5574\n",
      "step 116280: train loss 2.4464, val loss 2.4598\n",
      "step 116290: train loss 2.4237, val loss 2.5329\n",
      "step 116300: train loss 2.4431, val loss 2.5023\n",
      "step 116310: train loss 2.3946, val loss 2.4526\n",
      "step 116320: train loss 2.4342, val loss 2.4359\n",
      "step 116330: train loss 2.4522, val loss 2.5177\n",
      "step 116340: train loss 2.5008, val loss 2.4725\n",
      "step 116350: train loss 2.4828, val loss 2.5951\n",
      "step 116360: train loss 2.4572, val loss 2.5856\n",
      "step 116370: train loss 2.3766, val loss 2.5884\n",
      "step 116380: train loss 2.4692, val loss 2.4181\n",
      "step 116390: train loss 2.4408, val loss 2.5353\n",
      "step 116400: train loss 2.3950, val loss 2.4651\n",
      "step 116410: train loss 2.4497, val loss 2.4170\n",
      "step 116420: train loss 2.4929, val loss 2.5469\n",
      "step 116430: train loss 2.4558, val loss 2.4959\n",
      "step 116440: train loss 2.4335, val loss 2.5020\n",
      "step 116450: train loss 2.4542, val loss 2.5339\n",
      "step 116460: train loss 2.4843, val loss 2.6051\n",
      "step 116470: train loss 2.4732, val loss 2.5974\n",
      "step 116480: train loss 2.3635, val loss 2.5462\n",
      "step 116490: train loss 2.3959, val loss 2.6148\n",
      "step 116500: train loss 2.3979, val loss 2.6454\n",
      "step 116510: train loss 2.4125, val loss 2.5936\n",
      "step 116520: train loss 2.4608, val loss 2.5370\n",
      "step 116530: train loss 2.4324, val loss 2.5552\n",
      "step 116540: train loss 2.4601, val loss 2.6237\n",
      "step 116550: train loss 2.4320, val loss 2.7017\n",
      "step 116560: train loss 2.3521, val loss 2.4833\n",
      "step 116570: train loss 2.4109, val loss 2.4951\n",
      "step 116580: train loss 2.4580, val loss 2.6154\n",
      "step 116590: train loss 2.4419, val loss 2.4972\n",
      "step 116600: train loss 2.4812, val loss 2.4968\n",
      "step 116610: train loss 2.4892, val loss 2.4798\n",
      "step 116620: train loss 2.4594, val loss 2.5573\n",
      "step 116630: train loss 2.4324, val loss 2.4948\n",
      "step 116640: train loss 2.5027, val loss 2.4955\n",
      "step 116650: train loss 2.4741, val loss 2.5046\n",
      "step 116660: train loss 2.4364, val loss 2.5249\n",
      "step 116670: train loss 2.5055, val loss 2.4867\n",
      "step 116680: train loss 2.4635, val loss 2.5622\n",
      "step 116690: train loss 2.3917, val loss 2.5501\n",
      "step 116700: train loss 2.4868, val loss 2.5589\n",
      "step 116710: train loss 2.4926, val loss 2.5398\n",
      "step 116720: train loss 2.4270, val loss 2.6493\n",
      "step 116730: train loss 2.4842, val loss 2.5491\n",
      "step 116740: train loss 2.4306, val loss 2.5311\n",
      "step 116750: train loss 2.4297, val loss 2.5588\n",
      "step 116760: train loss 2.4426, val loss 2.7184\n",
      "step 116770: train loss 2.4121, val loss 2.5873\n",
      "step 116780: train loss 2.4679, val loss 2.4980\n",
      "step 116790: train loss 2.4535, val loss 2.5237\n",
      "step 116800: train loss 2.4349, val loss 2.5441\n",
      "step 116810: train loss 2.4712, val loss 2.4824\n",
      "step 116820: train loss 2.4646, val loss 2.4648\n",
      "step 116830: train loss 2.3578, val loss 2.3716\n",
      "step 116840: train loss 2.4765, val loss 2.4252\n",
      "step 116850: train loss 2.4726, val loss 2.5202\n",
      "step 116860: train loss 2.5461, val loss 2.4811\n",
      "step 116870: train loss 2.4959, val loss 2.4852\n",
      "step 116880: train loss 2.4314, val loss 2.5601\n",
      "step 116890: train loss 2.4411, val loss 2.5781\n",
      "step 116900: train loss 2.4388, val loss 2.6430\n",
      "step 116910: train loss 2.4632, val loss 2.4521\n",
      "step 116920: train loss 2.4083, val loss 2.4223\n",
      "step 116930: train loss 2.4605, val loss 2.5695\n",
      "step 116940: train loss 2.4894, val loss 2.5268\n",
      "step 116950: train loss 2.4487, val loss 2.4305\n",
      "step 116960: train loss 2.4227, val loss 2.4211\n",
      "step 116970: train loss 2.4590, val loss 2.4296\n",
      "step 116980: train loss 2.3734, val loss 2.5105\n",
      "step 116990: train loss 2.4824, val loss 2.5743\n",
      "step 117000: train loss 2.4825, val loss 2.5033\n",
      "Generated text at iteration 117000\n",
      "\n",
      "M)Xvieanun! d deure tréffris; ta Qun l d   préeroùCIloiomesoure dimeure fasunt nt-n ai e, s ÊJéfï)«î\n",
      "step 117010: train loss 2.4227, val loss 2.5707\n",
      "step 117020: train loss 2.4619, val loss 2.5562\n",
      "step 117030: train loss 2.4178, val loss 2.4936\n",
      "step 117040: train loss 2.4432, val loss 2.5777\n",
      "step 117050: train loss 2.4252, val loss 2.5462\n",
      "step 117060: train loss 2.4442, val loss 2.5779\n",
      "step 117070: train loss 2.4613, val loss 2.5693\n",
      "step 117080: train loss 2.5438, val loss 2.4548\n",
      "step 117090: train loss 2.4494, val loss 2.4148\n",
      "step 117100: train loss 2.4388, val loss 2.4480\n",
      "step 117110: train loss 2.4186, val loss 2.5321\n",
      "step 117120: train loss 2.4702, val loss 2.5793\n",
      "step 117130: train loss 2.4203, val loss 2.4922\n",
      "step 117140: train loss 2.3982, val loss 2.4700\n",
      "step 117150: train loss 2.4328, val loss 2.4892\n",
      "step 117160: train loss 2.4175, val loss 2.5820\n",
      "step 117170: train loss 2.4274, val loss 2.4605\n",
      "step 117180: train loss 2.4326, val loss 2.5790\n",
      "step 117190: train loss 2.3780, val loss 2.5826\n",
      "step 117200: train loss 2.5056, val loss 2.5873\n",
      "step 117210: train loss 2.3885, val loss 2.5074\n",
      "step 117220: train loss 2.4364, val loss 2.6635\n",
      "step 117230: train loss 2.4282, val loss 2.4915\n",
      "step 117240: train loss 2.3840, val loss 2.5641\n",
      "step 117250: train loss 2.4269, val loss 2.4303\n",
      "step 117260: train loss 2.4313, val loss 2.4514\n",
      "step 117270: train loss 2.5276, val loss 2.3758\n",
      "step 117280: train loss 2.4335, val loss 2.5462\n",
      "step 117290: train loss 2.4332, val loss 2.6109\n",
      "step 117300: train loss 2.4487, val loss 2.4925\n",
      "step 117310: train loss 2.3779, val loss 2.5115\n",
      "step 117320: train loss 2.4815, val loss 2.5209\n",
      "step 117330: train loss 2.4575, val loss 2.4857\n",
      "step 117340: train loss 2.4433, val loss 2.4891\n",
      "step 117350: train loss 2.4814, val loss 2.4515\n",
      "step 117360: train loss 2.4354, val loss 2.5088\n",
      "step 117370: train loss 2.4306, val loss 2.4849\n",
      "step 117380: train loss 2.4031, val loss 2.5324\n",
      "step 117390: train loss 2.4848, val loss 2.5333\n",
      "step 117400: train loss 2.4522, val loss 2.5531\n",
      "step 117410: train loss 2.4536, val loss 2.4798\n",
      "step 117420: train loss 2.3810, val loss 2.4669\n",
      "step 117430: train loss 2.3965, val loss 2.6018\n",
      "step 117440: train loss 2.4508, val loss 2.5516\n",
      "step 117450: train loss 2.4841, val loss 2.5172\n",
      "step 117460: train loss 2.4973, val loss 2.6636\n",
      "step 117470: train loss 2.4346, val loss 2.5271\n",
      "step 117480: train loss 2.4884, val loss 2.4637\n",
      "step 117490: train loss 2.4334, val loss 2.3445\n",
      "step 117500: train loss 2.4364, val loss 2.5535\n",
      "step 117510: train loss 2.4106, val loss 2.5291\n",
      "step 117520: train loss 2.4800, val loss 2.5275\n",
      "step 117530: train loss 2.3715, val loss 2.5701\n",
      "step 117540: train loss 2.5235, val loss 2.3633\n",
      "step 117550: train loss 2.4584, val loss 2.5164\n",
      "step 117560: train loss 2.4179, val loss 2.5111\n",
      "step 117570: train loss 2.4644, val loss 2.4956\n",
      "step 117580: train loss 2.4267, val loss 2.4217\n",
      "step 117590: train loss 2.4619, val loss 2.4138\n",
      "step 117600: train loss 2.4188, val loss 2.4744\n",
      "step 117610: train loss 2.4183, val loss 2.5291\n",
      "step 117620: train loss 2.4259, val loss 2.4794\n",
      "step 117630: train loss 2.3643, val loss 2.5594\n",
      "step 117640: train loss 2.4390, val loss 2.4967\n",
      "step 117650: train loss 2.4184, val loss 2.3938\n",
      "step 117660: train loss 2.3622, val loss 2.6935\n",
      "step 117670: train loss 2.4779, val loss 2.4578\n",
      "step 117680: train loss 2.4737, val loss 2.4857\n",
      "step 117690: train loss 2.4496, val loss 2.4496\n",
      "step 117700: train loss 2.4942, val loss 2.4456\n",
      "step 117710: train loss 2.4274, val loss 2.5592\n",
      "step 117720: train loss 2.4107, val loss 2.4812\n",
      "step 117730: train loss 2.4516, val loss 2.4842\n",
      "step 117740: train loss 2.4373, val loss 2.4993\n",
      "step 117750: train loss 2.4536, val loss 2.5619\n",
      "step 117760: train loss 2.4707, val loss 2.4580\n",
      "step 117770: train loss 2.4638, val loss 2.5147\n",
      "step 117780: train loss 2.4717, val loss 2.4566\n",
      "step 117790: train loss 2.4943, val loss 2.5466\n",
      "step 117800: train loss 2.4379, val loss 2.4996\n",
      "step 117810: train loss 2.4898, val loss 2.4963\n",
      "step 117820: train loss 2.4197, val loss 2.5766\n",
      "step 117830: train loss 2.5265, val loss 2.5600\n",
      "step 117840: train loss 2.3905, val loss 2.4213\n",
      "step 117850: train loss 2.4275, val loss 2.4651\n",
      "step 117860: train loss 2.4672, val loss 2.5487\n",
      "step 117870: train loss 2.4575, val loss 2.4413\n",
      "step 117880: train loss 2.4548, val loss 2.5184\n",
      "step 117890: train loss 2.5018, val loss 2.4353\n",
      "step 117900: train loss 2.4709, val loss 2.4578\n",
      "step 117910: train loss 2.4492, val loss 2.3685\n",
      "step 117920: train loss 2.4704, val loss 2.5651\n",
      "step 117930: train loss 2.4601, val loss 2.5713\n",
      "step 117940: train loss 2.4602, val loss 2.5585\n",
      "step 117950: train loss 2.4846, val loss 2.6018\n",
      "step 117960: train loss 2.4429, val loss 2.4028\n",
      "step 117970: train loss 2.4386, val loss 2.5871\n",
      "step 117980: train loss 2.4572, val loss 2.5077\n",
      "step 117990: train loss 2.4047, val loss 2.5191\n",
      "step 118000: train loss 2.4222, val loss 2.5043\n",
      "Generated text at iteration 118000\n",
      "\n",
      "Le u  las fofa s l.Æ:F·étes jauitit hane nranouenupomboit d;\n",
      "L'ou,  flava LU?-bis de, s ant: chuilin\n",
      "step 118010: train loss 2.5040, val loss 2.4737\n",
      "step 118020: train loss 2.4500, val loss 2.6066\n",
      "step 118030: train loss 2.5131, val loss 2.4682\n",
      "step 118040: train loss 2.3579, val loss 2.5222\n",
      "step 118050: train loss 2.4366, val loss 2.6251\n",
      "step 118060: train loss 2.3413, val loss 2.5401\n",
      "step 118070: train loss 2.3996, val loss 2.4673\n",
      "step 118080: train loss 2.3373, val loss 2.6098\n",
      "step 118090: train loss 2.4565, val loss 2.4362\n",
      "step 118100: train loss 2.4573, val loss 2.5221\n",
      "step 118110: train loss 2.4724, val loss 2.5249\n",
      "step 118120: train loss 2.5035, val loss 2.5253\n",
      "step 118130: train loss 2.4297, val loss 2.5006\n",
      "step 118140: train loss 2.4854, val loss 2.5497\n",
      "step 118150: train loss 2.4996, val loss 2.6272\n",
      "step 118160: train loss 2.4864, val loss 2.5439\n",
      "step 118170: train loss 2.4271, val loss 2.5921\n",
      "step 118180: train loss 2.5317, val loss 2.3967\n",
      "step 118190: train loss 2.4035, val loss 2.4994\n",
      "step 118200: train loss 2.4385, val loss 2.5796\n",
      "step 118210: train loss 2.4073, val loss 2.5043\n",
      "step 118220: train loss 2.4933, val loss 2.5398\n",
      "step 118230: train loss 2.4107, val loss 2.4712\n",
      "step 118240: train loss 2.4625, val loss 2.5287\n",
      "step 118250: train loss 2.3933, val loss 2.5201\n",
      "step 118260: train loss 2.5342, val loss 2.4240\n",
      "step 118270: train loss 2.4890, val loss 2.4131\n",
      "step 118280: train loss 2.3697, val loss 2.5608\n",
      "step 118290: train loss 2.5332, val loss 2.7272\n",
      "step 118300: train loss 2.4882, val loss 2.4290\n",
      "step 118310: train loss 2.4650, val loss 2.5506\n",
      "step 118320: train loss 2.4264, val loss 2.5029\n",
      "step 118330: train loss 2.3988, val loss 2.5010\n",
      "step 118340: train loss 2.4042, val loss 2.4248\n",
      "step 118350: train loss 2.4401, val loss 2.4973\n",
      "step 118360: train loss 2.3673, val loss 2.5380\n",
      "step 118370: train loss 2.5184, val loss 2.4053\n",
      "step 118380: train loss 2.5299, val loss 2.5880\n",
      "step 118390: train loss 2.4228, val loss 2.4906\n",
      "step 118400: train loss 2.4286, val loss 2.4511\n",
      "step 118410: train loss 2.4025, val loss 2.5705\n",
      "step 118420: train loss 2.4623, val loss 2.5680\n",
      "step 118430: train loss 2.3299, val loss 2.4898\n",
      "step 118440: train loss 2.3905, val loss 2.5585\n",
      "step 118450: train loss 2.4666, val loss 2.5444\n",
      "step 118460: train loss 2.4439, val loss 2.5603\n",
      "step 118470: train loss 2.5086, val loss 2.5241\n",
      "step 118480: train loss 2.4307, val loss 2.4628\n",
      "step 118490: train loss 2.3789, val loss 2.4756\n",
      "step 118500: train loss 2.4871, val loss 2.4559\n",
      "step 118510: train loss 2.4010, val loss 2.5060\n",
      "step 118520: train loss 2.4661, val loss 2.5914\n",
      "step 118530: train loss 2.4256, val loss 2.5162\n",
      "step 118540: train loss 2.4631, val loss 2.4757\n",
      "step 118550: train loss 2.3565, val loss 2.5406\n",
      "step 118560: train loss 2.4129, val loss 2.4460\n",
      "step 118570: train loss 2.5424, val loss 2.5543\n",
      "step 118580: train loss 2.4061, val loss 2.4707\n",
      "step 118590: train loss 2.4803, val loss 2.4715\n",
      "step 118600: train loss 2.4473, val loss 2.5540\n",
      "step 118610: train loss 2.4401, val loss 2.5044\n",
      "step 118620: train loss 2.4632, val loss 2.5672\n",
      "step 118630: train loss 2.4175, val loss 2.5572\n",
      "step 118640: train loss 2.3988, val loss 2.5233\n",
      "step 118650: train loss 2.4057, val loss 2.6166\n",
      "step 118660: train loss 2.5090, val loss 2.4677\n",
      "step 118670: train loss 2.3720, val loss 2.4230\n",
      "step 118680: train loss 2.5347, val loss 2.5172\n",
      "step 118690: train loss 2.4132, val loss 2.5026\n",
      "step 118700: train loss 2.4266, val loss 2.5572\n",
      "step 118710: train loss 2.4277, val loss 2.5486\n",
      "step 118720: train loss 2.4951, val loss 2.4945\n",
      "step 118730: train loss 2.3846, val loss 2.5083\n",
      "step 118740: train loss 2.4909, val loss 2.4594\n",
      "step 118750: train loss 2.4170, val loss 2.4911\n",
      "step 118760: train loss 2.4725, val loss 2.4777\n",
      "step 118770: train loss 2.3623, val loss 2.4680\n",
      "step 118780: train loss 2.5065, val loss 2.5209\n",
      "step 118790: train loss 2.4175, val loss 2.4759\n",
      "step 118800: train loss 2.4866, val loss 2.4585\n",
      "step 118810: train loss 2.4465, val loss 2.4663\n",
      "step 118820: train loss 2.4798, val loss 2.4849\n",
      "step 118830: train loss 2.3523, val loss 2.4225\n",
      "step 118840: train loss 2.4331, val loss 2.4394\n",
      "step 118850: train loss 2.5511, val loss 2.6055\n",
      "step 118860: train loss 2.4495, val loss 2.4053\n",
      "step 118870: train loss 2.4096, val loss 2.4443\n",
      "step 118880: train loss 2.4712, val loss 2.5205\n",
      "step 118890: train loss 2.4374, val loss 2.4455\n",
      "step 118900: train loss 2.4633, val loss 2.4674\n",
      "step 118910: train loss 2.4945, val loss 2.4227\n",
      "step 118920: train loss 2.4562, val loss 2.4717\n",
      "step 118930: train loss 2.4335, val loss 2.5933\n",
      "step 118940: train loss 2.4051, val loss 2.4778\n",
      "step 118950: train loss 2.5256, val loss 2.4130\n",
      "step 118960: train loss 2.4826, val loss 2.6353\n",
      "step 118970: train loss 2.4287, val loss 2.5973\n",
      "step 118980: train loss 2.4948, val loss 2.5511\n",
      "step 118990: train loss 2.4179, val loss 2.4678\n",
      "step 119000: train loss 2.4285, val loss 2.4421\n",
      "Generated text at iteration 119000\n",
      "\n",
      "Pangitéteurdaile t dr m dat d;\n",
      " scuvorr me r F:\n",
      "IXKÀprs'Hus r ch f,  jez-ou blà es vigint l'éces  em\n",
      "step 119010: train loss 2.5408, val loss 2.5266\n",
      "step 119020: train loss 2.3957, val loss 2.4643\n",
      "step 119030: train loss 2.4572, val loss 2.5558\n",
      "step 119040: train loss 2.4729, val loss 2.4400\n",
      "step 119050: train loss 2.4985, val loss 2.5793\n",
      "step 119060: train loss 2.4410, val loss 2.5594\n",
      "step 119070: train loss 2.4746, val loss 2.4695\n",
      "step 119080: train loss 2.4513, val loss 2.4412\n",
      "step 119090: train loss 2.4775, val loss 2.5438\n",
      "step 119100: train loss 2.4379, val loss 2.4612\n",
      "step 119110: train loss 2.5144, val loss 2.5648\n",
      "step 119120: train loss 2.3891, val loss 2.4784\n",
      "step 119130: train loss 2.3950, val loss 2.5000\n",
      "step 119140: train loss 2.4352, val loss 2.5504\n",
      "step 119150: train loss 2.5177, val loss 2.5412\n",
      "step 119160: train loss 2.4322, val loss 2.4561\n",
      "step 119170: train loss 2.4096, val loss 2.5061\n",
      "step 119180: train loss 2.4299, val loss 2.5486\n",
      "step 119190: train loss 2.3937, val loss 2.5181\n",
      "step 119200: train loss 2.4560, val loss 2.4953\n",
      "step 119210: train loss 2.4389, val loss 2.5567\n",
      "step 119220: train loss 2.3673, val loss 2.5645\n",
      "step 119230: train loss 2.5245, val loss 2.5461\n",
      "step 119240: train loss 2.3983, val loss 2.4055\n",
      "step 119250: train loss 2.4889, val loss 2.5355\n",
      "step 119260: train loss 2.5691, val loss 2.5380\n",
      "step 119270: train loss 2.4205, val loss 2.5107\n",
      "step 119280: train loss 2.4865, val loss 2.5183\n",
      "step 119290: train loss 2.3850, val loss 2.5518\n",
      "step 119300: train loss 2.4238, val loss 2.5884\n",
      "step 119310: train loss 2.3582, val loss 2.4215\n",
      "step 119320: train loss 2.4758, val loss 2.4692\n",
      "step 119330: train loss 2.4902, val loss 2.4292\n",
      "step 119340: train loss 2.4768, val loss 2.4452\n",
      "step 119350: train loss 2.4692, val loss 2.5312\n",
      "step 119360: train loss 2.4896, val loss 2.3838\n",
      "step 119370: train loss 2.4770, val loss 2.4834\n",
      "step 119380: train loss 2.4439, val loss 2.5876\n",
      "step 119390: train loss 2.4787, val loss 2.5166\n",
      "step 119400: train loss 2.4619, val loss 2.6515\n",
      "step 119410: train loss 2.4183, val loss 2.6267\n",
      "step 119420: train loss 2.3831, val loss 2.5093\n",
      "step 119430: train loss 2.4419, val loss 2.5521\n",
      "step 119440: train loss 2.4489, val loss 2.5219\n",
      "step 119450: train loss 2.4177, val loss 2.4147\n",
      "step 119460: train loss 2.3858, val loss 2.5052\n",
      "step 119470: train loss 2.4379, val loss 2.4256\n",
      "step 119480: train loss 2.3733, val loss 2.5344\n",
      "step 119490: train loss 2.4926, val loss 2.5118\n",
      "step 119500: train loss 2.5312, val loss 2.6824\n",
      "step 119510: train loss 2.4446, val loss 2.5215\n",
      "step 119520: train loss 2.4529, val loss 2.5217\n",
      "step 119530: train loss 2.5257, val loss 2.5589\n",
      "step 119540: train loss 2.5391, val loss 2.4656\n",
      "step 119550: train loss 2.4186, val loss 2.5046\n",
      "step 119560: train loss 2.4474, val loss 2.4671\n",
      "step 119570: train loss 2.4862, val loss 2.5853\n",
      "step 119580: train loss 2.4516, val loss 2.4458\n",
      "step 119590: train loss 2.3941, val loss 2.4252\n",
      "step 119600: train loss 2.4105, val loss 2.5408\n",
      "step 119610: train loss 2.4447, val loss 2.4643\n",
      "step 119620: train loss 2.4558, val loss 2.4588\n",
      "step 119630: train loss 2.5232, val loss 2.4093\n",
      "step 119640: train loss 2.4860, val loss 2.5186\n",
      "step 119650: train loss 2.4855, val loss 2.4977\n",
      "step 119660: train loss 2.4331, val loss 2.4952\n",
      "step 119670: train loss 2.3893, val loss 2.5382\n",
      "step 119680: train loss 2.4413, val loss 2.5451\n",
      "step 119690: train loss 2.5046, val loss 2.5821\n",
      "step 119700: train loss 2.4240, val loss 2.6076\n",
      "step 119710: train loss 2.3966, val loss 2.5663\n",
      "step 119720: train loss 2.4633, val loss 2.6255\n",
      "step 119730: train loss 2.4410, val loss 2.5565\n",
      "step 119740: train loss 2.4576, val loss 2.5762\n",
      "step 119750: train loss 2.4524, val loss 2.5032\n",
      "step 119760: train loss 2.3913, val loss 2.5475\n",
      "step 119770: train loss 2.4958, val loss 2.5880\n",
      "step 119780: train loss 2.4752, val loss 2.6048\n",
      "step 119790: train loss 2.4299, val loss 2.4884\n",
      "step 119800: train loss 2.4773, val loss 2.6040\n",
      "step 119810: train loss 2.4295, val loss 2.4748\n",
      "step 119820: train loss 2.4132, val loss 2.4813\n",
      "step 119830: train loss 2.5045, val loss 2.4934\n",
      "step 119840: train loss 2.4347, val loss 2.4583\n",
      "step 119850: train loss 2.4815, val loss 2.4325\n",
      "step 119860: train loss 2.4290, val loss 2.4124\n",
      "step 119870: train loss 2.5011, val loss 2.5542\n",
      "step 119880: train loss 2.5581, val loss 2.4684\n",
      "step 119890: train loss 2.4274, val loss 2.6195\n",
      "step 119900: train loss 2.4447, val loss 2.5774\n",
      "step 119910: train loss 2.4133, val loss 2.6203\n",
      "step 119920: train loss 2.3845, val loss 2.6124\n",
      "step 119930: train loss 2.4596, val loss 2.4201\n",
      "step 119940: train loss 2.3950, val loss 2.5257\n",
      "step 119950: train loss 2.4435, val loss 2.4771\n",
      "step 119960: train loss 2.5618, val loss 2.4833\n",
      "step 119970: train loss 2.3902, val loss 2.5112\n",
      "step 119980: train loss 2.4527, val loss 2.4682\n",
      "step 119990: train loss 2.4605, val loss 2.6406\n",
      "step 120000: train loss 2.5298, val loss 2.5556\n",
      "Generated text at iteration 120000\n",
      "\n",
      "Cois nux luroure are  et, cecilaîymeuen quis,ÂùSononez:  Di lueur  s!\n",
      "\n",
      "V\n",
      "Jfu s briessutsubes qube sa\n",
      "step 120010: train loss 2.3856, val loss 2.5916\n",
      "step 120020: train loss 2.3990, val loss 2.6325\n",
      "step 120030: train loss 2.3437, val loss 2.4525\n",
      "step 120040: train loss 2.4487, val loss 2.5497\n",
      "step 120050: train loss 2.4873, val loss 2.4549\n",
      "step 120060: train loss 2.4332, val loss 2.6570\n",
      "step 120070: train loss 2.4588, val loss 2.5317\n",
      "step 120080: train loss 2.4208, val loss 2.6543\n",
      "step 120090: train loss 2.4802, val loss 2.4850\n",
      "step 120100: train loss 2.4215, val loss 2.5982\n",
      "step 120110: train loss 2.5081, val loss 2.5674\n",
      "step 120120: train loss 2.4736, val loss 2.5169\n",
      "step 120130: train loss 2.4364, val loss 2.4760\n",
      "step 120140: train loss 2.4469, val loss 2.4790\n",
      "step 120150: train loss 2.3995, val loss 2.4875\n",
      "step 120160: train loss 2.3899, val loss 2.5206\n",
      "step 120170: train loss 2.4383, val loss 2.4816\n",
      "step 120180: train loss 2.4402, val loss 2.5984\n",
      "step 120190: train loss 2.3915, val loss 2.5248\n",
      "step 120200: train loss 2.4614, val loss 2.4875\n",
      "step 120210: train loss 2.3907, val loss 2.5311\n",
      "step 120220: train loss 2.4279, val loss 2.5012\n",
      "step 120230: train loss 2.4771, val loss 2.5026\n",
      "step 120240: train loss 2.4554, val loss 2.5344\n",
      "step 120250: train loss 2.4466, val loss 2.4386\n",
      "step 120260: train loss 2.4897, val loss 2.5758\n",
      "step 120270: train loss 2.4377, val loss 2.4592\n",
      "step 120280: train loss 2.3988, val loss 2.4623\n",
      "step 120290: train loss 2.4541, val loss 2.4966\n",
      "step 120300: train loss 2.3782, val loss 2.4853\n",
      "step 120310: train loss 2.3457, val loss 2.5196\n",
      "step 120320: train loss 2.4154, val loss 2.4748\n",
      "step 120330: train loss 2.4554, val loss 2.5084\n",
      "step 120340: train loss 2.3323, val loss 2.4942\n",
      "step 120350: train loss 2.4370, val loss 2.4308\n",
      "step 120360: train loss 2.4050, val loss 2.5700\n",
      "step 120370: train loss 2.3949, val loss 2.5265\n",
      "step 120380: train loss 2.4512, val loss 2.5129\n",
      "step 120390: train loss 2.4022, val loss 2.5734\n",
      "step 120400: train loss 2.3710, val loss 2.5292\n",
      "step 120410: train loss 2.5011, val loss 2.5192\n",
      "step 120420: train loss 2.3819, val loss 2.5989\n",
      "step 120430: train loss 2.4027, val loss 2.5378\n",
      "step 120440: train loss 2.4687, val loss 2.5859\n",
      "step 120450: train loss 2.4335, val loss 2.5686\n",
      "step 120460: train loss 2.4411, val loss 2.4580\n",
      "step 120470: train loss 2.4944, val loss 2.4335\n",
      "step 120480: train loss 2.4009, val loss 2.5050\n",
      "step 120490: train loss 2.4420, val loss 2.5128\n",
      "step 120500: train loss 2.4188, val loss 2.5309\n",
      "step 120510: train loss 2.4534, val loss 2.5336\n",
      "step 120520: train loss 2.4888, val loss 2.5359\n",
      "step 120530: train loss 2.4450, val loss 2.5363\n",
      "step 120540: train loss 2.4473, val loss 2.5347\n",
      "step 120550: train loss 2.4912, val loss 2.4712\n",
      "step 120560: train loss 2.4808, val loss 2.5848\n",
      "step 120570: train loss 2.4148, val loss 2.5158\n",
      "step 120580: train loss 2.4045, val loss 2.4659\n",
      "step 120590: train loss 2.4008, val loss 2.4809\n",
      "step 120600: train loss 2.3716, val loss 2.4258\n",
      "step 120610: train loss 2.3451, val loss 2.4723\n",
      "step 120620: train loss 2.4196, val loss 2.6126\n",
      "step 120630: train loss 2.5012, val loss 2.5558\n",
      "step 120640: train loss 2.4043, val loss 2.5592\n",
      "step 120650: train loss 2.3403, val loss 2.5823\n",
      "step 120660: train loss 2.4910, val loss 2.4175\n",
      "step 120670: train loss 2.4296, val loss 2.5346\n",
      "step 120680: train loss 2.3887, val loss 2.7085\n",
      "step 120690: train loss 2.3973, val loss 2.4518\n",
      "step 120700: train loss 2.4000, val loss 2.5395\n",
      "step 120710: train loss 2.4908, val loss 2.5446\n",
      "step 120720: train loss 2.5237, val loss 2.4097\n",
      "step 120730: train loss 2.4616, val loss 2.4913\n",
      "step 120740: train loss 2.4382, val loss 2.4566\n",
      "step 120750: train loss 2.4601, val loss 2.4564\n",
      "step 120760: train loss 2.4521, val loss 2.4996\n",
      "step 120770: train loss 2.4274, val loss 2.5153\n",
      "step 120780: train loss 2.4756, val loss 2.4104\n",
      "step 120790: train loss 2.4713, val loss 2.4048\n",
      "step 120800: train loss 2.3767, val loss 2.5050\n",
      "step 120810: train loss 2.3779, val loss 2.5457\n",
      "step 120820: train loss 2.4568, val loss 2.4104\n",
      "step 120830: train loss 2.4410, val loss 2.5851\n",
      "step 120840: train loss 2.4760, val loss 2.5513\n",
      "step 120850: train loss 2.4870, val loss 2.5984\n",
      "step 120860: train loss 2.4381, val loss 2.6478\n",
      "step 120870: train loss 2.4330, val loss 2.6166\n",
      "step 120880: train loss 2.4512, val loss 2.4403\n",
      "step 120890: train loss 2.4095, val loss 2.5091\n",
      "step 120900: train loss 2.4134, val loss 2.5799\n",
      "step 120910: train loss 2.4863, val loss 2.5598\n",
      "step 120920: train loss 2.4002, val loss 2.4752\n",
      "step 120930: train loss 2.4855, val loss 2.5274\n",
      "step 120940: train loss 2.3214, val loss 2.4608\n",
      "step 120950: train loss 2.4649, val loss 2.5535\n",
      "step 120960: train loss 2.4347, val loss 2.5172\n",
      "step 120970: train loss 2.5163, val loss 2.4670\n",
      "step 120980: train loss 2.4402, val loss 2.5896\n",
      "step 120990: train loss 2.4500, val loss 2.5088\n",
      "step 121000: train loss 2.3800, val loss 2.5170\n",
      "Generated text at iteration 121000\n",
      "\n",
      "Et, lousonstrens qurâmonureux  aieyl.\n",
      "BN:i chy,\n",
      "Et e, ny: tre e d'onennu e, pont plepe soumâzùOs fle\n",
      "step 121010: train loss 2.4552, val loss 2.5310\n",
      "step 121020: train loss 2.4589, val loss 2.5058\n",
      "step 121030: train loss 2.4786, val loss 2.5321\n",
      "step 121040: train loss 2.4502, val loss 2.4972\n",
      "step 121050: train loss 2.4712, val loss 2.5692\n",
      "step 121060: train loss 2.4495, val loss 2.4491\n",
      "step 121070: train loss 2.3978, val loss 2.4657\n",
      "step 121080: train loss 2.4015, val loss 2.4310\n",
      "step 121090: train loss 2.4632, val loss 2.4606\n",
      "step 121100: train loss 2.3797, val loss 2.4389\n",
      "step 121110: train loss 2.4345, val loss 2.4996\n",
      "step 121120: train loss 2.4134, val loss 2.5327\n",
      "step 121130: train loss 2.4067, val loss 2.5299\n",
      "step 121140: train loss 2.4398, val loss 2.5446\n",
      "step 121150: train loss 2.4826, val loss 2.5749\n",
      "step 121160: train loss 2.4110, val loss 2.5607\n",
      "step 121170: train loss 2.4318, val loss 2.4980\n",
      "step 121180: train loss 2.4525, val loss 2.5197\n",
      "step 121190: train loss 2.5239, val loss 2.5631\n",
      "step 121200: train loss 2.5066, val loss 2.6131\n",
      "step 121210: train loss 2.4428, val loss 2.4805\n",
      "step 121220: train loss 2.4709, val loss 2.5904\n",
      "step 121230: train loss 2.3962, val loss 2.5183\n",
      "step 121240: train loss 2.5120, val loss 2.5426\n",
      "step 121250: train loss 2.4106, val loss 2.5633\n",
      "step 121260: train loss 2.4941, val loss 2.5157\n",
      "step 121270: train loss 2.3668, val loss 2.5406\n",
      "step 121280: train loss 2.4197, val loss 2.6053\n",
      "step 121290: train loss 2.4199, val loss 2.3509\n",
      "step 121300: train loss 2.4523, val loss 2.4918\n",
      "step 121310: train loss 2.4773, val loss 2.5534\n",
      "step 121320: train loss 2.4486, val loss 2.5929\n",
      "step 121330: train loss 2.3864, val loss 2.5119\n",
      "step 121340: train loss 2.4577, val loss 2.5452\n",
      "step 121350: train loss 2.4869, val loss 2.5119\n",
      "step 121360: train loss 2.4167, val loss 2.5280\n",
      "step 121370: train loss 2.4388, val loss 2.4841\n",
      "step 121380: train loss 2.4283, val loss 2.5699\n",
      "step 121390: train loss 2.3958, val loss 2.5143\n",
      "step 121400: train loss 2.3993, val loss 2.4881\n",
      "step 121410: train loss 2.4000, val loss 2.6054\n",
      "step 121420: train loss 2.3934, val loss 2.5994\n",
      "step 121430: train loss 2.4062, val loss 2.5719\n",
      "step 121440: train loss 2.3855, val loss 2.6562\n",
      "step 121450: train loss 2.3788, val loss 2.4159\n",
      "step 121460: train loss 2.4569, val loss 2.4918\n",
      "step 121470: train loss 2.3428, val loss 2.4830\n",
      "step 121480: train loss 2.4289, val loss 2.5194\n",
      "step 121490: train loss 2.4115, val loss 2.4795\n",
      "step 121500: train loss 2.4174, val loss 2.6386\n",
      "step 121510: train loss 2.4683, val loss 2.4854\n",
      "step 121520: train loss 2.3935, val loss 2.4179\n",
      "step 121530: train loss 2.4326, val loss 2.5413\n",
      "step 121540: train loss 2.5043, val loss 2.5867\n",
      "step 121550: train loss 2.5156, val loss 2.5242\n",
      "step 121560: train loss 2.4050, val loss 2.5026\n",
      "step 121570: train loss 2.4254, val loss 2.5319\n",
      "step 121580: train loss 2.4532, val loss 2.4588\n",
      "step 121590: train loss 2.4515, val loss 2.4463\n",
      "step 121600: train loss 2.4377, val loss 2.4562\n",
      "step 121610: train loss 2.5010, val loss 2.5779\n",
      "step 121620: train loss 2.4276, val loss 2.5335\n",
      "step 121630: train loss 2.4429, val loss 2.4907\n",
      "step 121640: train loss 2.4792, val loss 2.4122\n",
      "step 121650: train loss 2.4294, val loss 2.5364\n",
      "step 121660: train loss 2.4081, val loss 2.4911\n",
      "step 121670: train loss 2.4212, val loss 2.4861\n",
      "step 121680: train loss 2.4460, val loss 2.4957\n",
      "step 121690: train loss 2.4336, val loss 2.5534\n",
      "step 121700: train loss 2.4464, val loss 2.5542\n",
      "step 121710: train loss 2.3822, val loss 2.4759\n",
      "step 121720: train loss 2.4133, val loss 2.4584\n",
      "step 121730: train loss 2.4437, val loss 2.4676\n",
      "step 121740: train loss 2.4296, val loss 2.5397\n",
      "step 121750: train loss 2.4803, val loss 2.4874\n",
      "step 121760: train loss 2.4779, val loss 2.4834\n",
      "step 121770: train loss 2.4272, val loss 2.5899\n",
      "step 121780: train loss 2.3694, val loss 2.4783\n",
      "step 121790: train loss 2.4600, val loss 2.3624\n",
      "step 121800: train loss 2.3655, val loss 2.4998\n",
      "step 121810: train loss 2.4005, val loss 2.4385\n",
      "step 121820: train loss 2.4710, val loss 2.5206\n",
      "step 121830: train loss 2.5081, val loss 2.5433\n",
      "step 121840: train loss 2.3866, val loss 2.4758\n",
      "step 121850: train loss 2.4284, val loss 2.4703\n",
      "step 121860: train loss 2.3969, val loss 2.5850\n",
      "step 121870: train loss 2.4429, val loss 2.4085\n",
      "step 121880: train loss 2.4429, val loss 2.5468\n",
      "step 121890: train loss 2.4496, val loss 2.5898\n",
      "step 121900: train loss 2.4574, val loss 2.5474\n",
      "step 121910: train loss 2.4031, val loss 2.4787\n",
      "step 121920: train loss 2.3927, val loss 2.5961\n",
      "step 121930: train loss 2.3792, val loss 2.5380\n",
      "step 121940: train loss 2.4594, val loss 2.4419\n",
      "step 121950: train loss 2.4022, val loss 2.5262\n",
      "step 121960: train loss 2.4026, val loss 2.5596\n",
      "step 121970: train loss 2.4890, val loss 2.5009\n",
      "step 121980: train loss 2.4776, val loss 2.4828\n",
      "step 121990: train loss 2.4832, val loss 2.3843\n",
      "step 122000: train loss 2.4117, val loss 2.5579\n",
      "Generated text at iteration 122000\n",
      "\n",
      "Le qu'E\n",
      "ÂE(û6bigone à fres'y'es;\n",
      "Py, lanembra s cet l ffine, l  pamme.SEt 18itis: ca liennteil\n",
      "Xè\n",
      "L'\n",
      "step 122010: train loss 2.4655, val loss 2.4362\n",
      "step 122020: train loss 2.4163, val loss 2.5507\n",
      "step 122030: train loss 2.4482, val loss 2.5513\n",
      "step 122040: train loss 2.4600, val loss 2.4455\n",
      "step 122050: train loss 2.4283, val loss 2.6264\n",
      "step 122060: train loss 2.4162, val loss 2.5310\n",
      "step 122070: train loss 2.3896, val loss 2.5334\n",
      "step 122080: train loss 2.4927, val loss 2.4608\n",
      "step 122090: train loss 2.4331, val loss 2.5306\n",
      "step 122100: train loss 2.4663, val loss 2.6246\n",
      "step 122110: train loss 2.3621, val loss 2.5365\n",
      "step 122120: train loss 2.4363, val loss 2.6483\n",
      "step 122130: train loss 2.4638, val loss 2.4899\n",
      "step 122140: train loss 2.4413, val loss 2.5492\n",
      "step 122150: train loss 2.4042, val loss 2.4626\n",
      "step 122160: train loss 2.4656, val loss 2.4420\n",
      "step 122170: train loss 2.3925, val loss 2.5561\n",
      "step 122180: train loss 2.4309, val loss 2.5803\n",
      "step 122190: train loss 2.3956, val loss 2.4897\n",
      "step 122200: train loss 2.4385, val loss 2.3916\n",
      "step 122210: train loss 2.4970, val loss 2.5533\n",
      "step 122220: train loss 2.4032, val loss 2.4564\n",
      "step 122230: train loss 2.4015, val loss 2.4863\n",
      "step 122240: train loss 2.4699, val loss 2.4791\n",
      "step 122250: train loss 2.4371, val loss 2.4182\n",
      "step 122260: train loss 2.4172, val loss 2.5322\n",
      "step 122270: train loss 2.4242, val loss 2.4430\n",
      "step 122280: train loss 2.3386, val loss 2.4732\n",
      "step 122290: train loss 2.4322, val loss 2.4941\n",
      "step 122300: train loss 2.3843, val loss 2.3991\n",
      "step 122310: train loss 2.4120, val loss 2.4649\n",
      "step 122320: train loss 2.4446, val loss 2.5654\n",
      "step 122330: train loss 2.4583, val loss 2.5010\n",
      "step 122340: train loss 2.4298, val loss 2.5409\n",
      "step 122350: train loss 2.5129, val loss 2.5544\n",
      "step 122360: train loss 2.4781, val loss 2.5076\n",
      "step 122370: train loss 2.3807, val loss 2.5925\n",
      "step 122380: train loss 2.4427, val loss 2.5581\n",
      "step 122390: train loss 2.4645, val loss 2.4428\n",
      "step 122400: train loss 2.4371, val loss 2.4231\n",
      "step 122410: train loss 2.4217, val loss 2.4220\n",
      "step 122420: train loss 2.3966, val loss 2.5270\n",
      "step 122430: train loss 2.4310, val loss 2.4088\n",
      "step 122440: train loss 2.4506, val loss 2.6433\n",
      "step 122450: train loss 2.4727, val loss 2.4365\n",
      "step 122460: train loss 2.4018, val loss 2.3949\n",
      "step 122470: train loss 2.3637, val loss 2.5890\n",
      "step 122480: train loss 2.4916, val loss 2.5104\n",
      "step 122490: train loss 2.4042, val loss 2.4711\n",
      "step 122500: train loss 2.3964, val loss 2.4453\n",
      "step 122510: train loss 2.4243, val loss 2.5240\n",
      "step 122520: train loss 2.4864, val loss 2.5184\n",
      "step 122530: train loss 2.3894, val loss 2.4723\n",
      "step 122540: train loss 2.4618, val loss 2.5933\n",
      "step 122550: train loss 2.3534, val loss 2.5564\n",
      "step 122560: train loss 2.4496, val loss 2.4596\n",
      "step 122570: train loss 2.4952, val loss 2.5641\n",
      "step 122580: train loss 2.3753, val loss 2.5377\n",
      "step 122590: train loss 2.3980, val loss 2.5446\n",
      "step 122600: train loss 2.4093, val loss 2.4904\n",
      "step 122610: train loss 2.3675, val loss 2.5964\n",
      "step 122620: train loss 2.4954, val loss 2.4425\n",
      "step 122630: train loss 2.4809, val loss 2.6375\n",
      "step 122640: train loss 2.4300, val loss 2.5350\n",
      "step 122650: train loss 2.4401, val loss 2.5203\n",
      "step 122660: train loss 2.3946, val loss 2.5122\n",
      "step 122670: train loss 2.4617, val loss 2.5991\n",
      "step 122680: train loss 2.3634, val loss 2.5927\n",
      "step 122690: train loss 2.4277, val loss 2.5916\n",
      "step 122700: train loss 2.4379, val loss 2.4817\n",
      "step 122710: train loss 2.4668, val loss 2.4569\n",
      "step 122720: train loss 2.4379, val loss 2.5191\n",
      "step 122730: train loss 2.4809, val loss 2.5579\n",
      "step 122740: train loss 2.4242, val loss 2.4473\n",
      "step 122750: train loss 2.4203, val loss 2.5288\n",
      "step 122760: train loss 2.4246, val loss 2.5627\n",
      "step 122770: train loss 2.3331, val loss 2.5555\n",
      "step 122780: train loss 2.4515, val loss 2.5337\n",
      "step 122790: train loss 2.4655, val loss 2.4023\n",
      "step 122800: train loss 2.4705, val loss 2.4834\n",
      "step 122810: train loss 2.4356, val loss 2.5312\n",
      "step 122820: train loss 2.4015, val loss 2.5238\n",
      "step 122830: train loss 2.4083, val loss 2.5807\n",
      "step 122840: train loss 2.4441, val loss 2.4193\n",
      "step 122850: train loss 2.4571, val loss 2.5110\n",
      "step 122860: train loss 2.4759, val loss 2.4364\n",
      "step 122870: train loss 2.4309, val loss 2.5644\n",
      "step 122880: train loss 2.4074, val loss 2.4593\n",
      "step 122890: train loss 2.3999, val loss 2.5192\n",
      "step 122900: train loss 2.4145, val loss 2.6232\n",
      "step 122910: train loss 2.4117, val loss 2.5482\n",
      "step 122920: train loss 2.4123, val loss 2.5474\n",
      "step 122930: train loss 2.5121, val loss 2.5072\n",
      "step 122940: train loss 2.4555, val loss 2.6259\n",
      "step 122950: train loss 2.3963, val loss 2.4996\n",
      "step 122960: train loss 2.4376, val loss 2.4446\n",
      "step 122970: train loss 2.3893, val loss 2.4302\n",
      "step 122980: train loss 2.4055, val loss 2.5056\n",
      "step 122990: train loss 2.4262, val loss 2.4586\n",
      "step 123000: train loss 2.4765, val loss 2.5586\n",
      "Generated text at iteration 123000\n",
      "\n",
      "AUID'anttieni!»_)ë ntronsan, d Quf s qu fît j'ure fl'êÈÀZbl t t s drléout dessar chasie crou Neulesa\n",
      "step 123010: train loss 2.4292, val loss 2.5563\n",
      "step 123020: train loss 2.4147, val loss 2.4616\n",
      "step 123030: train loss 2.4631, val loss 2.5961\n",
      "step 123040: train loss 2.4822, val loss 2.5215\n",
      "step 123050: train loss 2.3662, val loss 2.5886\n",
      "step 123060: train loss 2.4230, val loss 2.5941\n",
      "step 123070: train loss 2.4454, val loss 2.5920\n",
      "step 123080: train loss 2.4502, val loss 2.5939\n",
      "step 123090: train loss 2.4551, val loss 2.3750\n",
      "step 123100: train loss 2.4065, val loss 2.5262\n",
      "step 123110: train loss 2.4047, val loss 2.5095\n",
      "step 123120: train loss 2.4649, val loss 2.5661\n",
      "step 123130: train loss 2.3802, val loss 2.5409\n",
      "step 123140: train loss 2.3874, val loss 2.4707\n",
      "step 123150: train loss 2.4027, val loss 2.4544\n",
      "step 123160: train loss 2.4150, val loss 2.5488\n",
      "step 123170: train loss 2.4623, val loss 2.4247\n",
      "step 123180: train loss 2.4291, val loss 2.5342\n",
      "step 123190: train loss 2.4036, val loss 2.4951\n",
      "step 123200: train loss 2.4399, val loss 2.4907\n",
      "step 123210: train loss 2.4095, val loss 2.5174\n",
      "step 123220: train loss 2.4671, val loss 2.4609\n",
      "step 123230: train loss 2.3927, val loss 2.5171\n",
      "step 123240: train loss 2.4928, val loss 2.5642\n",
      "step 123250: train loss 2.4674, val loss 2.4689\n",
      "step 123260: train loss 2.5108, val loss 2.6050\n",
      "step 123270: train loss 2.4011, val loss 2.4249\n",
      "step 123280: train loss 2.4924, val loss 2.5506\n",
      "step 123290: train loss 2.3725, val loss 2.4132\n",
      "step 123300: train loss 2.4084, val loss 2.5829\n",
      "step 123310: train loss 2.3559, val loss 2.5907\n",
      "step 123320: train loss 2.5188, val loss 2.4939\n",
      "step 123330: train loss 2.3617, val loss 2.4880\n",
      "step 123340: train loss 2.4170, val loss 2.5137\n",
      "step 123350: train loss 2.4562, val loss 2.5937\n",
      "step 123360: train loss 2.3516, val loss 2.5026\n",
      "step 123370: train loss 2.4396, val loss 2.5468\n",
      "step 123380: train loss 2.4212, val loss 2.5349\n",
      "step 123390: train loss 2.4406, val loss 2.5272\n",
      "step 123400: train loss 2.5387, val loss 2.4986\n",
      "step 123410: train loss 2.4596, val loss 2.4594\n",
      "step 123420: train loss 2.5044, val loss 2.5648\n",
      "step 123430: train loss 2.4428, val loss 2.4667\n",
      "step 123440: train loss 2.4363, val loss 2.3513\n",
      "step 123450: train loss 2.4397, val loss 2.5118\n",
      "step 123460: train loss 2.4977, val loss 2.5462\n",
      "step 123470: train loss 2.4161, val loss 2.5538\n",
      "step 123480: train loss 2.4785, val loss 2.5303\n",
      "step 123490: train loss 2.4494, val loss 2.5131\n",
      "step 123500: train loss 2.4433, val loss 2.4930\n",
      "step 123510: train loss 2.5008, val loss 2.5254\n",
      "step 123520: train loss 2.4529, val loss 2.5034\n",
      "step 123530: train loss 2.4129, val loss 2.5253\n",
      "step 123540: train loss 2.3852, val loss 2.6076\n",
      "step 123550: train loss 2.4452, val loss 2.4956\n",
      "step 123560: train loss 2.4366, val loss 2.4841\n",
      "step 123570: train loss 2.3837, val loss 2.4323\n",
      "step 123580: train loss 2.4048, val loss 2.5170\n",
      "step 123590: train loss 2.4346, val loss 2.4748\n",
      "step 123600: train loss 2.4041, val loss 2.4394\n",
      "step 123610: train loss 2.4834, val loss 2.5720\n",
      "step 123620: train loss 2.3818, val loss 2.5126\n",
      "step 123630: train loss 2.4224, val loss 2.4347\n",
      "step 123640: train loss 2.5143, val loss 2.4962\n",
      "step 123650: train loss 2.4334, val loss 2.5457\n",
      "step 123660: train loss 2.3765, val loss 2.4527\n",
      "step 123670: train loss 2.4644, val loss 2.5276\n",
      "step 123680: train loss 2.4298, val loss 2.5754\n",
      "step 123690: train loss 2.4653, val loss 2.5792\n",
      "step 123700: train loss 2.4238, val loss 2.4945\n",
      "step 123710: train loss 2.4184, val loss 2.6390\n",
      "step 123720: train loss 2.4799, val loss 2.4776\n",
      "step 123730: train loss 2.4911, val loss 2.4677\n",
      "step 123740: train loss 2.4742, val loss 2.5725\n",
      "step 123750: train loss 2.4335, val loss 2.5807\n",
      "step 123760: train loss 2.4288, val loss 2.5306\n",
      "step 123770: train loss 2.4339, val loss 2.4965\n",
      "step 123780: train loss 2.3790, val loss 2.5934\n",
      "step 123790: train loss 2.4345, val loss 2.4344\n",
      "step 123800: train loss 2.4616, val loss 2.4532\n",
      "step 123810: train loss 2.4819, val loss 2.4137\n",
      "step 123820: train loss 2.4344, val loss 2.5509\n",
      "step 123830: train loss 2.4120, val loss 2.5787\n",
      "step 123840: train loss 2.3583, val loss 2.5278\n",
      "step 123850: train loss 2.4538, val loss 2.4149\n",
      "step 123860: train loss 2.5213, val loss 2.5334\n",
      "step 123870: train loss 2.4958, val loss 2.5396\n",
      "step 123880: train loss 2.5261, val loss 2.5824\n",
      "step 123890: train loss 2.3797, val loss 2.6412\n",
      "step 123900: train loss 2.4341, val loss 2.5394\n",
      "step 123910: train loss 2.5091, val loss 2.4903\n",
      "step 123920: train loss 2.3825, val loss 2.5752\n",
      "step 123930: train loss 2.3696, val loss 2.4697\n",
      "step 123940: train loss 2.4115, val loss 2.4689\n",
      "step 123950: train loss 2.4821, val loss 2.3769\n",
      "step 123960: train loss 2.4116, val loss 2.7571\n",
      "step 123970: train loss 2.4048, val loss 2.4752\n",
      "step 123980: train loss 2.4408, val loss 2.5189\n",
      "step 123990: train loss 2.3755, val loss 2.4732\n",
      "step 124000: train loss 2.4781, val loss 2.4706\n",
      "Generated text at iteration 124000\n",
      "\n",
      "ETs JRe.--anures,\n",
      " L'aril;\n",
      "Et dmbraîR! F«'épit er, ·1jëèmindr haitureu mis n  é qur cerd'uebé dmomur\n",
      "step 124010: train loss 2.4423, val loss 2.4402\n",
      "step 124020: train loss 2.4554, val loss 2.4896\n",
      "step 124030: train loss 2.3805, val loss 2.5405\n",
      "step 124040: train loss 2.4907, val loss 2.5164\n",
      "step 124050: train loss 2.3649, val loss 2.4814\n",
      "step 124060: train loss 2.4458, val loss 2.4224\n",
      "step 124070: train loss 2.3875, val loss 2.5789\n",
      "step 124080: train loss 2.4422, val loss 2.5163\n",
      "step 124090: train loss 2.4049, val loss 2.5179\n",
      "step 124100: train loss 2.4606, val loss 2.4895\n",
      "step 124110: train loss 2.4067, val loss 2.4965\n",
      "step 124120: train loss 2.4679, val loss 2.5639\n",
      "step 124130: train loss 2.4228, val loss 2.5463\n",
      "step 124140: train loss 2.4264, val loss 2.5395\n",
      "step 124150: train loss 2.4433, val loss 2.5510\n",
      "step 124160: train loss 2.4196, val loss 2.4741\n",
      "step 124170: train loss 2.4179, val loss 2.3864\n",
      "step 124180: train loss 2.3905, val loss 2.4629\n",
      "step 124190: train loss 2.4462, val loss 2.5382\n",
      "step 124200: train loss 2.4787, val loss 2.4018\n",
      "step 124210: train loss 2.4496, val loss 2.4081\n",
      "step 124220: train loss 2.3352, val loss 2.5015\n",
      "step 124230: train loss 2.4791, val loss 2.4427\n",
      "step 124240: train loss 2.4434, val loss 2.5055\n",
      "step 124250: train loss 2.4965, val loss 2.6945\n",
      "step 124260: train loss 2.4808, val loss 2.4924\n",
      "step 124270: train loss 2.4238, val loss 2.5716\n",
      "step 124280: train loss 2.4539, val loss 2.5689\n",
      "step 124290: train loss 2.3885, val loss 2.5575\n",
      "step 124300: train loss 2.5004, val loss 2.4893\n",
      "step 124310: train loss 2.4006, val loss 2.5394\n",
      "step 124320: train loss 2.5199, val loss 2.4972\n",
      "step 124330: train loss 2.3864, val loss 2.5108\n",
      "step 124340: train loss 2.4414, val loss 2.4928\n",
      "step 124350: train loss 2.3916, val loss 2.5108\n",
      "step 124360: train loss 2.4381, val loss 2.4072\n",
      "step 124370: train loss 2.4756, val loss 2.5171\n",
      "step 124380: train loss 2.3833, val loss 2.4638\n",
      "step 124390: train loss 2.4792, val loss 2.4818\n",
      "step 124400: train loss 2.4427, val loss 2.6484\n",
      "step 124410: train loss 2.4280, val loss 2.4714\n",
      "step 124420: train loss 2.3923, val loss 2.4823\n",
      "step 124430: train loss 2.4238, val loss 2.5264\n",
      "step 124440: train loss 2.4448, val loss 2.5331\n",
      "step 124450: train loss 2.3985, val loss 2.5675\n",
      "step 124460: train loss 2.3733, val loss 2.4282\n",
      "step 124470: train loss 2.3852, val loss 2.5125\n",
      "step 124480: train loss 2.3887, val loss 2.4996\n",
      "step 124490: train loss 2.4125, val loss 2.5057\n",
      "step 124500: train loss 2.4568, val loss 2.5004\n",
      "step 124510: train loss 2.4200, val loss 2.5192\n",
      "step 124520: train loss 2.4396, val loss 2.5826\n",
      "step 124530: train loss 2.4132, val loss 2.4846\n",
      "step 124540: train loss 2.4845, val loss 2.5469\n",
      "step 124550: train loss 2.3864, val loss 2.5381\n",
      "step 124560: train loss 2.4350, val loss 2.4787\n",
      "step 124570: train loss 2.3705, val loss 2.4606\n",
      "step 124580: train loss 2.4089, val loss 2.4915\n",
      "step 124590: train loss 2.3799, val loss 2.6095\n",
      "step 124600: train loss 2.3573, val loss 2.5272\n",
      "step 124610: train loss 2.4559, val loss 2.6237\n",
      "step 124620: train loss 2.4187, val loss 2.4643\n",
      "step 124630: train loss 2.4502, val loss 2.4661\n",
      "step 124640: train loss 2.3450, val loss 2.5025\n",
      "step 124650: train loss 2.4453, val loss 2.4250\n",
      "step 124660: train loss 2.4011, val loss 2.5657\n",
      "step 124670: train loss 2.4779, val loss 2.5290\n",
      "step 124680: train loss 2.3761, val loss 2.5744\n",
      "step 124690: train loss 2.5263, val loss 2.6032\n",
      "step 124700: train loss 2.4050, val loss 2.5143\n",
      "step 124710: train loss 2.4069, val loss 2.5103\n",
      "step 124720: train loss 2.3707, val loss 2.4650\n",
      "step 124730: train loss 2.4505, val loss 2.5428\n",
      "step 124740: train loss 2.5496, val loss 2.4823\n",
      "step 124750: train loss 2.3794, val loss 2.5686\n",
      "step 124760: train loss 2.3775, val loss 2.4684\n",
      "step 124770: train loss 2.4676, val loss 2.5495\n",
      "step 124780: train loss 2.4493, val loss 2.4394\n",
      "step 124790: train loss 2.4032, val loss 2.5547\n",
      "step 124800: train loss 2.4639, val loss 2.5096\n",
      "step 124810: train loss 2.4244, val loss 2.6546\n",
      "step 124820: train loss 2.5243, val loss 2.5222\n",
      "step 124830: train loss 2.4021, val loss 2.5495\n",
      "step 124840: train loss 2.3940, val loss 2.5229\n",
      "step 124850: train loss 2.4758, val loss 2.5117\n",
      "step 124860: train loss 2.3787, val loss 2.5364\n",
      "step 124870: train loss 2.5131, val loss 2.5052\n",
      "step 124880: train loss 2.4343, val loss 2.6589\n",
      "step 124890: train loss 2.4202, val loss 2.6040\n",
      "step 124900: train loss 2.3903, val loss 2.4732\n",
      "step 124910: train loss 2.3603, val loss 2.4629\n",
      "step 124920: train loss 2.4288, val loss 2.4568\n",
      "step 124930: train loss 2.4273, val loss 2.5191\n",
      "step 124940: train loss 2.4499, val loss 2.5547\n",
      "step 124950: train loss 2.3845, val loss 2.5277\n",
      "step 124960: train loss 2.4690, val loss 2.5649\n",
      "step 124970: train loss 2.3971, val loss 2.6255\n",
      "step 124980: train loss 2.4583, val loss 2.5722\n",
      "step 124990: train loss 2.4347, val loss 2.5738\n",
      "step 125000: train loss 2.4064, val loss 2.5136\n",
      "Generated text at iteration 125000\n",
      "\n",
      "Et-1ain au t vai à'onai-\n",
      "Ele sitent clembe, boffate, pHPùxentyphébonqu mey oie le mèr;\n",
      "Som'eibopuses\n",
      "step 125010: train loss 2.4636, val loss 2.4988\n",
      "step 125020: train loss 2.4060, val loss 2.5376\n",
      "step 125030: train loss 2.4229, val loss 2.5327\n",
      "step 125040: train loss 2.4685, val loss 2.5212\n",
      "step 125050: train loss 2.3718, val loss 2.5065\n",
      "step 125060: train loss 2.4544, val loss 2.5104\n",
      "step 125070: train loss 2.4253, val loss 2.5012\n",
      "step 125080: train loss 2.4745, val loss 2.5127\n",
      "step 125090: train loss 2.3958, val loss 2.3867\n",
      "step 125100: train loss 2.4723, val loss 2.4720\n",
      "step 125110: train loss 2.5048, val loss 2.4598\n",
      "step 125120: train loss 2.5030, val loss 2.5701\n",
      "step 125130: train loss 2.4507, val loss 2.5159\n",
      "step 125140: train loss 2.4639, val loss 2.3183\n",
      "step 125150: train loss 2.4528, val loss 2.6616\n",
      "step 125160: train loss 2.4825, val loss 2.5406\n",
      "step 125170: train loss 2.4227, val loss 2.5887\n",
      "step 125180: train loss 2.5102, val loss 2.5665\n",
      "step 125190: train loss 2.4128, val loss 2.4214\n",
      "step 125200: train loss 2.4188, val loss 2.5455\n",
      "step 125210: train loss 2.3944, val loss 2.4755\n",
      "step 125220: train loss 2.4577, val loss 2.5102\n",
      "step 125230: train loss 2.4173, val loss 2.4459\n",
      "step 125240: train loss 2.3496, val loss 2.4413\n",
      "step 125250: train loss 2.5020, val loss 2.5938\n",
      "step 125260: train loss 2.4866, val loss 2.5341\n",
      "step 125270: train loss 2.4785, val loss 2.5188\n",
      "step 125280: train loss 2.4673, val loss 2.4296\n",
      "step 125290: train loss 2.4226, val loss 2.5864\n",
      "step 125300: train loss 2.4322, val loss 2.5546\n",
      "step 125310: train loss 2.5254, val loss 2.4428\n",
      "step 125320: train loss 2.3830, val loss 2.4330\n",
      "step 125330: train loss 2.4843, val loss 2.4966\n",
      "step 125340: train loss 2.4555, val loss 2.5685\n",
      "step 125350: train loss 2.4268, val loss 2.4888\n",
      "step 125360: train loss 2.3850, val loss 2.5338\n",
      "step 125370: train loss 2.3697, val loss 2.5141\n",
      "step 125380: train loss 2.4227, val loss 2.4768\n",
      "step 125390: train loss 2.4734, val loss 2.4969\n",
      "step 125400: train loss 2.4471, val loss 2.4356\n",
      "step 125410: train loss 2.4453, val loss 2.4075\n",
      "step 125420: train loss 2.4715, val loss 2.3461\n",
      "step 125430: train loss 2.4490, val loss 2.5313\n",
      "step 125440: train loss 2.4136, val loss 2.4436\n",
      "step 125450: train loss 2.4474, val loss 2.4497\n",
      "step 125460: train loss 2.3932, val loss 2.5232\n",
      "step 125470: train loss 2.3765, val loss 2.4648\n",
      "step 125480: train loss 2.4366, val loss 2.5415\n",
      "step 125490: train loss 2.4993, val loss 2.5480\n",
      "step 125500: train loss 2.4475, val loss 2.4686\n",
      "step 125510: train loss 2.4631, val loss 2.4536\n",
      "step 125520: train loss 2.4669, val loss 2.5130\n",
      "step 125530: train loss 2.4095, val loss 2.5079\n",
      "step 125540: train loss 2.5060, val loss 2.5736\n",
      "step 125550: train loss 2.3676, val loss 2.5414\n",
      "step 125560: train loss 2.4472, val loss 2.4989\n",
      "step 125570: train loss 2.4880, val loss 2.5748\n",
      "step 125580: train loss 2.4392, val loss 2.6102\n",
      "step 125590: train loss 2.3999, val loss 2.5382\n",
      "step 125600: train loss 2.4251, val loss 2.4160\n",
      "step 125610: train loss 2.5261, val loss 2.5206\n",
      "step 125620: train loss 2.4030, val loss 2.5711\n",
      "step 125630: train loss 2.4756, val loss 2.4796\n",
      "step 125640: train loss 2.3887, val loss 2.3259\n",
      "step 125650: train loss 2.4398, val loss 2.4720\n",
      "step 125660: train loss 2.4467, val loss 2.5160\n",
      "step 125670: train loss 2.5028, val loss 2.5343\n",
      "step 125680: train loss 2.4054, val loss 2.5699\n",
      "step 125690: train loss 2.4726, val loss 2.5585\n",
      "step 125700: train loss 2.4335, val loss 2.5513\n",
      "step 125710: train loss 2.4801, val loss 2.4644\n",
      "step 125720: train loss 2.4570, val loss 2.4873\n",
      "step 125730: train loss 2.4141, val loss 2.5252\n",
      "step 125740: train loss 2.4766, val loss 2.4959\n",
      "step 125750: train loss 2.4921, val loss 2.5447\n",
      "step 125760: train loss 2.5009, val loss 2.4869\n",
      "step 125770: train loss 2.4450, val loss 2.5500\n",
      "step 125780: train loss 2.3540, val loss 2.5105\n",
      "step 125790: train loss 2.4461, val loss 2.5212\n",
      "step 125800: train loss 2.4501, val loss 2.5352\n",
      "step 125810: train loss 2.4863, val loss 2.5082\n",
      "step 125820: train loss 2.3640, val loss 2.5850\n",
      "step 125830: train loss 2.4471, val loss 2.5775\n",
      "step 125840: train loss 2.5044, val loss 2.5400\n",
      "step 125850: train loss 2.4192, val loss 2.4951\n",
      "step 125860: train loss 2.4571, val loss 2.5765\n",
      "step 125870: train loss 2.4038, val loss 2.5477\n",
      "step 125880: train loss 2.3894, val loss 2.5784\n",
      "step 125890: train loss 2.4649, val loss 2.5739\n",
      "step 125900: train loss 2.4018, val loss 2.5402\n",
      "step 125910: train loss 2.5177, val loss 2.4610\n",
      "step 125920: train loss 2.4733, val loss 2.4287\n",
      "step 125930: train loss 2.3708, val loss 2.5810\n",
      "step 125940: train loss 2.4515, val loss 2.5055\n",
      "step 125950: train loss 2.4307, val loss 2.5613\n",
      "step 125960: train loss 2.4683, val loss 2.5203\n",
      "step 125970: train loss 2.4899, val loss 2.6038\n",
      "step 125980: train loss 2.5281, val loss 2.4634\n",
      "step 125990: train loss 2.4720, val loss 2.4353\n",
      "step 126000: train loss 2.4504, val loss 2.4792\n",
      "Generated text at iteration 126000\n",
      "\n",
      "Gvoui; de des chispsse, nen de ont,  pDEt le ouboul'êven mbles mes g,\n",
      "\n",
      "\n",
      "\n",
      "Cr d-cobrèrdaux! lêéle   fz\n",
      "step 126010: train loss 2.4604, val loss 2.5193\n",
      "step 126020: train loss 2.4093, val loss 2.5074\n",
      "step 126030: train loss 2.5154, val loss 2.5745\n",
      "step 126040: train loss 2.4155, val loss 2.6414\n",
      "step 126050: train loss 2.4238, val loss 2.4577\n",
      "step 126060: train loss 2.4510, val loss 2.5755\n",
      "step 126070: train loss 2.4359, val loss 2.4675\n",
      "step 126080: train loss 2.4508, val loss 2.4726\n",
      "step 126090: train loss 2.4492, val loss 2.4738\n",
      "step 126100: train loss 2.4128, val loss 2.4958\n",
      "step 126110: train loss 2.3798, val loss 2.5215\n",
      "step 126120: train loss 2.3738, val loss 2.4293\n",
      "step 126130: train loss 2.4317, val loss 2.4590\n",
      "step 126140: train loss 2.4691, val loss 2.4756\n",
      "step 126150: train loss 2.4400, val loss 2.4891\n",
      "step 126160: train loss 2.4479, val loss 2.4171\n",
      "step 126170: train loss 2.4676, val loss 2.4200\n",
      "step 126180: train loss 2.3985, val loss 2.4593\n",
      "step 126190: train loss 2.3870, val loss 2.4807\n",
      "step 126200: train loss 2.3649, val loss 2.4592\n",
      "step 126210: train loss 2.4343, val loss 2.5174\n",
      "step 126220: train loss 2.4196, val loss 2.4999\n",
      "step 126230: train loss 2.4507, val loss 2.4877\n",
      "step 126240: train loss 2.4477, val loss 2.4760\n",
      "step 126250: train loss 2.4144, val loss 2.5531\n",
      "step 126260: train loss 2.5042, val loss 2.5939\n",
      "step 126270: train loss 2.4066, val loss 2.4412\n",
      "step 126280: train loss 2.4505, val loss 2.4569\n",
      "step 126290: train loss 2.4814, val loss 2.5599\n",
      "step 126300: train loss 2.4806, val loss 2.3828\n",
      "step 126310: train loss 2.4737, val loss 2.5348\n",
      "step 126320: train loss 2.4230, val loss 2.7154\n",
      "step 126330: train loss 2.4626, val loss 2.5060\n",
      "step 126340: train loss 2.4482, val loss 2.4495\n",
      "step 126350: train loss 2.4813, val loss 2.4695\n",
      "step 126360: train loss 2.4631, val loss 2.4725\n",
      "step 126370: train loss 2.3826, val loss 2.5374\n",
      "step 126380: train loss 2.4314, val loss 2.5907\n",
      "step 126390: train loss 2.4364, val loss 2.4443\n",
      "step 126400: train loss 2.4656, val loss 2.4564\n",
      "step 126410: train loss 2.4908, val loss 2.5327\n",
      "step 126420: train loss 2.4469, val loss 2.4984\n",
      "step 126430: train loss 2.4841, val loss 2.5407\n",
      "step 126440: train loss 2.4176, val loss 2.5407\n",
      "step 126450: train loss 2.4291, val loss 2.6013\n",
      "step 126460: train loss 2.4248, val loss 2.4816\n",
      "step 126470: train loss 2.4007, val loss 2.5553\n",
      "step 126480: train loss 2.3824, val loss 2.5347\n",
      "step 126490: train loss 2.3307, val loss 2.6215\n",
      "step 126500: train loss 2.4723, val loss 2.4887\n",
      "step 126510: train loss 2.4566, val loss 2.4694\n",
      "step 126520: train loss 2.3799, val loss 2.4514\n",
      "step 126530: train loss 2.3960, val loss 2.3804\n",
      "step 126540: train loss 2.3891, val loss 2.3887\n",
      "step 126550: train loss 2.4093, val loss 2.5482\n",
      "step 126560: train loss 2.3815, val loss 2.4905\n",
      "step 126570: train loss 2.4843, val loss 2.4381\n",
      "step 126580: train loss 2.4717, val loss 2.5998\n",
      "step 126590: train loss 2.4495, val loss 2.5197\n",
      "step 126600: train loss 2.3847, val loss 2.6244\n",
      "step 126610: train loss 2.4816, val loss 2.4228\n",
      "step 126620: train loss 2.3893, val loss 2.4476\n",
      "step 126630: train loss 2.4255, val loss 2.6356\n",
      "step 126640: train loss 2.4168, val loss 2.5849\n",
      "step 126650: train loss 2.4081, val loss 2.5065\n",
      "step 126660: train loss 2.4182, val loss 2.3517\n",
      "step 126670: train loss 2.4245, val loss 2.5933\n",
      "step 126680: train loss 2.4854, val loss 2.5147\n",
      "step 126690: train loss 2.4672, val loss 2.4251\n",
      "step 126700: train loss 2.4106, val loss 2.5549\n",
      "step 126710: train loss 2.3797, val loss 2.5185\n",
      "step 126720: train loss 2.4521, val loss 2.4757\n",
      "step 126730: train loss 2.3361, val loss 2.3684\n",
      "step 126740: train loss 2.3631, val loss 2.4743\n",
      "step 126750: train loss 2.3713, val loss 2.4278\n",
      "step 126760: train loss 2.4654, val loss 2.5398\n",
      "step 126770: train loss 2.4130, val loss 2.5163\n",
      "step 126780: train loss 2.4567, val loss 2.6080\n",
      "step 126790: train loss 2.5284, val loss 2.6309\n",
      "step 126800: train loss 2.4081, val loss 2.5106\n",
      "step 126810: train loss 2.4681, val loss 2.4641\n",
      "step 126820: train loss 2.3800, val loss 2.5734\n",
      "step 126830: train loss 2.5342, val loss 2.6211\n",
      "step 126840: train loss 2.4408, val loss 2.4569\n",
      "step 126850: train loss 2.4362, val loss 2.3980\n",
      "step 126860: train loss 2.3298, val loss 2.4899\n",
      "step 126870: train loss 2.4879, val loss 2.4818\n",
      "step 126880: train loss 2.4335, val loss 2.5297\n",
      "step 126890: train loss 2.4323, val loss 2.5156\n",
      "step 126900: train loss 2.4779, val loss 2.5277\n",
      "step 126910: train loss 2.3877, val loss 2.6185\n",
      "step 126920: train loss 2.5391, val loss 2.4321\n",
      "step 126930: train loss 2.3983, val loss 2.4455\n",
      "step 126940: train loss 2.4370, val loss 2.5710\n",
      "step 126950: train loss 2.3996, val loss 2.4659\n",
      "step 126960: train loss 2.4559, val loss 2.5097\n",
      "step 126970: train loss 2.4254, val loss 2.5292\n",
      "step 126980: train loss 2.4210, val loss 2.5980\n",
      "step 126990: train loss 2.5018, val loss 2.4988\n",
      "step 127000: train loss 2.4873, val loss 2.5243\n",
      "Generated text at iteration 127000\n",
      "\n",
      "  leue, UChe phelyeures meurs,\n",
      "Qut qur mêôz.\n",
      "E\n",
      "Rle le,\n",
      "Qus,\n",
      "Hqêles  letre;\n",
      "Jelaf a  meueil;âfi len c\n",
      "step 127010: train loss 2.4540, val loss 2.5906\n",
      "step 127020: train loss 2.3697, val loss 2.4397\n",
      "step 127030: train loss 2.4547, val loss 2.5371\n",
      "step 127040: train loss 2.4722, val loss 2.5467\n",
      "step 127050: train loss 2.4866, val loss 2.4211\n",
      "step 127060: train loss 2.4200, val loss 2.5562\n",
      "step 127070: train loss 2.4078, val loss 2.4942\n",
      "step 127080: train loss 2.3983, val loss 2.6561\n",
      "step 127090: train loss 2.4658, val loss 2.5200\n",
      "step 127100: train loss 2.4254, val loss 2.4931\n",
      "step 127110: train loss 2.3894, val loss 2.5028\n",
      "step 127120: train loss 2.4740, val loss 2.4779\n",
      "step 127130: train loss 2.4594, val loss 2.6101\n",
      "step 127140: train loss 2.4703, val loss 2.3991\n",
      "step 127150: train loss 2.4648, val loss 2.5773\n",
      "step 127160: train loss 2.3978, val loss 2.4834\n",
      "step 127170: train loss 2.4356, val loss 2.5500\n",
      "step 127180: train loss 2.4422, val loss 2.6409\n",
      "step 127190: train loss 2.4057, val loss 2.5307\n",
      "step 127200: train loss 2.3742, val loss 2.6317\n",
      "step 127210: train loss 2.4237, val loss 2.4742\n",
      "step 127220: train loss 2.4672, val loss 2.5299\n",
      "step 127230: train loss 2.4019, val loss 2.5396\n",
      "step 127240: train loss 2.4735, val loss 2.4615\n",
      "step 127250: train loss 2.3898, val loss 2.5943\n",
      "step 127260: train loss 2.3926, val loss 2.6094\n",
      "step 127270: train loss 2.4710, val loss 2.3973\n",
      "step 127280: train loss 2.4633, val loss 2.6858\n",
      "step 127290: train loss 2.4926, val loss 2.5901\n",
      "step 127300: train loss 2.5130, val loss 2.5064\n",
      "step 127310: train loss 2.4129, val loss 2.4403\n",
      "step 127320: train loss 2.4538, val loss 2.5591\n",
      "step 127330: train loss 2.4825, val loss 2.5564\n",
      "step 127340: train loss 2.3949, val loss 2.4689\n",
      "step 127350: train loss 2.4444, val loss 2.5379\n",
      "step 127360: train loss 2.4102, val loss 2.4038\n",
      "step 127370: train loss 2.3855, val loss 2.3732\n",
      "step 127380: train loss 2.3653, val loss 2.5625\n",
      "step 127390: train loss 2.4482, val loss 2.6403\n",
      "step 127400: train loss 2.4659, val loss 2.3591\n",
      "step 127410: train loss 2.3477, val loss 2.5567\n",
      "step 127420: train loss 2.3736, val loss 2.4337\n",
      "step 127430: train loss 2.4386, val loss 2.4290\n",
      "step 127440: train loss 2.4383, val loss 2.3782\n",
      "step 127450: train loss 2.3494, val loss 2.4392\n",
      "step 127460: train loss 2.3886, val loss 2.5565\n",
      "step 127470: train loss 2.4120, val loss 2.5057\n",
      "step 127480: train loss 2.4375, val loss 2.4487\n",
      "step 127490: train loss 2.3601, val loss 2.4511\n",
      "step 127500: train loss 2.4224, val loss 2.5198\n",
      "step 127510: train loss 2.4006, val loss 2.5081\n",
      "step 127520: train loss 2.4698, val loss 2.6640\n",
      "step 127530: train loss 2.4233, val loss 2.4554\n",
      "step 127540: train loss 2.3734, val loss 2.4506\n",
      "step 127550: train loss 2.3850, val loss 2.4602\n",
      "step 127560: train loss 2.4301, val loss 2.5372\n",
      "step 127570: train loss 2.4382, val loss 2.5745\n",
      "step 127580: train loss 2.4560, val loss 2.5821\n",
      "step 127590: train loss 2.4786, val loss 2.4974\n",
      "step 127600: train loss 2.5200, val loss 2.3405\n",
      "step 127610: train loss 2.4075, val loss 2.4497\n",
      "step 127620: train loss 2.4167, val loss 2.4881\n",
      "step 127630: train loss 2.3765, val loss 2.5545\n",
      "step 127640: train loss 2.4081, val loss 2.5470\n",
      "step 127650: train loss 2.4269, val loss 2.4361\n",
      "step 127660: train loss 2.4310, val loss 2.5814\n",
      "step 127670: train loss 2.3199, val loss 2.4912\n",
      "step 127680: train loss 2.4160, val loss 2.4727\n",
      "step 127690: train loss 2.4525, val loss 2.5253\n",
      "step 127700: train loss 2.3648, val loss 2.4825\n",
      "step 127710: train loss 2.4296, val loss 2.4963\n",
      "step 127720: train loss 2.4324, val loss 2.4926\n",
      "step 127730: train loss 2.4795, val loss 2.5995\n",
      "step 127740: train loss 2.4689, val loss 2.4107\n",
      "step 127750: train loss 2.4128, val loss 2.4296\n",
      "step 127760: train loss 2.4218, val loss 2.5447\n",
      "step 127770: train loss 2.3618, val loss 2.5494\n",
      "step 127780: train loss 2.4087, val loss 2.3868\n",
      "step 127790: train loss 2.4018, val loss 2.4760\n",
      "step 127800: train loss 2.4052, val loss 2.5016\n",
      "step 127810: train loss 2.4458, val loss 2.5009\n",
      "step 127820: train loss 2.4928, val loss 2.5121\n",
      "step 127830: train loss 2.4189, val loss 2.4895\n",
      "step 127840: train loss 2.4319, val loss 2.4771\n",
      "step 127850: train loss 2.3842, val loss 2.5766\n",
      "step 127860: train loss 2.3933, val loss 2.4731\n",
      "step 127870: train loss 2.4509, val loss 2.4976\n",
      "step 127880: train loss 2.4108, val loss 2.5520\n",
      "step 127890: train loss 2.4046, val loss 2.4464\n",
      "step 127900: train loss 2.4150, val loss 2.5591\n",
      "step 127910: train loss 2.3147, val loss 2.5578\n",
      "step 127920: train loss 2.4250, val loss 2.5933\n",
      "step 127930: train loss 2.5255, val loss 2.4810\n",
      "step 127940: train loss 2.4270, val loss 2.4103\n",
      "step 127950: train loss 2.3985, val loss 2.6323\n",
      "step 127960: train loss 2.5468, val loss 2.5883\n",
      "step 127970: train loss 2.4127, val loss 2.5636\n",
      "step 127980: train loss 2.3723, val loss 2.4034\n",
      "step 127990: train loss 2.4714, val loss 2.5163\n",
      "step 128000: train loss 2.4749, val loss 2.4412\n",
      "Generated text at iteration 128000\n",
      "\n",
      "Comuche mmboue tan'AÂbestt   mme mairie gr, Onspou'ase;\n",
      "N·êÆù mos ntéve ai dere mbit  lanttrès san c\n",
      "step 128010: train loss 2.4062, val loss 2.4580\n",
      "step 128020: train loss 2.4055, val loss 2.4538\n",
      "step 128030: train loss 2.3744, val loss 2.5312\n",
      "step 128040: train loss 2.3529, val loss 2.5388\n",
      "step 128050: train loss 2.4094, val loss 2.5569\n",
      "step 128060: train loss 2.4470, val loss 2.4091\n",
      "step 128070: train loss 2.4291, val loss 2.5827\n",
      "step 128080: train loss 2.3982, val loss 2.4494\n",
      "step 128090: train loss 2.3954, val loss 2.5568\n",
      "step 128100: train loss 2.4651, val loss 2.5688\n",
      "step 128110: train loss 2.4644, val loss 2.4948\n",
      "step 128120: train loss 2.4082, val loss 2.5323\n",
      "step 128130: train loss 2.4218, val loss 2.3922\n",
      "step 128140: train loss 2.4383, val loss 2.4539\n",
      "step 128150: train loss 2.4823, val loss 2.4931\n",
      "step 128160: train loss 2.4005, val loss 2.5421\n",
      "step 128170: train loss 2.4334, val loss 2.4375\n",
      "step 128180: train loss 2.4199, val loss 2.5898\n",
      "step 128190: train loss 2.3758, val loss 2.5838\n",
      "step 128200: train loss 2.3817, val loss 2.5016\n",
      "step 128210: train loss 2.5046, val loss 2.3900\n",
      "step 128220: train loss 2.4586, val loss 2.5237\n",
      "step 128230: train loss 2.3823, val loss 2.4856\n",
      "step 128240: train loss 2.5786, val loss 2.6669\n",
      "step 128250: train loss 2.3517, val loss 2.4444\n",
      "step 128260: train loss 2.4909, val loss 2.4342\n",
      "step 128270: train loss 2.5039, val loss 2.5009\n",
      "step 128280: train loss 2.4186, val loss 2.4497\n",
      "step 128290: train loss 2.4247, val loss 2.5821\n",
      "step 128300: train loss 2.4320, val loss 2.4945\n",
      "step 128310: train loss 2.5071, val loss 2.3677\n",
      "step 128320: train loss 2.4578, val loss 2.4107\n",
      "step 128330: train loss 2.3655, val loss 2.4175\n",
      "step 128340: train loss 2.4522, val loss 2.4826\n",
      "step 128350: train loss 2.3765, val loss 2.4670\n",
      "step 128360: train loss 2.3836, val loss 2.5816\n",
      "step 128370: train loss 2.4170, val loss 2.3990\n",
      "step 128380: train loss 2.4270, val loss 2.4839\n",
      "step 128390: train loss 2.4480, val loss 2.4037\n",
      "step 128400: train loss 2.4377, val loss 2.5165\n",
      "step 128410: train loss 2.3193, val loss 2.5288\n",
      "step 128420: train loss 2.3610, val loss 2.5312\n",
      "step 128430: train loss 2.4500, val loss 2.4496\n",
      "step 128440: train loss 2.4795, val loss 2.5278\n",
      "step 128450: train loss 2.4919, val loss 2.4616\n",
      "step 128460: train loss 2.4410, val loss 2.5314\n",
      "step 128470: train loss 2.4384, val loss 2.4598\n",
      "step 128480: train loss 2.3775, val loss 2.5280\n",
      "step 128490: train loss 2.4539, val loss 2.4355\n",
      "step 128500: train loss 2.3879, val loss 2.5548\n",
      "step 128510: train loss 2.4980, val loss 2.4726\n",
      "step 128520: train loss 2.4682, val loss 2.5902\n",
      "step 128530: train loss 2.4381, val loss 2.4125\n",
      "step 128540: train loss 2.4096, val loss 2.6567\n",
      "step 128550: train loss 2.4351, val loss 2.4905\n",
      "step 128560: train loss 2.4180, val loss 2.5705\n",
      "step 128570: train loss 2.4684, val loss 2.4278\n",
      "step 128580: train loss 2.4272, val loss 2.4376\n",
      "step 128590: train loss 2.4597, val loss 2.6105\n",
      "step 128600: train loss 2.4277, val loss 2.4919\n",
      "step 128610: train loss 2.4047, val loss 2.5191\n",
      "step 128620: train loss 2.4190, val loss 2.4946\n",
      "step 128630: train loss 2.4813, val loss 2.6645\n",
      "step 128640: train loss 2.3729, val loss 2.4696\n",
      "step 128650: train loss 2.4628, val loss 2.5338\n",
      "step 128660: train loss 2.4730, val loss 2.5452\n",
      "step 128670: train loss 2.4229, val loss 2.6247\n",
      "step 128680: train loss 2.5286, val loss 2.4290\n",
      "step 128690: train loss 2.4310, val loss 2.6151\n",
      "step 128700: train loss 2.3903, val loss 2.5063\n",
      "step 128710: train loss 2.4453, val loss 2.4870\n",
      "step 128720: train loss 2.4593, val loss 2.5165\n",
      "step 128730: train loss 2.3939, val loss 2.4828\n",
      "step 128740: train loss 2.4232, val loss 2.4932\n",
      "step 128750: train loss 2.3749, val loss 2.4291\n",
      "step 128760: train loss 2.4076, val loss 2.4346\n",
      "step 128770: train loss 2.3877, val loss 2.5292\n",
      "step 128780: train loss 2.3361, val loss 2.5704\n",
      "step 128790: train loss 2.4376, val loss 2.5644\n",
      "step 128800: train loss 2.3708, val loss 2.4885\n",
      "step 128810: train loss 2.4197, val loss 2.4764\n",
      "step 128820: train loss 2.3985, val loss 2.4196\n",
      "step 128830: train loss 2.4022, val loss 2.4983\n",
      "step 128840: train loss 2.3944, val loss 2.5452\n",
      "step 128850: train loss 2.4547, val loss 2.3678\n",
      "step 128860: train loss 2.3928, val loss 2.5348\n",
      "step 128870: train loss 2.4220, val loss 2.4613\n",
      "step 128880: train loss 2.3436, val loss 2.4298\n",
      "step 128890: train loss 2.4277, val loss 2.5481\n",
      "step 128900: train loss 2.4233, val loss 2.4883\n",
      "step 128910: train loss 2.4307, val loss 2.4237\n",
      "step 128920: train loss 2.3639, val loss 2.4339\n",
      "step 128930: train loss 2.4178, val loss 2.4791\n",
      "step 128940: train loss 2.4910, val loss 2.4988\n",
      "step 128950: train loss 2.3617, val loss 2.4867\n",
      "step 128960: train loss 2.3471, val loss 2.6578\n",
      "step 128970: train loss 2.4453, val loss 2.5821\n",
      "step 128980: train loss 2.4159, val loss 2.3464\n",
      "step 128990: train loss 2.4386, val loss 2.6003\n",
      "step 129000: train loss 2.5015, val loss 2.4649\n",
      "Generated text at iteration 129000\n",
      "\n",
      "D'é me con   nta Uneresalebrue! dan,\n",
      "Qd'it dordan eut.Q55 queme UNgloie  paint mos me beutrbr  rouje\n",
      "step 129010: train loss 2.4327, val loss 2.5750\n",
      "step 129020: train loss 2.4417, val loss 2.6772\n",
      "step 129030: train loss 2.4492, val loss 2.4599\n",
      "step 129040: train loss 2.4306, val loss 2.4485\n",
      "step 129050: train loss 2.4076, val loss 2.3586\n",
      "step 129060: train loss 2.3998, val loss 2.4766\n",
      "step 129070: train loss 2.3744, val loss 2.4025\n",
      "step 129080: train loss 2.4018, val loss 2.5180\n",
      "step 129090: train loss 2.4166, val loss 2.4460\n",
      "step 129100: train loss 2.4698, val loss 2.4380\n",
      "step 129110: train loss 2.4409, val loss 2.4930\n",
      "step 129120: train loss 2.4609, val loss 2.5544\n",
      "step 129130: train loss 2.3986, val loss 2.5035\n",
      "step 129140: train loss 2.3565, val loss 2.4832\n",
      "step 129150: train loss 2.4167, val loss 2.5178\n",
      "step 129160: train loss 2.4531, val loss 2.5264\n",
      "step 129170: train loss 2.3851, val loss 2.4823\n",
      "step 129180: train loss 2.3995, val loss 2.5133\n",
      "step 129190: train loss 2.4668, val loss 2.4882\n",
      "step 129200: train loss 2.4391, val loss 2.5006\n",
      "step 129210: train loss 2.4388, val loss 2.4129\n",
      "step 129220: train loss 2.4085, val loss 2.4573\n",
      "step 129230: train loss 2.3751, val loss 2.4999\n",
      "step 129240: train loss 2.4926, val loss 2.4301\n",
      "step 129250: train loss 2.4184, val loss 2.4623\n",
      "step 129260: train loss 2.3915, val loss 2.4181\n",
      "step 129270: train loss 2.4898, val loss 2.5548\n",
      "step 129280: train loss 2.4881, val loss 2.4119\n",
      "step 129290: train loss 2.4506, val loss 2.5507\n",
      "step 129300: train loss 2.4072, val loss 2.4959\n",
      "step 129310: train loss 2.4730, val loss 2.5510\n",
      "step 129320: train loss 2.4088, val loss 2.4304\n",
      "step 129330: train loss 2.4702, val loss 2.5230\n",
      "step 129340: train loss 2.4384, val loss 2.5005\n",
      "step 129350: train loss 2.4911, val loss 2.4206\n",
      "step 129360: train loss 2.4465, val loss 2.4884\n",
      "step 129370: train loss 2.4808, val loss 2.6181\n",
      "step 129380: train loss 2.3586, val loss 2.5177\n",
      "step 129390: train loss 2.4407, val loss 2.5924\n",
      "step 129400: train loss 2.4998, val loss 2.5728\n",
      "step 129410: train loss 2.4172, val loss 2.6329\n",
      "step 129420: train loss 2.3395, val loss 2.5314\n",
      "step 129430: train loss 2.3777, val loss 2.4397\n",
      "step 129440: train loss 2.4582, val loss 2.3931\n",
      "step 129450: train loss 2.4630, val loss 2.4923\n",
      "step 129460: train loss 2.3947, val loss 2.4737\n",
      "step 129470: train loss 2.4652, val loss 2.4482\n",
      "step 129480: train loss 2.4451, val loss 2.5205\n",
      "step 129490: train loss 2.4837, val loss 2.5467\n",
      "step 129500: train loss 2.3579, val loss 2.4953\n",
      "step 129510: train loss 2.4146, val loss 2.6241\n",
      "step 129520: train loss 2.4760, val loss 2.5355\n",
      "step 129530: train loss 2.3753, val loss 2.4542\n",
      "step 129540: train loss 2.4402, val loss 2.5263\n",
      "step 129550: train loss 2.4221, val loss 2.4676\n",
      "step 129560: train loss 2.3721, val loss 2.5067\n",
      "step 129570: train loss 2.5134, val loss 2.5375\n",
      "step 129580: train loss 2.3784, val loss 2.5262\n",
      "step 129590: train loss 2.4502, val loss 2.5290\n",
      "step 129600: train loss 2.4130, val loss 2.6424\n",
      "step 129610: train loss 2.3815, val loss 2.4970\n",
      "step 129620: train loss 2.4048, val loss 2.6118\n",
      "step 129630: train loss 2.3691, val loss 2.5110\n",
      "step 129640: train loss 2.4069, val loss 2.6093\n",
      "step 129650: train loss 2.4765, val loss 2.5098\n",
      "step 129660: train loss 2.3104, val loss 2.5016\n",
      "step 129670: train loss 2.4644, val loss 2.6071\n",
      "step 129680: train loss 2.5168, val loss 2.5046\n",
      "step 129690: train loss 2.4775, val loss 2.5707\n",
      "step 129700: train loss 2.4943, val loss 2.4752\n",
      "step 129710: train loss 2.4824, val loss 2.4195\n",
      "step 129720: train loss 2.4164, val loss 2.4461\n",
      "step 129730: train loss 2.4727, val loss 2.4408\n",
      "step 129740: train loss 2.4374, val loss 2.5599\n",
      "step 129750: train loss 2.3402, val loss 2.5554\n",
      "step 129760: train loss 2.4506, val loss 2.5960\n",
      "step 129770: train loss 2.4225, val loss 2.4931\n",
      "step 129780: train loss 2.4368, val loss 2.4973\n",
      "step 129790: train loss 2.4414, val loss 2.4704\n",
      "step 129800: train loss 2.4219, val loss 2.4328\n",
      "step 129810: train loss 2.4064, val loss 2.4242\n",
      "step 129820: train loss 2.3952, val loss 2.4950\n",
      "step 129830: train loss 2.4638, val loss 2.3589\n",
      "step 129840: train loss 2.4052, val loss 2.4977\n",
      "step 129850: train loss 2.4153, val loss 2.5175\n",
      "step 129860: train loss 2.4480, val loss 2.5174\n",
      "step 129870: train loss 2.4577, val loss 2.4976\n",
      "step 129880: train loss 2.4978, val loss 2.4868\n",
      "step 129890: train loss 2.4175, val loss 2.5032\n",
      "step 129900: train loss 2.4197, val loss 2.5409\n",
      "step 129910: train loss 2.4257, val loss 2.4653\n",
      "step 129920: train loss 2.4170, val loss 2.5268\n",
      "step 129930: train loss 2.4219, val loss 2.5129\n",
      "step 129940: train loss 2.4274, val loss 2.3636\n",
      "step 129950: train loss 2.3982, val loss 2.5288\n",
      "step 129960: train loss 2.4023, val loss 2.6022\n",
      "step 129970: train loss 2.4629, val loss 2.3823\n",
      "step 129980: train loss 2.3866, val loss 2.4637\n",
      "step 129990: train loss 2.4323, val loss 2.5232\n",
      "step 130000: train loss 2.4907, val loss 2.4869\n",
      "Generated text at iteration 130000\n",
      "\n",
      "Pons, de ailengZomna tôt,\n",
      "\n",
      "Din vet QNÀ4foumsus leren dou-duie qur c De, peupind;  fail flandenuraisè\n",
      "step 130010: train loss 2.4648, val loss 2.4330\n",
      "step 130020: train loss 2.4837, val loss 2.5355\n",
      "step 130030: train loss 2.3203, val loss 2.5169\n",
      "step 130040: train loss 2.3197, val loss 2.6587\n",
      "step 130050: train loss 2.4484, val loss 2.4885\n",
      "step 130060: train loss 2.4084, val loss 2.5210\n",
      "step 130070: train loss 2.3904, val loss 2.4383\n",
      "step 130080: train loss 2.4879, val loss 2.4801\n",
      "step 130090: train loss 2.3759, val loss 2.5667\n",
      "step 130100: train loss 2.4385, val loss 2.4181\n",
      "step 130110: train loss 2.3474, val loss 2.5298\n",
      "step 130120: train loss 2.4206, val loss 2.4962\n",
      "step 130130: train loss 2.5008, val loss 2.6435\n",
      "step 130140: train loss 2.4026, val loss 2.5036\n",
      "step 130150: train loss 2.4663, val loss 2.5937\n",
      "step 130160: train loss 2.4069, val loss 2.4066\n",
      "step 130170: train loss 2.4232, val loss 2.5516\n",
      "step 130180: train loss 2.4392, val loss 2.4777\n",
      "step 130190: train loss 2.4458, val loss 2.6371\n",
      "step 130200: train loss 2.3889, val loss 2.5146\n",
      "step 130210: train loss 2.4703, val loss 2.6154\n",
      "step 130220: train loss 2.3920, val loss 2.4964\n",
      "step 130230: train loss 2.4391, val loss 2.5485\n",
      "step 130240: train loss 2.4057, val loss 2.4700\n",
      "step 130250: train loss 2.4034, val loss 2.5617\n",
      "step 130260: train loss 2.3627, val loss 2.5824\n",
      "step 130270: train loss 2.3918, val loss 2.4976\n",
      "step 130280: train loss 2.3969, val loss 2.4959\n",
      "step 130290: train loss 2.4282, val loss 2.3576\n",
      "step 130300: train loss 2.3770, val loss 2.4678\n",
      "step 130310: train loss 2.3909, val loss 2.5016\n",
      "step 130320: train loss 2.3627, val loss 2.3907\n",
      "step 130330: train loss 2.4872, val loss 2.5774\n",
      "step 130340: train loss 2.4034, val loss 2.4084\n",
      "step 130350: train loss 2.4423, val loss 2.6088\n",
      "step 130360: train loss 2.4345, val loss 2.5059\n",
      "step 130370: train loss 2.3754, val loss 2.3989\n",
      "step 130380: train loss 2.4230, val loss 2.5182\n",
      "step 130390: train loss 2.3846, val loss 2.4132\n",
      "step 130400: train loss 2.4286, val loss 2.3891\n",
      "step 130410: train loss 2.3819, val loss 2.5463\n",
      "step 130420: train loss 2.4323, val loss 2.4924\n",
      "step 130430: train loss 2.4058, val loss 2.5010\n",
      "step 130440: train loss 2.4978, val loss 2.5796\n",
      "step 130450: train loss 2.4326, val loss 2.4940\n",
      "step 130460: train loss 2.4393, val loss 2.5471\n",
      "step 130470: train loss 2.3718, val loss 2.5498\n",
      "step 130480: train loss 2.4537, val loss 2.5587\n",
      "step 130490: train loss 2.4150, val loss 2.4763\n",
      "step 130500: train loss 2.4058, val loss 2.5540\n",
      "step 130510: train loss 2.4398, val loss 2.5277\n",
      "step 130520: train loss 2.4306, val loss 2.4999\n",
      "step 130530: train loss 2.5180, val loss 2.4968\n",
      "step 130540: train loss 2.3914, val loss 2.5684\n",
      "step 130550: train loss 2.4729, val loss 2.4960\n",
      "step 130560: train loss 2.5200, val loss 2.4340\n",
      "step 130570: train loss 2.3833, val loss 2.4915\n",
      "step 130580: train loss 2.4404, val loss 2.5850\n",
      "step 130590: train loss 2.3671, val loss 2.4569\n",
      "step 130600: train loss 2.3908, val loss 2.5360\n",
      "step 130610: train loss 2.4593, val loss 2.6269\n",
      "step 130620: train loss 2.4640, val loss 2.4608\n",
      "step 130630: train loss 2.4408, val loss 2.5224\n",
      "step 130640: train loss 2.4050, val loss 2.5912\n",
      "step 130650: train loss 2.4948, val loss 2.5926\n",
      "step 130660: train loss 2.4028, val loss 2.5290\n",
      "step 130670: train loss 2.3743, val loss 2.4834\n",
      "step 130680: train loss 2.3658, val loss 2.4183\n",
      "step 130690: train loss 2.4344, val loss 2.5494\n",
      "step 130700: train loss 2.4108, val loss 2.5021\n",
      "step 130710: train loss 2.3943, val loss 2.6473\n",
      "step 130720: train loss 2.3816, val loss 2.3303\n",
      "step 130730: train loss 2.3717, val loss 2.6144\n",
      "step 130740: train loss 2.3981, val loss 2.5054\n",
      "step 130750: train loss 2.3916, val loss 2.4260\n",
      "step 130760: train loss 2.3818, val loss 2.5063\n",
      "step 130770: train loss 2.3749, val loss 2.3796\n",
      "step 130780: train loss 2.5410, val loss 2.4793\n",
      "step 130790: train loss 2.4800, val loss 2.5093\n",
      "step 130800: train loss 2.4231, val loss 2.4934\n",
      "step 130810: train loss 2.4109, val loss 2.4151\n",
      "step 130820: train loss 2.4129, val loss 2.5123\n",
      "step 130830: train loss 2.4814, val loss 2.5325\n",
      "step 130840: train loss 2.4275, val loss 2.4728\n",
      "step 130850: train loss 2.4036, val loss 2.4566\n",
      "step 130860: train loss 2.4331, val loss 2.5388\n",
      "step 130870: train loss 2.4323, val loss 2.5103\n",
      "step 130880: train loss 2.3430, val loss 2.4844\n",
      "step 130890: train loss 2.4283, val loss 2.5216\n",
      "step 130900: train loss 2.4261, val loss 2.5365\n",
      "step 130910: train loss 2.3333, val loss 2.5371\n",
      "step 130920: train loss 2.3716, val loss 2.4652\n",
      "step 130930: train loss 2.5115, val loss 2.4888\n",
      "step 130940: train loss 2.3906, val loss 2.5323\n",
      "step 130950: train loss 2.4643, val loss 2.5024\n",
      "step 130960: train loss 2.4492, val loss 2.4948\n",
      "step 130970: train loss 2.3909, val loss 2.5483\n",
      "step 130980: train loss 2.4535, val loss 2.5316\n",
      "step 130990: train loss 2.4563, val loss 2.4360\n",
      "step 131000: train loss 2.4104, val loss 2.6748\n",
      "Generated text at iteration 131000\n",
      "\n",
      "Shecr vot uile de qure   ls: aiffr tur meprmpèrs lait «éere;\n",
      "T(]wrsé!  Jcemeze trchormée lere,\n",
      "V\n",
      "\n",
      "C'\n",
      "step 131010: train loss 2.3756, val loss 2.5668\n",
      "step 131020: train loss 2.3783, val loss 2.5336\n",
      "step 131030: train loss 2.4831, val loss 2.4532\n",
      "step 131040: train loss 2.4380, val loss 2.5042\n",
      "step 131050: train loss 2.4291, val loss 2.5307\n",
      "step 131060: train loss 2.4042, val loss 2.3537\n",
      "step 131070: train loss 2.4897, val loss 2.5865\n",
      "step 131080: train loss 2.4120, val loss 2.4662\n",
      "step 131090: train loss 2.4722, val loss 2.3822\n",
      "step 131100: train loss 2.4521, val loss 2.4330\n",
      "step 131110: train loss 2.4376, val loss 2.5293\n",
      "step 131120: train loss 2.3891, val loss 2.4721\n",
      "step 131130: train loss 2.4859, val loss 2.4410\n",
      "step 131140: train loss 2.4520, val loss 2.4885\n",
      "step 131150: train loss 2.3715, val loss 2.5284\n",
      "step 131160: train loss 2.3809, val loss 2.5892\n",
      "step 131170: train loss 2.4140, val loss 2.4284\n",
      "step 131180: train loss 2.4544, val loss 2.5694\n",
      "step 131190: train loss 2.3791, val loss 2.5386\n",
      "step 131200: train loss 2.4111, val loss 2.5082\n",
      "step 131210: train loss 2.4402, val loss 2.4811\n",
      "step 131220: train loss 2.4579, val loss 2.4666\n",
      "step 131230: train loss 2.4156, val loss 2.4019\n",
      "step 131240: train loss 2.4260, val loss 2.4726\n",
      "step 131250: train loss 2.4339, val loss 2.6108\n",
      "step 131260: train loss 2.4130, val loss 2.4341\n",
      "step 131270: train loss 2.4582, val loss 2.5929\n",
      "step 131280: train loss 2.3573, val loss 2.5466\n",
      "step 131290: train loss 2.4488, val loss 2.5392\n",
      "step 131300: train loss 2.3893, val loss 2.4657\n",
      "step 131310: train loss 2.4414, val loss 2.5325\n",
      "step 131320: train loss 2.4104, val loss 2.4696\n",
      "step 131330: train loss 2.5162, val loss 2.5799\n",
      "step 131340: train loss 2.4011, val loss 2.5227\n",
      "step 131350: train loss 2.4265, val loss 2.5201\n",
      "step 131360: train loss 2.3518, val loss 2.5969\n",
      "step 131370: train loss 2.4969, val loss 2.6002\n",
      "step 131380: train loss 2.4352, val loss 2.4795\n",
      "step 131390: train loss 2.5104, val loss 2.4916\n",
      "step 131400: train loss 2.4633, val loss 2.4764\n",
      "step 131410: train loss 2.3733, val loss 2.4778\n",
      "step 131420: train loss 2.4874, val loss 2.5178\n",
      "step 131430: train loss 2.4802, val loss 2.5386\n",
      "step 131440: train loss 2.4417, val loss 2.4347\n",
      "step 131450: train loss 2.4330, val loss 2.4650\n",
      "step 131460: train loss 2.4074, val loss 2.5604\n",
      "step 131470: train loss 2.5255, val loss 2.5694\n",
      "step 131480: train loss 2.4712, val loss 2.5749\n",
      "step 131490: train loss 2.4591, val loss 2.4819\n",
      "step 131500: train loss 2.4605, val loss 2.6146\n",
      "step 131510: train loss 2.4013, val loss 2.4596\n",
      "step 131520: train loss 2.4250, val loss 2.5506\n",
      "step 131530: train loss 2.3784, val loss 2.5851\n",
      "step 131540: train loss 2.4277, val loss 2.5218\n",
      "step 131550: train loss 2.4359, val loss 2.5192\n",
      "step 131560: train loss 2.4189, val loss 2.4568\n",
      "step 131570: train loss 2.4386, val loss 2.4440\n",
      "step 131580: train loss 2.3961, val loss 2.5657\n",
      "step 131590: train loss 2.4064, val loss 2.4499\n",
      "step 131600: train loss 2.4172, val loss 2.4284\n",
      "step 131610: train loss 2.4612, val loss 2.4814\n",
      "step 131620: train loss 2.4621, val loss 2.4504\n",
      "step 131630: train loss 2.4488, val loss 2.5254\n",
      "step 131640: train loss 2.4944, val loss 2.5914\n",
      "step 131650: train loss 2.4589, val loss 2.5004\n",
      "step 131660: train loss 2.3625, val loss 2.4656\n",
      "step 131670: train loss 2.3886, val loss 2.4691\n",
      "step 131680: train loss 2.4900, val loss 2.4511\n",
      "step 131690: train loss 2.4019, val loss 2.4792\n",
      "step 131700: train loss 2.3801, val loss 2.6666\n",
      "step 131710: train loss 2.4787, val loss 2.5791\n",
      "step 131720: train loss 2.3803, val loss 2.4838\n",
      "step 131730: train loss 2.4242, val loss 2.4884\n",
      "step 131740: train loss 2.4641, val loss 2.5578\n",
      "step 131750: train loss 2.3730, val loss 2.5163\n",
      "step 131760: train loss 2.4528, val loss 2.5328\n",
      "step 131770: train loss 2.5385, val loss 2.4698\n",
      "step 131780: train loss 2.4094, val loss 2.5155\n",
      "step 131790: train loss 2.4732, val loss 2.3804\n",
      "step 131800: train loss 2.4157, val loss 2.5159\n",
      "step 131810: train loss 2.3914, val loss 2.4382\n",
      "step 131820: train loss 2.4622, val loss 2.5407\n",
      "step 131830: train loss 2.4051, val loss 2.4528\n",
      "step 131840: train loss 2.4037, val loss 2.5712\n",
      "step 131850: train loss 2.5096, val loss 2.4266\n",
      "step 131860: train loss 2.4164, val loss 2.5658\n",
      "step 131870: train loss 2.4712, val loss 2.4672\n",
      "step 131880: train loss 2.5263, val loss 2.4432\n",
      "step 131890: train loss 2.4638, val loss 2.4350\n",
      "step 131900: train loss 2.4429, val loss 2.4281\n",
      "step 131910: train loss 2.3988, val loss 2.4712\n",
      "step 131920: train loss 2.4293, val loss 2.4937\n",
      "step 131930: train loss 2.4323, val loss 2.5023\n",
      "step 131940: train loss 2.4323, val loss 2.5573\n",
      "step 131950: train loss 2.4666, val loss 2.4481\n",
      "step 131960: train loss 2.3829, val loss 2.4466\n",
      "step 131970: train loss 2.4057, val loss 2.5235\n",
      "step 131980: train loss 2.4101, val loss 2.5907\n",
      "step 131990: train loss 2.3884, val loss 2.6206\n",
      "step 132000: train loss 2.4495, val loss 2.5737\n",
      "Generated text at iteration 132000\n",
      "\n",
      "A lut Les de mos fâFQunens ser heuétre merentese pux aione juierlevil'auiecheu daseux, lep ntr!  de,\n",
      "step 132010: train loss 2.4218, val loss 2.6278\n",
      "step 132020: train loss 2.4523, val loss 2.5399\n",
      "step 132030: train loss 2.3790, val loss 2.4993\n",
      "step 132040: train loss 2.3977, val loss 2.5239\n",
      "step 132050: train loss 2.4352, val loss 2.4757\n",
      "step 132060: train loss 2.4279, val loss 2.5051\n",
      "step 132070: train loss 2.4104, val loss 2.5643\n",
      "step 132080: train loss 2.4894, val loss 2.5823\n",
      "step 132090: train loss 2.4203, val loss 2.5280\n",
      "step 132100: train loss 2.3953, val loss 2.5036\n",
      "step 132110: train loss 2.4460, val loss 2.5557\n",
      "step 132120: train loss 2.3836, val loss 2.3981\n",
      "step 132130: train loss 2.4666, val loss 2.5249\n",
      "step 132140: train loss 2.4291, val loss 2.4708\n",
      "step 132150: train loss 2.4452, val loss 2.4673\n",
      "step 132160: train loss 2.3740, val loss 2.5536\n",
      "step 132170: train loss 2.3893, val loss 2.5136\n",
      "step 132180: train loss 2.3519, val loss 2.5272\n",
      "step 132190: train loss 2.4938, val loss 2.4521\n",
      "step 132200: train loss 2.4409, val loss 2.5281\n",
      "step 132210: train loss 2.4473, val loss 2.6156\n",
      "step 132220: train loss 2.4512, val loss 2.4173\n",
      "step 132230: train loss 2.3954, val loss 2.5246\n",
      "step 132240: train loss 2.3744, val loss 2.5734\n",
      "step 132250: train loss 2.4834, val loss 2.5306\n",
      "step 132260: train loss 2.4032, val loss 2.4664\n",
      "step 132270: train loss 2.4460, val loss 2.5734\n",
      "step 132280: train loss 2.3479, val loss 2.4761\n",
      "step 132290: train loss 2.4154, val loss 2.4450\n",
      "step 132300: train loss 2.4020, val loss 2.5242\n",
      "step 132310: train loss 2.4057, val loss 2.4515\n",
      "step 132320: train loss 2.4235, val loss 2.6130\n",
      "step 132330: train loss 2.3652, val loss 2.6697\n",
      "step 132340: train loss 2.4162, val loss 2.4808\n",
      "step 132350: train loss 2.4012, val loss 2.5157\n",
      "step 132360: train loss 2.4425, val loss 2.4745\n",
      "step 132370: train loss 2.4959, val loss 2.4482\n",
      "step 132380: train loss 2.4052, val loss 2.4172\n",
      "step 132390: train loss 2.4057, val loss 2.5655\n",
      "step 132400: train loss 2.3923, val loss 2.4719\n",
      "step 132410: train loss 2.4192, val loss 2.4604\n",
      "step 132420: train loss 2.3846, val loss 2.4890\n",
      "step 132430: train loss 2.4602, val loss 2.5120\n",
      "step 132440: train loss 2.4460, val loss 2.4608\n",
      "step 132450: train loss 2.5593, val loss 2.5503\n",
      "step 132460: train loss 2.3902, val loss 2.5245\n",
      "step 132470: train loss 2.4385, val loss 2.4745\n",
      "step 132480: train loss 2.4632, val loss 2.5302\n",
      "step 132490: train loss 2.3871, val loss 2.4440\n",
      "step 132500: train loss 2.4235, val loss 2.4362\n",
      "step 132510: train loss 2.4467, val loss 2.4619\n",
      "step 132520: train loss 2.3641, val loss 2.5406\n",
      "step 132530: train loss 2.3564, val loss 2.5426\n",
      "step 132540: train loss 2.4428, val loss 2.3765\n",
      "step 132550: train loss 2.4365, val loss 2.4193\n",
      "step 132560: train loss 2.4645, val loss 2.5101\n",
      "step 132570: train loss 2.4313, val loss 2.5596\n",
      "step 132580: train loss 2.4820, val loss 2.5002\n",
      "step 132590: train loss 2.4753, val loss 2.4141\n",
      "step 132600: train loss 2.3764, val loss 2.4929\n",
      "step 132610: train loss 2.3989, val loss 2.5217\n",
      "step 132620: train loss 2.4213, val loss 2.4770\n",
      "step 132630: train loss 2.4759, val loss 2.5274\n",
      "step 132640: train loss 2.3654, val loss 2.5114\n",
      "step 132650: train loss 2.4322, val loss 2.5555\n",
      "step 132660: train loss 2.3834, val loss 2.4812\n",
      "step 132670: train loss 2.3418, val loss 2.4834\n",
      "step 132680: train loss 2.3797, val loss 2.4523\n",
      "step 132690: train loss 2.3950, val loss 2.4253\n",
      "step 132700: train loss 2.4434, val loss 2.5857\n",
      "step 132710: train loss 2.4047, val loss 2.5481\n",
      "step 132720: train loss 2.4134, val loss 2.4384\n",
      "step 132730: train loss 2.3805, val loss 2.5252\n",
      "step 132740: train loss 2.4305, val loss 2.3850\n",
      "step 132750: train loss 2.4256, val loss 2.4661\n",
      "step 132760: train loss 2.3908, val loss 2.5924\n",
      "step 132770: train loss 2.4170, val loss 2.5585\n",
      "step 132780: train loss 2.4835, val loss 2.3949\n",
      "step 132790: train loss 2.4491, val loss 2.4937\n",
      "step 132800: train loss 2.3675, val loss 2.3603\n",
      "step 132810: train loss 2.3729, val loss 2.5320\n",
      "step 132820: train loss 2.4004, val loss 2.4536\n",
      "step 132830: train loss 2.2938, val loss 2.6063\n",
      "step 132840: train loss 2.3723, val loss 2.4959\n",
      "step 132850: train loss 2.3935, val loss 2.6122\n",
      "step 132860: train loss 2.3840, val loss 2.4963\n",
      "step 132870: train loss 2.4379, val loss 2.5870\n",
      "step 132880: train loss 2.4558, val loss 2.4559\n",
      "step 132890: train loss 2.3826, val loss 2.5219\n",
      "step 132900: train loss 2.3862, val loss 2.5563\n",
      "step 132910: train loss 2.4545, val loss 2.5791\n",
      "step 132920: train loss 2.4634, val loss 2.5491\n",
      "step 132930: train loss 2.4564, val loss 2.4532\n",
      "step 132940: train loss 2.4270, val loss 2.4600\n",
      "step 132950: train loss 2.4024, val loss 2.4634\n",
      "step 132960: train loss 2.3461, val loss 2.4221\n",
      "step 132970: train loss 2.4630, val loss 2.4578\n",
      "step 132980: train loss 2.3941, val loss 2.5263\n",
      "step 132990: train loss 2.4296, val loss 2.5069\n",
      "step 133000: train loss 2.4701, val loss 2.5124\n",
      "Generated text at iteration 133000\n",
      "\n",
      "Quirs ay lavis'Ela lar emaureseuresiz-Hrremyromoendorez! Ayaure quoiouvilain fis,\n",
      "Lntul'ent!\n",
      "E\n",
      "\n",
      "Ly,\n",
      "\n",
      "step 133010: train loss 2.4765, val loss 2.3948\n",
      "step 133020: train loss 2.4426, val loss 2.4440\n",
      "step 133030: train loss 2.4210, val loss 2.5634\n",
      "step 133040: train loss 2.4382, val loss 2.3517\n",
      "step 133050: train loss 2.3723, val loss 2.5561\n",
      "step 133060: train loss 2.4253, val loss 2.4442\n",
      "step 133070: train loss 2.4799, val loss 2.5490\n",
      "step 133080: train loss 2.4162, val loss 2.4269\n",
      "step 133090: train loss 2.3618, val loss 2.4686\n",
      "step 133100: train loss 2.3358, val loss 2.4913\n",
      "step 133110: train loss 2.4466, val loss 2.4626\n",
      "step 133120: train loss 2.3347, val loss 2.5181\n",
      "step 133130: train loss 2.3921, val loss 2.4471\n",
      "step 133140: train loss 2.4975, val loss 2.5567\n",
      "step 133150: train loss 2.4193, val loss 2.4994\n",
      "step 133160: train loss 2.3807, val loss 2.5568\n",
      "step 133170: train loss 2.4132, val loss 2.5416\n",
      "step 133180: train loss 2.4248, val loss 2.5501\n",
      "step 133190: train loss 2.4420, val loss 2.5099\n",
      "step 133200: train loss 2.3981, val loss 2.4616\n",
      "step 133210: train loss 2.4573, val loss 2.5268\n",
      "step 133220: train loss 2.4441, val loss 2.4297\n",
      "step 133230: train loss 2.5153, val loss 2.4741\n",
      "step 133240: train loss 2.4922, val loss 2.6061\n",
      "step 133250: train loss 2.3860, val loss 2.4736\n",
      "step 133260: train loss 2.4526, val loss 2.4192\n",
      "step 133270: train loss 2.4611, val loss 2.5381\n",
      "step 133280: train loss 2.3437, val loss 2.6035\n",
      "step 133290: train loss 2.4244, val loss 2.5585\n",
      "step 133300: train loss 2.3498, val loss 2.3921\n",
      "step 133310: train loss 2.4827, val loss 2.5300\n",
      "step 133320: train loss 2.4090, val loss 2.4600\n",
      "step 133330: train loss 2.4406, val loss 2.6499\n",
      "step 133340: train loss 2.4595, val loss 2.4252\n",
      "step 133350: train loss 2.3816, val loss 2.6009\n",
      "step 133360: train loss 2.4495, val loss 2.4239\n",
      "step 133370: train loss 2.3932, val loss 2.5508\n",
      "step 133380: train loss 2.4861, val loss 2.4785\n",
      "step 133390: train loss 2.4249, val loss 2.5144\n",
      "step 133400: train loss 2.3761, val loss 2.4951\n",
      "step 133410: train loss 2.4933, val loss 2.4131\n",
      "step 133420: train loss 2.4761, val loss 2.4801\n",
      "step 133430: train loss 2.4241, val loss 2.4749\n",
      "step 133440: train loss 2.3970, val loss 2.5102\n",
      "step 133450: train loss 2.3630, val loss 2.5540\n",
      "step 133460: train loss 2.4152, val loss 2.5049\n",
      "step 133470: train loss 2.4009, val loss 2.5597\n",
      "step 133480: train loss 2.4268, val loss 2.5541\n",
      "step 133490: train loss 2.4543, val loss 2.4693\n",
      "step 133500: train loss 2.4299, val loss 2.4744\n",
      "step 133510: train loss 2.4796, val loss 2.4732\n",
      "step 133520: train loss 2.3636, val loss 2.6618\n",
      "step 133530: train loss 2.4961, val loss 2.4864\n",
      "step 133540: train loss 2.4877, val loss 2.5060\n",
      "step 133550: train loss 2.3246, val loss 2.5563\n",
      "step 133560: train loss 2.4038, val loss 2.4725\n",
      "step 133570: train loss 2.3155, val loss 2.5506\n",
      "step 133580: train loss 2.3861, val loss 2.5262\n",
      "step 133590: train loss 2.4393, val loss 2.5023\n",
      "step 133600: train loss 2.4065, val loss 2.4868\n",
      "step 133610: train loss 2.5138, val loss 2.3558\n",
      "step 133620: train loss 2.4616, val loss 2.5725\n",
      "step 133630: train loss 2.4575, val loss 2.5428\n",
      "step 133640: train loss 2.4077, val loss 2.5455\n",
      "step 133650: train loss 2.4452, val loss 2.4490\n",
      "step 133660: train loss 2.4028, val loss 2.4312\n",
      "step 133670: train loss 2.4700, val loss 2.4494\n",
      "step 133680: train loss 2.4576, val loss 2.4720\n",
      "step 133690: train loss 2.4220, val loss 2.5160\n",
      "step 133700: train loss 2.3990, val loss 2.4653\n",
      "step 133710: train loss 2.3463, val loss 2.4815\n",
      "step 133720: train loss 2.4945, val loss 2.4480\n",
      "step 133730: train loss 2.4328, val loss 2.4651\n",
      "step 133740: train loss 2.4573, val loss 2.5494\n",
      "step 133750: train loss 2.4965, val loss 2.4639\n",
      "step 133760: train loss 2.4218, val loss 2.4299\n",
      "step 133770: train loss 2.4507, val loss 2.5637\n",
      "step 133780: train loss 2.3979, val loss 2.5411\n",
      "step 133790: train loss 2.4088, val loss 2.4753\n",
      "step 133800: train loss 2.3714, val loss 2.4140\n",
      "step 133810: train loss 2.3536, val loss 2.4226\n",
      "step 133820: train loss 2.4245, val loss 2.4758\n",
      "step 133830: train loss 2.3920, val loss 2.5395\n",
      "step 133840: train loss 2.5223, val loss 2.4999\n",
      "step 133850: train loss 2.3669, val loss 2.3757\n",
      "step 133860: train loss 2.3687, val loss 2.5459\n",
      "step 133870: train loss 2.4622, val loss 2.5847\n",
      "step 133880: train loss 2.4049, val loss 2.4893\n",
      "step 133890: train loss 2.4399, val loss 2.5309\n",
      "step 133900: train loss 2.4092, val loss 2.4685\n",
      "step 133910: train loss 2.4193, val loss 2.4829\n",
      "step 133920: train loss 2.3813, val loss 2.6325\n",
      "step 133930: train loss 2.4241, val loss 2.5292\n",
      "step 133940: train loss 2.4485, val loss 2.4425\n",
      "step 133950: train loss 2.4403, val loss 2.5456\n",
      "step 133960: train loss 2.4955, val loss 2.5688\n",
      "step 133970: train loss 2.4991, val loss 2.4706\n",
      "step 133980: train loss 2.5186, val loss 2.4482\n",
      "step 133990: train loss 2.4317, val loss 2.4907\n",
      "step 134000: train loss 2.4134, val loss 2.4882\n",
      "Generated text at iteration 134000\n",
      "\n",
      "18ù brt mmprntot s lete.\n",
      "TAD'ans TZÆDi ir drple,\n",
      "A5Kls abr; ce! s lansassaufffrteigenun ITurt plavoi\n",
      "step 134010: train loss 2.4127, val loss 2.4659\n",
      "step 134020: train loss 2.4504, val loss 2.5418\n",
      "step 134030: train loss 2.3837, val loss 2.4370\n",
      "step 134040: train loss 2.4439, val loss 2.4804\n",
      "step 134050: train loss 2.4408, val loss 2.5210\n",
      "step 134060: train loss 2.4633, val loss 2.4996\n",
      "step 134070: train loss 2.4300, val loss 2.4447\n",
      "step 134080: train loss 2.3859, val loss 2.4267\n",
      "step 134090: train loss 2.4563, val loss 2.5932\n",
      "step 134100: train loss 2.4430, val loss 2.4650\n",
      "step 134110: train loss 2.5018, val loss 2.5182\n",
      "step 134120: train loss 2.3919, val loss 2.4736\n",
      "step 134130: train loss 2.4056, val loss 2.5515\n",
      "step 134140: train loss 2.5047, val loss 2.5364\n",
      "step 134150: train loss 2.4261, val loss 2.5066\n",
      "step 134160: train loss 2.4523, val loss 2.4761\n",
      "step 134170: train loss 2.4675, val loss 2.5205\n",
      "step 134180: train loss 2.3831, val loss 2.5387\n",
      "step 134190: train loss 2.4507, val loss 2.4306\n",
      "step 134200: train loss 2.3670, val loss 2.5228\n",
      "step 134210: train loss 2.3759, val loss 2.5170\n",
      "step 134220: train loss 2.3799, val loss 2.5287\n",
      "step 134230: train loss 2.5077, val loss 2.4813\n",
      "step 134240: train loss 2.3756, val loss 2.5766\n",
      "step 134250: train loss 2.4024, val loss 2.4654\n",
      "step 134260: train loss 2.4853, val loss 2.4160\n",
      "step 134270: train loss 2.4028, val loss 2.5281\n",
      "step 134280: train loss 2.4850, val loss 2.5262\n",
      "step 134290: train loss 2.4643, val loss 2.5449\n",
      "step 134300: train loss 2.3351, val loss 2.3875\n",
      "step 134310: train loss 2.4366, val loss 2.5714\n",
      "step 134320: train loss 2.4274, val loss 2.4857\n",
      "step 134330: train loss 2.4360, val loss 2.5540\n",
      "step 134340: train loss 2.3957, val loss 2.4657\n",
      "step 134350: train loss 2.4615, val loss 2.4536\n",
      "step 134360: train loss 2.4752, val loss 2.4512\n",
      "step 134370: train loss 2.4587, val loss 2.4249\n",
      "step 134380: train loss 2.3971, val loss 2.5321\n",
      "step 134390: train loss 2.5385, val loss 2.5514\n",
      "step 134400: train loss 2.4080, val loss 2.5132\n",
      "step 134410: train loss 2.4183, val loss 2.5502\n",
      "step 134420: train loss 2.4504, val loss 2.5150\n",
      "step 134430: train loss 2.4072, val loss 2.4760\n",
      "step 134440: train loss 2.4363, val loss 2.5576\n",
      "step 134450: train loss 2.3561, val loss 2.4710\n",
      "step 134460: train loss 2.4765, val loss 2.5109\n",
      "step 134470: train loss 2.4329, val loss 2.5326\n",
      "step 134480: train loss 2.4453, val loss 2.5602\n",
      "step 134490: train loss 2.4844, val loss 2.4755\n",
      "step 134500: train loss 2.4116, val loss 2.5381\n",
      "step 134510: train loss 2.4151, val loss 2.6505\n",
      "step 134520: train loss 2.3906, val loss 2.4588\n",
      "step 134530: train loss 2.3923, val loss 2.5001\n",
      "step 134540: train loss 2.4580, val loss 2.5735\n",
      "step 134550: train loss 2.4598, val loss 2.4736\n",
      "step 134560: train loss 2.4925, val loss 2.6087\n",
      "step 134570: train loss 2.3661, val loss 2.3963\n",
      "step 134580: train loss 2.4410, val loss 2.4545\n",
      "step 134590: train loss 2.3696, val loss 2.4960\n",
      "step 134600: train loss 2.4440, val loss 2.4291\n",
      "step 134610: train loss 2.3879, val loss 2.5842\n",
      "step 134620: train loss 2.3984, val loss 2.5703\n",
      "step 134630: train loss 2.4839, val loss 2.5075\n",
      "step 134640: train loss 2.4304, val loss 2.5076\n",
      "step 134650: train loss 2.3921, val loss 2.5643\n",
      "step 134660: train loss 2.4449, val loss 2.4595\n",
      "step 134670: train loss 2.4892, val loss 2.5669\n",
      "step 134680: train loss 2.4000, val loss 2.4522\n",
      "step 134690: train loss 2.5428, val loss 2.4649\n",
      "step 134700: train loss 2.3804, val loss 2.5620\n",
      "step 134710: train loss 2.4543, val loss 2.4167\n",
      "step 134720: train loss 2.4835, val loss 2.5391\n",
      "step 134730: train loss 2.4018, val loss 2.6228\n",
      "step 134740: train loss 2.4395, val loss 2.5707\n",
      "step 134750: train loss 2.4337, val loss 2.4676\n",
      "step 134760: train loss 2.4214, val loss 2.5365\n",
      "step 134770: train loss 2.4714, val loss 2.6068\n",
      "step 134780: train loss 2.5034, val loss 2.5009\n",
      "step 134790: train loss 2.4198, val loss 2.4456\n",
      "step 134800: train loss 2.4269, val loss 2.4419\n",
      "step 134810: train loss 2.4531, val loss 2.5320\n",
      "step 134820: train loss 2.4755, val loss 2.4270\n",
      "step 134830: train loss 2.4441, val loss 2.5021\n",
      "step 134840: train loss 2.4143, val loss 2.5177\n",
      "step 134850: train loss 2.3938, val loss 2.4095\n",
      "step 134860: train loss 2.4162, val loss 2.5207\n",
      "step 134870: train loss 2.4516, val loss 2.5344\n",
      "step 134880: train loss 2.4145, val loss 2.4173\n",
      "step 134890: train loss 2.4285, val loss 2.4651\n",
      "step 134900: train loss 2.3863, val loss 2.5213\n",
      "step 134910: train loss 2.4700, val loss 2.5597\n",
      "step 134920: train loss 2.4201, val loss 2.4602\n",
      "step 134930: train loss 2.4883, val loss 2.4593\n",
      "step 134940: train loss 2.3981, val loss 2.4460\n",
      "step 134950: train loss 2.4828, val loss 2.4975\n",
      "step 134960: train loss 2.4475, val loss 2.5921\n",
      "step 134970: train loss 2.4016, val loss 2.6225\n",
      "step 134980: train loss 2.4664, val loss 2.4376\n",
      "step 134990: train loss 2.3755, val loss 2.5342\n",
      "step 135000: train loss 2.4003, val loss 2.4738\n",
      "Generated text at iteration 135000\n",
      "\n",
      "Peur   s lan fubre  lat lande qu'e p s s unt cbetrie Suts is va chustiesonos  ssséges d'e das l'a ja\n",
      "step 135010: train loss 2.3971, val loss 2.4016\n",
      "step 135020: train loss 2.4592, val loss 2.4180\n",
      "step 135030: train loss 2.3670, val loss 2.5332\n",
      "step 135040: train loss 2.3943, val loss 2.3882\n",
      "step 135050: train loss 2.4716, val loss 2.4971\n",
      "step 135060: train loss 2.4166, val loss 2.5024\n",
      "step 135070: train loss 2.4596, val loss 2.6547\n",
      "step 135080: train loss 2.3805, val loss 2.5374\n",
      "step 135090: train loss 2.3997, val loss 2.5165\n",
      "step 135100: train loss 2.4463, val loss 2.4797\n",
      "step 135110: train loss 2.4726, val loss 2.4990\n",
      "step 135120: train loss 2.4388, val loss 2.4816\n",
      "step 135130: train loss 2.4609, val loss 2.4086\n",
      "step 135140: train loss 2.3910, val loss 2.4764\n",
      "step 135150: train loss 2.3572, val loss 2.5008\n",
      "step 135160: train loss 2.4333, val loss 2.4317\n",
      "step 135170: train loss 2.3845, val loss 2.5497\n",
      "step 135180: train loss 2.4561, val loss 2.4701\n",
      "step 135190: train loss 2.4485, val loss 2.4467\n",
      "step 135200: train loss 2.4105, val loss 2.5229\n",
      "step 135210: train loss 2.3494, val loss 2.4355\n",
      "step 135220: train loss 2.4038, val loss 2.5137\n",
      "step 135230: train loss 2.4035, val loss 2.5628\n",
      "step 135240: train loss 2.4558, val loss 2.5747\n",
      "step 135250: train loss 2.4564, val loss 2.4664\n",
      "step 135260: train loss 2.3872, val loss 2.5727\n",
      "step 135270: train loss 2.4760, val loss 2.4491\n",
      "step 135280: train loss 2.5060, val loss 2.3896\n",
      "step 135290: train loss 2.4247, val loss 2.4467\n",
      "step 135300: train loss 2.3896, val loss 2.4468\n",
      "step 135310: train loss 2.4304, val loss 2.5020\n",
      "step 135320: train loss 2.4510, val loss 2.5195\n",
      "step 135330: train loss 2.4366, val loss 2.5232\n",
      "step 135340: train loss 2.3751, val loss 2.4042\n",
      "step 135350: train loss 2.4306, val loss 2.5263\n",
      "step 135360: train loss 2.4685, val loss 2.4548\n",
      "step 135370: train loss 2.4386, val loss 2.5257\n",
      "step 135380: train loss 2.4834, val loss 2.5148\n",
      "step 135390: train loss 2.3781, val loss 2.4590\n",
      "step 135400: train loss 2.3659, val loss 2.4224\n",
      "step 135410: train loss 2.4597, val loss 2.4524\n",
      "step 135420: train loss 2.3983, val loss 2.4342\n",
      "step 135430: train loss 2.4140, val loss 2.5360\n",
      "step 135440: train loss 2.3854, val loss 2.4831\n",
      "step 135450: train loss 2.4234, val loss 2.3571\n",
      "step 135460: train loss 2.3977, val loss 2.4332\n",
      "step 135470: train loss 2.4978, val loss 2.4924\n",
      "step 135480: train loss 2.4355, val loss 2.5442\n",
      "step 135490: train loss 2.4319, val loss 2.5865\n",
      "step 135500: train loss 2.3882, val loss 2.4685\n",
      "step 135510: train loss 2.4189, val loss 2.5308\n",
      "step 135520: train loss 2.5195, val loss 2.4588\n",
      "step 135530: train loss 2.4276, val loss 2.4206\n",
      "step 135540: train loss 2.3962, val loss 2.4904\n",
      "step 135550: train loss 2.4452, val loss 2.4714\n",
      "step 135560: train loss 2.4044, val loss 2.4149\n",
      "step 135570: train loss 2.3788, val loss 2.6338\n",
      "step 135580: train loss 2.3873, val loss 2.4293\n",
      "step 135590: train loss 2.4713, val loss 2.4129\n",
      "step 135600: train loss 2.4399, val loss 2.5012\n",
      "step 135610: train loss 2.4380, val loss 2.5232\n",
      "step 135620: train loss 2.5319, val loss 2.5543\n",
      "step 135630: train loss 2.4301, val loss 2.5694\n",
      "step 135640: train loss 2.3302, val loss 2.3867\n",
      "step 135650: train loss 2.3554, val loss 2.5355\n",
      "step 135660: train loss 2.5209, val loss 2.4713\n",
      "step 135670: train loss 2.4386, val loss 2.6714\n",
      "step 135680: train loss 2.4385, val loss 2.5424\n",
      "step 135690: train loss 2.4663, val loss 2.4713\n",
      "step 135700: train loss 2.4922, val loss 2.5530\n",
      "step 135710: train loss 2.3761, val loss 2.5647\n",
      "step 135720: train loss 2.5031, val loss 2.4849\n",
      "step 135730: train loss 2.4277, val loss 2.4752\n",
      "step 135740: train loss 2.4078, val loss 2.6106\n",
      "step 135750: train loss 2.4147, val loss 2.3762\n",
      "step 135760: train loss 2.4814, val loss 2.5219\n",
      "step 135770: train loss 2.4674, val loss 2.3534\n",
      "step 135780: train loss 2.4045, val loss 2.5252\n",
      "step 135790: train loss 2.4207, val loss 2.5815\n",
      "step 135800: train loss 2.5082, val loss 2.4883\n",
      "step 135810: train loss 2.4508, val loss 2.5941\n",
      "step 135820: train loss 2.3846, val loss 2.5462\n",
      "step 135830: train loss 2.4631, val loss 2.5400\n",
      "step 135840: train loss 2.4650, val loss 2.5410\n",
      "step 135850: train loss 2.3704, val loss 2.4577\n",
      "step 135860: train loss 2.4638, val loss 2.4520\n",
      "step 135870: train loss 2.4014, val loss 2.4767\n",
      "step 135880: train loss 2.3836, val loss 2.5249\n",
      "step 135890: train loss 2.3197, val loss 2.4927\n",
      "step 135900: train loss 2.5265, val loss 2.5014\n",
      "step 135910: train loss 2.4148, val loss 2.5019\n",
      "step 135920: train loss 2.4416, val loss 2.4456\n",
      "step 135930: train loss 2.4404, val loss 2.5359\n",
      "step 135940: train loss 2.4557, val loss 2.4362\n",
      "step 135950: train loss 2.4255, val loss 2.5487\n",
      "step 135960: train loss 2.4258, val loss 2.4984\n",
      "step 135970: train loss 2.3860, val loss 2.5622\n",
      "step 135980: train loss 2.4185, val loss 2.3809\n",
      "step 135990: train loss 2.3852, val loss 2.5349\n",
      "step 136000: train loss 2.4492, val loss 2.4509\n",
      "Generated text at iteration 136000\n",
      "\n",
      " l'AU-LAAË4qure, lest  s rampous papurue t?9V2yH?\n",
      " eje,\n",
      "Qut, laya-Mere?\n",
      "Doi  a Ety4H.\n",
      "BUr les,\n",
      " à cc\n",
      "step 136010: train loss 2.4817, val loss 2.4569\n",
      "step 136020: train loss 2.4392, val loss 2.4883\n",
      "step 136030: train loss 2.4008, val loss 2.4389\n",
      "step 136040: train loss 2.3959, val loss 2.4888\n",
      "step 136050: train loss 2.4237, val loss 2.4839\n",
      "step 136060: train loss 2.4043, val loss 2.5553\n",
      "step 136070: train loss 2.4872, val loss 2.4340\n",
      "step 136080: train loss 2.5161, val loss 2.5120\n",
      "step 136090: train loss 2.3929, val loss 2.4999\n",
      "step 136100: train loss 2.4816, val loss 2.4910\n",
      "step 136110: train loss 2.5224, val loss 2.3739\n",
      "step 136120: train loss 2.3927, val loss 2.4642\n",
      "step 136130: train loss 2.4456, val loss 2.4757\n",
      "step 136140: train loss 2.4030, val loss 2.4866\n",
      "step 136150: train loss 2.3591, val loss 2.4537\n",
      "step 136160: train loss 2.3654, val loss 2.5099\n",
      "step 136170: train loss 2.4446, val loss 2.4913\n",
      "step 136180: train loss 2.4419, val loss 2.4270\n",
      "step 136190: train loss 2.3570, val loss 2.4015\n",
      "step 136200: train loss 2.4835, val loss 2.5483\n",
      "step 136210: train loss 2.3644, val loss 2.4475\n",
      "step 136220: train loss 2.4086, val loss 2.4680\n",
      "step 136230: train loss 2.3962, val loss 2.5041\n",
      "step 136240: train loss 2.4961, val loss 2.4425\n",
      "step 136250: train loss 2.4056, val loss 2.4326\n",
      "step 136260: train loss 2.4153, val loss 2.5015\n",
      "step 136270: train loss 2.3765, val loss 2.4695\n",
      "step 136280: train loss 2.3733, val loss 2.5095\n",
      "step 136290: train loss 2.4246, val loss 2.5737\n",
      "step 136300: train loss 2.4505, val loss 2.5285\n",
      "step 136310: train loss 2.4391, val loss 2.5588\n",
      "step 136320: train loss 2.4307, val loss 2.5628\n",
      "step 136330: train loss 2.4037, val loss 2.5131\n",
      "step 136340: train loss 2.3769, val loss 2.4783\n",
      "step 136350: train loss 2.3890, val loss 2.4887\n",
      "step 136360: train loss 2.4567, val loss 2.4888\n",
      "step 136370: train loss 2.4452, val loss 2.5519\n",
      "step 136380: train loss 2.3895, val loss 2.4719\n",
      "step 136390: train loss 2.5038, val loss 2.4254\n",
      "step 136400: train loss 2.4521, val loss 2.6141\n",
      "step 136410: train loss 2.5007, val loss 2.4526\n",
      "step 136420: train loss 2.4254, val loss 2.5121\n",
      "step 136430: train loss 2.3807, val loss 2.5505\n",
      "step 136440: train loss 2.3454, val loss 2.5092\n",
      "step 136450: train loss 2.4656, val loss 2.5131\n",
      "step 136460: train loss 2.4143, val loss 2.5152\n",
      "step 136470: train loss 2.4859, val loss 2.4359\n",
      "step 136480: train loss 2.4679, val loss 2.4469\n",
      "step 136490: train loss 2.3319, val loss 2.5147\n",
      "step 136500: train loss 2.3945, val loss 2.5226\n",
      "step 136510: train loss 2.4605, val loss 2.4267\n",
      "step 136520: train loss 2.4685, val loss 2.5438\n",
      "step 136530: train loss 2.3862, val loss 2.5245\n",
      "step 136540: train loss 2.4457, val loss 2.4449\n",
      "step 136550: train loss 2.4923, val loss 2.6023\n",
      "step 136560: train loss 2.4230, val loss 2.4941\n",
      "step 136570: train loss 2.4428, val loss 2.4933\n",
      "step 136580: train loss 2.4892, val loss 2.4577\n",
      "step 136590: train loss 2.4082, val loss 2.5432\n",
      "step 136600: train loss 2.4552, val loss 2.5908\n",
      "step 136610: train loss 2.3543, val loss 2.5270\n",
      "step 136620: train loss 2.4113, val loss 2.5314\n",
      "step 136630: train loss 2.4368, val loss 2.4984\n",
      "step 136640: train loss 2.3707, val loss 2.5537\n",
      "step 136650: train loss 2.3919, val loss 2.4898\n",
      "step 136660: train loss 2.4474, val loss 2.5840\n",
      "step 136670: train loss 2.3411, val loss 2.5441\n",
      "step 136680: train loss 2.4083, val loss 2.6087\n",
      "step 136690: train loss 2.4715, val loss 2.5740\n",
      "step 136700: train loss 2.3723, val loss 2.4741\n",
      "step 136710: train loss 2.5214, val loss 2.5237\n",
      "step 136720: train loss 2.4865, val loss 2.5007\n",
      "step 136730: train loss 2.4378, val loss 2.4019\n",
      "step 136740: train loss 2.4827, val loss 2.5293\n",
      "step 136750: train loss 2.5114, val loss 2.4407\n",
      "step 136760: train loss 2.3955, val loss 2.4531\n",
      "step 136770: train loss 2.4244, val loss 2.4759\n",
      "step 136780: train loss 2.4382, val loss 2.5282\n",
      "step 136790: train loss 2.3962, val loss 2.4875\n",
      "step 136800: train loss 2.3950, val loss 2.5009\n",
      "step 136810: train loss 2.4330, val loss 2.4484\n",
      "step 136820: train loss 2.4484, val loss 2.6360\n",
      "step 136830: train loss 2.4874, val loss 2.6280\n",
      "step 136840: train loss 2.5139, val loss 2.5076\n",
      "step 136850: train loss 2.3615, val loss 2.5600\n",
      "step 136860: train loss 2.4784, val loss 2.4712\n",
      "step 136870: train loss 2.4366, val loss 2.5235\n",
      "step 136880: train loss 2.4044, val loss 2.4807\n",
      "step 136890: train loss 2.4358, val loss 2.4631\n",
      "step 136900: train loss 2.3859, val loss 2.5101\n",
      "step 136910: train loss 2.4653, val loss 2.4907\n",
      "step 136920: train loss 2.4505, val loss 2.5456\n",
      "step 136930: train loss 2.4037, val loss 2.4673\n",
      "step 136940: train loss 2.3653, val loss 2.5482\n",
      "step 136950: train loss 2.4853, val loss 2.4472\n",
      "step 136960: train loss 2.4509, val loss 2.5067\n",
      "step 136970: train loss 2.4291, val loss 2.5197\n",
      "step 136980: train loss 2.4095, val loss 2.5212\n",
      "step 136990: train loss 2.4100, val loss 2.4671\n",
      "step 137000: train loss 2.4823, val loss 2.4238\n",
      "Generated text at iteration 137000\n",
      "\n",
      "Ce ffsoi aqunc ussta Onches qu?Kpainezice pibations bèt tautans, chépugenstis,  ds su UCr da lomlesa\n",
      "step 137010: train loss 2.3736, val loss 2.5423\n",
      "step 137020: train loss 2.4622, val loss 2.6814\n",
      "step 137030: train loss 2.4590, val loss 2.4477\n",
      "step 137040: train loss 2.3763, val loss 2.4195\n",
      "step 137050: train loss 2.4187, val loss 2.4186\n",
      "step 137060: train loss 2.4493, val loss 2.4550\n",
      "step 137070: train loss 2.5247, val loss 2.4172\n",
      "step 137080: train loss 2.3802, val loss 2.4235\n",
      "step 137090: train loss 2.5512, val loss 2.3459\n",
      "step 137100: train loss 2.4803, val loss 2.4799\n",
      "step 137110: train loss 2.5065, val loss 2.5179\n",
      "step 137120: train loss 2.4197, val loss 2.5552\n",
      "step 137130: train loss 2.4482, val loss 2.5394\n",
      "step 137140: train loss 2.4585, val loss 2.4039\n",
      "step 137150: train loss 2.3889, val loss 2.4961\n",
      "step 137160: train loss 2.4523, val loss 2.4169\n",
      "step 137170: train loss 2.4659, val loss 2.4630\n",
      "step 137180: train loss 2.3815, val loss 2.4986\n",
      "step 137190: train loss 2.4933, val loss 2.5598\n",
      "step 137200: train loss 2.3806, val loss 2.5120\n",
      "step 137210: train loss 2.3939, val loss 2.5741\n",
      "step 137220: train loss 2.4674, val loss 2.5558\n",
      "step 137230: train loss 2.3060, val loss 2.4904\n",
      "step 137240: train loss 2.4424, val loss 2.4859\n",
      "step 137250: train loss 2.4233, val loss 2.5206\n",
      "step 137260: train loss 2.4782, val loss 2.5064\n",
      "step 137270: train loss 2.3759, val loss 2.4957\n",
      "step 137280: train loss 2.4638, val loss 2.5077\n",
      "step 137290: train loss 2.4073, val loss 2.4435\n",
      "step 137300: train loss 2.4212, val loss 2.5231\n",
      "step 137310: train loss 2.3953, val loss 2.4300\n",
      "step 137320: train loss 2.4858, val loss 2.5138\n",
      "step 137330: train loss 2.3967, val loss 2.5345\n",
      "step 137340: train loss 2.4074, val loss 2.4763\n",
      "step 137350: train loss 2.4326, val loss 2.4991\n",
      "step 137360: train loss 2.3340, val loss 2.4485\n",
      "step 137370: train loss 2.3932, val loss 2.5222\n",
      "step 137380: train loss 2.4365, val loss 2.4385\n",
      "step 137390: train loss 2.4514, val loss 2.6205\n",
      "step 137400: train loss 2.4353, val loss 2.5174\n",
      "step 137410: train loss 2.3483, val loss 2.5061\n",
      "step 137420: train loss 2.4785, val loss 2.4534\n",
      "step 137430: train loss 2.5123, val loss 2.5557\n",
      "step 137440: train loss 2.4945, val loss 2.4464\n",
      "step 137450: train loss 2.3990, val loss 2.4614\n",
      "step 137460: train loss 2.4601, val loss 2.6115\n",
      "step 137470: train loss 2.3460, val loss 2.5428\n",
      "step 137480: train loss 2.4491, val loss 2.4130\n",
      "step 137490: train loss 2.4110, val loss 2.4493\n",
      "step 137500: train loss 2.4268, val loss 2.5156\n",
      "step 137510: train loss 2.4272, val loss 2.4034\n",
      "step 137520: train loss 2.4760, val loss 2.4659\n",
      "step 137530: train loss 2.4432, val loss 2.5692\n",
      "step 137540: train loss 2.4663, val loss 2.4699\n",
      "step 137550: train loss 2.4768, val loss 2.5666\n",
      "step 137560: train loss 2.4153, val loss 2.4761\n",
      "step 137570: train loss 2.3866, val loss 2.4632\n",
      "step 137580: train loss 2.4490, val loss 2.4770\n",
      "step 137590: train loss 2.4372, val loss 2.5699\n",
      "step 137600: train loss 2.3572, val loss 2.6315\n",
      "step 137610: train loss 2.3944, val loss 2.4681\n",
      "step 137620: train loss 2.3844, val loss 2.5077\n",
      "step 137630: train loss 2.4638, val loss 2.5112\n",
      "step 137640: train loss 2.4088, val loss 2.5095\n",
      "step 137650: train loss 2.4857, val loss 2.5139\n",
      "step 137660: train loss 2.3911, val loss 2.4024\n",
      "step 137670: train loss 2.4520, val loss 2.4835\n",
      "step 137680: train loss 2.4758, val loss 2.5065\n",
      "step 137690: train loss 2.4169, val loss 2.4809\n",
      "step 137700: train loss 2.4232, val loss 2.4267\n",
      "step 137710: train loss 2.3661, val loss 2.4781\n",
      "step 137720: train loss 2.4218, val loss 2.4715\n",
      "step 137730: train loss 2.4589, val loss 2.5179\n",
      "step 137740: train loss 2.4606, val loss 2.4296\n",
      "step 137750: train loss 2.4709, val loss 2.5006\n",
      "step 137760: train loss 2.4613, val loss 2.5397\n",
      "step 137770: train loss 2.4647, val loss 2.4488\n",
      "step 137780: train loss 2.3910, val loss 2.4041\n",
      "step 137790: train loss 2.3834, val loss 2.5051\n",
      "step 137800: train loss 2.4490, val loss 2.4789\n",
      "step 137810: train loss 2.4403, val loss 2.6860\n",
      "step 137820: train loss 2.3834, val loss 2.5560\n",
      "step 137830: train loss 2.3639, val loss 2.4745\n",
      "step 137840: train loss 2.3946, val loss 2.5687\n",
      "step 137850: train loss 2.4230, val loss 2.6005\n",
      "step 137860: train loss 2.4160, val loss 2.6069\n",
      "step 137870: train loss 2.3800, val loss 2.4427\n",
      "step 137880: train loss 2.3687, val loss 2.5445\n",
      "step 137890: train loss 2.4045, val loss 2.4731\n",
      "step 137900: train loss 2.4020, val loss 2.4853\n",
      "step 137910: train loss 2.4202, val loss 2.5442\n",
      "step 137920: train loss 2.4518, val loss 2.5037\n",
      "step 137930: train loss 2.4382, val loss 2.4407\n",
      "step 137940: train loss 2.4579, val loss 2.4456\n",
      "step 137950: train loss 2.4486, val loss 2.5285\n",
      "step 137960: train loss 2.3408, val loss 2.5213\n",
      "step 137970: train loss 2.4285, val loss 2.4346\n",
      "step 137980: train loss 2.4293, val loss 2.5203\n",
      "step 137990: train loss 2.4005, val loss 2.3294\n",
      "step 138000: train loss 2.4109, val loss 2.6156\n",
      "Generated text at iteration 138000\n",
      "\n",
      "Ochatose!\n",
      "Eclestéti chitôqurme d'oreresai  lcha jufièrorgre fâmmou.\n",
      "J1éluces d,\n",
      "Qupi lés s, a l'erfa\n",
      "step 138010: train loss 2.4186, val loss 2.4151\n",
      "step 138020: train loss 2.4486, val loss 2.5613\n",
      "step 138030: train loss 2.3277, val loss 2.4398\n",
      "step 138040: train loss 2.4157, val loss 2.5693\n",
      "step 138050: train loss 2.4900, val loss 2.5101\n",
      "step 138060: train loss 2.4670, val loss 2.4918\n",
      "step 138070: train loss 2.3595, val loss 2.3986\n",
      "step 138080: train loss 2.4531, val loss 2.5554\n",
      "step 138090: train loss 2.4321, val loss 2.6194\n",
      "step 138100: train loss 2.3632, val loss 2.4693\n",
      "step 138110: train loss 2.4151, val loss 2.5171\n",
      "step 138120: train loss 2.3683, val loss 2.5003\n",
      "step 138130: train loss 2.3551, val loss 2.5281\n",
      "step 138140: train loss 2.4259, val loss 2.5828\n",
      "step 138150: train loss 2.4117, val loss 2.4707\n",
      "step 138160: train loss 2.4291, val loss 2.5119\n",
      "step 138170: train loss 2.4299, val loss 2.5202\n",
      "step 138180: train loss 2.4061, val loss 2.4851\n",
      "step 138190: train loss 2.4497, val loss 2.5046\n",
      "step 138200: train loss 2.3564, val loss 2.5818\n",
      "step 138210: train loss 2.4977, val loss 2.5628\n",
      "step 138220: train loss 2.3810, val loss 2.5475\n",
      "step 138230: train loss 2.4760, val loss 2.4936\n",
      "step 138240: train loss 2.4299, val loss 2.4490\n",
      "step 138250: train loss 2.4236, val loss 2.4774\n",
      "step 138260: train loss 2.3406, val loss 2.4770\n",
      "step 138270: train loss 2.4496, val loss 2.4829\n",
      "step 138280: train loss 2.3580, val loss 2.5159\n",
      "step 138290: train loss 2.4089, val loss 2.4388\n",
      "step 138300: train loss 2.4624, val loss 2.4848\n",
      "step 138310: train loss 2.4063, val loss 2.5032\n",
      "step 138320: train loss 2.3863, val loss 2.4751\n",
      "step 138330: train loss 2.4056, val loss 2.4157\n",
      "step 138340: train loss 2.3932, val loss 2.4682\n",
      "step 138350: train loss 2.4458, val loss 2.5323\n",
      "step 138360: train loss 2.3999, val loss 2.4993\n",
      "step 138370: train loss 2.3851, val loss 2.5235\n",
      "step 138380: train loss 2.3810, val loss 2.5604\n",
      "step 138390: train loss 2.4176, val loss 2.5802\n",
      "step 138400: train loss 2.4065, val loss 2.5971\n",
      "step 138410: train loss 2.4766, val loss 2.5839\n",
      "step 138420: train loss 2.4530, val loss 2.5557\n",
      "step 138430: train loss 2.3904, val loss 2.4320\n",
      "step 138440: train loss 2.3745, val loss 2.5627\n",
      "step 138450: train loss 2.4774, val loss 2.5562\n",
      "step 138460: train loss 2.4203, val loss 2.5310\n",
      "step 138470: train loss 2.3979, val loss 2.5694\n",
      "step 138480: train loss 2.4335, val loss 2.3960\n",
      "step 138490: train loss 2.4751, val loss 2.5290\n",
      "step 138500: train loss 2.4722, val loss 2.4770\n",
      "step 138510: train loss 2.4715, val loss 2.6001\n",
      "step 138520: train loss 2.4324, val loss 2.4639\n",
      "step 138530: train loss 2.4369, val loss 2.4354\n",
      "step 138540: train loss 2.4431, val loss 2.4662\n",
      "step 138550: train loss 2.4293, val loss 2.4748\n",
      "step 138560: train loss 2.4023, val loss 2.4414\n",
      "step 138570: train loss 2.4864, val loss 2.5006\n",
      "step 138580: train loss 2.3376, val loss 2.4314\n",
      "step 138590: train loss 2.4231, val loss 2.5737\n",
      "step 138600: train loss 2.3969, val loss 2.4191\n",
      "step 138610: train loss 2.4082, val loss 2.4944\n",
      "step 138620: train loss 2.4364, val loss 2.4596\n",
      "step 138630: train loss 2.4675, val loss 2.5108\n",
      "step 138640: train loss 2.4835, val loss 2.5148\n",
      "step 138650: train loss 2.4156, val loss 2.4633\n",
      "step 138660: train loss 2.4391, val loss 2.5212\n",
      "step 138670: train loss 2.4380, val loss 2.5557\n",
      "step 138680: train loss 2.3701, val loss 2.4371\n",
      "step 138690: train loss 2.3997, val loss 2.4889\n",
      "step 138700: train loss 2.4153, val loss 2.5408\n",
      "step 138710: train loss 2.3426, val loss 2.4984\n",
      "step 138720: train loss 2.3536, val loss 2.4857\n",
      "step 138730: train loss 2.3921, val loss 2.5675\n",
      "step 138740: train loss 2.5071, val loss 2.4652\n",
      "step 138750: train loss 2.4362, val loss 2.5084\n",
      "step 138760: train loss 2.4225, val loss 2.6364\n",
      "step 138770: train loss 2.4652, val loss 2.6439\n",
      "step 138780: train loss 2.3511, val loss 2.5132\n",
      "step 138790: train loss 2.4687, val loss 2.4505\n",
      "step 138800: train loss 2.3330, val loss 2.5134\n",
      "step 138810: train loss 2.4064, val loss 2.5269\n",
      "step 138820: train loss 2.4880, val loss 2.4236\n",
      "step 138830: train loss 2.4529, val loss 2.5552\n",
      "step 138840: train loss 2.4465, val loss 2.4565\n",
      "step 138850: train loss 2.4519, val loss 2.4333\n",
      "step 138860: train loss 2.4115, val loss 2.5518\n",
      "step 138870: train loss 2.3998, val loss 2.4826\n",
      "step 138880: train loss 2.4227, val loss 2.6556\n",
      "step 138890: train loss 2.4460, val loss 2.4494\n",
      "step 138900: train loss 2.3488, val loss 2.4618\n",
      "step 138910: train loss 2.4536, val loss 2.5257\n",
      "step 138920: train loss 2.4531, val loss 2.4789\n",
      "step 138930: train loss 2.4041, val loss 2.4991\n",
      "step 138940: train loss 2.3758, val loss 2.4873\n",
      "step 138950: train loss 2.4341, val loss 2.4414\n",
      "step 138960: train loss 2.4129, val loss 2.3731\n",
      "step 138970: train loss 2.4911, val loss 2.4660\n",
      "step 138980: train loss 2.4594, val loss 2.4574\n",
      "step 138990: train loss 2.4321, val loss 2.5023\n",
      "step 139000: train loss 2.4742, val loss 2.4445\n",
      "Generated text at iteration 139000\n",
      "\n",
      "Qfourt,\n",
      "À'êvauoura vontribon fairgrchecangrilagél HUG.\n",
      "N(ît schaus!tésuppouenntrunesseuts frausoirer\n",
      "step 139010: train loss 2.4817, val loss 2.4561\n",
      "step 139020: train loss 2.4240, val loss 2.6215\n",
      "step 139030: train loss 2.4151, val loss 2.4746\n",
      "step 139040: train loss 2.4331, val loss 2.4505\n",
      "step 139050: train loss 2.4209, val loss 2.3677\n",
      "step 139060: train loss 2.4937, val loss 2.6084\n",
      "step 139070: train loss 2.4718, val loss 2.4563\n",
      "step 139080: train loss 2.3793, val loss 2.4602\n",
      "step 139090: train loss 2.4250, val loss 2.5405\n",
      "step 139100: train loss 2.4115, val loss 2.5510\n",
      "step 139110: train loss 2.4164, val loss 2.5167\n",
      "step 139120: train loss 2.3766, val loss 2.4637\n",
      "step 139130: train loss 2.4444, val loss 2.4569\n",
      "step 139140: train loss 2.4121, val loss 2.4100\n",
      "step 139150: train loss 2.4606, val loss 2.4636\n",
      "step 139160: train loss 2.3913, val loss 2.4671\n",
      "step 139170: train loss 2.3923, val loss 2.5383\n",
      "step 139180: train loss 2.4040, val loss 2.4032\n",
      "step 139190: train loss 2.4344, val loss 2.5139\n",
      "step 139200: train loss 2.4328, val loss 2.5352\n",
      "step 139210: train loss 2.4086, val loss 2.4987\n",
      "step 139220: train loss 2.4012, val loss 2.4593\n",
      "step 139230: train loss 2.4272, val loss 2.5380\n",
      "step 139240: train loss 2.3870, val loss 2.4883\n",
      "step 139250: train loss 2.3697, val loss 2.4413\n",
      "step 139260: train loss 2.4228, val loss 2.5039\n",
      "step 139270: train loss 2.3602, val loss 2.5345\n",
      "step 139280: train loss 2.3739, val loss 2.4056\n",
      "step 139290: train loss 2.3885, val loss 2.4669\n",
      "step 139300: train loss 2.4625, val loss 2.4838\n",
      "step 139310: train loss 2.4205, val loss 2.4448\n",
      "step 139320: train loss 2.4321, val loss 2.4795\n",
      "step 139330: train loss 2.3859, val loss 2.4242\n",
      "step 139340: train loss 2.4351, val loss 2.5442\n",
      "step 139350: train loss 2.4399, val loss 2.6110\n",
      "step 139360: train loss 2.4959, val loss 2.5475\n",
      "step 139370: train loss 2.2950, val loss 2.4796\n",
      "step 139380: train loss 2.4786, val loss 2.4222\n",
      "step 139390: train loss 2.4769, val loss 2.4369\n",
      "step 139400: train loss 2.3893, val loss 2.4557\n",
      "step 139410: train loss 2.4201, val loss 2.3937\n",
      "step 139420: train loss 2.4195, val loss 2.5152\n",
      "step 139430: train loss 2.4123, val loss 2.5168\n",
      "step 139440: train loss 2.4962, val loss 2.5005\n",
      "step 139450: train loss 2.4126, val loss 2.4230\n",
      "step 139460: train loss 2.4716, val loss 2.5018\n",
      "step 139470: train loss 2.3843, val loss 2.5692\n",
      "step 139480: train loss 2.4347, val loss 2.4604\n",
      "step 139490: train loss 2.3556, val loss 2.4436\n",
      "step 139500: train loss 2.3982, val loss 2.4952\n",
      "step 139510: train loss 2.4084, val loss 2.6150\n",
      "step 139520: train loss 2.4496, val loss 2.5482\n",
      "step 139530: train loss 2.4073, val loss 2.4408\n",
      "step 139540: train loss 2.3690, val loss 2.5691\n",
      "step 139550: train loss 2.4756, val loss 2.5042\n",
      "step 139560: train loss 2.3819, val loss 2.5161\n",
      "step 139570: train loss 2.3557, val loss 2.5877\n",
      "step 139580: train loss 2.3612, val loss 2.5664\n",
      "step 139590: train loss 2.4367, val loss 2.4700\n",
      "step 139600: train loss 2.4041, val loss 2.5532\n",
      "step 139610: train loss 2.3814, val loss 2.3917\n",
      "step 139620: train loss 2.4201, val loss 2.4501\n",
      "step 139630: train loss 2.4587, val loss 2.4499\n",
      "step 139640: train loss 2.4108, val loss 2.4829\n",
      "step 139650: train loss 2.3407, val loss 2.5290\n",
      "step 139660: train loss 2.5096, val loss 2.4263\n",
      "step 139670: train loss 2.4004, val loss 2.4656\n",
      "step 139680: train loss 2.4242, val loss 2.5177\n",
      "step 139690: train loss 2.4186, val loss 2.4093\n",
      "step 139700: train loss 2.3933, val loss 2.6026\n",
      "step 139710: train loss 2.4374, val loss 2.6025\n",
      "step 139720: train loss 2.4218, val loss 2.4858\n",
      "step 139730: train loss 2.3336, val loss 2.4165\n",
      "step 139740: train loss 2.3972, val loss 2.4492\n",
      "step 139750: train loss 2.4292, val loss 2.4718\n",
      "step 139760: train loss 2.4854, val loss 2.5603\n",
      "step 139770: train loss 2.3963, val loss 2.6516\n",
      "step 139780: train loss 2.4064, val loss 2.5078\n",
      "step 139790: train loss 2.3923, val loss 2.5030\n",
      "step 139800: train loss 2.4505, val loss 2.5419\n",
      "step 139810: train loss 2.3788, val loss 2.4578\n",
      "step 139820: train loss 2.4042, val loss 2.4804\n",
      "step 139830: train loss 2.3668, val loss 2.5536\n",
      "step 139840: train loss 2.4374, val loss 2.4600\n",
      "step 139850: train loss 2.4512, val loss 2.4908\n",
      "step 139860: train loss 2.5105, val loss 2.5363\n",
      "step 139870: train loss 2.4121, val loss 2.5651\n",
      "step 139880: train loss 2.4254, val loss 2.5551\n",
      "step 139890: train loss 2.4063, val loss 2.5678\n",
      "step 139900: train loss 2.3942, val loss 2.4359\n",
      "step 139910: train loss 2.4334, val loss 2.5269\n",
      "step 139920: train loss 2.3328, val loss 2.5157\n",
      "step 139930: train loss 2.4101, val loss 2.5128\n",
      "step 139940: train loss 2.4048, val loss 2.5636\n",
      "step 139950: train loss 2.3471, val loss 2.4858\n",
      "step 139960: train loss 2.4518, val loss 2.4517\n",
      "step 139970: train loss 2.4268, val loss 2.4714\n",
      "step 139980: train loss 2.4061, val loss 2.4738\n",
      "step 139990: train loss 2.4454, val loss 2.5398\n",
      "step 140000: train loss 2.3794, val loss 2.5883\n",
      "Generated text at iteration 140000\n",
      "\n",
      "Le phouie  s chiespanuesansé s  r len n mourgesses  dé,\n",
      "RËégle,\n",
      "Dess Sonenoiaidezré, s li e  paus RI\n",
      "step 140010: train loss 2.4194, val loss 2.4923\n",
      "step 140020: train loss 2.4134, val loss 2.5201\n",
      "step 140030: train loss 2.4006, val loss 2.4985\n",
      "step 140040: train loss 2.4213, val loss 2.5636\n",
      "step 140050: train loss 2.4367, val loss 2.4721\n",
      "step 140060: train loss 2.4130, val loss 2.5517\n",
      "step 140070: train loss 2.4451, val loss 2.4385\n",
      "step 140080: train loss 2.3855, val loss 2.4331\n",
      "step 140090: train loss 2.4215, val loss 2.4569\n",
      "step 140100: train loss 2.4251, val loss 2.5114\n",
      "step 140110: train loss 2.4217, val loss 2.4770\n",
      "step 140120: train loss 2.5098, val loss 2.5453\n",
      "step 140130: train loss 2.4071, val loss 2.4664\n",
      "step 140140: train loss 2.3473, val loss 2.5012\n",
      "step 140150: train loss 2.3754, val loss 2.5942\n",
      "step 140160: train loss 2.3980, val loss 2.4621\n",
      "step 140170: train loss 2.4160, val loss 2.5092\n",
      "step 140180: train loss 2.4918, val loss 2.4741\n",
      "step 140190: train loss 2.4841, val loss 2.5366\n",
      "step 140200: train loss 2.4557, val loss 2.6141\n",
      "step 140210: train loss 2.4671, val loss 2.4495\n",
      "step 140220: train loss 2.4525, val loss 2.4647\n",
      "step 140230: train loss 2.4237, val loss 2.4793\n",
      "step 140240: train loss 2.4803, val loss 2.5152\n",
      "step 140250: train loss 2.4159, val loss 2.4780\n",
      "step 140260: train loss 2.4245, val loss 2.5114\n",
      "step 140270: train loss 2.4799, val loss 2.4667\n",
      "step 140280: train loss 2.4031, val loss 2.5023\n",
      "step 140290: train loss 2.4407, val loss 2.5395\n",
      "step 140300: train loss 2.4395, val loss 2.5169\n",
      "step 140310: train loss 2.4552, val loss 2.5071\n",
      "step 140320: train loss 2.4374, val loss 2.4591\n",
      "step 140330: train loss 2.3835, val loss 2.4751\n",
      "step 140340: train loss 2.4181, val loss 2.3893\n",
      "step 140350: train loss 2.4091, val loss 2.5621\n",
      "step 140360: train loss 2.3751, val loss 2.4973\n",
      "step 140370: train loss 2.3808, val loss 2.4350\n",
      "step 140380: train loss 2.4796, val loss 2.4838\n",
      "step 140390: train loss 2.3649, val loss 2.4951\n",
      "step 140400: train loss 2.4771, val loss 2.6113\n",
      "step 140410: train loss 2.4630, val loss 2.5171\n",
      "step 140420: train loss 2.4597, val loss 2.4844\n",
      "step 140430: train loss 2.4503, val loss 2.4587\n",
      "step 140440: train loss 2.3604, val loss 2.4559\n",
      "step 140450: train loss 2.4245, val loss 2.4749\n",
      "step 140460: train loss 2.4091, val loss 2.4622\n",
      "step 140470: train loss 2.3981, val loss 2.5191\n",
      "step 140480: train loss 2.4080, val loss 2.4446\n",
      "step 140490: train loss 2.4687, val loss 2.4543\n",
      "step 140500: train loss 2.4450, val loss 2.5074\n",
      "step 140510: train loss 2.4962, val loss 2.4979\n",
      "step 140520: train loss 2.4338, val loss 2.5853\n",
      "step 140530: train loss 2.4054, val loss 2.5788\n",
      "step 140540: train loss 2.4148, val loss 2.4424\n",
      "step 140550: train loss 2.4017, val loss 2.5399\n",
      "step 140560: train loss 2.4339, val loss 2.4374\n",
      "step 140570: train loss 2.3512, val loss 2.3784\n",
      "step 140580: train loss 2.4639, val loss 2.4608\n",
      "step 140590: train loss 2.3065, val loss 2.5127\n",
      "step 140600: train loss 2.4183, val loss 2.6672\n",
      "step 140610: train loss 2.4328, val loss 2.5122\n",
      "step 140620: train loss 2.4339, val loss 2.5886\n",
      "step 140630: train loss 2.4026, val loss 2.4711\n",
      "step 140640: train loss 2.4015, val loss 2.6962\n",
      "step 140650: train loss 2.3928, val loss 2.3730\n",
      "step 140660: train loss 2.4248, val loss 2.5748\n",
      "step 140670: train loss 2.3392, val loss 2.4733\n",
      "step 140680: train loss 2.4012, val loss 2.4596\n",
      "step 140690: train loss 2.4022, val loss 2.4231\n",
      "step 140700: train loss 2.4627, val loss 2.5608\n",
      "step 140710: train loss 2.3881, val loss 2.6265\n",
      "step 140720: train loss 2.3511, val loss 2.5366\n",
      "step 140730: train loss 2.4355, val loss 2.3437\n",
      "step 140740: train loss 2.5286, val loss 2.4588\n",
      "step 140750: train loss 2.4034, val loss 2.6095\n",
      "step 140760: train loss 2.3682, val loss 2.6200\n",
      "step 140770: train loss 2.4351, val loss 2.5520\n",
      "step 140780: train loss 2.3597, val loss 2.4861\n",
      "step 140790: train loss 2.4157, val loss 2.3866\n",
      "step 140800: train loss 2.4326, val loss 2.4974\n",
      "step 140810: train loss 2.4255, val loss 2.5244\n",
      "step 140820: train loss 2.4481, val loss 2.5543\n",
      "step 140830: train loss 2.4331, val loss 2.4097\n",
      "step 140840: train loss 2.3848, val loss 2.5277\n",
      "step 140850: train loss 2.4236, val loss 2.5641\n",
      "step 140860: train loss 2.4079, val loss 2.4021\n",
      "step 140870: train loss 2.3806, val loss 2.5580\n",
      "step 140880: train loss 2.4223, val loss 2.4766\n",
      "step 140890: train loss 2.4033, val loss 2.4829\n",
      "step 140900: train loss 2.3777, val loss 2.4582\n",
      "step 140910: train loss 2.4645, val loss 2.5777\n",
      "step 140920: train loss 2.4912, val loss 2.4358\n",
      "step 140930: train loss 2.3672, val loss 2.6017\n",
      "step 140940: train loss 2.3863, val loss 2.5171\n",
      "step 140950: train loss 2.4335, val loss 2.4885\n",
      "step 140960: train loss 2.4098, val loss 2.4534\n",
      "step 140970: train loss 2.4068, val loss 2.4094\n",
      "step 140980: train loss 2.4474, val loss 2.4076\n",
      "step 140990: train loss 2.4460, val loss 2.4977\n",
      "step 141000: train loss 2.4007, val loss 2.4477\n",
      "Generated text at iteration 141000\n",
      "\n",
      "\n",
      "N'eupe\n",
      "A rouifl'erere Laroierèrueuploiorént mez ve EÉde;\n",
      "Eserc,\n",
      "L'holamarêëÊEt c mentuet de t l'a l\n",
      "step 141010: train loss 2.4699, val loss 2.3903\n",
      "step 141020: train loss 2.4035, val loss 2.5514\n",
      "step 141030: train loss 2.4492, val loss 2.4056\n",
      "step 141040: train loss 2.4084, val loss 2.5415\n",
      "step 141050: train loss 2.4591, val loss 2.4488\n",
      "step 141060: train loss 2.4282, val loss 2.4887\n",
      "step 141070: train loss 2.3238, val loss 2.4245\n",
      "step 141080: train loss 2.4077, val loss 2.4744\n",
      "step 141090: train loss 2.3878, val loss 2.4998\n",
      "step 141100: train loss 2.4328, val loss 2.3697\n",
      "step 141110: train loss 2.4004, val loss 2.4442\n",
      "step 141120: train loss 2.4191, val loss 2.5950\n",
      "step 141130: train loss 2.4108, val loss 2.5667\n",
      "step 141140: train loss 2.4977, val loss 2.4575\n",
      "step 141150: train loss 2.4465, val loss 2.4898\n",
      "step 141160: train loss 2.4286, val loss 2.5435\n",
      "step 141170: train loss 2.4205, val loss 2.4881\n",
      "step 141180: train loss 2.4003, val loss 2.4429\n",
      "step 141190: train loss 2.4611, val loss 2.4812\n",
      "step 141200: train loss 2.4459, val loss 2.4555\n",
      "step 141210: train loss 2.4374, val loss 2.3908\n",
      "step 141220: train loss 2.4536, val loss 2.4509\n",
      "step 141230: train loss 2.3622, val loss 2.4834\n",
      "step 141240: train loss 2.4246, val loss 2.4832\n",
      "step 141250: train loss 2.4113, val loss 2.4989\n",
      "step 141260: train loss 2.4893, val loss 2.5483\n",
      "step 141270: train loss 2.4258, val loss 2.4459\n",
      "step 141280: train loss 2.4102, val loss 2.4781\n",
      "step 141290: train loss 2.3665, val loss 2.5155\n",
      "step 141300: train loss 2.4503, val loss 2.5613\n",
      "step 141310: train loss 2.4129, val loss 2.5148\n",
      "step 141320: train loss 2.4361, val loss 2.4923\n",
      "step 141330: train loss 2.4410, val loss 2.4666\n",
      "step 141340: train loss 2.3954, val loss 2.5374\n",
      "step 141350: train loss 2.3601, val loss 2.6400\n",
      "step 141360: train loss 2.4236, val loss 2.4997\n",
      "step 141370: train loss 2.4523, val loss 2.5803\n",
      "step 141380: train loss 2.4351, val loss 2.4713\n",
      "step 141390: train loss 2.4317, val loss 2.5092\n",
      "step 141400: train loss 2.3522, val loss 2.4966\n",
      "step 141410: train loss 2.3695, val loss 2.5076\n",
      "step 141420: train loss 2.4493, val loss 2.4373\n",
      "step 141430: train loss 2.3919, val loss 2.4585\n",
      "step 141440: train loss 2.5102, val loss 2.4490\n",
      "step 141450: train loss 2.3954, val loss 2.4488\n",
      "step 141460: train loss 2.4056, val loss 2.5249\n",
      "step 141470: train loss 2.3950, val loss 2.4997\n",
      "step 141480: train loss 2.3876, val loss 2.4873\n",
      "step 141490: train loss 2.4095, val loss 2.6104\n",
      "step 141500: train loss 2.4289, val loss 2.5966\n",
      "step 141510: train loss 2.3917, val loss 2.4641\n",
      "step 141520: train loss 2.3713, val loss 2.5487\n",
      "step 141530: train loss 2.4266, val loss 2.4286\n",
      "step 141540: train loss 2.4839, val loss 2.4031\n",
      "step 141550: train loss 2.3116, val loss 2.4404\n",
      "step 141560: train loss 2.3785, val loss 2.5816\n",
      "step 141570: train loss 2.4159, val loss 2.5282\n",
      "step 141580: train loss 2.3399, val loss 2.5422\n",
      "step 141590: train loss 2.4428, val loss 2.4985\n",
      "step 141600: train loss 2.4520, val loss 2.4431\n",
      "step 141610: train loss 2.5171, val loss 2.5162\n",
      "step 141620: train loss 2.4301, val loss 2.4268\n",
      "step 141630: train loss 2.4261, val loss 2.4514\n",
      "step 141640: train loss 2.4407, val loss 2.4358\n",
      "step 141650: train loss 2.4107, val loss 2.5500\n",
      "step 141660: train loss 2.4180, val loss 2.4193\n",
      "step 141670: train loss 2.4156, val loss 2.5965\n",
      "step 141680: train loss 2.3904, val loss 2.3995\n",
      "step 141690: train loss 2.4049, val loss 2.5803\n",
      "step 141700: train loss 2.4421, val loss 2.4686\n",
      "step 141710: train loss 2.4462, val loss 2.4765\n",
      "step 141720: train loss 2.4353, val loss 2.3878\n",
      "step 141730: train loss 2.4642, val loss 2.4126\n",
      "step 141740: train loss 2.4946, val loss 2.4940\n",
      "step 141750: train loss 2.4986, val loss 2.6042\n",
      "step 141760: train loss 2.4546, val loss 2.5279\n",
      "step 141770: train loss 2.3971, val loss 2.6744\n",
      "step 141780: train loss 2.4078, val loss 2.4054\n",
      "step 141790: train loss 2.4039, val loss 2.6042\n",
      "step 141800: train loss 2.4415, val loss 2.4228\n",
      "step 141810: train loss 2.4880, val loss 2.4969\n",
      "step 141820: train loss 2.4469, val loss 2.5258\n",
      "step 141830: train loss 2.3963, val loss 2.5251\n",
      "step 141840: train loss 2.3932, val loss 2.4298\n",
      "step 141850: train loss 2.4102, val loss 2.5398\n",
      "step 141860: train loss 2.4468, val loss 2.6590\n",
      "step 141870: train loss 2.4618, val loss 2.5329\n",
      "step 141880: train loss 2.3991, val loss 2.6044\n",
      "step 141890: train loss 2.5376, val loss 2.5021\n",
      "step 141900: train loss 2.3516, val loss 2.5167\n",
      "step 141910: train loss 2.5143, val loss 2.4284\n",
      "step 141920: train loss 2.4606, val loss 2.4376\n",
      "step 141930: train loss 2.5092, val loss 2.5914\n",
      "step 141940: train loss 2.3968, val loss 2.5084\n",
      "step 141950: train loss 2.4092, val loss 2.4668\n",
      "step 141960: train loss 2.4340, val loss 2.4085\n",
      "step 141970: train loss 2.4392, val loss 2.5235\n",
      "step 141980: train loss 2.4293, val loss 2.5088\n",
      "step 141990: train loss 2.3348, val loss 2.4470\n",
      "step 142000: train loss 2.3302, val loss 2.5262\n",
      "Generated text at iteration 142000\n",
      "\n",
      "\n",
      "JHXboure d,\n",
      "Re:de vr jens des, ar prloue pole\n",
      "\n",
      "Pocen'i,  nde\n",
      "Ont eurit cois qu'avilens  e frt,\n",
      "LAus\n",
      "step 142010: train loss 2.4667, val loss 2.5032\n",
      "step 142020: train loss 2.4261, val loss 2.4629\n",
      "step 142030: train loss 2.4631, val loss 2.4531\n",
      "step 142040: train loss 2.4148, val loss 2.5224\n",
      "step 142050: train loss 2.3911, val loss 2.5141\n",
      "step 142060: train loss 2.4981, val loss 2.3976\n",
      "step 142070: train loss 2.4609, val loss 2.4399\n",
      "step 142080: train loss 2.3486, val loss 2.4407\n",
      "step 142090: train loss 2.4417, val loss 2.4863\n",
      "step 142100: train loss 2.4498, val loss 2.4595\n",
      "step 142110: train loss 2.3915, val loss 2.4681\n",
      "step 142120: train loss 2.3773, val loss 2.5768\n",
      "step 142130: train loss 2.4214, val loss 2.4673\n",
      "step 142140: train loss 2.4365, val loss 2.4496\n",
      "step 142150: train loss 2.4099, val loss 2.5296\n",
      "step 142160: train loss 2.3332, val loss 2.4963\n",
      "step 142170: train loss 2.4859, val loss 2.5128\n",
      "step 142180: train loss 2.4112, val loss 2.5144\n",
      "step 142190: train loss 2.5217, val loss 2.4182\n",
      "step 142200: train loss 2.3834, val loss 2.4391\n",
      "step 142210: train loss 2.4300, val loss 2.4897\n",
      "step 142220: train loss 2.3587, val loss 2.5009\n",
      "step 142230: train loss 2.4631, val loss 2.4565\n",
      "step 142240: train loss 2.4994, val loss 2.4377\n",
      "step 142250: train loss 2.3691, val loss 2.3880\n",
      "step 142260: train loss 2.4473, val loss 2.5884\n",
      "step 142270: train loss 2.3359, val loss 2.4405\n",
      "step 142280: train loss 2.3698, val loss 2.4968\n",
      "step 142290: train loss 2.4231, val loss 2.4585\n",
      "step 142300: train loss 2.3326, val loss 2.5739\n",
      "step 142310: train loss 2.3971, val loss 2.5282\n",
      "step 142320: train loss 2.4871, val loss 2.4323\n",
      "step 142330: train loss 2.4637, val loss 2.4509\n",
      "step 142340: train loss 2.4842, val loss 2.4273\n",
      "step 142350: train loss 2.4430, val loss 2.3742\n",
      "step 142360: train loss 2.4007, val loss 2.5186\n",
      "step 142370: train loss 2.4199, val loss 2.5248\n",
      "step 142380: train loss 2.4651, val loss 2.4510\n",
      "step 142390: train loss 2.4900, val loss 2.5896\n",
      "step 142400: train loss 2.3627, val loss 2.5393\n",
      "step 142410: train loss 2.5042, val loss 2.4797\n",
      "step 142420: train loss 2.4294, val loss 2.5153\n",
      "step 142430: train loss 2.4104, val loss 2.5236\n",
      "step 142440: train loss 2.4055, val loss 2.5610\n",
      "step 142450: train loss 2.3980, val loss 2.4734\n",
      "step 142460: train loss 2.4320, val loss 2.5294\n",
      "step 142470: train loss 2.4421, val loss 2.5105\n",
      "step 142480: train loss 2.4645, val loss 2.4312\n",
      "step 142490: train loss 2.4618, val loss 2.4995\n",
      "step 142500: train loss 2.4280, val loss 2.4670\n",
      "step 142510: train loss 2.3817, val loss 2.6256\n",
      "step 142520: train loss 2.4312, val loss 2.5105\n",
      "step 142530: train loss 2.3729, val loss 2.5059\n",
      "step 142540: train loss 2.4662, val loss 2.5190\n",
      "step 142550: train loss 2.4081, val loss 2.5253\n",
      "step 142560: train loss 2.4469, val loss 2.5251\n",
      "step 142570: train loss 2.4876, val loss 2.4895\n",
      "step 142580: train loss 2.3532, val loss 2.5241\n",
      "step 142590: train loss 2.4394, val loss 2.4167\n",
      "step 142600: train loss 2.3728, val loss 2.4720\n",
      "step 142610: train loss 2.4662, val loss 2.4121\n",
      "step 142620: train loss 2.4253, val loss 2.5634\n",
      "step 142630: train loss 2.4797, val loss 2.6928\n",
      "step 142640: train loss 2.4415, val loss 2.4289\n",
      "step 142650: train loss 2.4115, val loss 2.4451\n",
      "step 142660: train loss 2.3753, val loss 2.5438\n",
      "step 142670: train loss 2.4387, val loss 2.5163\n",
      "step 142680: train loss 2.4375, val loss 2.5612\n",
      "step 142690: train loss 2.4309, val loss 2.5300\n",
      "step 142700: train loss 2.3932, val loss 2.4650\n",
      "step 142710: train loss 2.4510, val loss 2.4462\n",
      "step 142720: train loss 2.4676, val loss 2.5717\n",
      "step 142730: train loss 2.4437, val loss 2.5877\n",
      "step 142740: train loss 2.4946, val loss 2.4855\n",
      "step 142750: train loss 2.4466, val loss 2.5324\n",
      "step 142760: train loss 2.5291, val loss 2.5310\n",
      "step 142770: train loss 2.3520, val loss 2.4997\n",
      "step 142780: train loss 2.3314, val loss 2.4787\n",
      "step 142790: train loss 2.4455, val loss 2.5082\n",
      "step 142800: train loss 2.3655, val loss 2.5731\n",
      "step 142810: train loss 2.4320, val loss 2.5100\n",
      "step 142820: train loss 2.4438, val loss 2.3640\n",
      "step 142830: train loss 2.4251, val loss 2.5185\n",
      "step 142840: train loss 2.4712, val loss 2.5038\n",
      "step 142850: train loss 2.3648, val loss 2.4404\n",
      "step 142860: train loss 2.3915, val loss 2.4760\n",
      "step 142870: train loss 2.3343, val loss 2.5924\n",
      "step 142880: train loss 2.4286, val loss 2.5618\n",
      "step 142890: train loss 2.3870, val loss 2.6549\n",
      "step 142900: train loss 2.3873, val loss 2.6008\n",
      "step 142910: train loss 2.4274, val loss 2.4718\n",
      "step 142920: train loss 2.3655, val loss 2.4372\n",
      "step 142930: train loss 2.4317, val loss 2.5075\n",
      "step 142940: train loss 2.4476, val loss 2.4954\n",
      "step 142950: train loss 2.4750, val loss 2.4466\n",
      "step 142960: train loss 2.4331, val loss 2.5449\n",
      "step 142970: train loss 2.4384, val loss 2.4729\n",
      "step 142980: train loss 2.4593, val loss 2.4046\n",
      "step 142990: train loss 2.3987, val loss 2.4827\n",
      "step 143000: train loss 2.3817, val loss 2.5535\n",
      "Generated text at iteration 143000\n",
      "\n",
      "S\n",
      "VA je d'ar,\n",
      "\n",
      "L'onécre, meurenfitavoù e he vene n pt doyaï3jons zS'ist laneux,\n",
      "\n",
      "Pant s Sputuronume \n",
      "step 143010: train loss 2.5296, val loss 2.5605\n",
      "step 143020: train loss 2.2932, val loss 2.5186\n",
      "step 143030: train loss 2.4582, val loss 2.5515\n",
      "step 143040: train loss 2.4145, val loss 2.5451\n",
      "step 143050: train loss 2.4308, val loss 2.5226\n",
      "step 143060: train loss 2.4032, val loss 2.5608\n",
      "step 143070: train loss 2.4148, val loss 2.4524\n",
      "step 143080: train loss 2.3884, val loss 2.5362\n",
      "step 143090: train loss 2.3733, val loss 2.4637\n",
      "step 143100: train loss 2.4065, val loss 2.4918\n",
      "step 143110: train loss 2.4408, val loss 2.5312\n",
      "step 143120: train loss 2.4488, val loss 2.5850\n",
      "step 143130: train loss 2.4438, val loss 2.4486\n",
      "step 143140: train loss 2.4064, val loss 2.4480\n",
      "step 143150: train loss 2.3461, val loss 2.4943\n",
      "step 143160: train loss 2.4059, val loss 2.5256\n",
      "step 143170: train loss 2.3750, val loss 2.5982\n",
      "step 143180: train loss 2.4146, val loss 2.5273\n",
      "step 143190: train loss 2.4809, val loss 2.5532\n",
      "step 143200: train loss 2.4670, val loss 2.4933\n",
      "step 143210: train loss 2.3324, val loss 2.5120\n",
      "step 143220: train loss 2.4096, val loss 2.5347\n",
      "step 143230: train loss 2.4348, val loss 2.4484\n",
      "step 143240: train loss 2.5159, val loss 2.5724\n",
      "step 143250: train loss 2.4477, val loss 2.5020\n",
      "step 143260: train loss 2.3649, val loss 2.4472\n",
      "step 143270: train loss 2.4006, val loss 2.5385\n",
      "step 143280: train loss 2.4006, val loss 2.5865\n",
      "step 143290: train loss 2.4520, val loss 2.5282\n",
      "step 143300: train loss 2.3860, val loss 2.5579\n",
      "step 143310: train loss 2.4207, val loss 2.5673\n",
      "step 143320: train loss 2.3434, val loss 2.5011\n",
      "step 143330: train loss 2.4699, val loss 2.4240\n",
      "step 143340: train loss 2.3871, val loss 2.4429\n",
      "step 143350: train loss 2.3764, val loss 2.5288\n",
      "step 143360: train loss 2.4212, val loss 2.5358\n",
      "step 143370: train loss 2.4983, val loss 2.5511\n",
      "step 143380: train loss 2.4397, val loss 2.5156\n",
      "step 143390: train loss 2.3899, val loss 2.4375\n",
      "step 143400: train loss 2.4744, val loss 2.5515\n",
      "step 143410: train loss 2.4181, val loss 2.5129\n",
      "step 143420: train loss 2.4003, val loss 2.3980\n",
      "step 143430: train loss 2.2885, val loss 2.5053\n",
      "step 143440: train loss 2.4417, val loss 2.6212\n",
      "step 143450: train loss 2.3726, val loss 2.4118\n",
      "step 143460: train loss 2.4388, val loss 2.4791\n",
      "step 143470: train loss 2.3901, val loss 2.5144\n",
      "step 143480: train loss 2.4202, val loss 2.4662\n",
      "step 143490: train loss 2.4454, val loss 2.5423\n",
      "step 143500: train loss 2.3458, val loss 2.4490\n",
      "step 143510: train loss 2.2844, val loss 2.4541\n",
      "step 143520: train loss 2.3972, val loss 2.5166\n",
      "step 143530: train loss 2.3633, val loss 2.5329\n",
      "step 143540: train loss 2.4770, val loss 2.5392\n",
      "step 143550: train loss 2.4773, val loss 2.4501\n",
      "step 143560: train loss 2.4644, val loss 2.5844\n",
      "step 143570: train loss 2.4048, val loss 2.4340\n",
      "step 143580: train loss 2.4381, val loss 2.4558\n",
      "step 143590: train loss 2.3921, val loss 2.5006\n",
      "step 143600: train loss 2.4962, val loss 2.3946\n",
      "step 143610: train loss 2.4256, val loss 2.4029\n",
      "step 143620: train loss 2.3729, val loss 2.5718\n",
      "step 143630: train loss 2.3831, val loss 2.5057\n",
      "step 143640: train loss 2.3718, val loss 2.5098\n",
      "step 143650: train loss 2.4763, val loss 2.5138\n",
      "step 143660: train loss 2.3841, val loss 2.5250\n",
      "step 143670: train loss 2.4795, val loss 2.5460\n",
      "step 143680: train loss 2.4846, val loss 2.5528\n",
      "step 143690: train loss 2.3688, val loss 2.4566\n",
      "step 143700: train loss 2.3249, val loss 2.4829\n",
      "step 143710: train loss 2.5056, val loss 2.5636\n",
      "step 143720: train loss 2.3641, val loss 2.4866\n",
      "step 143730: train loss 2.4083, val loss 2.4349\n",
      "step 143740: train loss 2.3883, val loss 2.4240\n",
      "step 143750: train loss 2.4282, val loss 2.3667\n",
      "step 143760: train loss 2.4429, val loss 2.4664\n",
      "step 143770: train loss 2.3241, val loss 2.3812\n",
      "step 143780: train loss 2.3952, val loss 2.4925\n",
      "step 143790: train loss 2.4281, val loss 2.5625\n",
      "step 143800: train loss 2.4363, val loss 2.3352\n",
      "step 143810: train loss 2.4241, val loss 2.4078\n",
      "step 143820: train loss 2.4721, val loss 2.5518\n",
      "step 143830: train loss 2.3759, val loss 2.3595\n",
      "step 143840: train loss 2.4538, val loss 2.4963\n",
      "step 143850: train loss 2.3985, val loss 2.4902\n",
      "step 143860: train loss 2.4448, val loss 2.5904\n",
      "step 143870: train loss 2.3857, val loss 2.5177\n",
      "step 143880: train loss 2.4517, val loss 2.5147\n",
      "step 143890: train loss 2.4231, val loss 2.6397\n",
      "step 143900: train loss 2.4198, val loss 2.5181\n",
      "step 143910: train loss 2.4067, val loss 2.4464\n",
      "step 143920: train loss 2.4377, val loss 2.3965\n",
      "step 143930: train loss 2.3707, val loss 2.5471\n",
      "step 143940: train loss 2.3779, val loss 2.4287\n",
      "step 143950: train loss 2.4490, val loss 2.5100\n",
      "step 143960: train loss 2.3921, val loss 2.5716\n",
      "step 143970: train loss 2.4646, val loss 2.5637\n",
      "step 143980: train loss 2.4449, val loss 2.4062\n",
      "step 143990: train loss 2.3798, val loss 2.6341\n",
      "step 144000: train loss 2.3555, val loss 2.5086\n",
      "Generated text at iteration 144000\n",
      "\n",
      "Aule fen lp ffit l'e,\n",
      "L'ux;\n",
      "Jut, êt dourrtoumbit cosat ve fauiouietrt trez t c'e! lefllai,\n",
      "U8ante di\n",
      "step 144010: train loss 2.4515, val loss 2.5477\n",
      "step 144020: train loss 2.3678, val loss 2.5638\n",
      "step 144030: train loss 2.4022, val loss 2.5327\n",
      "step 144040: train loss 2.3713, val loss 2.3768\n",
      "step 144050: train loss 2.4136, val loss 2.4943\n",
      "step 144060: train loss 2.4215, val loss 2.5078\n",
      "step 144070: train loss 2.3384, val loss 2.4206\n",
      "step 144080: train loss 2.4758, val loss 2.5505\n",
      "step 144090: train loss 2.4426, val loss 2.4635\n",
      "step 144100: train loss 2.4329, val loss 2.4528\n",
      "step 144110: train loss 2.4628, val loss 2.4816\n",
      "step 144120: train loss 2.4267, val loss 2.4992\n",
      "step 144130: train loss 2.3962, val loss 2.4142\n",
      "step 144140: train loss 2.3545, val loss 2.5206\n",
      "step 144150: train loss 2.3983, val loss 2.4466\n",
      "step 144160: train loss 2.3744, val loss 2.5382\n",
      "step 144170: train loss 2.3710, val loss 2.5053\n",
      "step 144180: train loss 2.3996, val loss 2.5198\n",
      "step 144190: train loss 2.3727, val loss 2.4480\n",
      "step 144200: train loss 2.4319, val loss 2.5920\n",
      "step 144210: train loss 2.3620, val loss 2.5341\n",
      "step 144220: train loss 2.4437, val loss 2.4048\n",
      "step 144230: train loss 2.4380, val loss 2.4799\n",
      "step 144240: train loss 2.3982, val loss 2.4712\n",
      "step 144250: train loss 2.3759, val loss 2.5278\n",
      "step 144260: train loss 2.3196, val loss 2.4497\n",
      "step 144270: train loss 2.4224, val loss 2.3828\n",
      "step 144280: train loss 2.4969, val loss 2.4951\n",
      "step 144290: train loss 2.4740, val loss 2.5733\n",
      "step 144300: train loss 2.4244, val loss 2.4673\n",
      "step 144310: train loss 2.3699, val loss 2.5330\n",
      "step 144320: train loss 2.3621, val loss 2.4631\n",
      "step 144330: train loss 2.4552, val loss 2.4645\n",
      "step 144340: train loss 2.4493, val loss 2.4431\n",
      "step 144350: train loss 2.4252, val loss 2.5241\n",
      "step 144360: train loss 2.4502, val loss 2.4989\n",
      "step 144370: train loss 2.4397, val loss 2.5485\n",
      "step 144380: train loss 2.4044, val loss 2.5744\n",
      "step 144390: train loss 2.4174, val loss 2.5323\n",
      "step 144400: train loss 2.4596, val loss 2.5347\n",
      "step 144410: train loss 2.4392, val loss 2.3921\n",
      "step 144420: train loss 2.4825, val loss 2.5888\n",
      "step 144430: train loss 2.4546, val loss 2.4962\n",
      "step 144440: train loss 2.3273, val loss 2.5207\n",
      "step 144450: train loss 2.4953, val loss 2.4982\n",
      "step 144460: train loss 2.4895, val loss 2.4954\n",
      "step 144470: train loss 2.4361, val loss 2.5848\n",
      "step 144480: train loss 2.4088, val loss 2.4360\n",
      "step 144490: train loss 2.4351, val loss 2.5122\n",
      "step 144500: train loss 2.4499, val loss 2.5817\n",
      "step 144510: train loss 2.4200, val loss 2.4377\n",
      "step 144520: train loss 2.4577, val loss 2.5064\n",
      "step 144530: train loss 2.3774, val loss 2.5144\n",
      "step 144540: train loss 2.4240, val loss 2.4699\n",
      "step 144550: train loss 2.3645, val loss 2.3819\n",
      "step 144560: train loss 2.4195, val loss 2.5012\n",
      "step 144570: train loss 2.4241, val loss 2.5725\n",
      "step 144580: train loss 2.3666, val loss 2.4904\n",
      "step 144590: train loss 2.4800, val loss 2.4677\n",
      "step 144600: train loss 2.3791, val loss 2.4608\n",
      "step 144610: train loss 2.4568, val loss 2.4868\n",
      "step 144620: train loss 2.4536, val loss 2.4433\n",
      "step 144630: train loss 2.3545, val loss 2.4818\n",
      "step 144640: train loss 2.4448, val loss 2.4816\n",
      "step 144650: train loss 2.4118, val loss 2.5567\n",
      "step 144660: train loss 2.4395, val loss 2.4365\n",
      "step 144670: train loss 2.3427, val loss 2.4349\n",
      "step 144680: train loss 2.4252, val loss 2.4410\n",
      "step 144690: train loss 2.3878, val loss 2.4153\n",
      "step 144700: train loss 2.4324, val loss 2.3880\n",
      "step 144710: train loss 2.3816, val loss 2.5082\n",
      "step 144720: train loss 2.3669, val loss 2.3859\n",
      "step 144730: train loss 2.4369, val loss 2.5422\n",
      "step 144740: train loss 2.4022, val loss 2.5135\n",
      "step 144750: train loss 2.4329, val loss 2.5403\n",
      "step 144760: train loss 2.4203, val loss 2.4092\n",
      "step 144770: train loss 2.4123, val loss 2.3654\n",
      "step 144780: train loss 2.4518, val loss 2.4909\n",
      "step 144790: train loss 2.4339, val loss 2.5154\n",
      "step 144800: train loss 2.4124, val loss 2.5325\n",
      "step 144810: train loss 2.4906, val loss 2.3478\n",
      "step 144820: train loss 2.4351, val loss 2.6859\n",
      "step 144830: train loss 2.4395, val loss 2.4161\n",
      "step 144840: train loss 2.4185, val loss 2.5660\n",
      "step 144850: train loss 2.3973, val loss 2.5293\n",
      "step 144860: train loss 2.3596, val loss 2.4772\n",
      "step 144870: train loss 2.4188, val loss 2.5127\n",
      "step 144880: train loss 2.4058, val loss 2.4799\n",
      "step 144890: train loss 2.4438, val loss 2.5268\n",
      "step 144900: train loss 2.3968, val loss 2.3978\n",
      "step 144910: train loss 2.4416, val loss 2.4686\n",
      "step 144920: train loss 2.3991, val loss 2.3133\n",
      "step 144930: train loss 2.5039, val loss 2.4525\n",
      "step 144940: train loss 2.4524, val loss 2.4613\n",
      "step 144950: train loss 2.4368, val loss 2.6074\n",
      "step 144960: train loss 2.4084, val loss 2.4692\n",
      "step 144970: train loss 2.4446, val loss 2.5534\n",
      "step 144980: train loss 2.3866, val loss 2.5242\n",
      "step 144990: train loss 2.4346, val loss 2.4924\n",
      "step 145000: train loss 2.3679, val loss 2.5507\n",
      "Generated text at iteration 145000\n",
      "\n",
      "\n",
      "Avor piqui;\n",
      "EU58au?\n",
      "Viteze! qu chounèÊ;\n",
      "X9Seme 4l l; e pasés rre   s didaivrt.\n",
      "Et ntouéloneu e, d'e\n",
      "step 145010: train loss 2.4469, val loss 2.5787\n",
      "step 145020: train loss 2.4470, val loss 2.4466\n",
      "step 145030: train loss 2.4522, val loss 2.4451\n",
      "step 145040: train loss 2.4512, val loss 2.3866\n",
      "step 145050: train loss 2.4074, val loss 2.4385\n",
      "step 145060: train loss 2.4459, val loss 2.4484\n",
      "step 145070: train loss 2.3736, val loss 2.5648\n",
      "step 145080: train loss 2.4025, val loss 2.6358\n",
      "step 145090: train loss 2.4120, val loss 2.4934\n",
      "step 145100: train loss 2.3958, val loss 2.5514\n",
      "step 145110: train loss 2.4944, val loss 2.5432\n",
      "step 145120: train loss 2.4124, val loss 2.5268\n",
      "step 145130: train loss 2.3820, val loss 2.5379\n",
      "step 145140: train loss 2.4401, val loss 2.4620\n",
      "step 145150: train loss 2.3943, val loss 2.5358\n",
      "step 145160: train loss 2.4178, val loss 2.5748\n",
      "step 145170: train loss 2.5011, val loss 2.4995\n",
      "step 145180: train loss 2.3886, val loss 2.4900\n",
      "step 145190: train loss 2.4078, val loss 2.4556\n",
      "step 145200: train loss 2.3413, val loss 2.5013\n",
      "step 145210: train loss 2.4012, val loss 2.4510\n",
      "step 145220: train loss 2.3411, val loss 2.4637\n",
      "step 145230: train loss 2.4905, val loss 2.4626\n",
      "step 145240: train loss 2.4339, val loss 2.4448\n",
      "step 145250: train loss 2.4528, val loss 2.4443\n",
      "step 145260: train loss 2.3755, val loss 2.4712\n",
      "step 145270: train loss 2.3989, val loss 2.4649\n",
      "step 145280: train loss 2.3905, val loss 2.5296\n",
      "step 145290: train loss 2.3709, val loss 2.5062\n",
      "step 145300: train loss 2.4369, val loss 2.5419\n",
      "step 145310: train loss 2.4151, val loss 2.5721\n",
      "step 145320: train loss 2.3925, val loss 2.4459\n",
      "step 145330: train loss 2.4429, val loss 2.4481\n",
      "step 145340: train loss 2.4105, val loss 2.4334\n",
      "step 145350: train loss 2.4527, val loss 2.5965\n",
      "step 145360: train loss 2.4130, val loss 2.3549\n",
      "step 145370: train loss 2.3871, val loss 2.4524\n",
      "step 145380: train loss 2.4395, val loss 2.5580\n",
      "step 145390: train loss 2.4132, val loss 2.5277\n",
      "step 145400: train loss 2.4422, val loss 2.6757\n",
      "step 145410: train loss 2.4009, val loss 2.4490\n",
      "step 145420: train loss 2.3150, val loss 2.5009\n",
      "step 145430: train loss 2.3987, val loss 2.5648\n",
      "step 145440: train loss 2.4474, val loss 2.4752\n",
      "step 145450: train loss 2.3662, val loss 2.5016\n",
      "step 145460: train loss 2.3761, val loss 2.4015\n",
      "step 145470: train loss 2.3726, val loss 2.4050\n",
      "step 145480: train loss 2.4160, val loss 2.4676\n",
      "step 145490: train loss 2.3773, val loss 2.4123\n",
      "step 145500: train loss 2.3561, val loss 2.5376\n",
      "step 145510: train loss 2.3939, val loss 2.4981\n",
      "step 145520: train loss 2.4235, val loss 2.5388\n",
      "step 145530: train loss 2.4821, val loss 2.5044\n",
      "step 145540: train loss 2.4407, val loss 2.4720\n",
      "step 145550: train loss 2.4757, val loss 2.3740\n",
      "step 145560: train loss 2.3390, val loss 2.5456\n",
      "step 145570: train loss 2.4012, val loss 2.5904\n",
      "step 145580: train loss 2.4524, val loss 2.5138\n",
      "step 145590: train loss 2.4038, val loss 2.3852\n",
      "step 145600: train loss 2.4083, val loss 2.4471\n",
      "step 145610: train loss 2.3855, val loss 2.4917\n",
      "step 145620: train loss 2.3974, val loss 2.5327\n",
      "step 145630: train loss 2.4536, val loss 2.3874\n",
      "step 145640: train loss 2.4974, val loss 2.5318\n",
      "step 145650: train loss 2.3793, val loss 2.5579\n",
      "step 145660: train loss 2.4950, val loss 2.4876\n",
      "step 145670: train loss 2.4530, val loss 2.5344\n",
      "step 145680: train loss 2.4248, val loss 2.4921\n",
      "step 145690: train loss 2.4319, val loss 2.5963\n",
      "step 145700: train loss 2.4281, val loss 2.5414\n",
      "step 145710: train loss 2.3316, val loss 2.4660\n",
      "step 145720: train loss 2.4111, val loss 2.6035\n",
      "step 145730: train loss 2.4712, val loss 2.4195\n",
      "step 145740: train loss 2.4228, val loss 2.4992\n",
      "step 145750: train loss 2.3484, val loss 2.4981\n",
      "step 145760: train loss 2.3934, val loss 2.4131\n",
      "step 145770: train loss 2.4874, val loss 2.3940\n",
      "step 145780: train loss 2.3739, val loss 2.5312\n",
      "step 145790: train loss 2.3446, val loss 2.5312\n",
      "step 145800: train loss 2.4347, val loss 2.4215\n",
      "step 145810: train loss 2.4147, val loss 2.5460\n",
      "step 145820: train loss 2.3775, val loss 2.5471\n",
      "step 145830: train loss 2.4059, val loss 2.5873\n",
      "step 145840: train loss 2.4294, val loss 2.6044\n",
      "step 145850: train loss 2.3992, val loss 2.3726\n",
      "step 145860: train loss 2.4355, val loss 2.4091\n",
      "step 145870: train loss 2.3794, val loss 2.5035\n",
      "step 145880: train loss 2.4322, val loss 2.4899\n",
      "step 145890: train loss 2.4577, val loss 2.4615\n",
      "step 145900: train loss 2.4036, val loss 2.4072\n",
      "step 145910: train loss 2.3971, val loss 2.5490\n",
      "step 145920: train loss 2.3962, val loss 2.5150\n",
      "step 145930: train loss 2.3814, val loss 2.4886\n",
      "step 145940: train loss 2.4914, val loss 2.4478\n",
      "step 145950: train loss 2.3312, val loss 2.5799\n",
      "step 145960: train loss 2.4836, val loss 2.4499\n",
      "step 145970: train loss 2.3521, val loss 2.4870\n",
      "step 145980: train loss 2.4269, val loss 2.5423\n",
      "step 145990: train loss 2.4542, val loss 2.5317\n",
      "step 146000: train loss 2.4461, val loss 2.5147\n",
      "Generated text at iteration 146000\n",
      "\n",
      "\n",
      "QUOns, Shoueselus-baittrrs nes   én, oùÊgs densiles,\n",
      "Et die.\n",
      "\n",
      "\n",
      "\n",
      "QU3«CRÈan;\n",
      "\n",
      "-elot pauss-vet-êlenest\n",
      "step 146010: train loss 2.4536, val loss 2.4401\n",
      "step 146020: train loss 2.3760, val loss 2.4998\n",
      "step 146030: train loss 2.4287, val loss 2.3907\n",
      "step 146040: train loss 2.3840, val loss 2.5478\n",
      "step 146050: train loss 2.4378, val loss 2.5584\n",
      "step 146060: train loss 2.3964, val loss 2.4940\n",
      "step 146070: train loss 2.4133, val loss 2.6283\n",
      "step 146080: train loss 2.3120, val loss 2.4126\n",
      "step 146090: train loss 2.4210, val loss 2.5670\n",
      "step 146100: train loss 2.3661, val loss 2.4421\n",
      "step 146110: train loss 2.4473, val loss 2.4448\n",
      "step 146120: train loss 2.5046, val loss 2.4247\n",
      "step 146130: train loss 2.4577, val loss 2.5367\n",
      "step 146140: train loss 2.4784, val loss 2.3652\n",
      "step 146150: train loss 2.4624, val loss 2.4256\n",
      "step 146160: train loss 2.4343, val loss 2.4779\n",
      "step 146170: train loss 2.4207, val loss 2.4611\n",
      "step 146180: train loss 2.4143, val loss 2.5624\n",
      "step 146190: train loss 2.4334, val loss 2.6081\n",
      "step 146200: train loss 2.4852, val loss 2.4535\n",
      "step 146210: train loss 2.3350, val loss 2.4753\n",
      "step 146220: train loss 2.4057, val loss 2.5002\n",
      "step 146230: train loss 2.4109, val loss 2.5047\n",
      "step 146240: train loss 2.4021, val loss 2.5348\n",
      "step 146250: train loss 2.4076, val loss 2.4876\n",
      "step 146260: train loss 2.4154, val loss 2.4070\n",
      "step 146270: train loss 2.3492, val loss 2.4325\n",
      "step 146280: train loss 2.4206, val loss 2.4682\n",
      "step 146290: train loss 2.3116, val loss 2.5175\n",
      "step 146300: train loss 2.3540, val loss 2.4309\n",
      "step 146310: train loss 2.3705, val loss 2.5280\n",
      "step 146320: train loss 2.4147, val loss 2.4366\n",
      "step 146330: train loss 2.3491, val loss 2.3870\n",
      "step 146340: train loss 2.4858, val loss 2.5134\n",
      "step 146350: train loss 2.4168, val loss 2.5723\n",
      "step 146360: train loss 2.4523, val loss 2.4645\n",
      "step 146370: train loss 2.4089, val loss 2.5423\n",
      "step 146380: train loss 2.4486, val loss 2.4121\n",
      "step 146390: train loss 2.3557, val loss 2.3835\n",
      "step 146400: train loss 2.4183, val loss 2.4523\n",
      "step 146410: train loss 2.4153, val loss 2.4987\n",
      "step 146420: train loss 2.3730, val loss 2.4994\n",
      "step 146430: train loss 2.3563, val loss 2.5633\n",
      "step 146440: train loss 2.4758, val loss 2.4714\n",
      "step 146450: train loss 2.4499, val loss 2.4891\n",
      "step 146460: train loss 2.4239, val loss 2.6544\n",
      "step 146470: train loss 2.3983, val loss 2.3970\n",
      "step 146480: train loss 2.3953, val loss 2.4897\n",
      "step 146490: train loss 2.4058, val loss 2.4712\n",
      "step 146500: train loss 2.3958, val loss 2.4677\n",
      "step 146510: train loss 2.4066, val loss 2.4729\n",
      "step 146520: train loss 2.3989, val loss 2.3859\n",
      "step 146530: train loss 2.4301, val loss 2.4314\n",
      "step 146540: train loss 2.4101, val loss 2.5684\n",
      "step 146550: train loss 2.4529, val loss 2.5528\n",
      "step 146560: train loss 2.4725, val loss 2.5265\n",
      "step 146570: train loss 2.4373, val loss 2.4862\n",
      "step 146580: train loss 2.3541, val loss 2.4219\n",
      "step 146590: train loss 2.3497, val loss 2.5203\n",
      "step 146600: train loss 2.4156, val loss 2.5538\n",
      "step 146610: train loss 2.3430, val loss 2.5007\n",
      "step 146620: train loss 2.4058, val loss 2.4730\n",
      "step 146630: train loss 2.3901, val loss 2.5209\n",
      "step 146640: train loss 2.4262, val loss 2.5095\n",
      "step 146650: train loss 2.4835, val loss 2.6481\n",
      "step 146660: train loss 2.3916, val loss 2.4769\n",
      "step 146670: train loss 2.4253, val loss 2.5431\n",
      "step 146680: train loss 2.4070, val loss 2.5060\n",
      "step 146690: train loss 2.4225, val loss 2.4379\n",
      "step 146700: train loss 2.3666, val loss 2.4676\n",
      "step 146710: train loss 2.3999, val loss 2.5823\n",
      "step 146720: train loss 2.4126, val loss 2.7514\n",
      "step 146730: train loss 2.4643, val loss 2.4326\n",
      "step 146740: train loss 2.3647, val loss 2.4844\n",
      "step 146750: train loss 2.3535, val loss 2.4817\n",
      "step 146760: train loss 2.4593, val loss 2.5023\n",
      "step 146770: train loss 2.3586, val loss 2.3511\n",
      "step 146780: train loss 2.4119, val loss 2.4518\n",
      "step 146790: train loss 2.3873, val loss 2.5026\n",
      "step 146800: train loss 2.4532, val loss 2.5143\n",
      "step 146810: train loss 2.3835, val loss 2.5240\n",
      "step 146820: train loss 2.3337, val loss 2.5217\n",
      "step 146830: train loss 2.3990, val loss 2.4790\n",
      "step 146840: train loss 2.3948, val loss 2.5721\n",
      "step 146850: train loss 2.4329, val loss 2.4717\n",
      "step 146860: train loss 2.3679, val loss 2.5088\n",
      "step 146870: train loss 2.3680, val loss 2.5757\n",
      "step 146880: train loss 2.3593, val loss 2.5951\n",
      "step 146890: train loss 2.3853, val loss 2.4517\n",
      "step 146900: train loss 2.4232, val loss 2.5805\n",
      "step 146910: train loss 2.3781, val loss 2.4773\n",
      "step 146920: train loss 2.3472, val loss 2.4448\n",
      "step 146930: train loss 2.3430, val loss 2.4896\n",
      "step 146940: train loss 2.4826, val loss 2.4964\n",
      "step 146950: train loss 2.4423, val loss 2.5007\n",
      "step 146960: train loss 2.4041, val loss 2.4644\n",
      "step 146970: train loss 2.3971, val loss 2.5039\n",
      "step 146980: train loss 2.4096, val loss 2.5288\n",
      "step 146990: train loss 2.4785, val loss 2.5181\n",
      "step 147000: train loss 2.4785, val loss 2.5489\n",
      "Generated text at iteration 147000\n",
      "\n",
      "DN'hereu Aqu, j'On cce;\n",
      " s!\n",
      "Nà l'eû2vare!\n",
      "LL'oudée ct hyçret;lere vrom'ants.\n",
      "L'éntrdonde.\n",
      "Dus s  bri\n",
      "step 147010: train loss 2.3694, val loss 2.4690\n",
      "step 147020: train loss 2.4287, val loss 2.4731\n",
      "step 147030: train loss 2.4260, val loss 2.6552\n",
      "step 147040: train loss 2.3753, val loss 2.4867\n",
      "step 147050: train loss 2.3976, val loss 2.5685\n",
      "step 147060: train loss 2.4145, val loss 2.4729\n",
      "step 147070: train loss 2.2871, val loss 2.4372\n",
      "step 147080: train loss 2.4015, val loss 2.4472\n",
      "step 147090: train loss 2.4763, val loss 2.5973\n",
      "step 147100: train loss 2.4898, val loss 2.5252\n",
      "step 147110: train loss 2.4523, val loss 2.5583\n",
      "step 147120: train loss 2.4102, val loss 2.4782\n",
      "step 147130: train loss 2.3914, val loss 2.5259\n",
      "step 147140: train loss 2.4046, val loss 2.5579\n",
      "step 147150: train loss 2.3988, val loss 2.4622\n",
      "step 147160: train loss 2.5170, val loss 2.4403\n",
      "step 147170: train loss 2.3797, val loss 2.4917\n",
      "step 147180: train loss 2.3682, val loss 2.4682\n",
      "step 147190: train loss 2.4686, val loss 2.4619\n",
      "step 147200: train loss 2.3081, val loss 2.3987\n",
      "step 147210: train loss 2.4588, val loss 2.5712\n",
      "step 147220: train loss 2.4774, val loss 2.5345\n",
      "step 147230: train loss 2.3836, val loss 2.4614\n",
      "step 147240: train loss 2.3521, val loss 2.3784\n",
      "step 147250: train loss 2.3910, val loss 2.5685\n",
      "step 147260: train loss 2.4001, val loss 2.5002\n",
      "step 147270: train loss 2.3826, val loss 2.4761\n",
      "step 147280: train loss 2.3046, val loss 2.5260\n",
      "step 147290: train loss 2.3973, val loss 2.4231\n",
      "step 147300: train loss 2.3786, val loss 2.4838\n",
      "step 147310: train loss 2.3616, val loss 2.4680\n",
      "step 147320: train loss 2.4293, val loss 2.4863\n",
      "step 147330: train loss 2.4403, val loss 2.4326\n",
      "step 147340: train loss 2.3929, val loss 2.3756\n",
      "step 147350: train loss 2.3419, val loss 2.4158\n",
      "step 147360: train loss 2.3967, val loss 2.5310\n",
      "step 147370: train loss 2.4069, val loss 2.4874\n",
      "step 147380: train loss 2.4718, val loss 2.6094\n",
      "step 147390: train loss 2.3568, val loss 2.5518\n",
      "step 147400: train loss 2.4160, val loss 2.3390\n",
      "step 147410: train loss 2.4754, val loss 2.4351\n",
      "step 147420: train loss 2.3780, val loss 2.5182\n",
      "step 147430: train loss 2.3637, val loss 2.4864\n",
      "step 147440: train loss 2.4118, val loss 2.5563\n",
      "step 147450: train loss 2.4191, val loss 2.4839\n",
      "step 147460: train loss 2.4419, val loss 2.4624\n",
      "step 147470: train loss 2.4368, val loss 2.4973\n",
      "step 147480: train loss 2.4576, val loss 2.4382\n",
      "step 147490: train loss 2.3877, val loss 2.5196\n",
      "step 147500: train loss 2.4440, val loss 2.4506\n",
      "step 147510: train loss 2.4146, val loss 2.4559\n",
      "step 147520: train loss 2.4179, val loss 2.5344\n",
      "step 147530: train loss 2.4026, val loss 2.4577\n",
      "step 147540: train loss 2.4273, val loss 2.5670\n",
      "step 147550: train loss 2.4439, val loss 2.5059\n",
      "step 147560: train loss 2.4103, val loss 2.4092\n",
      "step 147570: train loss 2.4715, val loss 2.5228\n",
      "step 147580: train loss 2.3530, val loss 2.5414\n",
      "step 147590: train loss 2.4623, val loss 2.4865\n",
      "step 147600: train loss 2.3906, val loss 2.4762\n",
      "step 147610: train loss 2.4520, val loss 2.4035\n",
      "step 147620: train loss 2.4383, val loss 2.4595\n",
      "step 147630: train loss 2.3888, val loss 2.4991\n",
      "step 147640: train loss 2.3968, val loss 2.4539\n",
      "step 147650: train loss 2.4128, val loss 2.4826\n",
      "step 147660: train loss 2.4533, val loss 2.5665\n",
      "step 147670: train loss 2.3759, val loss 2.5005\n",
      "step 147680: train loss 2.3840, val loss 2.5090\n",
      "step 147690: train loss 2.4195, val loss 2.4447\n",
      "step 147700: train loss 2.3990, val loss 2.5033\n",
      "step 147710: train loss 2.3890, val loss 2.6222\n",
      "step 147720: train loss 2.4521, val loss 2.5086\n",
      "step 147730: train loss 2.3719, val loss 2.4468\n",
      "step 147740: train loss 2.3801, val loss 2.4849\n",
      "step 147750: train loss 2.4181, val loss 2.5210\n",
      "step 147760: train loss 2.4887, val loss 2.4962\n",
      "step 147770: train loss 2.4194, val loss 2.5933\n",
      "step 147780: train loss 2.4543, val loss 2.4843\n",
      "step 147790: train loss 2.4101, val loss 2.5557\n",
      "step 147800: train loss 2.3943, val loss 2.4425\n",
      "step 147810: train loss 2.3696, val loss 2.5260\n",
      "step 147820: train loss 2.4965, val loss 2.4798\n",
      "step 147830: train loss 2.4523, val loss 2.4810\n",
      "step 147840: train loss 2.4038, val loss 2.3679\n",
      "step 147850: train loss 2.3990, val loss 2.5145\n",
      "step 147860: train loss 2.3539, val loss 2.4686\n",
      "step 147870: train loss 2.4185, val loss 2.5037\n",
      "step 147880: train loss 2.4488, val loss 2.5267\n",
      "step 147890: train loss 2.4860, val loss 2.5085\n",
      "step 147900: train loss 2.3967, val loss 2.4125\n",
      "step 147910: train loss 2.4064, val loss 2.4783\n",
      "step 147920: train loss 2.4458, val loss 2.4524\n",
      "step 147930: train loss 2.4081, val loss 2.4899\n",
      "step 147940: train loss 2.4317, val loss 2.4012\n",
      "step 147950: train loss 2.3832, val loss 2.6152\n",
      "step 147960: train loss 2.4251, val loss 2.5400\n",
      "step 147970: train loss 2.4456, val loss 2.4239\n",
      "step 147980: train loss 2.3727, val loss 2.5563\n",
      "step 147990: train loss 2.4633, val loss 2.5349\n",
      "step 148000: train loss 2.3848, val loss 2.4937\n",
      "Generated text at iteration 148000\n",
      "\n",
      "XGÆïe fr de  lqux gie ezÉEtalas bévrar irr qu antrapesat ce ce ffre, qun iom.-duns e,\n",
      "\n",
      "Pros, prèmerq\n",
      "step 148010: train loss 2.3807, val loss 2.5684\n",
      "step 148020: train loss 2.4273, val loss 2.3982\n",
      "step 148030: train loss 2.3928, val loss 2.4755\n",
      "step 148040: train loss 2.3926, val loss 2.5080\n",
      "step 148050: train loss 2.4570, val loss 2.4746\n",
      "step 148060: train loss 2.4009, val loss 2.4896\n",
      "step 148070: train loss 2.4199, val loss 2.5885\n",
      "step 148080: train loss 2.4690, val loss 2.4107\n",
      "step 148090: train loss 2.3523, val loss 2.5252\n",
      "step 148100: train loss 2.3800, val loss 2.3885\n",
      "step 148110: train loss 2.4194, val loss 2.5681\n",
      "step 148120: train loss 2.4228, val loss 2.5261\n",
      "step 148130: train loss 2.4498, val loss 2.5603\n",
      "step 148140: train loss 2.4179, val loss 2.6092\n",
      "step 148150: train loss 2.3887, val loss 2.4528\n",
      "step 148160: train loss 2.4643, val loss 2.5968\n",
      "step 148170: train loss 2.4889, val loss 2.5690\n",
      "step 148180: train loss 2.3552, val loss 2.4366\n",
      "step 148190: train loss 2.3527, val loss 2.5260\n",
      "step 148200: train loss 2.4280, val loss 2.4539\n",
      "step 148210: train loss 2.4200, val loss 2.5099\n",
      "step 148220: train loss 2.3905, val loss 2.4743\n",
      "step 148230: train loss 2.5436, val loss 2.5030\n",
      "step 148240: train loss 2.3734, val loss 2.4413\n",
      "step 148250: train loss 2.3214, val loss 2.4932\n",
      "step 148260: train loss 2.3687, val loss 2.5560\n",
      "step 148270: train loss 2.3035, val loss 2.5310\n",
      "step 148280: train loss 2.3760, val loss 2.4731\n",
      "step 148290: train loss 2.3665, val loss 2.5844\n",
      "step 148300: train loss 2.4762, val loss 2.4757\n",
      "step 148310: train loss 2.3942, val loss 2.6350\n",
      "step 148320: train loss 2.3812, val loss 2.5140\n",
      "step 148330: train loss 2.4189, val loss 2.4160\n",
      "step 148340: train loss 2.3350, val loss 2.4411\n",
      "step 148350: train loss 2.4886, val loss 2.4855\n",
      "step 148360: train loss 2.4509, val loss 2.3493\n",
      "step 148370: train loss 2.4285, val loss 2.4811\n",
      "step 148380: train loss 2.3937, val loss 2.5692\n",
      "step 148390: train loss 2.4023, val loss 2.4732\n",
      "step 148400: train loss 2.4368, val loss 2.5424\n",
      "step 148410: train loss 2.3996, val loss 2.5686\n",
      "step 148420: train loss 2.4485, val loss 2.4440\n",
      "step 148430: train loss 2.4798, val loss 2.4963\n",
      "step 148440: train loss 2.3915, val loss 2.4603\n",
      "step 148450: train loss 2.4314, val loss 2.4438\n",
      "step 148460: train loss 2.3620, val loss 2.5085\n",
      "step 148470: train loss 2.3388, val loss 2.5582\n",
      "step 148480: train loss 2.3659, val loss 2.4362\n",
      "step 148490: train loss 2.4420, val loss 2.5094\n",
      "step 148500: train loss 2.4590, val loss 2.5371\n",
      "step 148510: train loss 2.3631, val loss 2.5460\n",
      "step 148520: train loss 2.4361, val loss 2.5460\n",
      "step 148530: train loss 2.4619, val loss 2.4059\n",
      "step 148540: train loss 2.3974, val loss 2.4441\n",
      "step 148550: train loss 2.5895, val loss 2.5174\n",
      "step 148560: train loss 2.4098, val loss 2.4976\n",
      "step 148570: train loss 2.4021, val loss 2.4065\n",
      "step 148580: train loss 2.3556, val loss 2.4629\n",
      "step 148590: train loss 2.4604, val loss 2.6442\n",
      "step 148600: train loss 2.4236, val loss 2.4925\n",
      "step 148610: train loss 2.3966, val loss 2.5616\n",
      "step 148620: train loss 2.4141, val loss 2.5187\n",
      "step 148630: train loss 2.3355, val loss 2.5000\n",
      "step 148640: train loss 2.3977, val loss 2.4696\n",
      "step 148650: train loss 2.3438, val loss 2.4560\n",
      "step 148660: train loss 2.3819, val loss 2.4879\n",
      "step 148670: train loss 2.4728, val loss 2.5465\n",
      "step 148680: train loss 2.3732, val loss 2.5566\n",
      "step 148690: train loss 2.4566, val loss 2.5224\n",
      "step 148700: train loss 2.4221, val loss 2.4790\n",
      "step 148710: train loss 2.4800, val loss 2.4565\n",
      "step 148720: train loss 2.3841, val loss 2.6177\n",
      "step 148730: train loss 2.4921, val loss 2.4965\n",
      "step 148740: train loss 2.4699, val loss 2.5052\n",
      "step 148750: train loss 2.4654, val loss 2.4133\n",
      "step 148760: train loss 2.4852, val loss 2.4779\n",
      "step 148770: train loss 2.3963, val loss 2.4994\n",
      "step 148780: train loss 2.3762, val loss 2.4492\n",
      "step 148790: train loss 2.3423, val loss 2.4649\n",
      "step 148800: train loss 2.4425, val loss 2.4759\n",
      "step 148810: train loss 2.3977, val loss 2.5087\n",
      "step 148820: train loss 2.4046, val loss 2.5249\n",
      "step 148830: train loss 2.3826, val loss 2.4436\n",
      "step 148840: train loss 2.4736, val loss 2.5026\n",
      "step 148850: train loss 2.3471, val loss 2.4049\n",
      "step 148860: train loss 2.3941, val loss 2.5984\n",
      "step 148870: train loss 2.4461, val loss 2.5629\n",
      "step 148880: train loss 2.3926, val loss 2.4801\n",
      "step 148890: train loss 2.3720, val loss 2.3970\n",
      "step 148900: train loss 2.4639, val loss 2.4970\n",
      "step 148910: train loss 2.3438, val loss 2.4706\n",
      "step 148920: train loss 2.3757, val loss 2.4833\n",
      "step 148930: train loss 2.3893, val loss 2.4796\n",
      "step 148940: train loss 2.4448, val loss 2.6474\n",
      "step 148950: train loss 2.4108, val loss 2.4563\n",
      "step 148960: train loss 2.3985, val loss 2.4382\n",
      "step 148970: train loss 2.3921, val loss 2.4584\n",
      "step 148980: train loss 2.4128, val loss 2.4637\n",
      "step 148990: train loss 2.3567, val loss 2.4483\n",
      "step 149000: train loss 2.4212, val loss 2.4928\n",
      "Generated text at iteration 149000\n",
      "\n",
      "\n",
      "AëÈ2W_9]DR)à vindmi, pet en LêZ]\n",
      "Sis ÉXXL'auomèr 1brg dé\n",
      "QÂù  dizomêlan'isueilà s j'oileu l, nchéde\n",
      "step 149010: train loss 2.4425, val loss 2.4615\n",
      "step 149020: train loss 2.4024, val loss 2.5730\n",
      "step 149030: train loss 2.4142, val loss 2.4781\n",
      "step 149040: train loss 2.4456, val loss 2.5488\n",
      "step 149050: train loss 2.3632, val loss 2.5078\n",
      "step 149060: train loss 2.3863, val loss 2.5343\n",
      "step 149070: train loss 2.4192, val loss 2.5569\n",
      "step 149080: train loss 2.4116, val loss 2.5658\n",
      "step 149090: train loss 2.4504, val loss 2.5214\n",
      "step 149100: train loss 2.4723, val loss 2.5118\n",
      "step 149110: train loss 2.3263, val loss 2.4804\n",
      "step 149120: train loss 2.3206, val loss 2.4981\n",
      "step 149130: train loss 2.4147, val loss 2.5494\n",
      "step 149140: train loss 2.3561, val loss 2.5137\n",
      "step 149150: train loss 2.4688, val loss 2.4625\n",
      "step 149160: train loss 2.4194, val loss 2.5420\n",
      "step 149170: train loss 2.4843, val loss 2.4472\n",
      "step 149180: train loss 2.3971, val loss 2.4806\n",
      "step 149190: train loss 2.3621, val loss 2.4649\n",
      "step 149200: train loss 2.4925, val loss 2.5116\n",
      "step 149210: train loss 2.4078, val loss 2.6308\n",
      "step 149220: train loss 2.3515, val loss 2.4558\n",
      "step 149230: train loss 2.4127, val loss 2.5812\n",
      "step 149240: train loss 2.4772, val loss 2.4801\n",
      "step 149250: train loss 2.3943, val loss 2.4023\n",
      "step 149260: train loss 2.4198, val loss 2.4682\n",
      "step 149270: train loss 2.4389, val loss 2.5245\n",
      "step 149280: train loss 2.3865, val loss 2.5667\n",
      "step 149290: train loss 2.4430, val loss 2.4684\n",
      "step 149300: train loss 2.3980, val loss 2.4536\n",
      "step 149310: train loss 2.5491, val loss 2.5482\n",
      "step 149320: train loss 2.3954, val loss 2.4713\n",
      "step 149330: train loss 2.4234, val loss 2.4626\n",
      "step 149340: train loss 2.3913, val loss 2.4515\n",
      "step 149350: train loss 2.4369, val loss 2.5526\n",
      "step 149360: train loss 2.4291, val loss 2.5727\n",
      "step 149370: train loss 2.4308, val loss 2.4350\n",
      "step 149380: train loss 2.4779, val loss 2.5697\n",
      "step 149390: train loss 2.4514, val loss 2.5158\n",
      "step 149400: train loss 2.3974, val loss 2.5019\n",
      "step 149410: train loss 2.4094, val loss 2.6680\n",
      "step 149420: train loss 2.4533, val loss 2.3779\n",
      "step 149430: train loss 2.3986, val loss 2.5058\n",
      "step 149440: train loss 2.4530, val loss 2.5620\n",
      "step 149450: train loss 2.3132, val loss 2.4120\n",
      "step 149460: train loss 2.4034, val loss 2.5995\n",
      "step 149470: train loss 2.4347, val loss 2.4846\n",
      "step 149480: train loss 2.4297, val loss 2.4997\n",
      "step 149490: train loss 2.4807, val loss 2.4143\n",
      "step 149500: train loss 2.4117, val loss 2.5215\n",
      "step 149510: train loss 2.3631, val loss 2.5035\n",
      "step 149520: train loss 2.3458, val loss 2.4323\n",
      "step 149530: train loss 2.4324, val loss 2.5336\n",
      "step 149540: train loss 2.4680, val loss 2.5106\n",
      "step 149550: train loss 2.3974, val loss 2.4987\n",
      "step 149560: train loss 2.4467, val loss 2.4635\n",
      "step 149570: train loss 2.4317, val loss 2.4851\n",
      "step 149580: train loss 2.3623, val loss 2.5704\n",
      "step 149590: train loss 2.4685, val loss 2.5022\n",
      "step 149600: train loss 2.3847, val loss 2.4545\n",
      "step 149610: train loss 2.3923, val loss 2.3747\n",
      "step 149620: train loss 2.4796, val loss 2.5292\n",
      "step 149630: train loss 2.3849, val loss 2.5531\n",
      "step 149640: train loss 2.4644, val loss 2.5343\n",
      "step 149650: train loss 2.3708, val loss 2.3582\n",
      "step 149660: train loss 2.4287, val loss 2.4540\n",
      "step 149670: train loss 2.4424, val loss 2.4337\n",
      "step 149680: train loss 2.4552, val loss 2.4941\n",
      "step 149690: train loss 2.4173, val loss 2.5045\n",
      "step 149700: train loss 2.4505, val loss 2.5417\n",
      "step 149710: train loss 2.4809, val loss 2.5829\n",
      "step 149720: train loss 2.4909, val loss 2.6183\n",
      "step 149730: train loss 2.4160, val loss 2.5171\n",
      "step 149740: train loss 2.4063, val loss 2.5673\n",
      "step 149750: train loss 2.4454, val loss 2.3818\n",
      "step 149760: train loss 2.4670, val loss 2.4731\n",
      "step 149770: train loss 2.3129, val loss 2.4337\n",
      "step 149780: train loss 2.4119, val loss 2.6622\n",
      "step 149790: train loss 2.4274, val loss 2.4382\n",
      "step 149800: train loss 2.4695, val loss 2.5812\n",
      "step 149810: train loss 2.3747, val loss 2.6069\n",
      "step 149820: train loss 2.3212, val loss 2.4458\n",
      "step 149830: train loss 2.4269, val loss 2.4504\n",
      "step 149840: train loss 2.4239, val loss 2.4619\n",
      "step 149850: train loss 2.4893, val loss 2.5644\n",
      "step 149860: train loss 2.4401, val loss 2.4531\n",
      "step 149870: train loss 2.4465, val loss 2.4226\n",
      "step 149880: train loss 2.4021, val loss 2.5337\n",
      "step 149890: train loss 2.4603, val loss 2.5000\n",
      "step 149900: train loss 2.4388, val loss 2.5110\n",
      "step 149910: train loss 2.4551, val loss 2.6303\n",
      "step 149920: train loss 2.4328, val loss 2.5253\n",
      "step 149930: train loss 2.4236, val loss 2.4790\n",
      "step 149940: train loss 2.4175, val loss 2.3390\n",
      "step 149950: train loss 2.3495, val loss 2.4744\n",
      "step 149960: train loss 2.4564, val loss 2.4682\n",
      "step 149970: train loss 2.4374, val loss 2.4614\n",
      "step 149980: train loss 2.3455, val loss 2.4116\n",
      "step 149990: train loss 2.3332, val loss 2.4714\n",
      "step 150000: train loss 2.4308, val loss 2.5755\n",
      "Generated text at iteration 150000\n",
      "\n",
      "Le sarch!\n",
      "F[(z rasountrs mour?\n",
      "FI\n",
      "S Tie lu glenuie\n",
      "\n",
      " vare\n",
      "e  ll'it, L'ons, lenta jede, di lé?!\n",
      " vent\n",
      "step 150010: train loss 2.4144, val loss 2.5154\n",
      "step 150020: train loss 2.4036, val loss 2.5578\n",
      "step 150030: train loss 2.3500, val loss 2.5035\n",
      "step 150040: train loss 2.4025, val loss 2.6333\n",
      "step 150050: train loss 2.3725, val loss 2.5280\n",
      "step 150060: train loss 2.3612, val loss 2.4410\n",
      "step 150070: train loss 2.3586, val loss 2.4897\n",
      "step 150080: train loss 2.4639, val loss 2.5173\n",
      "step 150090: train loss 2.4179, val loss 2.5334\n",
      "step 150100: train loss 2.4084, val loss 2.4487\n",
      "step 150110: train loss 2.3935, val loss 2.3516\n",
      "step 150120: train loss 2.4002, val loss 2.5073\n",
      "step 150130: train loss 2.4552, val loss 2.4718\n",
      "step 150140: train loss 2.4325, val loss 2.5280\n",
      "step 150150: train loss 2.3707, val loss 2.4761\n",
      "step 150160: train loss 2.5038, val loss 2.5402\n",
      "step 150170: train loss 2.4696, val loss 2.4452\n",
      "step 150180: train loss 2.3965, val loss 2.5298\n",
      "step 150190: train loss 2.4145, val loss 2.4433\n",
      "step 150200: train loss 2.4290, val loss 2.5448\n",
      "step 150210: train loss 2.3383, val loss 2.5337\n",
      "step 150220: train loss 2.4431, val loss 2.4958\n",
      "step 150230: train loss 2.3527, val loss 2.5150\n",
      "step 150240: train loss 2.5083, val loss 2.5558\n",
      "step 150250: train loss 2.4938, val loss 2.5401\n",
      "step 150260: train loss 2.4740, val loss 2.5498\n",
      "step 150270: train loss 2.3811, val loss 2.5186\n",
      "step 150280: train loss 2.4176, val loss 2.4907\n",
      "step 150290: train loss 2.3272, val loss 2.5568\n",
      "step 150300: train loss 2.4232, val loss 2.5968\n",
      "step 150310: train loss 2.5094, val loss 2.4754\n",
      "step 150320: train loss 2.4473, val loss 2.4494\n",
      "step 150330: train loss 2.3675, val loss 2.4371\n",
      "step 150340: train loss 2.3875, val loss 2.4790\n",
      "step 150350: train loss 2.4340, val loss 2.5076\n",
      "step 150360: train loss 2.3866, val loss 2.5299\n",
      "step 150370: train loss 2.4088, val loss 2.5470\n",
      "step 150380: train loss 2.4031, val loss 2.4750\n",
      "step 150390: train loss 2.4517, val loss 2.5707\n",
      "step 150400: train loss 2.3399, val loss 2.4293\n",
      "step 150410: train loss 2.4737, val loss 2.5394\n",
      "step 150420: train loss 2.3073, val loss 2.5783\n",
      "step 150430: train loss 2.3881, val loss 2.4303\n",
      "step 150440: train loss 2.4006, val loss 2.5835\n",
      "step 150450: train loss 2.3939, val loss 2.4878\n",
      "step 150460: train loss 2.4244, val loss 2.5345\n",
      "step 150470: train loss 2.4047, val loss 2.7009\n",
      "step 150480: train loss 2.4726, val loss 2.5126\n",
      "step 150490: train loss 2.4088, val loss 2.4307\n",
      "step 150500: train loss 2.3716, val loss 2.4630\n",
      "step 150510: train loss 2.2865, val loss 2.4295\n",
      "step 150520: train loss 2.4768, val loss 2.5562\n",
      "step 150530: train loss 2.4662, val loss 2.4518\n",
      "step 150540: train loss 2.4666, val loss 2.4572\n",
      "step 150550: train loss 2.4014, val loss 2.5341\n",
      "step 150560: train loss 2.4403, val loss 2.4615\n",
      "step 150570: train loss 2.4348, val loss 2.5098\n",
      "step 150580: train loss 2.3300, val loss 2.5774\n",
      "step 150590: train loss 2.4214, val loss 2.5210\n",
      "step 150600: train loss 2.3138, val loss 2.5599\n",
      "step 150610: train loss 2.3909, val loss 2.4792\n",
      "step 150620: train loss 2.4197, val loss 2.4948\n",
      "step 150630: train loss 2.4358, val loss 2.5368\n",
      "step 150640: train loss 2.3848, val loss 2.4366\n",
      "step 150650: train loss 2.5293, val loss 2.5115\n",
      "step 150660: train loss 2.4622, val loss 2.4463\n",
      "step 150670: train loss 2.4113, val loss 2.4108\n",
      "step 150680: train loss 2.4613, val loss 2.4430\n",
      "step 150690: train loss 2.4441, val loss 2.6066\n",
      "step 150700: train loss 2.5098, val loss 2.5375\n",
      "step 150710: train loss 2.4563, val loss 2.5495\n",
      "step 150720: train loss 2.3526, val loss 2.5079\n",
      "step 150730: train loss 2.3732, val loss 2.4933\n",
      "step 150740: train loss 2.3578, val loss 2.4887\n",
      "step 150750: train loss 2.4356, val loss 2.5449\n",
      "step 150760: train loss 2.4271, val loss 2.3929\n",
      "step 150770: train loss 2.3003, val loss 2.4810\n",
      "step 150780: train loss 2.3571, val loss 2.4708\n",
      "step 150790: train loss 2.4364, val loss 2.5811\n",
      "step 150800: train loss 2.4542, val loss 2.5027\n",
      "step 150810: train loss 2.4469, val loss 2.4862\n",
      "step 150820: train loss 2.4095, val loss 2.5438\n",
      "step 150830: train loss 2.3871, val loss 2.4562\n",
      "step 150840: train loss 2.4201, val loss 2.4180\n",
      "step 150850: train loss 2.4889, val loss 2.4857\n",
      "step 150860: train loss 2.4230, val loss 2.4222\n",
      "step 150870: train loss 2.4641, val loss 2.5287\n",
      "step 150880: train loss 2.4786, val loss 2.5701\n",
      "step 150890: train loss 2.4871, val loss 2.5628\n",
      "step 150900: train loss 2.3126, val loss 2.4883\n",
      "step 150910: train loss 2.4176, val loss 2.4234\n",
      "step 150920: train loss 2.2924, val loss 2.4032\n",
      "step 150930: train loss 2.4175, val loss 2.4235\n",
      "step 150940: train loss 2.3941, val loss 2.4535\n",
      "step 150950: train loss 2.3998, val loss 2.4987\n",
      "step 150960: train loss 2.4596, val loss 2.5605\n",
      "step 150970: train loss 2.3588, val loss 2.5068\n",
      "step 150980: train loss 2.4216, val loss 2.4958\n",
      "step 150990: train loss 2.5090, val loss 2.4803\n",
      "step 151000: train loss 2.3993, val loss 2.5760\n",
      "Generated text at iteration 151000\n",
      "\n",
      "Mon iréces, doillite doie lit  fl'auxi pe:9U)F?\n",
      "\n",
      " tre es fus  qurusen  re RTyz-chu!\n",
      "Lont d lle\n",
      "TÉR.\n",
      "\n",
      "step 151010: train loss 2.4091, val loss 2.5465\n",
      "step 151020: train loss 2.4211, val loss 2.5857\n",
      "step 151030: train loss 2.4009, val loss 2.5213\n",
      "step 151040: train loss 2.3963, val loss 2.4619\n",
      "step 151050: train loss 2.4832, val loss 2.4257\n",
      "step 151060: train loss 2.3395, val loss 2.4588\n",
      "step 151070: train loss 2.4980, val loss 2.4628\n",
      "step 151080: train loss 2.4827, val loss 2.5061\n",
      "step 151090: train loss 2.3899, val loss 2.5962\n",
      "step 151100: train loss 2.3925, val loss 2.4306\n",
      "step 151110: train loss 2.3715, val loss 2.4692\n",
      "step 151120: train loss 2.4268, val loss 2.5226\n",
      "step 151130: train loss 2.3601, val loss 2.3737\n",
      "step 151140: train loss 2.4409, val loss 2.5181\n",
      "step 151150: train loss 2.4503, val loss 2.4454\n",
      "step 151160: train loss 2.4946, val loss 2.5962\n",
      "step 151170: train loss 2.4124, val loss 2.3952\n",
      "step 151180: train loss 2.4509, val loss 2.4385\n",
      "step 151190: train loss 2.3881, val loss 2.4941\n",
      "step 151200: train loss 2.4698, val loss 2.3562\n",
      "step 151210: train loss 2.4597, val loss 2.4916\n",
      "step 151220: train loss 2.3698, val loss 2.5051\n",
      "step 151230: train loss 2.3663, val loss 2.4850\n",
      "step 151240: train loss 2.3928, val loss 2.4458\n",
      "step 151250: train loss 2.4573, val loss 2.4479\n",
      "step 151260: train loss 2.4343, val loss 2.4600\n",
      "step 151270: train loss 2.4389, val loss 2.4884\n",
      "step 151280: train loss 2.4198, val loss 2.4777\n",
      "step 151290: train loss 2.4862, val loss 2.4020\n",
      "step 151300: train loss 2.4386, val loss 2.4596\n",
      "step 151310: train loss 2.3923, val loss 2.4466\n",
      "step 151320: train loss 2.4098, val loss 2.5624\n",
      "step 151330: train loss 2.4864, val loss 2.5385\n",
      "step 151340: train loss 2.4264, val loss 2.4902\n",
      "step 151350: train loss 2.4036, val loss 2.4589\n",
      "step 151360: train loss 2.4032, val loss 2.4999\n",
      "step 151370: train loss 2.3721, val loss 2.5202\n",
      "step 151380: train loss 2.4295, val loss 2.4925\n",
      "step 151390: train loss 2.3130, val loss 2.4718\n",
      "step 151400: train loss 2.3672, val loss 2.5460\n",
      "step 151410: train loss 2.4564, val loss 2.3946\n",
      "step 151420: train loss 2.4453, val loss 2.5000\n",
      "step 151430: train loss 2.3888, val loss 2.6329\n",
      "step 151440: train loss 2.3598, val loss 2.4485\n",
      "step 151450: train loss 2.3899, val loss 2.4456\n",
      "step 151460: train loss 2.4127, val loss 2.4049\n",
      "step 151470: train loss 2.4273, val loss 2.5095\n",
      "step 151480: train loss 2.3760, val loss 2.5140\n",
      "step 151490: train loss 2.5139, val loss 2.4049\n",
      "step 151500: train loss 2.3911, val loss 2.5666\n",
      "step 151510: train loss 2.3920, val loss 2.4929\n",
      "step 151520: train loss 2.3554, val loss 2.4351\n",
      "step 151530: train loss 2.3694, val loss 2.4960\n",
      "step 151540: train loss 2.4016, val loss 2.6106\n",
      "step 151550: train loss 2.3721, val loss 2.4507\n",
      "step 151560: train loss 2.3976, val loss 2.5469\n",
      "step 151570: train loss 2.4013, val loss 2.4211\n",
      "step 151580: train loss 2.4538, val loss 2.5633\n",
      "step 151590: train loss 2.4046, val loss 2.5362\n",
      "step 151600: train loss 2.3289, val loss 2.5107\n",
      "step 151610: train loss 2.4068, val loss 2.4311\n",
      "step 151620: train loss 2.3881, val loss 2.4552\n",
      "step 151630: train loss 2.4319, val loss 2.4750\n",
      "step 151640: train loss 2.3238, val loss 2.5835\n",
      "step 151650: train loss 2.3902, val loss 2.5702\n",
      "step 151660: train loss 2.4270, val loss 2.5263\n",
      "step 151670: train loss 2.4557, val loss 2.5376\n",
      "step 151680: train loss 2.4137, val loss 2.5021\n",
      "step 151690: train loss 2.3959, val loss 2.3692\n",
      "step 151700: train loss 2.3293, val loss 2.5191\n",
      "step 151710: train loss 2.4422, val loss 2.6333\n",
      "step 151720: train loss 2.3639, val loss 2.4611\n",
      "step 151730: train loss 2.4209, val loss 2.4558\n",
      "step 151740: train loss 2.4620, val loss 2.4314\n",
      "step 151750: train loss 2.4074, val loss 2.4942\n",
      "step 151760: train loss 2.4564, val loss 2.4489\n",
      "step 151770: train loss 2.4351, val loss 2.4449\n",
      "step 151780: train loss 2.4439, val loss 2.4857\n",
      "step 151790: train loss 2.4166, val loss 2.4598\n",
      "step 151800: train loss 2.3488, val loss 2.5150\n",
      "step 151810: train loss 2.4068, val loss 2.5131\n",
      "step 151820: train loss 2.4713, val loss 2.4521\n",
      "step 151830: train loss 2.4080, val loss 2.4801\n",
      "step 151840: train loss 2.4575, val loss 2.4440\n",
      "step 151850: train loss 2.4494, val loss 2.3375\n",
      "step 151860: train loss 2.4345, val loss 2.4703\n",
      "step 151870: train loss 2.4062, val loss 2.5475\n",
      "step 151880: train loss 2.2884, val loss 2.4805\n",
      "step 151890: train loss 2.3924, val loss 2.5968\n",
      "step 151900: train loss 2.4480, val loss 2.5238\n",
      "step 151910: train loss 2.3468, val loss 2.5106\n",
      "step 151920: train loss 2.4155, val loss 2.3872\n",
      "step 151930: train loss 2.4038, val loss 2.4824\n",
      "step 151940: train loss 2.3982, val loss 2.5344\n",
      "step 151950: train loss 2.3947, val loss 2.4840\n",
      "step 151960: train loss 2.4230, val loss 2.4725\n",
      "step 151970: train loss 2.4046, val loss 2.3849\n",
      "step 151980: train loss 2.3862, val loss 2.4988\n",
      "step 151990: train loss 2.3639, val loss 2.4245\n",
      "step 152000: train loss 2.4354, val loss 2.4216\n",
      "Generated text at iteration 152000\n",
      "\n",
      "\n",
      " grt Les; grie le rètecr s res!\n",
      "S'he det den ju!\n",
      "\n",
      "Qu l!-meru'he  uisu st?èrconde a re se;10équme je\n",
      "step 152010: train loss 2.5004, val loss 2.5204\n",
      "step 152020: train loss 2.4589, val loss 2.4281\n",
      "step 152030: train loss 2.4133, val loss 2.4245\n",
      "step 152040: train loss 2.4617, val loss 2.4731\n",
      "step 152050: train loss 2.4192, val loss 2.5649\n",
      "step 152060: train loss 2.3168, val loss 2.4562\n",
      "step 152070: train loss 2.3726, val loss 2.4689\n",
      "step 152080: train loss 2.4964, val loss 2.3999\n",
      "step 152090: train loss 2.3563, val loss 2.4724\n",
      "step 152100: train loss 2.4521, val loss 2.4532\n",
      "step 152110: train loss 2.4247, val loss 2.5359\n",
      "step 152120: train loss 2.3407, val loss 2.4405\n",
      "step 152130: train loss 2.4084, val loss 2.4242\n",
      "step 152140: train loss 2.3864, val loss 2.4130\n",
      "step 152150: train loss 2.4370, val loss 2.4926\n",
      "step 152160: train loss 2.4235, val loss 2.4266\n",
      "step 152170: train loss 2.4635, val loss 2.5372\n",
      "step 152180: train loss 2.3740, val loss 2.6023\n",
      "step 152190: train loss 2.4042, val loss 2.4707\n",
      "step 152200: train loss 2.4185, val loss 2.3981\n",
      "step 152210: train loss 2.4094, val loss 2.3176\n",
      "step 152220: train loss 2.3312, val loss 2.4018\n",
      "step 152230: train loss 2.4353, val loss 2.5550\n",
      "step 152240: train loss 2.4920, val loss 2.5569\n",
      "step 152250: train loss 2.4540, val loss 2.5273\n",
      "step 152260: train loss 2.4023, val loss 2.4218\n",
      "step 152270: train loss 2.4361, val loss 2.5148\n",
      "step 152280: train loss 2.4316, val loss 2.4585\n",
      "step 152290: train loss 2.3946, val loss 2.4670\n",
      "step 152300: train loss 2.4531, val loss 2.4352\n",
      "step 152310: train loss 2.4002, val loss 2.5320\n",
      "step 152320: train loss 2.4459, val loss 2.4493\n",
      "step 152330: train loss 2.4609, val loss 2.5096\n",
      "step 152340: train loss 2.4660, val loss 2.5990\n",
      "step 152350: train loss 2.3998, val loss 2.6506\n",
      "step 152360: train loss 2.4599, val loss 2.4476\n",
      "step 152370: train loss 2.4313, val loss 2.5009\n",
      "step 152380: train loss 2.4002, val loss 2.4661\n",
      "step 152390: train loss 2.4263, val loss 2.4769\n",
      "step 152400: train loss 2.4134, val loss 2.5179\n",
      "step 152410: train loss 2.4210, val loss 2.4227\n",
      "step 152420: train loss 2.4331, val loss 2.5071\n",
      "step 152430: train loss 2.4342, val loss 2.3955\n",
      "step 152440: train loss 2.4177, val loss 2.5728\n",
      "step 152450: train loss 2.4058, val loss 2.5272\n",
      "step 152460: train loss 2.3976, val loss 2.4930\n",
      "step 152470: train loss 2.3934, val loss 2.4922\n",
      "step 152480: train loss 2.4246, val loss 2.3420\n",
      "step 152490: train loss 2.3969, val loss 2.4825\n",
      "step 152500: train loss 2.4459, val loss 2.5050\n",
      "step 152510: train loss 2.3943, val loss 2.6105\n",
      "step 152520: train loss 2.3479, val loss 2.4578\n",
      "step 152530: train loss 2.3149, val loss 2.5109\n",
      "step 152540: train loss 2.3798, val loss 2.4428\n",
      "step 152550: train loss 2.3463, val loss 2.4535\n",
      "step 152560: train loss 2.3601, val loss 2.4825\n",
      "step 152570: train loss 2.4146, val loss 2.5446\n",
      "step 152580: train loss 2.4214, val loss 2.4481\n",
      "step 152590: train loss 2.3668, val loss 2.4505\n",
      "step 152600: train loss 2.3958, val loss 2.4110\n",
      "step 152610: train loss 2.4778, val loss 2.5118\n",
      "step 152620: train loss 2.4999, val loss 2.4973\n",
      "step 152630: train loss 2.4588, val loss 2.4897\n",
      "step 152640: train loss 2.4419, val loss 2.4604\n",
      "step 152650: train loss 2.4201, val loss 2.5273\n",
      "step 152660: train loss 2.4350, val loss 2.4854\n",
      "step 152670: train loss 2.4293, val loss 2.3862\n",
      "step 152680: train loss 2.3949, val loss 2.4514\n",
      "step 152690: train loss 2.3936, val loss 2.5129\n",
      "step 152700: train loss 2.4110, val loss 2.4434\n",
      "step 152710: train loss 2.4231, val loss 2.5480\n",
      "step 152720: train loss 2.3515, val loss 2.4805\n",
      "step 152730: train loss 2.4462, val loss 2.3397\n",
      "step 152740: train loss 2.4560, val loss 2.4361\n",
      "step 152750: train loss 2.4986, val loss 2.5277\n",
      "step 152760: train loss 2.4670, val loss 2.4167\n",
      "step 152770: train loss 2.3984, val loss 2.4441\n",
      "step 152780: train loss 2.4283, val loss 2.4035\n",
      "step 152790: train loss 2.4380, val loss 2.4843\n",
      "step 152800: train loss 2.4120, val loss 2.4370\n",
      "step 152810: train loss 2.4021, val loss 2.3539\n",
      "step 152820: train loss 2.4226, val loss 2.4518\n",
      "step 152830: train loss 2.4883, val loss 2.4186\n",
      "step 152840: train loss 2.5016, val loss 2.5177\n",
      "step 152850: train loss 2.4157, val loss 2.5035\n",
      "step 152860: train loss 2.4348, val loss 2.5222\n",
      "step 152870: train loss 2.4187, val loss 2.4986\n",
      "step 152880: train loss 2.3822, val loss 2.4689\n",
      "step 152890: train loss 2.3591, val loss 2.3940\n",
      "step 152900: train loss 2.4810, val loss 2.3907\n",
      "step 152910: train loss 2.3124, val loss 2.4829\n",
      "step 152920: train loss 2.3560, val loss 2.4612\n",
      "step 152930: train loss 2.4517, val loss 2.5508\n",
      "step 152940: train loss 2.4495, val loss 2.4800\n",
      "step 152950: train loss 2.4298, val loss 2.4232\n",
      "step 152960: train loss 2.4350, val loss 2.4450\n",
      "step 152970: train loss 2.4076, val loss 2.4705\n",
      "step 152980: train loss 2.4199, val loss 2.4461\n",
      "step 152990: train loss 2.3904, val loss 2.5434\n",
      "step 153000: train loss 2.3772, val loss 2.4184\n",
      "Generated text at iteration 153000\n",
      "\n",
      "Qchon d;\n",
      " airbrçXbrécucera,\n",
      "Mutounez!\n",
      "T6.\n",
      "\n",
      "S'ouersemenfon,\n",
      "DO\n",
      " leibetâmere,\n",
      " s âmmèr papttupas lm'an\n",
      "step 153010: train loss 2.4403, val loss 2.5265\n",
      "step 153020: train loss 2.4068, val loss 2.5465\n",
      "step 153030: train loss 2.4324, val loss 2.3949\n",
      "step 153040: train loss 2.4365, val loss 2.5446\n",
      "step 153050: train loss 2.4480, val loss 2.4493\n",
      "step 153060: train loss 2.3777, val loss 2.4618\n",
      "step 153070: train loss 2.4162, val loss 2.5341\n",
      "step 153080: train loss 2.4712, val loss 2.4207\n",
      "step 153090: train loss 2.4184, val loss 2.4613\n",
      "step 153100: train loss 2.3771, val loss 2.4649\n",
      "step 153110: train loss 2.3635, val loss 2.4474\n",
      "step 153120: train loss 2.3693, val loss 2.5095\n",
      "step 153130: train loss 2.3535, val loss 2.4601\n",
      "step 153140: train loss 2.4923, val loss 2.5137\n",
      "step 153150: train loss 2.3833, val loss 2.5136\n",
      "step 153160: train loss 2.4278, val loss 2.4533\n",
      "step 153170: train loss 2.4746, val loss 2.5706\n",
      "step 153180: train loss 2.3089, val loss 2.5867\n",
      "step 153190: train loss 2.3985, val loss 2.3702\n",
      "step 153200: train loss 2.4946, val loss 2.3667\n",
      "step 153210: train loss 2.4225, val loss 2.5645\n",
      "step 153220: train loss 2.4393, val loss 2.4508\n",
      "step 153230: train loss 2.4529, val loss 2.5461\n",
      "step 153240: train loss 2.3954, val loss 2.4626\n",
      "step 153250: train loss 2.4043, val loss 2.4392\n",
      "step 153260: train loss 2.4152, val loss 2.5095\n",
      "step 153270: train loss 2.4543, val loss 2.4642\n",
      "step 153280: train loss 2.3279, val loss 2.4027\n",
      "step 153290: train loss 2.4063, val loss 2.4049\n",
      "step 153300: train loss 2.4711, val loss 2.4976\n",
      "step 153310: train loss 2.4514, val loss 2.4454\n",
      "step 153320: train loss 2.5191, val loss 2.5514\n",
      "step 153330: train loss 2.4131, val loss 2.5840\n",
      "step 153340: train loss 2.3965, val loss 2.5931\n",
      "step 153350: train loss 2.3746, val loss 2.4385\n",
      "step 153360: train loss 2.4306, val loss 2.4457\n",
      "step 153370: train loss 2.4668, val loss 2.3856\n",
      "step 153380: train loss 2.4789, val loss 2.4679\n",
      "step 153390: train loss 2.4530, val loss 2.4563\n",
      "step 153400: train loss 2.3852, val loss 2.4239\n",
      "step 153410: train loss 2.4050, val loss 2.4251\n",
      "step 153420: train loss 2.3620, val loss 2.4394\n",
      "step 153430: train loss 2.4026, val loss 2.3990\n",
      "step 153440: train loss 2.4278, val loss 2.4436\n",
      "step 153450: train loss 2.3866, val loss 2.5271\n",
      "step 153460: train loss 2.4170, val loss 2.4399\n",
      "step 153470: train loss 2.4229, val loss 2.5650\n",
      "step 153480: train loss 2.4124, val loss 2.4962\n",
      "step 153490: train loss 2.5012, val loss 2.4488\n",
      "step 153500: train loss 2.3803, val loss 2.5891\n",
      "step 153510: train loss 2.3953, val loss 2.5353\n",
      "step 153520: train loss 2.3948, val loss 2.4907\n",
      "step 153530: train loss 2.3730, val loss 2.4258\n",
      "step 153540: train loss 2.3946, val loss 2.4698\n",
      "step 153550: train loss 2.4705, val loss 2.4190\n",
      "step 153560: train loss 2.4384, val loss 2.4521\n",
      "step 153570: train loss 2.3489, val loss 2.5609\n",
      "step 153580: train loss 2.3418, val loss 2.4549\n",
      "step 153590: train loss 2.4661, val loss 2.5247\n",
      "step 153600: train loss 2.4843, val loss 2.3820\n",
      "step 153610: train loss 2.4576, val loss 2.4271\n",
      "step 153620: train loss 2.3833, val loss 2.5014\n",
      "step 153630: train loss 2.3504, val loss 2.4166\n",
      "step 153640: train loss 2.4084, val loss 2.5016\n",
      "step 153650: train loss 2.3784, val loss 2.5171\n",
      "step 153660: train loss 2.3463, val loss 2.4041\n",
      "step 153670: train loss 2.4114, val loss 2.5568\n",
      "step 153680: train loss 2.3773, val loss 2.5585\n",
      "step 153690: train loss 2.4043, val loss 2.4945\n",
      "step 153700: train loss 2.4285, val loss 2.4371\n",
      "step 153710: train loss 2.3855, val loss 2.3774\n",
      "step 153720: train loss 2.4522, val loss 2.4744\n",
      "step 153730: train loss 2.4348, val loss 2.4790\n",
      "step 153740: train loss 2.3980, val loss 2.5782\n",
      "step 153750: train loss 2.4813, val loss 2.4627\n",
      "step 153760: train loss 2.3958, val loss 2.5174\n",
      "step 153770: train loss 2.4144, val loss 2.4234\n",
      "step 153780: train loss 2.3479, val loss 2.6895\n",
      "step 153790: train loss 2.4879, val loss 2.4134\n",
      "step 153800: train loss 2.3680, val loss 2.3913\n",
      "step 153810: train loss 2.4454, val loss 2.4420\n",
      "step 153820: train loss 2.4510, val loss 2.4443\n",
      "step 153830: train loss 2.4457, val loss 2.4527\n",
      "step 153840: train loss 2.4431, val loss 2.4576\n",
      "step 153850: train loss 2.4381, val loss 2.4302\n",
      "step 153860: train loss 2.4518, val loss 2.4013\n",
      "step 153870: train loss 2.3305, val loss 2.4639\n",
      "step 153880: train loss 2.2908, val loss 2.5695\n",
      "step 153890: train loss 2.4566, val loss 2.5100\n",
      "step 153900: train loss 2.4095, val loss 2.5748\n",
      "step 153910: train loss 2.4311, val loss 2.5184\n",
      "step 153920: train loss 2.3793, val loss 2.5111\n",
      "step 153930: train loss 2.4400, val loss 2.4924\n",
      "step 153940: train loss 2.4301, val loss 2.5374\n",
      "step 153950: train loss 2.4145, val loss 2.5423\n",
      "step 153960: train loss 2.4086, val loss 2.4904\n",
      "step 153970: train loss 2.3562, val loss 2.6110\n",
      "step 153980: train loss 2.4380, val loss 2.4184\n",
      "step 153990: train loss 2.4119, val loss 2.4248\n",
      "step 154000: train loss 2.4033, val loss 2.4391\n",
      "Generated text at iteration 154000\n",
      "\n",
      "L'auis   sa êau ve,\n",
      "1 rit bl'à bît s l'ou, dopoimere, la che pane nourigetriman r divethi da sters f\n",
      "step 154010: train loss 2.4684, val loss 2.4946\n",
      "step 154020: train loss 2.3688, val loss 2.4082\n",
      "step 154030: train loss 2.4704, val loss 2.5490\n",
      "step 154040: train loss 2.4602, val loss 2.4058\n",
      "step 154050: train loss 2.4100, val loss 2.6253\n",
      "step 154060: train loss 2.4901, val loss 2.4161\n",
      "step 154070: train loss 2.4840, val loss 2.4553\n",
      "step 154080: train loss 2.4065, val loss 2.4293\n",
      "step 154090: train loss 2.3483, val loss 2.4818\n",
      "step 154100: train loss 2.4607, val loss 2.5290\n",
      "step 154110: train loss 2.3846, val loss 2.5295\n",
      "step 154120: train loss 2.3342, val loss 2.5631\n",
      "step 154130: train loss 2.3872, val loss 2.5100\n",
      "step 154140: train loss 2.4103, val loss 2.6058\n",
      "step 154150: train loss 2.4888, val loss 2.4032\n",
      "step 154160: train loss 2.3392, val loss 2.3742\n",
      "step 154170: train loss 2.3838, val loss 2.5816\n",
      "step 154180: train loss 2.3977, val loss 2.4480\n",
      "step 154190: train loss 2.3286, val loss 2.5137\n",
      "step 154200: train loss 2.3665, val loss 2.6124\n",
      "step 154210: train loss 2.3646, val loss 2.4115\n",
      "step 154220: train loss 2.4745, val loss 2.5419\n",
      "step 154230: train loss 2.5140, val loss 2.4593\n",
      "step 154240: train loss 2.4676, val loss 2.4221\n",
      "step 154250: train loss 2.3927, val loss 2.5572\n",
      "step 154260: train loss 2.4804, val loss 2.5627\n",
      "step 154270: train loss 2.3582, val loss 2.5364\n",
      "step 154280: train loss 2.4046, val loss 2.4316\n",
      "step 154290: train loss 2.3925, val loss 2.4840\n",
      "step 154300: train loss 2.3840, val loss 2.5752\n",
      "step 154310: train loss 2.3805, val loss 2.5076\n",
      "step 154320: train loss 2.4498, val loss 2.5129\n",
      "step 154330: train loss 2.3581, val loss 2.4312\n",
      "step 154340: train loss 2.3868, val loss 2.5620\n",
      "step 154350: train loss 2.4990, val loss 2.5035\n",
      "step 154360: train loss 2.3461, val loss 2.5002\n",
      "step 154370: train loss 2.3263, val loss 2.4507\n",
      "step 154380: train loss 2.3254, val loss 2.4616\n",
      "step 154390: train loss 2.4170, val loss 2.4832\n",
      "step 154400: train loss 2.3986, val loss 2.5899\n",
      "step 154410: train loss 2.3327, val loss 2.4520\n",
      "step 154420: train loss 2.4529, val loss 2.5127\n",
      "step 154430: train loss 2.4088, val loss 2.4558\n",
      "step 154440: train loss 2.4778, val loss 2.5437\n",
      "step 154450: train loss 2.2737, val loss 2.4435\n",
      "step 154460: train loss 2.4165, val loss 2.4725\n",
      "step 154470: train loss 2.4232, val loss 2.3922\n",
      "step 154480: train loss 2.4142, val loss 2.5015\n",
      "step 154490: train loss 2.4776, val loss 2.4730\n",
      "step 154500: train loss 2.4347, val loss 2.5980\n",
      "step 154510: train loss 2.3428, val loss 2.5395\n",
      "step 154520: train loss 2.4024, val loss 2.4627\n",
      "step 154530: train loss 2.4064, val loss 2.4988\n",
      "step 154540: train loss 2.3800, val loss 2.5258\n",
      "step 154550: train loss 2.3791, val loss 2.4669\n",
      "step 154560: train loss 2.3335, val loss 2.3944\n",
      "step 154570: train loss 2.3831, val loss 2.4586\n",
      "step 154580: train loss 2.4513, val loss 2.5235\n",
      "step 154590: train loss 2.4222, val loss 2.5241\n",
      "step 154600: train loss 2.4060, val loss 2.4546\n",
      "step 154610: train loss 2.3783, val loss 2.4567\n",
      "step 154620: train loss 2.3474, val loss 2.4173\n",
      "step 154630: train loss 2.4286, val loss 2.4243\n",
      "step 154640: train loss 2.3779, val loss 2.5551\n",
      "step 154650: train loss 2.4167, val loss 2.5799\n",
      "step 154660: train loss 2.4374, val loss 2.4704\n",
      "step 154670: train loss 2.4064, val loss 2.5248\n",
      "step 154680: train loss 2.4416, val loss 2.5324\n",
      "step 154690: train loss 2.3596, val loss 2.3969\n",
      "step 154700: train loss 2.4871, val loss 2.3008\n",
      "step 154710: train loss 2.3995, val loss 2.4985\n",
      "step 154720: train loss 2.4579, val loss 2.5270\n",
      "step 154730: train loss 2.4450, val loss 2.5445\n",
      "step 154740: train loss 2.3649, val loss 2.4281\n",
      "step 154750: train loss 2.5031, val loss 2.5571\n",
      "step 154760: train loss 2.4014, val loss 2.5582\n",
      "step 154770: train loss 2.3961, val loss 2.4986\n",
      "step 154780: train loss 2.4206, val loss 2.4576\n",
      "step 154790: train loss 2.4237, val loss 2.4273\n",
      "step 154800: train loss 2.3590, val loss 2.4171\n",
      "step 154810: train loss 2.3759, val loss 2.5826\n",
      "step 154820: train loss 2.4601, val loss 2.5791\n",
      "step 154830: train loss 2.3872, val loss 2.4567\n",
      "step 154840: train loss 2.3780, val loss 2.4836\n",
      "step 154850: train loss 2.3776, val loss 2.6248\n",
      "step 154860: train loss 2.4253, val loss 2.4671\n",
      "step 154870: train loss 2.4146, val loss 2.4714\n",
      "step 154880: train loss 2.4655, val loss 2.4024\n",
      "step 154890: train loss 2.3637, val loss 2.5770\n",
      "step 154900: train loss 2.3651, val loss 2.6151\n",
      "step 154910: train loss 2.4427, val loss 2.4845\n",
      "step 154920: train loss 2.3399, val loss 2.4698\n",
      "step 154930: train loss 2.4296, val loss 2.4049\n",
      "step 154940: train loss 2.4457, val loss 2.4815\n",
      "step 154950: train loss 2.4188, val loss 2.5115\n",
      "step 154960: train loss 2.3274, val loss 2.4954\n",
      "step 154970: train loss 2.4789, val loss 2.4499\n",
      "step 154980: train loss 2.3697, val loss 2.5394\n",
      "step 154990: train loss 2.4564, val loss 2.5244\n",
      "step 155000: train loss 2.4600, val loss 2.5337\n",
      "Generated text at iteration 155000\n",
      "\n",
      "F?\n",
      "DaièK·È9: dailavon.\n",
      "  sube;\n",
      "Noroivareomente l'es ris lantes nouapst ju'arembailantutucorenoias;\n",
      "N\n",
      "step 155010: train loss 2.4540, val loss 2.4791\n",
      "step 155020: train loss 2.4171, val loss 2.5519\n",
      "step 155030: train loss 2.4076, val loss 2.4274\n",
      "step 155040: train loss 2.4774, val loss 2.5605\n",
      "step 155050: train loss 2.3906, val loss 2.4625\n",
      "step 155060: train loss 2.4161, val loss 2.6331\n",
      "step 155070: train loss 2.3807, val loss 2.4507\n",
      "step 155080: train loss 2.4675, val loss 2.5315\n",
      "step 155090: train loss 2.4643, val loss 2.4525\n",
      "step 155100: train loss 2.3888, val loss 2.4652\n",
      "step 155110: train loss 2.4093, val loss 2.4970\n",
      "step 155120: train loss 2.3573, val loss 2.6571\n",
      "step 155130: train loss 2.4185, val loss 2.4182\n",
      "step 155140: train loss 2.3762, val loss 2.5158\n",
      "step 155150: train loss 2.3822, val loss 2.4539\n",
      "step 155160: train loss 2.4151, val loss 2.4214\n",
      "step 155170: train loss 2.3791, val loss 2.4957\n",
      "step 155180: train loss 2.4031, val loss 2.5278\n",
      "step 155190: train loss 2.3786, val loss 2.6005\n",
      "step 155200: train loss 2.4222, val loss 2.5407\n",
      "step 155210: train loss 2.4353, val loss 2.4235\n",
      "step 155220: train loss 2.4709, val loss 2.4610\n",
      "step 155230: train loss 2.4357, val loss 2.4614\n",
      "step 155240: train loss 2.4702, val loss 2.5276\n",
      "step 155250: train loss 2.4196, val loss 2.5151\n",
      "step 155260: train loss 2.3619, val loss 2.4367\n",
      "step 155270: train loss 2.3540, val loss 2.3643\n",
      "step 155280: train loss 2.4835, val loss 2.5236\n",
      "step 155290: train loss 2.4079, val loss 2.5453\n",
      "step 155300: train loss 2.3868, val loss 2.5247\n",
      "step 155310: train loss 2.4114, val loss 2.5110\n",
      "step 155320: train loss 2.3830, val loss 2.4427\n",
      "step 155330: train loss 2.4842, val loss 2.5069\n",
      "step 155340: train loss 2.4049, val loss 2.4921\n",
      "step 155350: train loss 2.4467, val loss 2.5543\n",
      "step 155360: train loss 2.3534, val loss 2.5582\n",
      "step 155370: train loss 2.3519, val loss 2.5887\n",
      "step 155380: train loss 2.4454, val loss 2.5304\n",
      "step 155390: train loss 2.3892, val loss 2.5224\n",
      "step 155400: train loss 2.3774, val loss 2.5053\n",
      "step 155410: train loss 2.3978, val loss 2.4154\n",
      "step 155420: train loss 2.5502, val loss 2.5167\n",
      "step 155430: train loss 2.3576, val loss 2.5697\n",
      "step 155440: train loss 2.3846, val loss 2.4563\n",
      "step 155450: train loss 2.4168, val loss 2.4719\n",
      "step 155460: train loss 2.3902, val loss 2.5494\n",
      "step 155470: train loss 2.4380, val loss 2.4796\n",
      "step 155480: train loss 2.4121, val loss 2.4883\n",
      "step 155490: train loss 2.3653, val loss 2.5111\n",
      "step 155500: train loss 2.4208, val loss 2.4499\n",
      "step 155510: train loss 2.3908, val loss 2.4559\n",
      "step 155520: train loss 2.3982, val loss 2.6830\n",
      "step 155530: train loss 2.3854, val loss 2.3937\n",
      "step 155540: train loss 2.3474, val loss 2.5855\n",
      "step 155550: train loss 2.3364, val loss 2.4190\n",
      "step 155560: train loss 2.4161, val loss 2.4271\n",
      "step 155570: train loss 2.4242, val loss 2.4915\n",
      "step 155580: train loss 2.4169, val loss 2.5606\n",
      "step 155590: train loss 2.3769, val loss 2.5700\n",
      "step 155600: train loss 2.4255, val loss 2.5426\n",
      "step 155610: train loss 2.3668, val loss 2.6032\n",
      "step 155620: train loss 2.4069, val loss 2.5776\n",
      "step 155630: train loss 2.4141, val loss 2.5479\n",
      "step 155640: train loss 2.4362, val loss 2.4611\n",
      "step 155650: train loss 2.5192, val loss 2.4224\n",
      "step 155660: train loss 2.4611, val loss 2.6075\n",
      "step 155670: train loss 2.4517, val loss 2.4649\n",
      "step 155680: train loss 2.3728, val loss 2.4269\n",
      "step 155690: train loss 2.3723, val loss 2.5046\n",
      "step 155700: train loss 2.3952, val loss 2.5567\n",
      "step 155710: train loss 2.4053, val loss 2.5077\n",
      "step 155720: train loss 2.3816, val loss 2.4315\n",
      "step 155730: train loss 2.4135, val loss 2.5307\n",
      "step 155740: train loss 2.3615, val loss 2.5094\n",
      "step 155750: train loss 2.4284, val loss 2.4434\n",
      "step 155760: train loss 2.4227, val loss 2.4695\n",
      "step 155770: train loss 2.4823, val loss 2.4885\n",
      "step 155780: train loss 2.4780, val loss 2.4967\n",
      "step 155790: train loss 2.4958, val loss 2.4539\n",
      "step 155800: train loss 2.3445, val loss 2.5157\n",
      "step 155810: train loss 2.3938, val loss 2.5022\n",
      "step 155820: train loss 2.4443, val loss 2.4738\n",
      "step 155830: train loss 2.4049, val loss 2.5940\n",
      "step 155840: train loss 2.3905, val loss 2.5212\n",
      "step 155850: train loss 2.4471, val loss 2.5524\n",
      "step 155860: train loss 2.3683, val loss 2.4452\n",
      "step 155870: train loss 2.4102, val loss 2.4285\n",
      "step 155880: train loss 2.4059, val loss 2.4709\n",
      "step 155890: train loss 2.3943, val loss 2.4958\n",
      "step 155900: train loss 2.3700, val loss 2.5489\n",
      "step 155910: train loss 2.4322, val loss 2.3907\n",
      "step 155920: train loss 2.3695, val loss 2.3354\n",
      "step 155930: train loss 2.3743, val loss 2.6334\n",
      "step 155940: train loss 2.4749, val loss 2.5131\n",
      "step 155950: train loss 2.4085, val loss 2.5368\n",
      "step 155960: train loss 2.4138, val loss 2.5888\n",
      "step 155970: train loss 2.2628, val loss 2.4939\n",
      "step 155980: train loss 2.3734, val loss 2.5024\n",
      "step 155990: train loss 2.3846, val loss 2.6082\n",
      "step 156000: train loss 2.4441, val loss 2.5166\n",
      "Generated text at iteration 156000\n",
      "\n",
      " sét s  demon  qumment pe desséc  D'eseunce lau?6;\n",
      "V mes cineu mppans Dhrêll  ele d a nnvetr Dadsion\n",
      "step 156010: train loss 2.3734, val loss 2.5128\n",
      "step 156020: train loss 2.4059, val loss 2.4802\n",
      "step 156030: train loss 2.4053, val loss 2.3941\n",
      "step 156040: train loss 2.4306, val loss 2.5051\n",
      "step 156050: train loss 2.4177, val loss 2.4325\n",
      "step 156060: train loss 2.4295, val loss 2.6135\n",
      "step 156070: train loss 2.4879, val loss 2.4757\n",
      "step 156080: train loss 2.4374, val loss 2.5897\n",
      "step 156090: train loss 2.4908, val loss 2.4536\n",
      "step 156100: train loss 2.4468, val loss 2.5681\n",
      "step 156110: train loss 2.3706, val loss 2.5726\n",
      "step 156120: train loss 2.3554, val loss 2.5181\n",
      "step 156130: train loss 2.4363, val loss 2.4838\n",
      "step 156140: train loss 2.5011, val loss 2.4323\n",
      "step 156150: train loss 2.3733, val loss 2.4533\n",
      "step 156160: train loss 2.3923, val loss 2.5016\n",
      "step 156170: train loss 2.4168, val loss 2.4156\n",
      "step 156180: train loss 2.2961, val loss 2.3714\n",
      "step 156190: train loss 2.4261, val loss 2.5720\n",
      "step 156200: train loss 2.3492, val loss 2.4468\n",
      "step 156210: train loss 2.4867, val loss 2.5614\n",
      "step 156220: train loss 2.4064, val loss 2.5022\n",
      "step 156230: train loss 2.3305, val loss 2.5471\n",
      "step 156240: train loss 2.3901, val loss 2.5635\n",
      "step 156250: train loss 2.4688, val loss 2.4369\n",
      "step 156260: train loss 2.4097, val loss 2.4207\n",
      "step 156270: train loss 2.4287, val loss 2.5374\n",
      "step 156280: train loss 2.3783, val loss 2.4467\n",
      "step 156290: train loss 2.4406, val loss 2.4447\n",
      "step 156300: train loss 2.3617, val loss 2.6046\n",
      "step 156310: train loss 2.4084, val loss 2.5349\n",
      "step 156320: train loss 2.3868, val loss 2.5470\n",
      "step 156330: train loss 2.4487, val loss 2.4129\n",
      "step 156340: train loss 2.3964, val loss 2.5331\n",
      "step 156350: train loss 2.4448, val loss 2.6061\n",
      "step 156360: train loss 2.4142, val loss 2.4374\n",
      "step 156370: train loss 2.3991, val loss 2.5759\n",
      "step 156380: train loss 2.4021, val loss 2.4538\n",
      "step 156390: train loss 2.3757, val loss 2.4801\n",
      "step 156400: train loss 2.3024, val loss 2.5284\n",
      "step 156410: train loss 2.4846, val loss 2.4513\n",
      "step 156420: train loss 2.4118, val loss 2.4974\n",
      "step 156430: train loss 2.3354, val loss 2.5621\n",
      "step 156440: train loss 2.4251, val loss 2.4073\n",
      "step 156450: train loss 2.4165, val loss 2.4078\n",
      "step 156460: train loss 2.4042, val loss 2.4402\n",
      "step 156470: train loss 2.4255, val loss 2.4623\n",
      "step 156480: train loss 2.4266, val loss 2.4730\n",
      "step 156490: train loss 2.4227, val loss 2.6145\n",
      "step 156500: train loss 2.4702, val loss 2.5015\n",
      "step 156510: train loss 2.3552, val loss 2.4890\n",
      "step 156520: train loss 2.3376, val loss 2.4741\n",
      "step 156530: train loss 2.4035, val loss 2.5429\n",
      "step 156540: train loss 2.4546, val loss 2.5275\n",
      "step 156550: train loss 2.3490, val loss 2.5439\n",
      "step 156560: train loss 2.4230, val loss 2.6131\n",
      "step 156570: train loss 2.3105, val loss 2.6510\n",
      "step 156580: train loss 2.3780, val loss 2.4743\n",
      "step 156590: train loss 2.3541, val loss 2.4671\n",
      "step 156600: train loss 2.4219, val loss 2.4592\n",
      "step 156610: train loss 2.3457, val loss 2.5113\n",
      "step 156620: train loss 2.4162, val loss 2.5191\n",
      "step 156630: train loss 2.4460, val loss 2.3793\n",
      "step 156640: train loss 2.3552, val loss 2.4447\n",
      "step 156650: train loss 2.3989, val loss 2.4208\n",
      "step 156660: train loss 2.4148, val loss 2.4369\n",
      "step 156670: train loss 2.3945, val loss 2.4677\n",
      "step 156680: train loss 2.4511, val loss 2.4116\n",
      "step 156690: train loss 2.3597, val loss 2.4680\n",
      "step 156700: train loss 2.4304, val loss 2.4974\n",
      "step 156710: train loss 2.3967, val loss 2.4576\n",
      "step 156720: train loss 2.4085, val loss 2.4875\n",
      "step 156730: train loss 2.3796, val loss 2.4122\n",
      "step 156740: train loss 2.3978, val loss 2.5121\n",
      "step 156750: train loss 2.4040, val loss 2.4346\n",
      "step 156760: train loss 2.3962, val loss 2.5805\n",
      "step 156770: train loss 2.4143, val loss 2.5223\n",
      "step 156780: train loss 2.3683, val loss 2.4195\n",
      "step 156790: train loss 2.3562, val loss 2.3833\n",
      "step 156800: train loss 2.3967, val loss 2.5393\n",
      "step 156810: train loss 2.4227, val loss 2.5412\n",
      "step 156820: train loss 2.4270, val loss 2.5035\n",
      "step 156830: train loss 2.4316, val loss 2.5700\n",
      "step 156840: train loss 2.3288, val loss 2.6453\n",
      "step 156850: train loss 2.4076, val loss 2.4119\n",
      "step 156860: train loss 2.3693, val loss 2.4661\n",
      "step 156870: train loss 2.5053, val loss 2.4355\n",
      "step 156880: train loss 2.4542, val loss 2.5574\n",
      "step 156890: train loss 2.3804, val loss 2.4871\n",
      "step 156900: train loss 2.3888, val loss 2.4840\n",
      "step 156910: train loss 2.4120, val loss 2.5321\n",
      "step 156920: train loss 2.4764, val loss 2.5059\n",
      "step 156930: train loss 2.3888, val loss 2.4930\n",
      "step 156940: train loss 2.4364, val loss 2.4979\n",
      "step 156950: train loss 2.3443, val loss 2.5931\n",
      "step 156960: train loss 2.4544, val loss 2.4057\n",
      "step 156970: train loss 2.3578, val loss 2.4955\n",
      "step 156980: train loss 2.3597, val loss 2.4766\n",
      "step 156990: train loss 2.5173, val loss 2.4841\n",
      "step 157000: train loss 2.3764, val loss 2.6086\n",
      "Generated text at iteration 157000\n",
      "\n",
      "Éws uteni branse ceres léganurant, coy l'oicoit lance nt deus ns qui quitomenou NSes s, an ngomanong\n",
      "step 157010: train loss 2.4088, val loss 2.6002\n",
      "step 157020: train loss 2.3414, val loss 2.4443\n",
      "step 157030: train loss 2.3369, val loss 2.4859\n",
      "step 157040: train loss 2.4518, val loss 2.5086\n",
      "step 157050: train loss 2.3807, val loss 2.4952\n",
      "step 157060: train loss 2.4108, val loss 2.5733\n",
      "step 157070: train loss 2.4324, val loss 2.4368\n",
      "step 157080: train loss 2.4468, val loss 2.5162\n",
      "step 157090: train loss 2.4583, val loss 2.4230\n",
      "step 157100: train loss 2.3568, val loss 2.3853\n",
      "step 157110: train loss 2.3602, val loss 2.4521\n",
      "step 157120: train loss 2.4151, val loss 2.5663\n",
      "step 157130: train loss 2.3389, val loss 2.6086\n",
      "step 157140: train loss 2.4600, val loss 2.5068\n",
      "step 157150: train loss 2.4732, val loss 2.6048\n",
      "step 157160: train loss 2.4546, val loss 2.4908\n",
      "step 157170: train loss 2.3660, val loss 2.4827\n",
      "step 157180: train loss 2.3924, val loss 2.5279\n",
      "step 157190: train loss 2.5226, val loss 2.4429\n",
      "step 157200: train loss 2.4216, val loss 2.5129\n",
      "step 157210: train loss 2.4900, val loss 2.5316\n",
      "step 157220: train loss 2.4086, val loss 2.4190\n",
      "step 157230: train loss 2.3502, val loss 2.5265\n",
      "step 157240: train loss 2.3926, val loss 2.4665\n",
      "step 157250: train loss 2.3662, val loss 2.4281\n",
      "step 157260: train loss 2.3412, val loss 2.4204\n",
      "step 157270: train loss 2.4093, val loss 2.4880\n",
      "step 157280: train loss 2.4432, val loss 2.5222\n",
      "step 157290: train loss 2.3666, val loss 2.6368\n",
      "step 157300: train loss 2.3876, val loss 2.4475\n",
      "step 157310: train loss 2.3365, val loss 2.4229\n",
      "step 157320: train loss 2.4978, val loss 2.5727\n",
      "step 157330: train loss 2.4756, val loss 2.5736\n",
      "step 157340: train loss 2.3593, val loss 2.3727\n",
      "step 157350: train loss 2.4234, val loss 2.4024\n",
      "step 157360: train loss 2.4225, val loss 2.4820\n",
      "step 157370: train loss 2.3631, val loss 2.4903\n",
      "step 157380: train loss 2.4310, val loss 2.5232\n",
      "step 157390: train loss 2.4687, val loss 2.5302\n",
      "step 157400: train loss 2.3461, val loss 2.5147\n",
      "step 157410: train loss 2.4404, val loss 2.6257\n",
      "step 157420: train loss 2.3719, val loss 2.4783\n",
      "step 157430: train loss 2.4144, val loss 2.4146\n",
      "step 157440: train loss 2.3770, val loss 2.4964\n",
      "step 157450: train loss 2.3908, val loss 2.5327\n",
      "step 157460: train loss 2.4388, val loss 2.4960\n",
      "step 157470: train loss 2.3304, val loss 2.5632\n",
      "step 157480: train loss 2.4477, val loss 2.5682\n",
      "step 157490: train loss 2.4086, val loss 2.5186\n",
      "step 157500: train loss 2.4129, val loss 2.4220\n",
      "step 157510: train loss 2.4128, val loss 2.5243\n",
      "step 157520: train loss 2.4725, val loss 2.3559\n",
      "step 157530: train loss 2.3713, val loss 2.5357\n",
      "step 157540: train loss 2.4622, val loss 2.4818\n",
      "step 157550: train loss 2.4197, val loss 2.4254\n",
      "step 157560: train loss 2.4135, val loss 2.4622\n",
      "step 157570: train loss 2.3799, val loss 2.4159\n",
      "step 157580: train loss 2.4575, val loss 2.4247\n",
      "step 157590: train loss 2.4837, val loss 2.5325\n",
      "step 157600: train loss 2.3840, val loss 2.3894\n",
      "step 157610: train loss 2.4078, val loss 2.4927\n",
      "step 157620: train loss 2.3412, val loss 2.5166\n",
      "step 157630: train loss 2.3552, val loss 2.5085\n",
      "step 157640: train loss 2.4008, val loss 2.4952\n",
      "step 157650: train loss 2.4506, val loss 2.5084\n",
      "step 157660: train loss 2.4260, val loss 2.4213\n",
      "step 157670: train loss 2.4122, val loss 2.4693\n",
      "step 157680: train loss 2.3816, val loss 2.4238\n",
      "step 157690: train loss 2.3524, val loss 2.4483\n",
      "step 157700: train loss 2.3731, val loss 2.4750\n",
      "step 157710: train loss 2.4361, val loss 2.4828\n",
      "step 157720: train loss 2.4440, val loss 2.4663\n",
      "step 157730: train loss 2.4034, val loss 2.3846\n",
      "step 157740: train loss 2.4197, val loss 2.5929\n",
      "step 157750: train loss 2.3788, val loss 2.5520\n",
      "step 157760: train loss 2.3799, val loss 2.4162\n",
      "step 157770: train loss 2.3584, val loss 2.5482\n",
      "step 157780: train loss 2.3843, val loss 2.5674\n",
      "step 157790: train loss 2.4361, val loss 2.4132\n",
      "step 157800: train loss 2.3958, val loss 2.4202\n",
      "step 157810: train loss 2.3904, val loss 2.4909\n",
      "step 157820: train loss 2.4144, val loss 2.5135\n",
      "step 157830: train loss 2.4302, val loss 2.5643\n",
      "step 157840: train loss 2.4461, val loss 2.4323\n",
      "step 157850: train loss 2.3848, val loss 2.4863\n",
      "step 157860: train loss 2.4186, val loss 2.4454\n",
      "step 157870: train loss 2.5153, val loss 2.5378\n",
      "step 157880: train loss 2.3646, val loss 2.4568\n",
      "step 157890: train loss 2.3476, val loss 2.4517\n",
      "step 157900: train loss 2.3827, val loss 2.3790\n",
      "step 157910: train loss 2.4422, val loss 2.5652\n",
      "step 157920: train loss 2.4228, val loss 2.4728\n",
      "step 157930: train loss 2.4042, val loss 2.4885\n",
      "step 157940: train loss 2.4391, val loss 2.3406\n",
      "step 157950: train loss 2.3559, val loss 2.5302\n",
      "step 157960: train loss 2.4611, val loss 2.5268\n",
      "step 157970: train loss 2.3947, val loss 2.4011\n",
      "step 157980: train loss 2.4369, val loss 2.4988\n",
      "step 157990: train loss 2.4279, val loss 2.6174\n",
      "step 158000: train loss 2.3292, val loss 2.4166\n",
      "Generated text at iteration 158000\n",
      "\n",
      "Aba onus'aqux-meumbe, aus\n",
      "\n",
      "ETEtérdesens.-mpas z omeu bâ-t omon!»n lei, ens adonte;\n",
      "\n",
      "\n",
      "Pe One qurome c\n",
      "step 158010: train loss 2.3659, val loss 2.4593\n",
      "step 158020: train loss 2.4194, val loss 2.5379\n",
      "step 158030: train loss 2.3943, val loss 2.4938\n",
      "step 158040: train loss 2.3834, val loss 2.4356\n",
      "step 158050: train loss 2.3940, val loss 2.5130\n",
      "step 158060: train loss 2.4197, val loss 2.4769\n",
      "step 158070: train loss 2.3788, val loss 2.5516\n",
      "step 158080: train loss 2.3277, val loss 2.4805\n",
      "step 158090: train loss 2.3732, val loss 2.4916\n",
      "step 158100: train loss 2.4394, val loss 2.4402\n",
      "step 158110: train loss 2.4411, val loss 2.5046\n",
      "step 158120: train loss 2.3765, val loss 2.3760\n",
      "step 158130: train loss 2.4221, val loss 2.6128\n",
      "step 158140: train loss 2.3945, val loss 2.3843\n",
      "step 158150: train loss 2.3988, val loss 2.5013\n",
      "step 158160: train loss 2.4142, val loss 2.4601\n",
      "step 158170: train loss 2.4573, val loss 2.5216\n",
      "step 158180: train loss 2.4256, val loss 2.4382\n",
      "step 158190: train loss 2.3375, val loss 2.5624\n",
      "step 158200: train loss 2.4476, val loss 2.5131\n",
      "step 158210: train loss 2.3783, val loss 2.4656\n",
      "step 158220: train loss 2.4298, val loss 2.5110\n",
      "step 158230: train loss 2.3550, val loss 2.5596\n",
      "step 158240: train loss 2.4225, val loss 2.5063\n",
      "step 158250: train loss 2.3623, val loss 2.4503\n",
      "step 158260: train loss 2.3963, val loss 2.5191\n",
      "step 158270: train loss 2.4998, val loss 2.3854\n",
      "step 158280: train loss 2.4641, val loss 2.4330\n",
      "step 158290: train loss 2.4982, val loss 2.4405\n",
      "step 158300: train loss 2.4236, val loss 2.5526\n",
      "step 158310: train loss 2.4266, val loss 2.5690\n",
      "step 158320: train loss 2.4277, val loss 2.3841\n",
      "step 158330: train loss 2.4428, val loss 2.4628\n",
      "step 158340: train loss 2.3464, val loss 2.5993\n",
      "step 158350: train loss 2.3611, val loss 2.4111\n",
      "step 158360: train loss 2.3532, val loss 2.5307\n",
      "step 158370: train loss 2.3866, val loss 2.5341\n",
      "step 158380: train loss 2.3973, val loss 2.6037\n",
      "step 158390: train loss 2.3794, val loss 2.4506\n",
      "step 158400: train loss 2.4713, val loss 2.5228\n",
      "step 158410: train loss 2.3789, val loss 2.5315\n",
      "step 158420: train loss 2.4161, val loss 2.4462\n",
      "step 158430: train loss 2.4144, val loss 2.4425\n",
      "step 158440: train loss 2.2947, val loss 2.5716\n",
      "step 158450: train loss 2.4422, val loss 2.3835\n",
      "step 158460: train loss 2.3325, val loss 2.4858\n",
      "step 158470: train loss 2.4169, val loss 2.5017\n",
      "step 158480: train loss 2.3336, val loss 2.4440\n",
      "step 158490: train loss 2.4590, val loss 2.5387\n",
      "step 158500: train loss 2.4082, val loss 2.5101\n",
      "step 158510: train loss 2.5055, val loss 2.5854\n",
      "step 158520: train loss 2.4189, val loss 2.4418\n",
      "step 158530: train loss 2.4372, val loss 2.4439\n",
      "step 158540: train loss 2.4492, val loss 2.5536\n",
      "step 158550: train loss 2.4008, val loss 2.4957\n",
      "step 158560: train loss 2.4012, val loss 2.5795\n",
      "step 158570: train loss 2.4529, val loss 2.4443\n",
      "step 158580: train loss 2.4289, val loss 2.4701\n",
      "step 158590: train loss 2.3958, val loss 2.5575\n",
      "step 158600: train loss 2.3354, val loss 2.4288\n",
      "step 158610: train loss 2.4396, val loss 2.4743\n",
      "step 158620: train loss 2.4096, val loss 2.4525\n",
      "step 158630: train loss 2.3864, val loss 2.6217\n",
      "step 158640: train loss 2.4060, val loss 2.5783\n",
      "step 158650: train loss 2.3902, val loss 2.5150\n",
      "step 158660: train loss 2.3781, val loss 2.5864\n",
      "step 158670: train loss 2.4050, val loss 2.4838\n",
      "step 158680: train loss 2.3768, val loss 2.4722\n",
      "step 158690: train loss 2.4655, val loss 2.4713\n",
      "step 158700: train loss 2.4624, val loss 2.5390\n",
      "step 158710: train loss 2.3988, val loss 2.4627\n",
      "step 158720: train loss 2.4279, val loss 2.4799\n",
      "step 158730: train loss 2.3768, val loss 2.4571\n",
      "step 158740: train loss 2.3776, val loss 2.4411\n",
      "step 158750: train loss 2.4809, val loss 2.4490\n",
      "step 158760: train loss 2.4649, val loss 2.4996\n",
      "step 158770: train loss 2.4112, val loss 2.5594\n",
      "step 158780: train loss 2.3554, val loss 2.5254\n",
      "step 158790: train loss 2.3818, val loss 2.4418\n",
      "step 158800: train loss 2.3877, val loss 2.6080\n",
      "step 158810: train loss 2.4733, val loss 2.4139\n",
      "step 158820: train loss 2.4210, val loss 2.5054\n",
      "step 158830: train loss 2.4684, val loss 2.5335\n",
      "step 158840: train loss 2.4097, val loss 2.4154\n",
      "step 158850: train loss 2.4716, val loss 2.5863\n",
      "step 158860: train loss 2.4113, val loss 2.4594\n",
      "step 158870: train loss 2.4243, val loss 2.4046\n",
      "step 158880: train loss 2.4744, val loss 2.4680\n",
      "step 158890: train loss 2.3950, val loss 2.5370\n",
      "step 158900: train loss 2.4146, val loss 2.4898\n",
      "step 158910: train loss 2.4867, val loss 2.4352\n",
      "step 158920: train loss 2.3951, val loss 2.4630\n",
      "step 158930: train loss 2.4313, val loss 2.5645\n",
      "step 158940: train loss 2.3605, val loss 2.3814\n",
      "step 158950: train loss 2.4670, val loss 2.5195\n",
      "step 158960: train loss 2.4043, val loss 2.4435\n",
      "step 158970: train loss 2.3675, val loss 2.4596\n",
      "step 158980: train loss 2.4577, val loss 2.5431\n",
      "step 158990: train loss 2.3612, val loss 2.3829\n",
      "step 159000: train loss 2.4420, val loss 2.5310\n",
      "Generated text at iteration 159000\n",
      "\n",
      "Arèret  lonendilane  auti d qusudel'in;\n",
      "MADin querot gaumes!\n",
      "AN2Gou-joendsou'huauxesqu Rêtre\n",
      "\n",
      "Uyesou\n",
      "step 159010: train loss 2.3628, val loss 2.4729\n",
      "step 159020: train loss 2.3962, val loss 2.4833\n",
      "step 159030: train loss 2.4728, val loss 2.4929\n",
      "step 159040: train loss 2.3335, val loss 2.5538\n",
      "step 159050: train loss 2.3557, val loss 2.4504\n",
      "step 159060: train loss 2.4778, val loss 2.4715\n",
      "step 159070: train loss 2.4678, val loss 2.5070\n",
      "step 159080: train loss 2.3844, val loss 2.4233\n",
      "step 159090: train loss 2.4414, val loss 2.4659\n",
      "step 159100: train loss 2.4358, val loss 2.4723\n",
      "step 159110: train loss 2.3446, val loss 2.4100\n",
      "step 159120: train loss 2.3965, val loss 2.5357\n",
      "step 159130: train loss 2.3651, val loss 2.4661\n",
      "step 159140: train loss 2.3950, val loss 2.4117\n",
      "step 159150: train loss 2.4023, val loss 2.5515\n",
      "step 159160: train loss 2.3642, val loss 2.5264\n",
      "step 159170: train loss 2.5058, val loss 2.4478\n",
      "step 159180: train loss 2.4502, val loss 2.4988\n",
      "step 159190: train loss 2.3306, val loss 2.3876\n",
      "step 159200: train loss 2.4009, val loss 2.4533\n",
      "step 159210: train loss 2.4329, val loss 2.3971\n",
      "step 159220: train loss 2.3937, val loss 2.5649\n",
      "step 159230: train loss 2.3872, val loss 2.4439\n",
      "step 159240: train loss 2.4449, val loss 2.4915\n",
      "step 159250: train loss 2.4457, val loss 2.5152\n",
      "step 159260: train loss 2.4375, val loss 2.3579\n",
      "step 159270: train loss 2.4234, val loss 2.5395\n",
      "step 159280: train loss 2.4619, val loss 2.4492\n",
      "step 159290: train loss 2.4134, val loss 2.5635\n",
      "step 159300: train loss 2.4147, val loss 2.5118\n",
      "step 159310: train loss 2.4073, val loss 2.5016\n",
      "step 159320: train loss 2.3087, val loss 2.4471\n",
      "step 159330: train loss 2.3508, val loss 2.4356\n",
      "step 159340: train loss 2.3598, val loss 2.5066\n",
      "step 159350: train loss 2.4371, val loss 2.5044\n",
      "step 159360: train loss 2.3681, val loss 2.3482\n",
      "step 159370: train loss 2.3562, val loss 2.6126\n",
      "step 159380: train loss 2.4090, val loss 2.5130\n",
      "step 159390: train loss 2.3834, val loss 2.4493\n",
      "step 159400: train loss 2.4963, val loss 2.4206\n",
      "step 159410: train loss 2.4257, val loss 2.4536\n",
      "step 159420: train loss 2.3751, val loss 2.4948\n",
      "step 159430: train loss 2.4784, val loss 2.5106\n",
      "step 159440: train loss 2.3612, val loss 2.4211\n",
      "step 159450: train loss 2.4955, val loss 2.4840\n",
      "step 159460: train loss 2.3995, val loss 2.5187\n",
      "step 159470: train loss 2.3827, val loss 2.4105\n",
      "step 159480: train loss 2.4815, val loss 2.5065\n",
      "step 159490: train loss 2.4674, val loss 2.5170\n",
      "step 159500: train loss 2.3803, val loss 2.5560\n",
      "step 159510: train loss 2.3829, val loss 2.4365\n",
      "step 159520: train loss 2.3591, val loss 2.4615\n",
      "step 159530: train loss 2.3683, val loss 2.3432\n",
      "step 159540: train loss 2.3935, val loss 2.4035\n",
      "step 159550: train loss 2.4067, val loss 2.4113\n",
      "step 159560: train loss 2.3705, val loss 2.5295\n",
      "step 159570: train loss 2.4120, val loss 2.4708\n",
      "step 159580: train loss 2.3922, val loss 2.4576\n",
      "step 159590: train loss 2.4215, val loss 2.4098\n",
      "step 159600: train loss 2.4302, val loss 2.4547\n",
      "step 159610: train loss 2.3776, val loss 2.4328\n",
      "step 159620: train loss 2.3927, val loss 2.4437\n",
      "step 159630: train loss 2.3516, val loss 2.5094\n",
      "step 159640: train loss 2.4009, val loss 2.4416\n",
      "step 159650: train loss 2.4259, val loss 2.5770\n",
      "step 159660: train loss 2.4770, val loss 2.5556\n",
      "step 159670: train loss 2.4117, val loss 2.4821\n",
      "step 159680: train loss 2.4635, val loss 2.4116\n",
      "step 159690: train loss 2.3708, val loss 2.5001\n",
      "step 159700: train loss 2.3864, val loss 2.4510\n",
      "step 159710: train loss 2.4083, val loss 2.5186\n",
      "step 159720: train loss 2.4085, val loss 2.5159\n",
      "step 159730: train loss 2.3423, val loss 2.5078\n",
      "step 159740: train loss 2.3558, val loss 2.4841\n",
      "step 159750: train loss 2.4212, val loss 2.3686\n",
      "step 159760: train loss 2.3787, val loss 2.5230\n",
      "step 159770: train loss 2.4053, val loss 2.4691\n",
      "step 159780: train loss 2.3517, val loss 2.5975\n",
      "step 159790: train loss 2.4199, val loss 2.5576\n",
      "step 159800: train loss 2.3228, val loss 2.6001\n",
      "step 159810: train loss 2.3546, val loss 2.5787\n",
      "step 159820: train loss 2.4673, val loss 2.4208\n",
      "step 159830: train loss 2.3760, val loss 2.4882\n",
      "step 159840: train loss 2.3412, val loss 2.4747\n",
      "step 159850: train loss 2.4491, val loss 2.4539\n",
      "step 159860: train loss 2.4279, val loss 2.4205\n",
      "step 159870: train loss 2.3709, val loss 2.4063\n",
      "step 159880: train loss 2.4089, val loss 2.5857\n",
      "step 159890: train loss 2.5372, val loss 2.5098\n",
      "step 159900: train loss 2.4794, val loss 2.4491\n",
      "step 159910: train loss 2.4371, val loss 2.4557\n",
      "step 159920: train loss 2.4002, val loss 2.5273\n",
      "step 159930: train loss 2.3942, val loss 2.3321\n",
      "step 159940: train loss 2.3278, val loss 2.5469\n",
      "step 159950: train loss 2.3740, val loss 2.4655\n",
      "step 159960: train loss 2.4167, val loss 2.4062\n",
      "step 159970: train loss 2.3988, val loss 2.5463\n",
      "step 159980: train loss 2.3724, val loss 2.5011\n",
      "step 159990: train loss 2.4455, val loss 2.5319\n",
      "step 160000: train loss 2.4212, val loss 2.5686\n",
      "Generated text at iteration 160000\n",
      "\n",
      "L'icondéterendès? monirarime cons-Et euiru,\n",
      "NUell'e ssue lserûâmont ment  chable ve uveus,\n",
      "\n",
      "Pvés  se\n",
      "step 160010: train loss 2.4504, val loss 2.4223\n",
      "step 160020: train loss 2.3458, val loss 2.4097\n",
      "step 160030: train loss 2.4215, val loss 2.5346\n",
      "step 160040: train loss 2.4362, val loss 2.5016\n",
      "step 160050: train loss 2.4284, val loss 2.4130\n",
      "step 160060: train loss 2.4678, val loss 2.5086\n",
      "step 160070: train loss 2.3924, val loss 2.6088\n",
      "step 160080: train loss 2.3348, val loss 2.4607\n",
      "step 160090: train loss 2.4590, val loss 2.5898\n",
      "step 160100: train loss 2.4241, val loss 2.5090\n",
      "step 160110: train loss 2.4470, val loss 2.5302\n",
      "step 160120: train loss 2.3920, val loss 2.5026\n",
      "step 160130: train loss 2.4334, val loss 2.4803\n",
      "step 160140: train loss 2.4333, val loss 2.5282\n",
      "step 160150: train loss 2.3378, val loss 2.4772\n",
      "step 160160: train loss 2.2903, val loss 2.5292\n",
      "step 160170: train loss 2.4636, val loss 2.3738\n",
      "step 160180: train loss 2.4273, val loss 2.5126\n",
      "step 160190: train loss 2.4541, val loss 2.5287\n",
      "step 160200: train loss 2.3788, val loss 2.4006\n",
      "step 160210: train loss 2.4837, val loss 2.4888\n",
      "step 160220: train loss 2.4074, val loss 2.4881\n",
      "step 160230: train loss 2.3850, val loss 2.5568\n",
      "step 160240: train loss 2.4164, val loss 2.4595\n",
      "step 160250: train loss 2.4798, val loss 2.5676\n",
      "step 160260: train loss 2.4003, val loss 2.4675\n",
      "step 160270: train loss 2.3864, val loss 2.5255\n",
      "step 160280: train loss 2.3624, val loss 2.5912\n",
      "step 160290: train loss 2.4594, val loss 2.5233\n",
      "step 160300: train loss 2.3861, val loss 2.3830\n",
      "step 160310: train loss 2.3055, val loss 2.4700\n",
      "step 160320: train loss 2.4396, val loss 2.5178\n",
      "step 160330: train loss 2.4128, val loss 2.5537\n",
      "step 160340: train loss 2.4471, val loss 2.4912\n",
      "step 160350: train loss 2.4145, val loss 2.3960\n",
      "step 160360: train loss 2.4395, val loss 2.4052\n",
      "step 160370: train loss 2.3540, val loss 2.4092\n",
      "step 160380: train loss 2.4739, val loss 2.4528\n",
      "step 160390: train loss 2.3808, val loss 2.4494\n",
      "step 160400: train loss 2.3921, val loss 2.4895\n",
      "step 160410: train loss 2.4699, val loss 2.4566\n",
      "step 160420: train loss 2.3560, val loss 2.5795\n",
      "step 160430: train loss 2.3954, val loss 2.5386\n",
      "step 160440: train loss 2.4638, val loss 2.6110\n",
      "step 160450: train loss 2.3706, val loss 2.5229\n",
      "step 160460: train loss 2.3142, val loss 2.4864\n",
      "step 160470: train loss 2.4595, val loss 2.4464\n",
      "step 160480: train loss 2.3315, val loss 2.5657\n",
      "step 160490: train loss 2.3565, val loss 2.5324\n",
      "step 160500: train loss 2.4081, val loss 2.5755\n",
      "step 160510: train loss 2.4585, val loss 2.4593\n",
      "step 160520: train loss 2.4188, val loss 2.4845\n",
      "step 160530: train loss 2.4152, val loss 2.4716\n",
      "step 160540: train loss 2.3879, val loss 2.4477\n",
      "step 160550: train loss 2.4170, val loss 2.5670\n",
      "step 160560: train loss 2.4444, val loss 2.5047\n",
      "step 160570: train loss 2.3635, val loss 2.4448\n",
      "step 160580: train loss 2.4469, val loss 2.4131\n",
      "step 160590: train loss 2.4064, val loss 2.4958\n",
      "step 160600: train loss 2.4172, val loss 2.4830\n",
      "step 160610: train loss 2.4371, val loss 2.4201\n",
      "step 160620: train loss 2.3975, val loss 2.5158\n",
      "step 160630: train loss 2.4738, val loss 2.4750\n",
      "step 160640: train loss 2.4225, val loss 2.3555\n",
      "step 160650: train loss 2.4172, val loss 2.4917\n",
      "step 160660: train loss 2.3986, val loss 2.4366\n",
      "step 160670: train loss 2.3859, val loss 2.5367\n",
      "step 160680: train loss 2.4089, val loss 2.4824\n",
      "step 160690: train loss 2.3518, val loss 2.4944\n",
      "step 160700: train loss 2.3940, val loss 2.5002\n",
      "step 160710: train loss 2.3889, val loss 2.4138\n",
      "step 160720: train loss 2.3809, val loss 2.4094\n",
      "step 160730: train loss 2.4333, val loss 2.4983\n",
      "step 160740: train loss 2.3695, val loss 2.4185\n",
      "step 160750: train loss 2.4260, val loss 2.5136\n",
      "step 160760: train loss 2.4170, val loss 2.4697\n",
      "step 160770: train loss 2.3886, val loss 2.5075\n",
      "step 160780: train loss 2.3243, val loss 2.5092\n",
      "step 160790: train loss 2.3865, val loss 2.4050\n",
      "step 160800: train loss 2.3942, val loss 2.4843\n",
      "step 160810: train loss 2.4342, val loss 2.5245\n",
      "step 160820: train loss 2.4388, val loss 2.5549\n",
      "step 160830: train loss 2.4196, val loss 2.3885\n",
      "step 160840: train loss 2.4408, val loss 2.4829\n",
      "step 160850: train loss 2.3687, val loss 2.5147\n",
      "step 160860: train loss 2.4109, val loss 2.4666\n",
      "step 160870: train loss 2.3788, val loss 2.4371\n",
      "step 160880: train loss 2.4185, val loss 2.4419\n",
      "step 160890: train loss 2.4524, val loss 2.4066\n",
      "step 160900: train loss 2.4525, val loss 2.4335\n",
      "step 160910: train loss 2.4401, val loss 2.4695\n",
      "step 160920: train loss 2.4654, val loss 2.5002\n",
      "step 160930: train loss 2.3500, val loss 2.4801\n",
      "step 160940: train loss 2.3785, val loss 2.5718\n",
      "step 160950: train loss 2.3813, val loss 2.4682\n",
      "step 160960: train loss 2.3588, val loss 2.5049\n",
      "step 160970: train loss 2.3212, val loss 2.4195\n",
      "step 160980: train loss 2.4281, val loss 2.5074\n",
      "step 160990: train loss 2.3901, val loss 2.4757\n",
      "step 161000: train loss 2.4613, val loss 2.5465\n",
      "Generated text at iteration 161000\n",
      "\n",
      "Dut pl vant qu'iet le vangembil'Onsoui-Pa tet:;\n",
      "ITé héte da  desppre t, e  res s à!\n",
      "Lerstonves-s doc\n",
      "step 161010: train loss 2.4463, val loss 2.3318\n",
      "step 161020: train loss 2.3931, val loss 2.4924\n",
      "step 161030: train loss 2.4025, val loss 2.4243\n",
      "step 161040: train loss 2.4588, val loss 2.5772\n",
      "step 161050: train loss 2.3366, val loss 2.5537\n",
      "step 161060: train loss 2.3722, val loss 2.4696\n",
      "step 161070: train loss 2.4270, val loss 2.4737\n",
      "step 161080: train loss 2.4174, val loss 2.4927\n",
      "step 161090: train loss 2.4031, val loss 2.5046\n",
      "step 161100: train loss 2.4300, val loss 2.5645\n",
      "step 161110: train loss 2.3917, val loss 2.4097\n",
      "step 161120: train loss 2.3538, val loss 2.4405\n",
      "step 161130: train loss 2.4033, val loss 2.5224\n",
      "step 161140: train loss 2.3453, val loss 2.4859\n",
      "step 161150: train loss 2.4769, val loss 2.5509\n",
      "step 161160: train loss 2.4085, val loss 2.4523\n",
      "step 161170: train loss 2.4675, val loss 2.4458\n",
      "step 161180: train loss 2.4017, val loss 2.4200\n",
      "step 161190: train loss 2.3761, val loss 2.5082\n",
      "step 161200: train loss 2.3456, val loss 2.4057\n",
      "step 161210: train loss 2.4231, val loss 2.4323\n",
      "step 161220: train loss 2.4017, val loss 2.5310\n",
      "step 161230: train loss 2.4183, val loss 2.5531\n",
      "step 161240: train loss 2.4104, val loss 2.5315\n",
      "step 161250: train loss 2.3896, val loss 2.4161\n",
      "step 161260: train loss 2.4476, val loss 2.4740\n",
      "step 161270: train loss 2.4502, val loss 2.4386\n",
      "step 161280: train loss 2.4358, val loss 2.5357\n",
      "step 161290: train loss 2.3948, val loss 2.5027\n",
      "step 161300: train loss 2.3251, val loss 2.4790\n",
      "step 161310: train loss 2.3960, val loss 2.3802\n",
      "step 161320: train loss 2.3534, val loss 2.4169\n",
      "step 161330: train loss 2.3602, val loss 2.4683\n",
      "step 161340: train loss 2.3913, val loss 2.4534\n",
      "step 161350: train loss 2.5433, val loss 2.4874\n",
      "step 161360: train loss 2.4407, val loss 2.4524\n",
      "step 161370: train loss 2.3860, val loss 2.5886\n",
      "step 161380: train loss 2.3649, val loss 2.4486\n",
      "step 161390: train loss 2.4174, val loss 2.4454\n",
      "step 161400: train loss 2.4502, val loss 2.5328\n",
      "step 161410: train loss 2.2973, val loss 2.5336\n",
      "step 161420: train loss 2.4233, val loss 2.6122\n",
      "step 161430: train loss 2.4784, val loss 2.4921\n",
      "step 161440: train loss 2.4165, val loss 2.3389\n",
      "step 161450: train loss 2.3446, val loss 2.5228\n",
      "step 161460: train loss 2.3637, val loss 2.5323\n",
      "step 161470: train loss 2.4318, val loss 2.3477\n",
      "step 161480: train loss 2.3905, val loss 2.5138\n",
      "step 161490: train loss 2.3979, val loss 2.4400\n",
      "step 161500: train loss 2.3409, val loss 2.4561\n",
      "step 161510: train loss 2.4006, val loss 2.5830\n",
      "step 161520: train loss 2.3961, val loss 2.4619\n",
      "step 161530: train loss 2.4158, val loss 2.5311\n",
      "step 161540: train loss 2.4154, val loss 2.4018\n",
      "step 161550: train loss 2.4239, val loss 2.4902\n",
      "step 161560: train loss 2.3603, val loss 2.4698\n",
      "step 161570: train loss 2.4296, val loss 2.3535\n",
      "step 161580: train loss 2.3282, val loss 2.6529\n",
      "step 161590: train loss 2.3301, val loss 2.4776\n",
      "step 161600: train loss 2.3615, val loss 2.4780\n",
      "step 161610: train loss 2.3858, val loss 2.3869\n",
      "step 161620: train loss 2.4418, val loss 2.4384\n",
      "step 161630: train loss 2.3879, val loss 2.4259\n",
      "step 161640: train loss 2.3923, val loss 2.4499\n",
      "step 161650: train loss 2.4277, val loss 2.5233\n",
      "step 161660: train loss 2.3572, val loss 2.4277\n",
      "step 161670: train loss 2.4061, val loss 2.5480\n",
      "step 161680: train loss 2.4525, val loss 2.4619\n",
      "step 161690: train loss 2.4152, val loss 2.4234\n",
      "step 161700: train loss 2.3840, val loss 2.3662\n",
      "step 161710: train loss 2.4143, val loss 2.4782\n",
      "step 161720: train loss 2.4092, val loss 2.5345\n",
      "step 161730: train loss 2.4119, val loss 2.4152\n",
      "step 161740: train loss 2.4555, val loss 2.5803\n",
      "step 161750: train loss 2.4128, val loss 2.4391\n",
      "step 161760: train loss 2.3287, val loss 2.4917\n",
      "step 161770: train loss 2.4632, val loss 2.4054\n",
      "step 161780: train loss 2.3675, val loss 2.5002\n",
      "step 161790: train loss 2.4225, val loss 2.5590\n",
      "step 161800: train loss 2.4764, val loss 2.5661\n",
      "step 161810: train loss 2.4369, val loss 2.4484\n",
      "step 161820: train loss 2.4223, val loss 2.5114\n",
      "step 161830: train loss 2.4378, val loss 2.5046\n",
      "step 161840: train loss 2.3837, val loss 2.4029\n",
      "step 161850: train loss 2.3792, val loss 2.5356\n",
      "step 161860: train loss 2.4925, val loss 2.5229\n",
      "step 161870: train loss 2.3738, val loss 2.4776\n",
      "step 161880: train loss 2.4518, val loss 2.5256\n",
      "step 161890: train loss 2.3313, val loss 2.5019\n",
      "step 161900: train loss 2.4084, val loss 2.4648\n",
      "step 161910: train loss 2.4212, val loss 2.4390\n",
      "step 161920: train loss 2.4192, val loss 2.5130\n",
      "step 161930: train loss 2.4210, val loss 2.3603\n",
      "step 161940: train loss 2.4163, val loss 2.5172\n",
      "step 161950: train loss 2.4586, val loss 2.4500\n",
      "step 161960: train loss 2.3212, val loss 2.4291\n",
      "step 161970: train loss 2.4135, val loss 2.4575\n",
      "step 161980: train loss 2.3448, val loss 2.4716\n",
      "step 161990: train loss 2.3228, val loss 2.4241\n",
      "step 162000: train loss 2.3816, val loss 2.5724\n",
      "Generated text at iteration 162000\n",
      "\n",
      "L'e,\n",
      "Le soùme,  n se cofamon omail'y'égrbe, hielus  Tu dautes le ux mmplèr pe  ue\n",
      "C'éaisémarss n s l\n",
      "step 162010: train loss 2.4096, val loss 2.4916\n",
      "step 162020: train loss 2.3388, val loss 2.5370\n",
      "step 162030: train loss 2.4445, val loss 2.4167\n",
      "step 162040: train loss 2.4130, val loss 2.4022\n",
      "step 162050: train loss 2.3926, val loss 2.4853\n",
      "step 162060: train loss 2.4445, val loss 2.5139\n",
      "step 162070: train loss 2.3785, val loss 2.5170\n",
      "step 162080: train loss 2.3872, val loss 2.4846\n",
      "step 162090: train loss 2.3920, val loss 2.4101\n",
      "step 162100: train loss 2.4201, val loss 2.6122\n",
      "step 162110: train loss 2.3170, val loss 2.5167\n",
      "step 162120: train loss 2.3503, val loss 2.4141\n",
      "step 162130: train loss 2.3714, val loss 2.5613\n",
      "step 162140: train loss 2.3635, val loss 2.5241\n",
      "step 162150: train loss 2.4081, val loss 2.5377\n",
      "step 162160: train loss 2.4329, val loss 2.5845\n",
      "step 162170: train loss 2.3975, val loss 2.4697\n",
      "step 162180: train loss 2.3892, val loss 2.6156\n",
      "step 162190: train loss 2.3835, val loss 2.5964\n",
      "step 162200: train loss 2.3767, val loss 2.4106\n",
      "step 162210: train loss 2.3488, val loss 2.5957\n",
      "step 162220: train loss 2.4365, val loss 2.4831\n",
      "step 162230: train loss 2.4366, val loss 2.5636\n",
      "step 162240: train loss 2.4599, val loss 2.4814\n",
      "step 162250: train loss 2.3545, val loss 2.4901\n",
      "step 162260: train loss 2.4524, val loss 2.6173\n",
      "step 162270: train loss 2.4484, val loss 2.5205\n",
      "step 162280: train loss 2.3664, val loss 2.3929\n",
      "step 162290: train loss 2.4762, val loss 2.4687\n",
      "step 162300: train loss 2.3555, val loss 2.6406\n",
      "step 162310: train loss 2.3618, val loss 2.4316\n",
      "step 162320: train loss 2.4516, val loss 2.4595\n",
      "step 162330: train loss 2.3488, val loss 2.4858\n",
      "step 162340: train loss 2.3407, val loss 2.5220\n",
      "step 162350: train loss 2.3661, val loss 2.4407\n",
      "step 162360: train loss 2.3604, val loss 2.3814\n",
      "step 162370: train loss 2.4038, val loss 2.5447\n",
      "step 162380: train loss 2.3880, val loss 2.4896\n",
      "step 162390: train loss 2.3193, val loss 2.5557\n",
      "step 162400: train loss 2.4332, val loss 2.4102\n",
      "step 162410: train loss 2.4013, val loss 2.4274\n",
      "step 162420: train loss 2.3398, val loss 2.5247\n",
      "step 162430: train loss 2.4402, val loss 2.4831\n",
      "step 162440: train loss 2.4307, val loss 2.5187\n",
      "step 162450: train loss 2.4183, val loss 2.5418\n",
      "step 162460: train loss 2.4185, val loss 2.4210\n",
      "step 162470: train loss 2.4483, val loss 2.5130\n",
      "step 162480: train loss 2.4508, val loss 2.4868\n",
      "step 162490: train loss 2.3982, val loss 2.4985\n",
      "step 162500: train loss 2.3806, val loss 2.6066\n",
      "step 162510: train loss 2.4421, val loss 2.4099\n",
      "step 162520: train loss 2.4578, val loss 2.4321\n",
      "step 162530: train loss 2.4018, val loss 2.5375\n",
      "step 162540: train loss 2.4374, val loss 2.4163\n",
      "step 162550: train loss 2.3753, val loss 2.4403\n",
      "step 162560: train loss 2.3396, val loss 2.4931\n",
      "step 162570: train loss 2.4467, val loss 2.4064\n",
      "step 162580: train loss 2.3460, val loss 2.5577\n",
      "step 162590: train loss 2.3235, val loss 2.4560\n",
      "step 162600: train loss 2.4542, val loss 2.4567\n",
      "step 162610: train loss 2.4727, val loss 2.5078\n",
      "step 162620: train loss 2.4149, val loss 2.4648\n",
      "step 162630: train loss 2.4411, val loss 2.5420\n",
      "step 162640: train loss 2.4991, val loss 2.4720\n",
      "step 162650: train loss 2.4559, val loss 2.4589\n",
      "step 162660: train loss 2.4053, val loss 2.5248\n",
      "step 162670: train loss 2.4635, val loss 2.5976\n",
      "step 162680: train loss 2.3992, val loss 2.4570\n",
      "step 162690: train loss 2.4222, val loss 2.4632\n",
      "step 162700: train loss 2.4690, val loss 2.5339\n",
      "step 162710: train loss 2.3790, val loss 2.5641\n",
      "step 162720: train loss 2.3865, val loss 2.5170\n",
      "step 162730: train loss 2.3316, val loss 2.5627\n",
      "step 162740: train loss 2.2720, val loss 2.5035\n",
      "step 162750: train loss 2.3943, val loss 2.5247\n",
      "step 162760: train loss 2.3817, val loss 2.6377\n",
      "step 162770: train loss 2.4088, val loss 2.4033\n",
      "step 162780: train loss 2.4047, val loss 2.4431\n",
      "step 162790: train loss 2.4135, val loss 2.4303\n",
      "step 162800: train loss 2.4018, val loss 2.5564\n",
      "step 162810: train loss 2.4022, val loss 2.6688\n",
      "step 162820: train loss 2.4314, val loss 2.5465\n",
      "step 162830: train loss 2.5106, val loss 2.4203\n",
      "step 162840: train loss 2.4742, val loss 2.4990\n",
      "step 162850: train loss 2.4679, val loss 2.5198\n",
      "step 162860: train loss 2.3952, val loss 2.4448\n",
      "step 162870: train loss 2.4001, val loss 2.4869\n",
      "step 162880: train loss 2.5000, val loss 2.4713\n",
      "step 162890: train loss 2.3672, val loss 2.5227\n",
      "step 162900: train loss 2.4317, val loss 2.4130\n",
      "step 162910: train loss 2.3467, val loss 2.4047\n",
      "step 162920: train loss 2.3684, val loss 2.4707\n",
      "step 162930: train loss 2.4015, val loss 2.5576\n",
      "step 162940: train loss 2.4404, val loss 2.4880\n",
      "step 162950: train loss 2.3578, val loss 2.4782\n",
      "step 162960: train loss 2.4043, val loss 2.5860\n",
      "step 162970: train loss 2.3754, val loss 2.4638\n",
      "step 162980: train loss 2.3786, val loss 2.4676\n",
      "step 162990: train loss 2.2915, val loss 2.5271\n",
      "step 163000: train loss 2.3973, val loss 2.4983\n",
      "Generated text at iteration 163000\n",
      "\n",
      "Chavea drs frant llibot pomoir  chau qutr penindave de pôtie les,\n",
      "ITom'alainares réfe  s blan-mez as\n",
      "step 163010: train loss 2.3715, val loss 2.4931\n",
      "step 163020: train loss 2.4202, val loss 2.4386\n",
      "step 163030: train loss 2.4091, val loss 2.5043\n",
      "step 163040: train loss 2.3863, val loss 2.5054\n",
      "step 163050: train loss 2.4911, val loss 2.4757\n",
      "step 163060: train loss 2.4206, val loss 2.4736\n",
      "step 163070: train loss 2.3977, val loss 2.4472\n",
      "step 163080: train loss 2.4013, val loss 2.5571\n",
      "step 163090: train loss 2.3930, val loss 2.4039\n",
      "step 163100: train loss 2.4198, val loss 2.4477\n",
      "step 163110: train loss 2.3829, val loss 2.4245\n",
      "step 163120: train loss 2.3351, val loss 2.4690\n",
      "step 163130: train loss 2.3504, val loss 2.5800\n",
      "step 163140: train loss 2.3598, val loss 2.3972\n",
      "step 163150: train loss 2.4778, val loss 2.5659\n",
      "step 163160: train loss 2.3984, val loss 2.5898\n",
      "step 163170: train loss 2.4310, val loss 2.4457\n",
      "step 163180: train loss 2.5046, val loss 2.4382\n",
      "step 163190: train loss 2.3959, val loss 2.4958\n",
      "step 163200: train loss 2.4156, val loss 2.4701\n",
      "step 163210: train loss 2.3786, val loss 2.4264\n",
      "step 163220: train loss 2.3588, val loss 2.4883\n",
      "step 163230: train loss 2.3732, val loss 2.4652\n",
      "step 163240: train loss 2.4333, val loss 2.4447\n",
      "step 163250: train loss 2.3468, val loss 2.5572\n",
      "step 163260: train loss 2.4187, val loss 2.5155\n",
      "step 163270: train loss 2.3590, val loss 2.5060\n",
      "step 163280: train loss 2.4833, val loss 2.4224\n",
      "step 163290: train loss 2.4211, val loss 2.4742\n",
      "step 163300: train loss 2.3668, val loss 2.4601\n",
      "step 163310: train loss 2.3780, val loss 2.4322\n",
      "step 163320: train loss 2.3632, val loss 2.6160\n",
      "step 163330: train loss 2.3880, val loss 2.5522\n",
      "step 163340: train loss 2.4024, val loss 2.4663\n",
      "step 163350: train loss 2.4868, val loss 2.5766\n",
      "step 163360: train loss 2.4494, val loss 2.4387\n",
      "step 163370: train loss 2.4315, val loss 2.5074\n",
      "step 163380: train loss 2.3305, val loss 2.4631\n",
      "step 163390: train loss 2.4361, val loss 2.4519\n",
      "step 163400: train loss 2.3878, val loss 2.4839\n",
      "step 163410: train loss 2.3969, val loss 2.5550\n",
      "step 163420: train loss 2.4098, val loss 2.6617\n",
      "step 163430: train loss 2.3344, val loss 2.4451\n",
      "step 163440: train loss 2.3956, val loss 2.5156\n",
      "step 163450: train loss 2.3865, val loss 2.4913\n",
      "step 163460: train loss 2.3815, val loss 2.4294\n",
      "step 163470: train loss 2.3713, val loss 2.5574\n",
      "step 163480: train loss 2.4654, val loss 2.4635\n",
      "step 163490: train loss 2.3752, val loss 2.4683\n",
      "step 163500: train loss 2.4268, val loss 2.5008\n",
      "step 163510: train loss 2.3911, val loss 2.5251\n",
      "step 163520: train loss 2.3906, val loss 2.5880\n",
      "step 163530: train loss 2.4228, val loss 2.5070\n",
      "step 163540: train loss 2.4263, val loss 2.5904\n",
      "step 163550: train loss 2.3498, val loss 2.3669\n",
      "step 163560: train loss 2.3943, val loss 2.3981\n",
      "step 163570: train loss 2.4649, val loss 2.4611\n",
      "step 163580: train loss 2.4718, val loss 2.4154\n",
      "step 163590: train loss 2.4184, val loss 2.5004\n",
      "step 163600: train loss 2.4072, val loss 2.5065\n",
      "step 163610: train loss 2.4133, val loss 2.4412\n",
      "step 163620: train loss 2.4159, val loss 2.5448\n",
      "step 163630: train loss 2.4570, val loss 2.4854\n",
      "step 163640: train loss 2.4338, val loss 2.5059\n",
      "step 163650: train loss 2.4954, val loss 2.5816\n",
      "step 163660: train loss 2.4189, val loss 2.4867\n",
      "step 163670: train loss 2.4149, val loss 2.4533\n",
      "step 163680: train loss 2.3797, val loss 2.5380\n",
      "step 163690: train loss 2.3816, val loss 2.5004\n",
      "step 163700: train loss 2.4023, val loss 2.5837\n",
      "step 163710: train loss 2.4101, val loss 2.5126\n",
      "step 163720: train loss 2.2546, val loss 2.4490\n",
      "step 163730: train loss 2.3700, val loss 2.3788\n",
      "step 163740: train loss 2.3889, val loss 2.5523\n",
      "step 163750: train loss 2.3205, val loss 2.4370\n",
      "step 163760: train loss 2.3886, val loss 2.6163\n",
      "step 163770: train loss 2.3141, val loss 2.4512\n",
      "step 163780: train loss 2.3999, val loss 2.6039\n",
      "step 163790: train loss 2.4491, val loss 2.5214\n",
      "step 163800: train loss 2.4582, val loss 2.4927\n",
      "step 163810: train loss 2.3985, val loss 2.5268\n",
      "step 163820: train loss 2.4148, val loss 2.5030\n",
      "step 163830: train loss 2.3344, val loss 2.5063\n",
      "step 163840: train loss 2.3984, val loss 2.4539\n",
      "step 163850: train loss 2.4579, val loss 2.4108\n",
      "step 163860: train loss 2.3915, val loss 2.4144\n",
      "step 163870: train loss 2.4395, val loss 2.5070\n",
      "step 163880: train loss 2.4225, val loss 2.4376\n",
      "step 163890: train loss 2.3295, val loss 2.4714\n",
      "step 163900: train loss 2.4375, val loss 2.3160\n",
      "step 163910: train loss 2.3955, val loss 2.5202\n",
      "step 163920: train loss 2.3582, val loss 2.5295\n",
      "step 163930: train loss 2.3939, val loss 2.6019\n",
      "step 163940: train loss 2.4231, val loss 2.3909\n",
      "step 163950: train loss 2.4387, val loss 2.4614\n",
      "step 163960: train loss 2.4075, val loss 2.3921\n",
      "step 163970: train loss 2.3702, val loss 2.4091\n",
      "step 163980: train loss 2.4339, val loss 2.5741\n",
      "step 163990: train loss 2.4325, val loss 2.4505\n",
      "step 164000: train loss 2.4440, val loss 2.4447\n",
      "Generated text at iteration 164000\n",
      "\n",
      "Bit  li deuroioisour quniesenou à dadairesems, aret'anderilol'aue\n",
      "\n",
      "Fà dune deniles C_6«-_oere as  t \n",
      "step 164010: train loss 2.5098, val loss 2.4368\n",
      "step 164020: train loss 2.4378, val loss 2.4894\n",
      "step 164030: train loss 2.4079, val loss 2.5601\n",
      "step 164040: train loss 2.4473, val loss 2.4512\n",
      "step 164050: train loss 2.3634, val loss 2.4432\n",
      "step 164060: train loss 2.3528, val loss 2.4318\n",
      "step 164070: train loss 2.3954, val loss 2.5595\n",
      "step 164080: train loss 2.4450, val loss 2.5697\n",
      "step 164090: train loss 2.4773, val loss 2.4278\n",
      "step 164100: train loss 2.4352, val loss 2.4064\n",
      "step 164110: train loss 2.3595, val loss 2.3788\n",
      "step 164120: train loss 2.4459, val loss 2.4754\n",
      "step 164130: train loss 2.3558, val loss 2.5036\n",
      "step 164140: train loss 2.3726, val loss 2.4821\n",
      "step 164150: train loss 2.4081, val loss 2.4108\n",
      "step 164160: train loss 2.4534, val loss 2.4819\n",
      "step 164170: train loss 2.3451, val loss 2.4481\n",
      "step 164180: train loss 2.3963, val loss 2.4581\n",
      "step 164190: train loss 2.4301, val loss 2.4358\n",
      "step 164200: train loss 2.4601, val loss 2.4837\n",
      "step 164210: train loss 2.3320, val loss 2.4919\n",
      "step 164220: train loss 2.3184, val loss 2.5165\n",
      "step 164230: train loss 2.3600, val loss 2.5783\n",
      "step 164240: train loss 2.4445, val loss 2.3957\n",
      "step 164250: train loss 2.4491, val loss 2.3535\n",
      "step 164260: train loss 2.4019, val loss 2.5462\n",
      "step 164270: train loss 2.5766, val loss 2.5530\n",
      "step 164280: train loss 2.4089, val loss 2.5337\n",
      "step 164290: train loss 2.3717, val loss 2.5131\n",
      "step 164300: train loss 2.4307, val loss 2.4520\n",
      "step 164310: train loss 2.4391, val loss 2.5353\n",
      "step 164320: train loss 2.3971, val loss 2.5409\n",
      "step 164330: train loss 2.4389, val loss 2.5310\n",
      "step 164340: train loss 2.4685, val loss 2.4898\n",
      "step 164350: train loss 2.3284, val loss 2.5699\n",
      "step 164360: train loss 2.3724, val loss 2.6485\n",
      "step 164370: train loss 2.4120, val loss 2.4477\n",
      "step 164380: train loss 2.4086, val loss 2.6198\n",
      "step 164390: train loss 2.4016, val loss 2.5554\n",
      "step 164400: train loss 2.2858, val loss 2.3962\n",
      "step 164410: train loss 2.3825, val loss 2.4477\n",
      "step 164420: train loss 2.3489, val loss 2.4489\n",
      "step 164430: train loss 2.3703, val loss 2.5245\n",
      "step 164440: train loss 2.4514, val loss 2.5553\n",
      "step 164450: train loss 2.4316, val loss 2.5313\n",
      "step 164460: train loss 2.3354, val loss 2.4820\n",
      "step 164470: train loss 2.3920, val loss 2.4972\n",
      "step 164480: train loss 2.3474, val loss 2.4960\n",
      "step 164490: train loss 2.3392, val loss 2.4846\n",
      "step 164500: train loss 2.4649, val loss 2.4580\n",
      "step 164510: train loss 2.3888, val loss 2.4006\n",
      "step 164520: train loss 2.4005, val loss 2.4206\n",
      "step 164530: train loss 2.4529, val loss 2.5781\n",
      "step 164540: train loss 2.3647, val loss 2.3373\n",
      "step 164550: train loss 2.4440, val loss 2.5046\n",
      "step 164560: train loss 2.3642, val loss 2.4204\n",
      "step 164570: train loss 2.4305, val loss 2.5183\n",
      "step 164580: train loss 2.3656, val loss 2.4251\n",
      "step 164590: train loss 2.3662, val loss 2.5746\n",
      "step 164600: train loss 2.4360, val loss 2.5330\n",
      "step 164610: train loss 2.4329, val loss 2.3891\n",
      "step 164620: train loss 2.4559, val loss 2.4879\n",
      "step 164630: train loss 2.4005, val loss 2.5506\n",
      "step 164640: train loss 2.4060, val loss 2.5230\n",
      "step 164650: train loss 2.3975, val loss 2.4254\n",
      "step 164660: train loss 2.3716, val loss 2.5439\n",
      "step 164670: train loss 2.4270, val loss 2.4758\n",
      "step 164680: train loss 2.4332, val loss 2.4946\n",
      "step 164690: train loss 2.4015, val loss 2.5885\n",
      "step 164700: train loss 2.4342, val loss 2.3809\n",
      "step 164710: train loss 2.4134, val loss 2.5853\n",
      "step 164720: train loss 2.3877, val loss 2.4064\n",
      "step 164730: train loss 2.3729, val loss 2.3531\n",
      "step 164740: train loss 2.4376, val loss 2.4039\n",
      "step 164750: train loss 2.4561, val loss 2.5423\n",
      "step 164760: train loss 2.3621, val loss 2.5138\n",
      "step 164770: train loss 2.4223, val loss 2.5241\n",
      "step 164780: train loss 2.3611, val loss 2.4550\n",
      "step 164790: train loss 2.3481, val loss 2.6679\n",
      "step 164800: train loss 2.3890, val loss 2.5604\n",
      "step 164810: train loss 2.4169, val loss 2.5566\n",
      "step 164820: train loss 2.4525, val loss 2.4514\n",
      "step 164830: train loss 2.3684, val loss 2.4759\n",
      "step 164840: train loss 2.4050, val loss 2.4396\n",
      "step 164850: train loss 2.3671, val loss 2.4814\n",
      "step 164860: train loss 2.4236, val loss 2.5427\n",
      "step 164870: train loss 2.4162, val loss 2.4775\n",
      "step 164880: train loss 2.3902, val loss 2.5344\n",
      "step 164890: train loss 2.4535, val loss 2.4337\n",
      "step 164900: train loss 2.3564, val loss 2.4579\n",
      "step 164910: train loss 2.4024, val loss 2.4815\n",
      "step 164920: train loss 2.4807, val loss 2.7355\n",
      "step 164930: train loss 2.4388, val loss 2.3708\n",
      "step 164940: train loss 2.3649, val loss 2.4099\n",
      "step 164950: train loss 2.3693, val loss 2.3920\n",
      "step 164960: train loss 2.3902, val loss 2.5059\n",
      "step 164970: train loss 2.4166, val loss 2.6472\n",
      "step 164980: train loss 2.4360, val loss 2.4554\n",
      "step 164990: train loss 2.3443, val loss 2.4905\n",
      "step 165000: train loss 2.4525, val loss 2.5021\n",
      "Generated text at iteration 165000\n",
      "\n",
      "\n",
      "Quffas voux.-Pant  jete, poilel'onsoù ffâmbou ches oituie, dirreataise dayYË!\n",
      "Turomprs diblues; ces\n",
      "step 165010: train loss 2.4271, val loss 2.4805\n",
      "step 165020: train loss 2.3261, val loss 2.5333\n",
      "step 165030: train loss 2.3778, val loss 2.5057\n",
      "step 165040: train loss 2.3766, val loss 2.5258\n",
      "step 165050: train loss 2.5009, val loss 2.5829\n",
      "step 165060: train loss 2.3612, val loss 2.5006\n",
      "step 165070: train loss 2.4166, val loss 2.5333\n",
      "step 165080: train loss 2.3990, val loss 2.4970\n",
      "step 165090: train loss 2.4081, val loss 2.5410\n",
      "step 165100: train loss 2.4053, val loss 2.5405\n",
      "step 165110: train loss 2.3946, val loss 2.5174\n",
      "step 165120: train loss 2.4058, val loss 2.5451\n",
      "step 165130: train loss 2.4501, val loss 2.4570\n",
      "step 165140: train loss 2.3886, val loss 2.3887\n",
      "step 165150: train loss 2.4388, val loss 2.5277\n",
      "step 165160: train loss 2.4121, val loss 2.4783\n",
      "step 165170: train loss 2.3907, val loss 2.4731\n",
      "step 165180: train loss 2.3492, val loss 2.6264\n",
      "step 165190: train loss 2.3433, val loss 2.4982\n",
      "step 165200: train loss 2.4140, val loss 2.4887\n",
      "step 165210: train loss 2.4308, val loss 2.5213\n",
      "step 165220: train loss 2.3895, val loss 2.4419\n",
      "step 165230: train loss 2.3865, val loss 2.5724\n",
      "step 165240: train loss 2.3622, val loss 2.5149\n",
      "step 165250: train loss 2.3654, val loss 2.4990\n",
      "step 165260: train loss 2.3723, val loss 2.6199\n",
      "step 165270: train loss 2.3919, val loss 2.4855\n",
      "step 165280: train loss 2.4359, val loss 2.4849\n",
      "step 165290: train loss 2.3376, val loss 2.5231\n",
      "step 165300: train loss 2.3679, val loss 2.4723\n",
      "step 165310: train loss 2.3854, val loss 2.4240\n",
      "step 165320: train loss 2.4266, val loss 2.4694\n",
      "step 165330: train loss 2.4382, val loss 2.5177\n",
      "step 165340: train loss 2.4049, val loss 2.4258\n",
      "step 165350: train loss 2.4477, val loss 2.4193\n",
      "step 165360: train loss 2.4026, val loss 2.5592\n",
      "step 165370: train loss 2.3666, val loss 2.4834\n",
      "step 165380: train loss 2.4305, val loss 2.5108\n",
      "step 165390: train loss 2.4826, val loss 2.4159\n",
      "step 165400: train loss 2.3844, val loss 2.4627\n",
      "step 165410: train loss 2.4084, val loss 2.3489\n",
      "step 165420: train loss 2.4859, val loss 2.3929\n",
      "step 165430: train loss 2.4127, val loss 2.4267\n",
      "step 165440: train loss 2.4387, val loss 2.3892\n",
      "step 165450: train loss 2.3819, val loss 2.6285\n",
      "step 165460: train loss 2.4508, val loss 2.4325\n",
      "step 165470: train loss 2.3637, val loss 2.5044\n",
      "step 165480: train loss 2.3632, val loss 2.6367\n",
      "step 165490: train loss 2.4281, val loss 2.4276\n",
      "step 165500: train loss 2.4646, val loss 2.4640\n",
      "step 165510: train loss 2.3990, val loss 2.3942\n",
      "step 165520: train loss 2.4097, val loss 2.4846\n",
      "step 165530: train loss 2.4149, val loss 2.5162\n",
      "step 165540: train loss 2.3754, val loss 2.6193\n",
      "step 165550: train loss 2.4137, val loss 2.5609\n",
      "step 165560: train loss 2.5009, val loss 2.6059\n",
      "step 165570: train loss 2.3264, val loss 2.5785\n",
      "step 165580: train loss 2.3800, val loss 2.3993\n",
      "step 165590: train loss 2.3855, val loss 2.4219\n",
      "step 165600: train loss 2.3708, val loss 2.4529\n",
      "step 165610: train loss 2.3988, val loss 2.3973\n",
      "step 165620: train loss 2.3945, val loss 2.5526\n",
      "step 165630: train loss 2.3177, val loss 2.5128\n",
      "step 165640: train loss 2.4137, val loss 2.4671\n",
      "step 165650: train loss 2.4015, val loss 2.4540\n",
      "step 165660: train loss 2.4374, val loss 2.4057\n",
      "step 165670: train loss 2.3741, val loss 2.5469\n",
      "step 165680: train loss 2.3435, val loss 2.4862\n",
      "step 165690: train loss 2.4585, val loss 2.5706\n",
      "step 165700: train loss 2.4422, val loss 2.5497\n",
      "step 165710: train loss 2.3606, val loss 2.4880\n",
      "step 165720: train loss 2.4050, val loss 2.3246\n",
      "step 165730: train loss 2.3983, val loss 2.4561\n",
      "step 165740: train loss 2.4917, val loss 2.6359\n",
      "step 165750: train loss 2.4010, val loss 2.5782\n",
      "step 165760: train loss 2.4464, val loss 2.4676\n",
      "step 165770: train loss 2.3843, val loss 2.4733\n",
      "step 165780: train loss 2.4105, val loss 2.4798\n",
      "step 165790: train loss 2.4240, val loss 2.5077\n",
      "step 165800: train loss 2.3650, val loss 2.4554\n",
      "step 165810: train loss 2.3643, val loss 2.5218\n",
      "step 165820: train loss 2.4710, val loss 2.4761\n",
      "step 165830: train loss 2.3598, val loss 2.4632\n",
      "step 165840: train loss 2.3588, val loss 2.4719\n",
      "step 165850: train loss 2.4034, val loss 2.5150\n",
      "step 165860: train loss 2.4567, val loss 2.4611\n",
      "step 165870: train loss 2.3737, val loss 2.5093\n",
      "step 165880: train loss 2.4383, val loss 2.5653\n",
      "step 165890: train loss 2.3612, val loss 2.5627\n",
      "step 165900: train loss 2.4193, val loss 2.4751\n",
      "step 165910: train loss 2.3624, val loss 2.5094\n",
      "step 165920: train loss 2.4211, val loss 2.5422\n",
      "step 165930: train loss 2.4168, val loss 2.4537\n",
      "step 165940: train loss 2.3779, val loss 2.5654\n",
      "step 165950: train loss 2.4020, val loss 2.5011\n",
      "step 165960: train loss 2.4270, val loss 2.5812\n",
      "step 165970: train loss 2.4041, val loss 2.4032\n",
      "step 165980: train loss 2.3245, val loss 2.5143\n",
      "step 165990: train loss 2.4495, val loss 2.4277\n",
      "step 166000: train loss 2.4847, val loss 2.4422\n",
      "Generated text at iteration 166000\n",
      "\n",
      "Eglllonerretre les pénetret,\n",
      " soir me\n",
      " n llalpit vr qurme meuie s  éeauiac e ve adroièneme  lurat am\n",
      "step 166010: train loss 2.4833, val loss 2.4859\n",
      "step 166020: train loss 2.3732, val loss 2.4648\n",
      "step 166030: train loss 2.3740, val loss 2.4889\n",
      "step 166040: train loss 2.3809, val loss 2.4204\n",
      "step 166050: train loss 2.5066, val loss 2.3991\n",
      "step 166060: train loss 2.4741, val loss 2.4882\n",
      "step 166070: train loss 2.4195, val loss 2.5414\n",
      "step 166080: train loss 2.3734, val loss 2.3827\n",
      "step 166090: train loss 2.4141, val loss 2.4068\n",
      "step 166100: train loss 2.4261, val loss 2.5329\n",
      "step 166110: train loss 2.3748, val loss 2.6141\n",
      "step 166120: train loss 2.4058, val loss 2.4836\n",
      "step 166130: train loss 2.4687, val loss 2.4596\n",
      "step 166140: train loss 2.3932, val loss 2.6182\n",
      "step 166150: train loss 2.4093, val loss 2.5131\n",
      "step 166160: train loss 2.3415, val loss 2.5354\n",
      "step 166170: train loss 2.4215, val loss 2.3340\n",
      "step 166180: train loss 2.3232, val loss 2.5920\n",
      "step 166190: train loss 2.4182, val loss 2.5266\n",
      "step 166200: train loss 2.4014, val loss 2.4315\n",
      "step 166210: train loss 2.3571, val loss 2.4886\n",
      "step 166220: train loss 2.3873, val loss 2.4550\n",
      "step 166230: train loss 2.4053, val loss 2.4943\n",
      "step 166240: train loss 2.4140, val loss 2.5876\n",
      "step 166250: train loss 2.4229, val loss 2.4882\n",
      "step 166260: train loss 2.3847, val loss 2.5090\n",
      "step 166270: train loss 2.4630, val loss 2.5482\n",
      "step 166280: train loss 2.3295, val loss 2.4969\n",
      "step 166290: train loss 2.3737, val loss 2.5916\n",
      "step 166300: train loss 2.3914, val loss 2.3919\n",
      "step 166310: train loss 2.4566, val loss 2.5820\n",
      "step 166320: train loss 2.4560, val loss 2.5047\n",
      "step 166330: train loss 2.3924, val loss 2.4800\n",
      "step 166340: train loss 2.4138, val loss 2.5217\n",
      "step 166350: train loss 2.3959, val loss 2.4372\n",
      "step 166360: train loss 2.4389, val loss 2.4682\n",
      "step 166370: train loss 2.4195, val loss 2.4908\n",
      "step 166380: train loss 2.3510, val loss 2.5616\n",
      "step 166390: train loss 2.5019, val loss 2.4179\n",
      "step 166400: train loss 2.4287, val loss 2.4784\n",
      "step 166410: train loss 2.3733, val loss 2.5834\n",
      "step 166420: train loss 2.3646, val loss 2.4566\n",
      "step 166430: train loss 2.4380, val loss 2.4268\n",
      "step 166440: train loss 2.3740, val loss 2.5826\n",
      "step 166450: train loss 2.4495, val loss 2.5513\n",
      "step 166460: train loss 2.4438, val loss 2.3623\n",
      "step 166470: train loss 2.3868, val loss 2.4697\n",
      "step 166480: train loss 2.4655, val loss 2.4764\n",
      "step 166490: train loss 2.4633, val loss 2.3617\n",
      "step 166500: train loss 2.3697, val loss 2.4970\n",
      "step 166510: train loss 2.3284, val loss 2.3578\n",
      "step 166520: train loss 2.4189, val loss 2.6087\n",
      "step 166530: train loss 2.3995, val loss 2.4845\n",
      "step 166540: train loss 2.3956, val loss 2.5033\n",
      "step 166550: train loss 2.3907, val loss 2.4898\n",
      "step 166560: train loss 2.4923, val loss 2.4186\n",
      "step 166570: train loss 2.4820, val loss 2.4565\n",
      "step 166580: train loss 2.4822, val loss 2.4812\n",
      "step 166590: train loss 2.3660, val loss 2.4268\n",
      "step 166600: train loss 2.4532, val loss 2.5306\n",
      "step 166610: train loss 2.4146, val loss 2.4759\n",
      "step 166620: train loss 2.4233, val loss 2.3530\n",
      "step 166630: train loss 2.4265, val loss 2.4855\n",
      "step 166640: train loss 2.3492, val loss 2.4363\n",
      "step 166650: train loss 2.4051, val loss 2.6417\n",
      "step 166660: train loss 2.3703, val loss 2.4903\n",
      "step 166670: train loss 2.2767, val loss 2.3856\n",
      "step 166680: train loss 2.3351, val loss 2.5085\n",
      "step 166690: train loss 2.4777, val loss 2.4743\n",
      "step 166700: train loss 2.3871, val loss 2.4152\n",
      "step 166710: train loss 2.4697, val loss 2.5139\n",
      "step 166720: train loss 2.4365, val loss 2.4318\n",
      "step 166730: train loss 2.4275, val loss 2.4991\n",
      "step 166740: train loss 2.3304, val loss 2.4514\n",
      "step 166750: train loss 2.4200, val loss 2.4202\n",
      "step 166760: train loss 2.3810, val loss 2.4508\n",
      "step 166770: train loss 2.3620, val loss 2.4861\n",
      "step 166780: train loss 2.4278, val loss 2.5058\n",
      "step 166790: train loss 2.3527, val loss 2.4991\n",
      "step 166800: train loss 2.4472, val loss 2.4389\n",
      "step 166810: train loss 2.4225, val loss 2.5331\n",
      "step 166820: train loss 2.4456, val loss 2.4640\n",
      "step 166830: train loss 2.3923, val loss 2.5738\n",
      "step 166840: train loss 2.3488, val loss 2.4218\n",
      "step 166850: train loss 2.4815, val loss 2.3996\n",
      "step 166860: train loss 2.4008, val loss 2.4537\n",
      "step 166870: train loss 2.4009, val loss 2.5871\n",
      "step 166880: train loss 2.3066, val loss 2.5645\n",
      "step 166890: train loss 2.3273, val loss 2.3954\n",
      "step 166900: train loss 2.4243, val loss 2.4269\n",
      "step 166910: train loss 2.3184, val loss 2.5010\n",
      "step 166920: train loss 2.3457, val loss 2.5961\n",
      "step 166930: train loss 2.3058, val loss 2.4799\n",
      "step 166940: train loss 2.4477, val loss 2.4936\n",
      "step 166950: train loss 2.4046, val loss 2.3760\n",
      "step 166960: train loss 2.4433, val loss 2.4343\n",
      "step 166970: train loss 2.4214, val loss 2.5045\n",
      "step 166980: train loss 2.3998, val loss 2.4798\n",
      "step 166990: train loss 2.3805, val loss 2.3685\n",
      "step 167000: train loss 2.3928, val loss 2.5948\n",
      "Generated text at iteration 167000\n",
      "\n",
      "Suirèrsten mesiront lau,\n",
      "\n",
      " brce hant leson chon re;\n",
      " voist.\n",
      "NENëqure mechoiri;  fou?\n",
      " etende nses,\n",
      "\n",
      "\n",
      "step 167010: train loss 2.3978, val loss 2.4849\n",
      "step 167020: train loss 2.3660, val loss 2.5077\n",
      "step 167030: train loss 2.3892, val loss 2.5506\n",
      "step 167040: train loss 2.3983, val loss 2.4510\n",
      "step 167050: train loss 2.3897, val loss 2.4364\n",
      "step 167060: train loss 2.4812, val loss 2.5179\n",
      "step 167070: train loss 2.4090, val loss 2.4800\n",
      "step 167080: train loss 2.4301, val loss 2.5361\n",
      "step 167090: train loss 2.3821, val loss 2.5799\n",
      "step 167100: train loss 2.4716, val loss 2.4345\n",
      "step 167110: train loss 2.3350, val loss 2.4903\n",
      "step 167120: train loss 2.3824, val loss 2.4339\n",
      "step 167130: train loss 2.5053, val loss 2.5456\n",
      "step 167140: train loss 2.3679, val loss 2.5193\n",
      "step 167150: train loss 2.3676, val loss 2.4173\n",
      "step 167160: train loss 2.4243, val loss 2.3688\n",
      "step 167170: train loss 2.4008, val loss 2.3965\n",
      "step 167180: train loss 2.4477, val loss 2.5067\n",
      "step 167190: train loss 2.4016, val loss 2.5276\n",
      "step 167200: train loss 2.4587, val loss 2.4574\n",
      "step 167210: train loss 2.4546, val loss 2.4654\n",
      "step 167220: train loss 2.4706, val loss 2.4412\n",
      "step 167230: train loss 2.4481, val loss 2.5508\n",
      "step 167240: train loss 2.3561, val loss 2.4826\n",
      "step 167250: train loss 2.3845, val loss 2.5214\n",
      "step 167260: train loss 2.4177, val loss 2.4126\n",
      "step 167270: train loss 2.3817, val loss 2.4487\n",
      "step 167280: train loss 2.4110, val loss 2.4217\n",
      "step 167290: train loss 2.4378, val loss 2.4481\n",
      "step 167300: train loss 2.3762, val loss 2.4639\n",
      "step 167310: train loss 2.4512, val loss 2.5246\n",
      "step 167320: train loss 2.3525, val loss 2.4922\n",
      "step 167330: train loss 2.4386, val loss 2.4116\n",
      "step 167340: train loss 2.4553, val loss 2.4612\n",
      "step 167350: train loss 2.4652, val loss 2.5267\n",
      "step 167360: train loss 2.3403, val loss 2.4747\n",
      "step 167370: train loss 2.3765, val loss 2.5147\n",
      "step 167380: train loss 2.3560, val loss 2.5404\n",
      "step 167390: train loss 2.3787, val loss 2.4454\n",
      "step 167400: train loss 2.4603, val loss 2.5537\n",
      "step 167410: train loss 2.3753, val loss 2.4841\n",
      "step 167420: train loss 2.4030, val loss 2.5561\n",
      "step 167430: train loss 2.3605, val loss 2.4680\n",
      "step 167440: train loss 2.3855, val loss 2.5519\n",
      "step 167450: train loss 2.4631, val loss 2.5146\n",
      "step 167460: train loss 2.3589, val loss 2.4973\n",
      "step 167470: train loss 2.3808, val loss 2.4704\n",
      "step 167480: train loss 2.3714, val loss 2.5013\n",
      "step 167490: train loss 2.3945, val loss 2.5965\n",
      "step 167500: train loss 2.3870, val loss 2.4161\n",
      "step 167510: train loss 2.3647, val loss 2.6061\n",
      "step 167520: train loss 2.3350, val loss 2.3390\n",
      "step 167530: train loss 2.3348, val loss 2.5343\n",
      "step 167540: train loss 2.3391, val loss 2.6342\n",
      "step 167550: train loss 2.4508, val loss 2.4862\n",
      "step 167560: train loss 2.3827, val loss 2.4941\n",
      "step 167570: train loss 2.4317, val loss 2.4791\n",
      "step 167580: train loss 2.3682, val loss 2.5461\n",
      "step 167590: train loss 2.4056, val loss 2.4800\n",
      "step 167600: train loss 2.4032, val loss 2.4800\n",
      "step 167610: train loss 2.4070, val loss 2.5716\n",
      "step 167620: train loss 2.4011, val loss 2.4557\n",
      "step 167630: train loss 2.3684, val loss 2.4076\n",
      "step 167640: train loss 2.3651, val loss 2.5937\n",
      "step 167650: train loss 2.3900, val loss 2.4336\n",
      "step 167660: train loss 2.3780, val loss 2.4764\n",
      "step 167670: train loss 2.3626, val loss 2.4084\n",
      "step 167680: train loss 2.4569, val loss 2.4122\n",
      "step 167690: train loss 2.4636, val loss 2.4506\n",
      "step 167700: train loss 2.4785, val loss 2.4669\n",
      "step 167710: train loss 2.4223, val loss 2.5002\n",
      "step 167720: train loss 2.4084, val loss 2.5146\n",
      "step 167730: train loss 2.4788, val loss 2.4272\n",
      "step 167740: train loss 2.3099, val loss 2.5216\n",
      "step 167750: train loss 2.4353, val loss 2.5373\n",
      "step 167760: train loss 2.3618, val loss 2.4557\n",
      "step 167770: train loss 2.4808, val loss 2.4867\n",
      "step 167780: train loss 2.4201, val loss 2.4947\n",
      "step 167790: train loss 2.4060, val loss 2.5720\n",
      "step 167800: train loss 2.3269, val loss 2.6064\n",
      "step 167810: train loss 2.3711, val loss 2.5120\n",
      "step 167820: train loss 2.4306, val loss 2.5211\n",
      "step 167830: train loss 2.5100, val loss 2.5021\n",
      "step 167840: train loss 2.3268, val loss 2.5881\n",
      "step 167850: train loss 2.4268, val loss 2.4329\n",
      "step 167860: train loss 2.3897, val loss 2.5405\n",
      "step 167870: train loss 2.4209, val loss 2.4515\n",
      "step 167880: train loss 2.3799, val loss 2.4842\n",
      "step 167890: train loss 2.4026, val loss 2.5963\n",
      "step 167900: train loss 2.2998, val loss 2.5120\n",
      "step 167910: train loss 2.4124, val loss 2.5827\n",
      "step 167920: train loss 2.4591, val loss 2.4253\n",
      "step 167930: train loss 2.4917, val loss 2.4021\n",
      "step 167940: train loss 2.3512, val loss 2.5688\n",
      "step 167950: train loss 2.3492, val loss 2.4809\n",
      "step 167960: train loss 2.4381, val loss 2.4189\n",
      "step 167970: train loss 2.4196, val loss 2.4179\n",
      "step 167980: train loss 2.3965, val loss 2.6156\n",
      "step 167990: train loss 2.4118, val loss 2.3115\n",
      "step 168000: train loss 2.4326, val loss 2.5547\n",
      "Generated text at iteration 168000\n",
      "\n",
      "Cérse suegrmau-ie e anfést à onsprt.\n",
      "J'hoilan te harrous: tainèr aioront te jUn le;\n",
      "Mau esonet! onss\n",
      "step 168010: train loss 2.3448, val loss 2.4158\n",
      "step 168020: train loss 2.2816, val loss 2.4344\n",
      "step 168030: train loss 2.3992, val loss 2.4263\n",
      "step 168040: train loss 2.3810, val loss 2.5404\n",
      "step 168050: train loss 2.3532, val loss 2.4468\n",
      "step 168060: train loss 2.4198, val loss 2.4264\n",
      "step 168070: train loss 2.3838, val loss 2.4144\n",
      "step 168080: train loss 2.3922, val loss 2.4317\n",
      "step 168090: train loss 2.3950, val loss 2.6436\n",
      "step 168100: train loss 2.4251, val loss 2.3738\n",
      "step 168110: train loss 2.4182, val loss 2.5227\n",
      "step 168120: train loss 2.3593, val loss 2.3682\n",
      "step 168130: train loss 2.4553, val loss 2.5278\n",
      "step 168140: train loss 2.4525, val loss 2.4668\n",
      "step 168150: train loss 2.4363, val loss 2.5612\n",
      "step 168160: train loss 2.3866, val loss 2.5659\n",
      "step 168170: train loss 2.4271, val loss 2.5032\n",
      "step 168180: train loss 2.3932, val loss 2.4992\n",
      "step 168190: train loss 2.3989, val loss 2.4041\n",
      "step 168200: train loss 2.4294, val loss 2.4955\n",
      "step 168210: train loss 2.3616, val loss 2.5193\n",
      "step 168220: train loss 2.3787, val loss 2.4050\n",
      "step 168230: train loss 2.3792, val loss 2.4015\n",
      "step 168240: train loss 2.3994, val loss 2.5122\n",
      "step 168250: train loss 2.3832, val loss 2.4888\n",
      "step 168260: train loss 2.4231, val loss 2.6421\n",
      "step 168270: train loss 2.4998, val loss 2.4247\n",
      "step 168280: train loss 2.3768, val loss 2.4284\n",
      "step 168290: train loss 2.4453, val loss 2.4057\n",
      "step 168300: train loss 2.3527, val loss 2.5176\n",
      "step 168310: train loss 2.4469, val loss 2.4440\n",
      "step 168320: train loss 2.4100, val loss 2.5524\n",
      "step 168330: train loss 2.3280, val loss 2.5172\n",
      "step 168340: train loss 2.4840, val loss 2.3253\n",
      "step 168350: train loss 2.4399, val loss 2.4179\n",
      "step 168360: train loss 2.5078, val loss 2.5758\n",
      "step 168370: train loss 2.3974, val loss 2.5964\n",
      "step 168380: train loss 2.4674, val loss 2.4871\n",
      "step 168390: train loss 2.4224, val loss 2.5968\n",
      "step 168400: train loss 2.4043, val loss 2.5719\n",
      "step 168410: train loss 2.3172, val loss 2.3542\n",
      "step 168420: train loss 2.4111, val loss 2.6380\n",
      "step 168430: train loss 2.3050, val loss 2.5378\n",
      "step 168440: train loss 2.4243, val loss 2.5067\n",
      "step 168450: train loss 2.3810, val loss 2.5377\n",
      "step 168460: train loss 2.3476, val loss 2.3860\n",
      "step 168470: train loss 2.4643, val loss 2.4948\n",
      "step 168480: train loss 2.4008, val loss 2.4950\n",
      "step 168490: train loss 2.4300, val loss 2.4726\n",
      "step 168500: train loss 2.4646, val loss 2.4019\n",
      "step 168510: train loss 2.3943, val loss 2.6263\n",
      "step 168520: train loss 2.3875, val loss 2.5862\n",
      "step 168530: train loss 2.3348, val loss 2.4245\n",
      "step 168540: train loss 2.3965, val loss 2.4217\n",
      "step 168550: train loss 2.3349, val loss 2.5036\n",
      "step 168560: train loss 2.4782, val loss 2.5902\n",
      "step 168570: train loss 2.4002, val loss 2.3862\n",
      "step 168580: train loss 2.4094, val loss 2.4453\n",
      "step 168590: train loss 2.3907, val loss 2.3515\n",
      "step 168600: train loss 2.3847, val loss 2.4442\n",
      "step 168610: train loss 2.4006, val loss 2.5456\n",
      "step 168620: train loss 2.3374, val loss 2.4585\n",
      "step 168630: train loss 2.4428, val loss 2.5058\n",
      "step 168640: train loss 2.3621, val loss 2.4570\n",
      "step 168650: train loss 2.4344, val loss 2.5583\n",
      "step 168660: train loss 2.4660, val loss 2.4957\n",
      "step 168670: train loss 2.4400, val loss 2.5734\n",
      "step 168680: train loss 2.3550, val loss 2.5381\n",
      "step 168690: train loss 2.3834, val loss 2.5394\n",
      "step 168700: train loss 2.4208, val loss 2.4680\n",
      "step 168710: train loss 2.3590, val loss 2.5082\n",
      "step 168720: train loss 2.3511, val loss 2.3943\n",
      "step 168730: train loss 2.4295, val loss 2.4547\n",
      "step 168740: train loss 2.3732, val loss 2.5504\n",
      "step 168750: train loss 2.4503, val loss 2.6865\n",
      "step 168760: train loss 2.3984, val loss 2.5418\n",
      "step 168770: train loss 2.4362, val loss 2.5141\n",
      "step 168780: train loss 2.4025, val loss 2.4822\n",
      "step 168790: train loss 2.4614, val loss 2.4881\n",
      "step 168800: train loss 2.4264, val loss 2.4426\n",
      "step 168810: train loss 2.4043, val loss 2.4825\n",
      "step 168820: train loss 2.3579, val loss 2.4963\n",
      "step 168830: train loss 2.3895, val loss 2.5219\n",
      "step 168840: train loss 2.4230, val loss 2.5408\n",
      "step 168850: train loss 2.4021, val loss 2.3916\n",
      "step 168860: train loss 2.4089, val loss 2.5557\n",
      "step 168870: train loss 2.4369, val loss 2.5713\n",
      "step 168880: train loss 2.3881, val loss 2.5533\n",
      "step 168890: train loss 2.3490, val loss 2.4719\n",
      "step 168900: train loss 2.5064, val loss 2.4325\n",
      "step 168910: train loss 2.4151, val loss 2.4404\n",
      "step 168920: train loss 2.4220, val loss 2.4196\n",
      "step 168930: train loss 2.4043, val loss 2.4651\n",
      "step 168940: train loss 2.3819, val loss 2.5293\n",
      "step 168950: train loss 2.4023, val loss 2.5054\n",
      "step 168960: train loss 2.4142, val loss 2.5471\n",
      "step 168970: train loss 2.4667, val loss 2.5543\n",
      "step 168980: train loss 2.4192, val loss 2.4379\n",
      "step 168990: train loss 2.4382, val loss 2.4964\n",
      "step 169000: train loss 2.3620, val loss 2.4802\n",
      "Generated text at iteration 169000\n",
      "\n",
      "a t l! deaielaute à lmar,\n",
      "\n",
      "AUÎcyre\n",
      "Ete t s.\n",
      "MESQcc'êt à;\n",
      "Sutent  s duerois s cre n, à terasou latran\n",
      "step 169010: train loss 2.3627, val loss 2.4387\n",
      "step 169020: train loss 2.4295, val loss 2.4306\n",
      "step 169030: train loss 2.4571, val loss 2.3734\n",
      "step 169040: train loss 2.4111, val loss 2.3733\n",
      "step 169050: train loss 2.3297, val loss 2.5938\n",
      "step 169060: train loss 2.3801, val loss 2.5535\n",
      "step 169070: train loss 2.4081, val loss 2.5529\n",
      "step 169080: train loss 2.4043, val loss 2.3380\n",
      "step 169090: train loss 2.4032, val loss 2.4159\n",
      "step 169100: train loss 2.3513, val loss 2.3514\n",
      "step 169110: train loss 2.4465, val loss 2.4190\n",
      "step 169120: train loss 2.4498, val loss 2.5178\n",
      "step 169130: train loss 2.3821, val loss 2.3680\n",
      "step 169140: train loss 2.3480, val loss 2.5071\n",
      "step 169150: train loss 2.4157, val loss 2.6481\n",
      "step 169160: train loss 2.3929, val loss 2.5343\n",
      "step 169170: train loss 2.3848, val loss 2.4493\n",
      "step 169180: train loss 2.3699, val loss 2.5123\n",
      "step 169190: train loss 2.4024, val loss 2.6140\n",
      "step 169200: train loss 2.3961, val loss 2.5738\n",
      "step 169210: train loss 2.4486, val loss 2.4788\n",
      "step 169220: train loss 2.3875, val loss 2.5348\n",
      "step 169230: train loss 2.4363, val loss 2.5025\n",
      "step 169240: train loss 2.4022, val loss 2.4401\n",
      "step 169250: train loss 2.4531, val loss 2.5652\n",
      "step 169260: train loss 2.4113, val loss 2.4425\n",
      "step 169270: train loss 2.4629, val loss 2.5247\n",
      "step 169280: train loss 2.3286, val loss 2.5422\n",
      "step 169290: train loss 2.3492, val loss 2.4428\n",
      "step 169300: train loss 2.3856, val loss 2.4921\n",
      "step 169310: train loss 2.4897, val loss 2.5926\n",
      "step 169320: train loss 2.4202, val loss 2.5956\n",
      "step 169330: train loss 2.4332, val loss 2.4887\n",
      "step 169340: train loss 2.4399, val loss 2.4969\n",
      "step 169350: train loss 2.3978, val loss 2.4650\n",
      "step 169360: train loss 2.3391, val loss 2.5184\n",
      "step 169370: train loss 2.4139, val loss 2.5041\n",
      "step 169380: train loss 2.4800, val loss 2.5457\n",
      "step 169390: train loss 2.4303, val loss 2.3987\n",
      "step 169400: train loss 2.4426, val loss 2.4073\n",
      "step 169410: train loss 2.4139, val loss 2.5435\n",
      "step 169420: train loss 2.2972, val loss 2.3514\n",
      "step 169430: train loss 2.4435, val loss 2.3971\n",
      "step 169440: train loss 2.4003, val loss 2.4403\n",
      "step 169450: train loss 2.4163, val loss 2.4020\n",
      "step 169460: train loss 2.3618, val loss 2.6049\n",
      "step 169470: train loss 2.3929, val loss 2.4763\n",
      "step 169480: train loss 2.4252, val loss 2.5965\n",
      "step 169490: train loss 2.4356, val loss 2.4907\n",
      "step 169500: train loss 2.4077, val loss 2.5311\n",
      "step 169510: train loss 2.3876, val loss 2.4528\n",
      "step 169520: train loss 2.4349, val loss 2.5582\n",
      "step 169530: train loss 2.3663, val loss 2.4157\n",
      "step 169540: train loss 2.4482, val loss 2.5115\n",
      "step 169550: train loss 2.3924, val loss 2.4547\n",
      "step 169560: train loss 2.3793, val loss 2.5029\n",
      "step 169570: train loss 2.4250, val loss 2.5725\n",
      "step 169580: train loss 2.3596, val loss 2.5895\n",
      "step 169590: train loss 2.3735, val loss 2.4178\n",
      "step 169600: train loss 2.4684, val loss 2.4649\n",
      "step 169610: train loss 2.3528, val loss 2.3976\n",
      "step 169620: train loss 2.4264, val loss 2.4540\n",
      "step 169630: train loss 2.4013, val loss 2.3925\n",
      "step 169640: train loss 2.3875, val loss 2.3834\n",
      "step 169650: train loss 2.3796, val loss 2.4535\n",
      "step 169660: train loss 2.4445, val loss 2.4727\n",
      "step 169670: train loss 2.3672, val loss 2.5343\n",
      "step 169680: train loss 2.4301, val loss 2.4648\n",
      "step 169690: train loss 2.3853, val loss 2.4405\n",
      "step 169700: train loss 2.4145, val loss 2.7165\n",
      "step 169710: train loss 2.3789, val loss 2.5183\n",
      "step 169720: train loss 2.4520, val loss 2.3505\n",
      "step 169730: train loss 2.4209, val loss 2.4379\n",
      "step 169740: train loss 2.3992, val loss 2.4159\n",
      "step 169750: train loss 2.3833, val loss 2.3741\n",
      "step 169760: train loss 2.5075, val loss 2.5208\n",
      "step 169770: train loss 2.4298, val loss 2.4933\n",
      "step 169780: train loss 2.3200, val loss 2.3789\n",
      "step 169790: train loss 2.3821, val loss 2.3689\n",
      "step 169800: train loss 2.3464, val loss 2.4415\n",
      "step 169810: train loss 2.4162, val loss 2.4726\n",
      "step 169820: train loss 2.3990, val loss 2.4491\n",
      "step 169830: train loss 2.4074, val loss 2.4741\n",
      "step 169840: train loss 2.3646, val loss 2.5429\n",
      "step 169850: train loss 2.4182, val loss 2.5608\n",
      "step 169860: train loss 2.4198, val loss 2.5453\n",
      "step 169870: train loss 2.3971, val loss 2.4393\n",
      "step 169880: train loss 2.4258, val loss 2.5834\n",
      "step 169890: train loss 2.5146, val loss 2.4822\n",
      "step 169900: train loss 2.4831, val loss 2.5773\n",
      "step 169910: train loss 2.3125, val loss 2.4926\n",
      "step 169920: train loss 2.4257, val loss 2.4283\n",
      "step 169930: train loss 2.5046, val loss 2.4658\n",
      "step 169940: train loss 2.4167, val loss 2.5146\n",
      "step 169950: train loss 2.4716, val loss 2.5236\n",
      "step 169960: train loss 2.3867, val loss 2.5156\n",
      "step 169970: train loss 2.5045, val loss 2.5449\n",
      "step 169980: train loss 2.4332, val loss 2.4781\n",
      "step 169990: train loss 2.4551, val loss 2.5163\n",
      "step 170000: train loss 2.3863, val loss 2.4306\n",
      "Generated text at iteration 170000\n",
      "\n",
      "\n",
      "OI\n",
      "Je panfffraquxpêÔL'êvas p  l'a 1Et  j;\n",
      "Qunt, fons s,\n",
      "\n",
      "Eméélonesqume de oitos le;\n",
      "Cere le tur is\n",
      "\n",
      "step 170010: train loss 2.3403, val loss 2.4601\n",
      "step 170020: train loss 2.3561, val loss 2.4846\n",
      "step 170030: train loss 2.3314, val loss 2.4956\n",
      "step 170040: train loss 2.4526, val loss 2.4216\n",
      "step 170050: train loss 2.3592, val loss 2.6157\n",
      "step 170060: train loss 2.4273, val loss 2.6228\n",
      "step 170070: train loss 2.3455, val loss 2.4417\n",
      "step 170080: train loss 2.4284, val loss 2.4936\n",
      "step 170090: train loss 2.4486, val loss 2.4728\n",
      "step 170100: train loss 2.3811, val loss 2.5085\n",
      "step 170110: train loss 2.3898, val loss 2.3953\n",
      "step 170120: train loss 2.3426, val loss 2.5911\n",
      "step 170130: train loss 2.3077, val loss 2.4623\n",
      "step 170140: train loss 2.5024, val loss 2.3431\n",
      "step 170150: train loss 2.4779, val loss 2.5346\n",
      "step 170160: train loss 2.4173, val loss 2.5587\n",
      "step 170170: train loss 2.4637, val loss 2.3844\n",
      "step 170180: train loss 2.4304, val loss 2.4678\n",
      "step 170190: train loss 2.4930, val loss 2.5075\n",
      "step 170200: train loss 2.4454, val loss 2.4742\n",
      "step 170210: train loss 2.4395, val loss 2.4828\n",
      "step 170220: train loss 2.4440, val loss 2.4958\n",
      "step 170230: train loss 2.3585, val loss 2.5815\n",
      "step 170240: train loss 2.4039, val loss 2.5204\n",
      "step 170250: train loss 2.4693, val loss 2.4823\n",
      "step 170260: train loss 2.4061, val loss 2.5043\n",
      "step 170270: train loss 2.4937, val loss 2.4314\n",
      "step 170280: train loss 2.3731, val loss 2.4583\n",
      "step 170290: train loss 2.3819, val loss 2.4827\n",
      "step 170300: train loss 2.4077, val loss 2.4272\n",
      "step 170310: train loss 2.4329, val loss 2.4728\n",
      "step 170320: train loss 2.3624, val loss 2.4278\n",
      "step 170330: train loss 2.3542, val loss 2.4955\n",
      "step 170340: train loss 2.4696, val loss 2.5013\n",
      "step 170350: train loss 2.3719, val loss 2.4188\n",
      "step 170360: train loss 2.3944, val loss 2.5042\n",
      "step 170370: train loss 2.4505, val loss 2.4912\n",
      "step 170380: train loss 2.4382, val loss 2.4238\n",
      "step 170390: train loss 2.3928, val loss 2.4707\n",
      "step 170400: train loss 2.3977, val loss 2.3860\n",
      "step 170410: train loss 2.4308, val loss 2.4903\n",
      "step 170420: train loss 2.3657, val loss 2.5093\n",
      "step 170430: train loss 2.3833, val loss 2.3629\n",
      "step 170440: train loss 2.4585, val loss 2.4801\n",
      "step 170450: train loss 2.3918, val loss 2.5105\n",
      "step 170460: train loss 2.4223, val loss 2.5218\n",
      "step 170470: train loss 2.3336, val loss 2.4488\n",
      "step 170480: train loss 2.4184, val loss 2.5007\n",
      "step 170490: train loss 2.3639, val loss 2.4334\n",
      "step 170500: train loss 2.4152, val loss 2.4068\n",
      "step 170510: train loss 2.3995, val loss 2.4691\n",
      "step 170520: train loss 2.3421, val loss 2.4492\n",
      "step 170530: train loss 2.4068, val loss 2.4912\n",
      "step 170540: train loss 2.3492, val loss 2.4850\n",
      "step 170550: train loss 2.3343, val loss 2.5108\n",
      "step 170560: train loss 2.3465, val loss 2.4428\n",
      "step 170570: train loss 2.4195, val loss 2.4269\n",
      "step 170580: train loss 2.4220, val loss 2.5086\n",
      "step 170590: train loss 2.3251, val loss 2.3687\n",
      "step 170600: train loss 2.3785, val loss 2.4778\n",
      "step 170610: train loss 2.4422, val loss 2.6082\n",
      "step 170620: train loss 2.3888, val loss 2.4980\n",
      "step 170630: train loss 2.3731, val loss 2.4772\n",
      "step 170640: train loss 2.4504, val loss 2.4038\n",
      "step 170650: train loss 2.3083, val loss 2.4223\n",
      "step 170660: train loss 2.3643, val loss 2.5595\n",
      "step 170670: train loss 2.4833, val loss 2.4660\n",
      "step 170680: train loss 2.3748, val loss 2.4148\n",
      "step 170690: train loss 2.3774, val loss 2.5085\n",
      "step 170700: train loss 2.4175, val loss 2.3310\n",
      "step 170710: train loss 2.3651, val loss 2.4386\n",
      "step 170720: train loss 2.3782, val loss 2.4418\n",
      "step 170730: train loss 2.4637, val loss 2.4312\n",
      "step 170740: train loss 2.4906, val loss 2.6458\n",
      "step 170750: train loss 2.3687, val loss 2.4641\n",
      "step 170760: train loss 2.4328, val loss 2.4752\n",
      "step 170770: train loss 2.4436, val loss 2.4394\n",
      "step 170780: train loss 2.4676, val loss 2.4733\n",
      "step 170790: train loss 2.3908, val loss 2.4260\n",
      "step 170800: train loss 2.3710, val loss 2.4255\n",
      "step 170810: train loss 2.4209, val loss 2.4557\n",
      "step 170820: train loss 2.4043, val loss 2.4966\n",
      "step 170830: train loss 2.3527, val loss 2.3842\n",
      "step 170840: train loss 2.3950, val loss 2.4123\n",
      "step 170850: train loss 2.4189, val loss 2.4968\n",
      "step 170860: train loss 2.3472, val loss 2.4647\n",
      "step 170870: train loss 2.4003, val loss 2.5635\n",
      "step 170880: train loss 2.4391, val loss 2.4644\n",
      "step 170890: train loss 2.4696, val loss 2.4685\n",
      "step 170900: train loss 2.3339, val loss 2.4508\n",
      "step 170910: train loss 2.4210, val loss 2.4352\n",
      "step 170920: train loss 2.3384, val loss 2.4194\n",
      "step 170930: train loss 2.3964, val loss 2.4519\n",
      "step 170940: train loss 2.4483, val loss 2.4183\n",
      "step 170950: train loss 2.4802, val loss 2.5285\n",
      "step 170960: train loss 2.4362, val loss 2.4932\n",
      "step 170970: train loss 2.3613, val loss 2.5606\n",
      "step 170980: train loss 2.3352, val loss 2.4476\n",
      "step 170990: train loss 2.3769, val loss 2.5559\n",
      "step 171000: train loss 2.4715, val loss 2.6003\n",
      "Generated text at iteration 171000\n",
      "\n",
      "Dée le échus s pirdunu surise! ée jontese mmomêt je glpoménun n prunorouvi---infiga tret vaurirû[u,\n",
      "\n",
      "step 171010: train loss 2.4018, val loss 2.5121\n",
      "step 171020: train loss 2.3601, val loss 2.5732\n",
      "step 171030: train loss 2.3853, val loss 2.4547\n",
      "step 171040: train loss 2.4066, val loss 2.4195\n",
      "step 171050: train loss 2.3767, val loss 2.5461\n",
      "step 171060: train loss 2.3944, val loss 2.3931\n",
      "step 171070: train loss 2.3292, val loss 2.4236\n",
      "step 171080: train loss 2.4877, val loss 2.4667\n",
      "step 171090: train loss 2.4546, val loss 2.4469\n",
      "step 171100: train loss 2.3627, val loss 2.5856\n",
      "step 171110: train loss 2.4182, val loss 2.4065\n",
      "step 171120: train loss 2.4733, val loss 2.5194\n",
      "step 171130: train loss 2.4289, val loss 2.5374\n",
      "step 171140: train loss 2.4048, val loss 2.4271\n",
      "step 171150: train loss 2.4230, val loss 2.4408\n",
      "step 171160: train loss 2.3107, val loss 2.5261\n",
      "step 171170: train loss 2.4256, val loss 2.5675\n",
      "step 171180: train loss 2.4077, val loss 2.5829\n",
      "step 171190: train loss 2.4611, val loss 2.3970\n",
      "step 171200: train loss 2.3898, val loss 2.4918\n",
      "step 171210: train loss 2.3672, val loss 2.4400\n",
      "step 171220: train loss 2.3595, val loss 2.5712\n",
      "step 171230: train loss 2.4108, val loss 2.6555\n",
      "step 171240: train loss 2.4506, val loss 2.4874\n",
      "step 171250: train loss 2.3384, val loss 2.4606\n",
      "step 171260: train loss 2.3972, val loss 2.5504\n",
      "step 171270: train loss 2.4051, val loss 2.4182\n",
      "step 171280: train loss 2.3899, val loss 2.3813\n",
      "step 171290: train loss 2.4147, val loss 2.4220\n",
      "step 171300: train loss 2.3230, val loss 2.6192\n",
      "step 171310: train loss 2.4694, val loss 2.5103\n",
      "step 171320: train loss 2.4198, val loss 2.6166\n",
      "step 171330: train loss 2.3995, val loss 2.4809\n",
      "step 171340: train loss 2.3332, val loss 2.4417\n",
      "step 171350: train loss 2.3325, val loss 2.4639\n",
      "step 171360: train loss 2.4228, val loss 2.4762\n",
      "step 171370: train loss 2.3848, val loss 2.6387\n",
      "step 171380: train loss 2.4045, val loss 2.5222\n",
      "step 171390: train loss 2.3515, val loss 2.4753\n",
      "step 171400: train loss 2.3820, val loss 2.4656\n",
      "step 171410: train loss 2.4090, val loss 2.4476\n",
      "step 171420: train loss 2.3855, val loss 2.4034\n",
      "step 171430: train loss 2.4256, val loss 2.4303\n",
      "step 171440: train loss 2.3815, val loss 2.4164\n",
      "step 171450: train loss 2.4497, val loss 2.2993\n",
      "step 171460: train loss 2.4135, val loss 2.5379\n",
      "step 171470: train loss 2.3497, val loss 2.5931\n",
      "step 171480: train loss 2.4389, val loss 2.4602\n",
      "step 171490: train loss 2.4374, val loss 2.5889\n",
      "step 171500: train loss 2.4132, val loss 2.4476\n",
      "step 171510: train loss 2.4516, val loss 2.5334\n",
      "step 171520: train loss 2.3704, val loss 2.4649\n",
      "step 171530: train loss 2.3953, val loss 2.4496\n",
      "step 171540: train loss 2.4221, val loss 2.4865\n",
      "step 171550: train loss 2.4032, val loss 2.4772\n",
      "step 171560: train loss 2.4362, val loss 2.4600\n",
      "step 171570: train loss 2.3999, val loss 2.4342\n",
      "step 171580: train loss 2.3914, val loss 2.5380\n",
      "step 171590: train loss 2.3640, val loss 2.3586\n",
      "step 171600: train loss 2.4015, val loss 2.4123\n",
      "step 171610: train loss 2.3648, val loss 2.5072\n",
      "step 171620: train loss 2.3819, val loss 2.4894\n",
      "step 171630: train loss 2.3915, val loss 2.5593\n",
      "step 171640: train loss 2.4714, val loss 2.5717\n",
      "step 171650: train loss 2.4012, val loss 2.5071\n",
      "step 171660: train loss 2.3850, val loss 2.5084\n",
      "step 171670: train loss 2.3699, val loss 2.4586\n",
      "step 171680: train loss 2.4006, val loss 2.5027\n",
      "step 171690: train loss 2.3631, val loss 2.3453\n",
      "step 171700: train loss 2.4150, val loss 2.4889\n",
      "step 171710: train loss 2.3704, val loss 2.4150\n",
      "step 171720: train loss 2.4102, val loss 2.4211\n",
      "step 171730: train loss 2.3916, val loss 2.5037\n",
      "step 171740: train loss 2.3733, val loss 2.4613\n",
      "step 171750: train loss 2.3849, val loss 2.3641\n",
      "step 171760: train loss 2.4310, val loss 2.4533\n",
      "step 171770: train loss 2.3926, val loss 2.5320\n",
      "step 171780: train loss 2.4035, val loss 2.5858\n",
      "step 171790: train loss 2.4069, val loss 2.4961\n",
      "step 171800: train loss 2.4442, val loss 2.4288\n",
      "step 171810: train loss 2.4476, val loss 2.5356\n",
      "step 171820: train loss 2.3675, val loss 2.4450\n",
      "step 171830: train loss 2.4255, val loss 2.4758\n",
      "step 171840: train loss 2.3766, val loss 2.4093\n",
      "step 171850: train loss 2.3479, val loss 2.5543\n",
      "step 171860: train loss 2.4505, val loss 2.4960\n",
      "step 171870: train loss 2.4308, val loss 2.4014\n",
      "step 171880: train loss 2.4259, val loss 2.5576\n",
      "step 171890: train loss 2.3835, val loss 2.4368\n",
      "step 171900: train loss 2.3986, val loss 2.6420\n",
      "step 171910: train loss 2.3866, val loss 2.4103\n",
      "step 171920: train loss 2.5660, val loss 2.4327\n",
      "step 171930: train loss 2.3676, val loss 2.3860\n",
      "step 171940: train loss 2.4342, val loss 2.6101\n",
      "step 171950: train loss 2.4380, val loss 2.5318\n",
      "step 171960: train loss 2.3959, val loss 2.5671\n",
      "step 171970: train loss 2.3528, val loss 2.4585\n",
      "step 171980: train loss 2.4214, val loss 2.4565\n",
      "step 171990: train loss 2.4723, val loss 2.5416\n",
      "step 172000: train loss 2.3947, val loss 2.4302\n",
      "Generated text at iteration 172000\n",
      "\n",
      "\n",
      "LIle!\n",
      "Areffomin hans e,  me ple et vole\n",
      "L'he  alount lamblsa lpe\n",
      " rayesplipesa chéve  poristorlon s\n",
      "step 172010: train loss 2.3464, val loss 2.5356\n",
      "step 172020: train loss 2.3875, val loss 2.3676\n",
      "step 172030: train loss 2.4836, val loss 2.4811\n",
      "step 172040: train loss 2.4012, val loss 2.4648\n",
      "step 172050: train loss 2.4180, val loss 2.4413\n",
      "step 172060: train loss 2.4002, val loss 2.4538\n",
      "step 172070: train loss 2.3972, val loss 2.5138\n",
      "step 172080: train loss 2.3939, val loss 2.4290\n",
      "step 172090: train loss 2.3395, val loss 2.4854\n",
      "step 172100: train loss 2.3551, val loss 2.4726\n",
      "step 172110: train loss 2.4116, val loss 2.4016\n",
      "step 172120: train loss 2.3714, val loss 2.5037\n",
      "step 172130: train loss 2.4246, val loss 2.4797\n",
      "step 172140: train loss 2.4195, val loss 2.4210\n",
      "step 172150: train loss 2.3660, val loss 2.4918\n",
      "step 172160: train loss 2.3571, val loss 2.4320\n",
      "step 172170: train loss 2.3604, val loss 2.5478\n",
      "step 172180: train loss 2.3741, val loss 2.5775\n",
      "step 172190: train loss 2.4199, val loss 2.4625\n",
      "step 172200: train loss 2.4103, val loss 2.5236\n",
      "step 172210: train loss 2.3855, val loss 2.4861\n",
      "step 172220: train loss 2.3546, val loss 2.4564\n",
      "step 172230: train loss 2.3585, val loss 2.4248\n",
      "step 172240: train loss 2.3648, val loss 2.5735\n",
      "step 172250: train loss 2.4455, val loss 2.4833\n",
      "step 172260: train loss 2.4897, val loss 2.5198\n",
      "step 172270: train loss 2.4130, val loss 2.6356\n",
      "step 172280: train loss 2.4055, val loss 2.4196\n",
      "step 172290: train loss 2.3013, val loss 2.4242\n",
      "step 172300: train loss 2.4061, val loss 2.4373\n",
      "step 172310: train loss 2.4379, val loss 2.4192\n",
      "step 172320: train loss 2.4912, val loss 2.4809\n",
      "step 172330: train loss 2.4331, val loss 2.4781\n",
      "step 172340: train loss 2.3526, val loss 2.4851\n",
      "step 172350: train loss 2.3832, val loss 2.5546\n",
      "step 172360: train loss 2.3755, val loss 2.5991\n",
      "step 172370: train loss 2.3629, val loss 2.4681\n",
      "step 172380: train loss 2.4602, val loss 2.5203\n",
      "step 172390: train loss 2.4052, val loss 2.4499\n",
      "step 172400: train loss 2.3897, val loss 2.5548\n",
      "step 172410: train loss 2.4548, val loss 2.5523\n",
      "step 172420: train loss 2.3453, val loss 2.4742\n",
      "step 172430: train loss 2.4320, val loss 2.5284\n",
      "step 172440: train loss 2.4534, val loss 2.3792\n",
      "step 172450: train loss 2.4777, val loss 2.5515\n",
      "step 172460: train loss 2.4517, val loss 2.4205\n",
      "step 172470: train loss 2.3977, val loss 2.4655\n",
      "step 172480: train loss 2.3444, val loss 2.5483\n",
      "step 172490: train loss 2.3501, val loss 2.4607\n",
      "step 172500: train loss 2.3645, val loss 2.4819\n",
      "step 172510: train loss 2.3476, val loss 2.4196\n",
      "step 172520: train loss 2.4535, val loss 2.5599\n",
      "step 172530: train loss 2.3743, val loss 2.4808\n",
      "step 172540: train loss 2.4025, val loss 2.5697\n",
      "step 172550: train loss 2.4396, val loss 2.4950\n",
      "step 172560: train loss 2.3771, val loss 2.5116\n",
      "step 172570: train loss 2.4198, val loss 2.5414\n",
      "step 172580: train loss 2.4116, val loss 2.4687\n",
      "step 172590: train loss 2.3936, val loss 2.5357\n",
      "step 172600: train loss 2.3773, val loss 2.5077\n",
      "step 172610: train loss 2.4513, val loss 2.4595\n",
      "step 172620: train loss 2.3759, val loss 2.4710\n",
      "step 172630: train loss 2.3680, val loss 2.5377\n",
      "step 172640: train loss 2.3239, val loss 2.4866\n",
      "step 172650: train loss 2.4061, val loss 2.5447\n",
      "step 172660: train loss 2.3313, val loss 2.4456\n",
      "step 172670: train loss 2.3723, val loss 2.4607\n",
      "step 172680: train loss 2.3903, val loss 2.4742\n",
      "step 172690: train loss 2.4434, val loss 2.4704\n",
      "step 172700: train loss 2.3505, val loss 2.4675\n",
      "step 172710: train loss 2.4475, val loss 2.4714\n",
      "step 172720: train loss 2.3684, val loss 2.5448\n",
      "step 172730: train loss 2.3297, val loss 2.5714\n",
      "step 172740: train loss 2.3917, val loss 2.4763\n",
      "step 172750: train loss 2.4532, val loss 2.5505\n",
      "step 172760: train loss 2.3906, val loss 2.4509\n",
      "step 172770: train loss 2.3568, val loss 2.4542\n",
      "step 172780: train loss 2.3832, val loss 2.4619\n",
      "step 172790: train loss 2.4360, val loss 2.4689\n",
      "step 172800: train loss 2.3502, val loss 2.5015\n",
      "step 172810: train loss 2.3391, val loss 2.4410\n",
      "step 172820: train loss 2.3976, val loss 2.4265\n",
      "step 172830: train loss 2.3766, val loss 2.4401\n",
      "step 172840: train loss 2.4605, val loss 2.4873\n",
      "step 172850: train loss 2.3895, val loss 2.4084\n",
      "step 172860: train loss 2.3823, val loss 2.3691\n",
      "step 172870: train loss 2.4805, val loss 2.4930\n",
      "step 172880: train loss 2.4165, val loss 2.5354\n",
      "step 172890: train loss 2.4034, val loss 2.4536\n",
      "step 172900: train loss 2.4657, val loss 2.4736\n",
      "step 172910: train loss 2.4345, val loss 2.4541\n",
      "step 172920: train loss 2.3937, val loss 2.5129\n",
      "step 172930: train loss 2.3803, val loss 2.4703\n",
      "step 172940: train loss 2.4214, val loss 2.4836\n",
      "step 172950: train loss 2.4164, val loss 2.5397\n",
      "step 172960: train loss 2.3808, val loss 2.4883\n",
      "step 172970: train loss 2.4343, val loss 2.4616\n",
      "step 172980: train loss 2.4226, val loss 2.5206\n",
      "step 172990: train loss 2.3623, val loss 2.4202\n",
      "step 173000: train loss 2.3993, val loss 2.5129\n",
      "Generated text at iteration 173000\n",
      "\n",
      "\n",
      "Noie li néarenc'e,\n",
      "Thar jTJeutris t drel spanais Jeubrus'ans O lere    u ches plausibîmmboù?\n",
      "Cla Ch\n",
      "step 173010: train loss 2.4612, val loss 2.4333\n",
      "step 173020: train loss 2.4594, val loss 2.5897\n",
      "step 173030: train loss 2.4408, val loss 2.5181\n",
      "step 173040: train loss 2.3894, val loss 2.4579\n",
      "step 173050: train loss 2.3490, val loss 2.4809\n",
      "step 173060: train loss 2.3725, val loss 2.5208\n",
      "step 173070: train loss 2.4168, val loss 2.3610\n",
      "step 173080: train loss 2.3669, val loss 2.4120\n",
      "step 173090: train loss 2.4200, val loss 2.5293\n",
      "step 173100: train loss 2.4187, val loss 2.5049\n",
      "step 173110: train loss 2.3960, val loss 2.5110\n",
      "step 173120: train loss 2.4314, val loss 2.4219\n",
      "step 173130: train loss 2.4387, val loss 2.5137\n",
      "step 173140: train loss 2.4115, val loss 2.5511\n",
      "step 173150: train loss 2.4247, val loss 2.4467\n",
      "step 173160: train loss 2.4136, val loss 2.5533\n",
      "step 173170: train loss 2.4757, val loss 2.5364\n",
      "step 173180: train loss 2.4021, val loss 2.4191\n",
      "step 173190: train loss 2.3674, val loss 2.4612\n",
      "step 173200: train loss 2.4276, val loss 2.4864\n",
      "step 173210: train loss 2.4077, val loss 2.4868\n",
      "step 173220: train loss 2.4401, val loss 2.3797\n",
      "step 173230: train loss 2.4757, val loss 2.5257\n",
      "step 173240: train loss 2.3175, val loss 2.4537\n",
      "step 173250: train loss 2.3683, val loss 2.3968\n",
      "step 173260: train loss 2.4618, val loss 2.4744\n",
      "step 173270: train loss 2.4259, val loss 2.5942\n",
      "step 173280: train loss 2.3515, val loss 2.4642\n",
      "step 173290: train loss 2.4231, val loss 2.5232\n",
      "step 173300: train loss 2.3947, val loss 2.4241\n",
      "step 173310: train loss 2.3241, val loss 2.4581\n",
      "step 173320: train loss 2.3792, val loss 2.4910\n",
      "step 173330: train loss 2.3741, val loss 2.4187\n",
      "step 173340: train loss 2.3198, val loss 2.3485\n",
      "step 173350: train loss 2.3707, val loss 2.4621\n",
      "step 173360: train loss 2.3670, val loss 2.4814\n",
      "step 173370: train loss 2.3376, val loss 2.3814\n",
      "step 173380: train loss 2.4404, val loss 2.5574\n",
      "step 173390: train loss 2.4083, val loss 2.4051\n",
      "step 173400: train loss 2.2941, val loss 2.3452\n",
      "step 173410: train loss 2.3894, val loss 2.4326\n",
      "step 173420: train loss 2.4292, val loss 2.4692\n",
      "step 173430: train loss 2.3649, val loss 2.4631\n",
      "step 173440: train loss 2.3661, val loss 2.4698\n",
      "step 173450: train loss 2.4602, val loss 2.5588\n",
      "step 173460: train loss 2.4186, val loss 2.4654\n",
      "step 173470: train loss 2.3751, val loss 2.4126\n",
      "step 173480: train loss 2.3660, val loss 2.4633\n",
      "step 173490: train loss 2.4089, val loss 2.4305\n",
      "step 173500: train loss 2.4674, val loss 2.4634\n",
      "step 173510: train loss 2.4086, val loss 2.3415\n",
      "step 173520: train loss 2.4273, val loss 2.5362\n",
      "step 173530: train loss 2.4271, val loss 2.4990\n",
      "step 173540: train loss 2.4161, val loss 2.4942\n",
      "step 173550: train loss 2.4250, val loss 2.4152\n",
      "step 173560: train loss 2.4258, val loss 2.4028\n",
      "step 173570: train loss 2.4276, val loss 2.4881\n",
      "step 173580: train loss 2.3930, val loss 2.4283\n",
      "step 173590: train loss 2.4064, val loss 2.3993\n",
      "step 173600: train loss 2.3581, val loss 2.4056\n",
      "step 173610: train loss 2.3748, val loss 2.4046\n",
      "step 173620: train loss 2.3812, val loss 2.4535\n",
      "step 173630: train loss 2.4376, val loss 2.4411\n",
      "step 173640: train loss 2.3874, val loss 2.5623\n",
      "step 173650: train loss 2.3836, val loss 2.3331\n",
      "step 173660: train loss 2.3948, val loss 2.5121\n",
      "step 173670: train loss 2.4656, val loss 2.5227\n",
      "step 173680: train loss 2.4214, val loss 2.5119\n",
      "step 173690: train loss 2.4067, val loss 2.6015\n",
      "step 173700: train loss 2.3912, val loss 2.4435\n",
      "step 173710: train loss 2.3327, val loss 2.4949\n",
      "step 173720: train loss 2.3020, val loss 2.5082\n",
      "step 173730: train loss 2.3942, val loss 2.5367\n",
      "step 173740: train loss 2.3686, val loss 2.4676\n",
      "step 173750: train loss 2.4284, val loss 2.4859\n",
      "step 173760: train loss 2.3872, val loss 2.5335\n",
      "step 173770: train loss 2.3774, val loss 2.4433\n",
      "step 173780: train loss 2.3501, val loss 2.4241\n",
      "step 173790: train loss 2.4562, val loss 2.5091\n",
      "step 173800: train loss 2.3850, val loss 2.5265\n",
      "step 173810: train loss 2.3756, val loss 2.5552\n",
      "step 173820: train loss 2.4030, val loss 2.5359\n",
      "step 173830: train loss 2.4228, val loss 2.4058\n",
      "step 173840: train loss 2.4676, val loss 2.4556\n",
      "step 173850: train loss 2.3835, val loss 2.4277\n",
      "step 173860: train loss 2.5085, val loss 2.4828\n",
      "step 173870: train loss 2.3122, val loss 2.4385\n",
      "step 173880: train loss 2.4237, val loss 2.4838\n",
      "step 173890: train loss 2.3483, val loss 2.5397\n",
      "step 173900: train loss 2.3569, val loss 2.5275\n",
      "step 173910: train loss 2.4011, val loss 2.4123\n",
      "step 173920: train loss 2.3846, val loss 2.4285\n",
      "step 173930: train loss 2.4257, val loss 2.5622\n",
      "step 173940: train loss 2.4536, val loss 2.5339\n",
      "step 173950: train loss 2.4145, val loss 2.4639\n",
      "step 173960: train loss 2.3497, val loss 2.5662\n",
      "step 173970: train loss 2.4140, val loss 2.5488\n",
      "step 173980: train loss 2.4191, val loss 2.4403\n",
      "step 173990: train loss 2.3941, val loss 2.4718\n",
      "step 174000: train loss 2.4033, val loss 2.5454\n",
      "Generated text at iteration 174000\n",
      "\n",
      "Quirie, menferilome Til.\n",
      "C'antoi;\n",
      "Etentas;\n",
      "TAlléainiez2! don vobet l'agre s'houbixitet s--nd'hauenes\n",
      "step 174010: train loss 2.4173, val loss 2.5283\n",
      "step 174020: train loss 2.4212, val loss 2.5055\n",
      "step 174030: train loss 2.3283, val loss 2.5448\n",
      "step 174040: train loss 2.4716, val loss 2.5181\n",
      "step 174050: train loss 2.4500, val loss 2.5109\n",
      "step 174060: train loss 2.4210, val loss 2.4414\n",
      "step 174070: train loss 2.3451, val loss 2.5517\n",
      "step 174080: train loss 2.3577, val loss 2.4776\n",
      "step 174090: train loss 2.3933, val loss 2.5614\n",
      "step 174100: train loss 2.4016, val loss 2.5246\n",
      "step 174110: train loss 2.4799, val loss 2.4779\n",
      "step 174120: train loss 2.4150, val loss 2.4551\n",
      "step 174130: train loss 2.3628, val loss 2.4320\n",
      "step 174140: train loss 2.3677, val loss 2.5170\n",
      "step 174150: train loss 2.4144, val loss 2.5382\n",
      "step 174160: train loss 2.3814, val loss 2.6055\n",
      "step 174170: train loss 2.3830, val loss 2.4344\n",
      "step 174180: train loss 2.3920, val loss 2.5084\n",
      "step 174190: train loss 2.3799, val loss 2.5204\n",
      "step 174200: train loss 2.3544, val loss 2.5337\n",
      "step 174210: train loss 2.3492, val loss 2.4886\n",
      "step 174220: train loss 2.4232, val loss 2.4937\n",
      "step 174230: train loss 2.3849, val loss 2.4286\n",
      "step 174240: train loss 2.3924, val loss 2.4381\n",
      "step 174250: train loss 2.4033, val loss 2.5197\n",
      "step 174260: train loss 2.4235, val loss 2.4135\n",
      "step 174270: train loss 2.4299, val loss 2.4924\n",
      "step 174280: train loss 2.3467, val loss 2.5002\n",
      "step 174290: train loss 2.3977, val loss 2.5701\n",
      "step 174300: train loss 2.4563, val loss 2.5350\n",
      "step 174310: train loss 2.3676, val loss 2.5424\n",
      "step 174320: train loss 2.3356, val loss 2.4704\n",
      "step 174330: train loss 2.3183, val loss 2.4382\n",
      "step 174340: train loss 2.3733, val loss 2.4501\n",
      "step 174350: train loss 2.4416, val loss 2.3653\n",
      "step 174360: train loss 2.3225, val loss 2.4722\n",
      "step 174370: train loss 2.3839, val loss 2.5826\n",
      "step 174380: train loss 2.4459, val loss 2.4487\n",
      "step 174390: train loss 2.3980, val loss 2.5635\n",
      "step 174400: train loss 2.4624, val loss 2.5070\n",
      "step 174410: train loss 2.4097, val loss 2.4385\n",
      "step 174420: train loss 2.3823, val loss 2.4426\n",
      "step 174430: train loss 2.3965, val loss 2.4729\n",
      "step 174440: train loss 2.3793, val loss 2.5579\n",
      "step 174450: train loss 2.4477, val loss 2.4917\n",
      "step 174460: train loss 2.4275, val loss 2.4396\n",
      "step 174470: train loss 2.3916, val loss 2.5182\n",
      "step 174480: train loss 2.3667, val loss 2.4626\n",
      "step 174490: train loss 2.4054, val loss 2.4893\n",
      "step 174500: train loss 2.3998, val loss 2.5810\n",
      "step 174510: train loss 2.3260, val loss 2.6128\n",
      "step 174520: train loss 2.4246, val loss 2.5151\n",
      "step 174530: train loss 2.3863, val loss 2.5519\n",
      "step 174540: train loss 2.4872, val loss 2.4822\n",
      "step 174550: train loss 2.4160, val loss 2.4868\n",
      "step 174560: train loss 2.4788, val loss 2.4492\n",
      "step 174570: train loss 2.4086, val loss 2.4167\n",
      "step 174580: train loss 2.3599, val loss 2.4960\n",
      "step 174590: train loss 2.4119, val loss 2.4787\n",
      "step 174600: train loss 2.4641, val loss 2.4241\n",
      "step 174610: train loss 2.3881, val loss 2.5162\n",
      "step 174620: train loss 2.3489, val loss 2.4485\n",
      "step 174630: train loss 2.3853, val loss 2.3849\n",
      "step 174640: train loss 2.3558, val loss 2.6839\n",
      "step 174650: train loss 2.4479, val loss 2.4241\n",
      "step 174660: train loss 2.3979, val loss 2.5271\n",
      "step 174670: train loss 2.4037, val loss 2.4182\n",
      "step 174680: train loss 2.4650, val loss 2.5372\n",
      "step 174690: train loss 2.3282, val loss 2.4931\n",
      "step 174700: train loss 2.4617, val loss 2.4667\n",
      "step 174710: train loss 2.4900, val loss 2.4466\n",
      "step 174720: train loss 2.4280, val loss 2.4727\n",
      "step 174730: train loss 2.3419, val loss 2.3884\n",
      "step 174740: train loss 2.4220, val loss 2.4856\n",
      "step 174750: train loss 2.4251, val loss 2.4755\n",
      "step 174760: train loss 2.3984, val loss 2.3413\n",
      "step 174770: train loss 2.3615, val loss 2.4967\n",
      "step 174780: train loss 2.3971, val loss 2.5475\n",
      "step 174790: train loss 2.3870, val loss 2.5600\n",
      "step 174800: train loss 2.4124, val loss 2.5490\n",
      "step 174810: train loss 2.3756, val loss 2.4380\n",
      "step 174820: train loss 2.5201, val loss 2.3661\n",
      "step 174830: train loss 2.3874, val loss 2.4413\n",
      "step 174840: train loss 2.4113, val loss 2.3747\n",
      "step 174850: train loss 2.4799, val loss 2.4418\n",
      "step 174860: train loss 2.4216, val loss 2.4478\n",
      "step 174870: train loss 2.3964, val loss 2.4340\n",
      "step 174880: train loss 2.4036, val loss 2.4626\n",
      "step 174890: train loss 2.3840, val loss 2.5313\n",
      "step 174900: train loss 2.3858, val loss 2.4003\n",
      "step 174910: train loss 2.5117, val loss 2.5189\n",
      "step 174920: train loss 2.3917, val loss 2.5490\n",
      "step 174930: train loss 2.3765, val loss 2.4633\n",
      "step 174940: train loss 2.4653, val loss 2.5041\n",
      "step 174950: train loss 2.3673, val loss 2.4603\n",
      "step 174960: train loss 2.4425, val loss 2.4502\n",
      "step 174970: train loss 2.4137, val loss 2.4292\n",
      "step 174980: train loss 2.4766, val loss 2.4296\n",
      "step 174990: train loss 2.3826, val loss 2.4406\n",
      "step 175000: train loss 2.4564, val loss 2.5317\n",
      "Generated text at iteration 175000\n",
      "\n",
      "Cois uir.\n",
      "Et,\n",
      "Vventt, Care s,   creullar le res, éanchatit ce RÉccrer mor lerdoux.-ccitèÔ]îBpomêl,\n",
      "D\n",
      "step 175010: train loss 2.3757, val loss 2.4924\n",
      "step 175020: train loss 2.4051, val loss 2.4325\n",
      "step 175030: train loss 2.4207, val loss 2.4629\n",
      "step 175040: train loss 2.3680, val loss 2.5667\n",
      "step 175050: train loss 2.3957, val loss 2.5362\n",
      "step 175060: train loss 2.3992, val loss 2.5400\n",
      "step 175070: train loss 2.3524, val loss 2.3773\n",
      "step 175080: train loss 2.4231, val loss 2.3908\n",
      "step 175090: train loss 2.4615, val loss 2.4194\n",
      "step 175100: train loss 2.4107, val loss 2.4577\n",
      "step 175110: train loss 2.4736, val loss 2.4019\n",
      "step 175120: train loss 2.3307, val loss 2.4447\n",
      "step 175130: train loss 2.4491, val loss 2.5306\n",
      "step 175140: train loss 2.3804, val loss 2.4559\n",
      "step 175150: train loss 2.4170, val loss 2.4015\n",
      "step 175160: train loss 2.4114, val loss 2.5082\n",
      "step 175170: train loss 2.3842, val loss 2.5540\n",
      "step 175180: train loss 2.4343, val loss 2.4744\n",
      "step 175190: train loss 2.4247, val loss 2.5632\n",
      "step 175200: train loss 2.4121, val loss 2.4892\n",
      "step 175210: train loss 2.4749, val loss 2.4320\n",
      "step 175220: train loss 2.4235, val loss 2.4577\n",
      "step 175230: train loss 2.3344, val loss 2.4688\n",
      "step 175240: train loss 2.4276, val loss 2.4931\n",
      "step 175250: train loss 2.4140, val loss 2.4898\n",
      "step 175260: train loss 2.3593, val loss 2.5549\n",
      "step 175270: train loss 2.4367, val loss 2.5980\n",
      "step 175280: train loss 2.4060, val loss 2.4575\n",
      "step 175290: train loss 2.4332, val loss 2.3807\n",
      "step 175300: train loss 2.3543, val loss 2.4207\n",
      "step 175310: train loss 2.3740, val loss 2.5747\n",
      "step 175320: train loss 2.4508, val loss 2.4870\n",
      "step 175330: train loss 2.4272, val loss 2.4815\n",
      "step 175340: train loss 2.3723, val loss 2.5494\n",
      "step 175350: train loss 2.3007, val loss 2.4084\n",
      "step 175360: train loss 2.3989, val loss 2.5873\n",
      "step 175370: train loss 2.4487, val loss 2.4381\n",
      "step 175380: train loss 2.3576, val loss 2.5217\n",
      "step 175390: train loss 2.3929, val loss 2.5453\n",
      "step 175400: train loss 2.4550, val loss 2.4460\n",
      "step 175410: train loss 2.4965, val loss 2.4943\n",
      "step 175420: train loss 2.4112, val loss 2.4239\n",
      "step 175430: train loss 2.3971, val loss 2.4758\n",
      "step 175440: train loss 2.4006, val loss 2.3451\n",
      "step 175450: train loss 2.4046, val loss 2.5733\n",
      "step 175460: train loss 2.4457, val loss 2.5920\n",
      "step 175470: train loss 2.3607, val loss 2.4435\n",
      "step 175480: train loss 2.3143, val loss 2.4509\n",
      "step 175490: train loss 2.3527, val loss 2.5085\n",
      "step 175500: train loss 2.5059, val loss 2.5972\n",
      "step 175510: train loss 2.3620, val loss 2.4157\n",
      "step 175520: train loss 2.4070, val loss 2.5080\n",
      "step 175530: train loss 2.3468, val loss 2.5000\n",
      "step 175540: train loss 2.3868, val loss 2.5181\n",
      "step 175550: train loss 2.4267, val loss 2.4822\n",
      "step 175560: train loss 2.4366, val loss 2.5707\n",
      "step 175570: train loss 2.3526, val loss 2.5438\n",
      "step 175580: train loss 2.4369, val loss 2.4899\n",
      "step 175590: train loss 2.3963, val loss 2.4552\n",
      "step 175600: train loss 2.4186, val loss 2.5858\n",
      "step 175610: train loss 2.4296, val loss 2.4523\n",
      "step 175620: train loss 2.4585, val loss 2.5594\n",
      "step 175630: train loss 2.4570, val loss 2.4542\n",
      "step 175640: train loss 2.4072, val loss 2.4928\n",
      "step 175650: train loss 2.4496, val loss 2.4483\n",
      "step 175660: train loss 2.4352, val loss 2.4521\n",
      "step 175670: train loss 2.3635, val loss 2.6569\n",
      "step 175680: train loss 2.3995, val loss 2.5989\n",
      "step 175690: train loss 2.4471, val loss 2.4675\n",
      "step 175700: train loss 2.3942, val loss 2.4625\n",
      "step 175710: train loss 2.4110, val loss 2.3870\n",
      "step 175720: train loss 2.4908, val loss 2.5385\n",
      "step 175730: train loss 2.3946, val loss 2.5617\n",
      "step 175740: train loss 2.3369, val loss 2.5214\n",
      "step 175750: train loss 2.5072, val loss 2.5393\n",
      "step 175760: train loss 2.5032, val loss 2.4815\n",
      "step 175770: train loss 2.3987, val loss 2.4900\n",
      "step 175780: train loss 2.3567, val loss 2.5482\n",
      "step 175790: train loss 2.4418, val loss 2.4107\n",
      "step 175800: train loss 2.4448, val loss 2.4335\n",
      "step 175810: train loss 2.4511, val loss 2.4460\n",
      "step 175820: train loss 2.4231, val loss 2.4347\n",
      "step 175830: train loss 2.4101, val loss 2.4711\n",
      "step 175840: train loss 2.4355, val loss 2.4538\n",
      "step 175850: train loss 2.4121, val loss 2.4109\n",
      "step 175860: train loss 2.4415, val loss 2.4199\n",
      "step 175870: train loss 2.3986, val loss 2.4848\n",
      "step 175880: train loss 2.4568, val loss 2.4991\n",
      "step 175890: train loss 2.4559, val loss 2.5512\n",
      "step 175900: train loss 2.3728, val loss 2.4365\n",
      "step 175910: train loss 2.4839, val loss 2.4395\n",
      "step 175920: train loss 2.4486, val loss 2.4524\n",
      "step 175930: train loss 2.3616, val loss 2.5028\n",
      "step 175940: train loss 2.4350, val loss 2.4499\n",
      "step 175950: train loss 2.4483, val loss 2.5074\n",
      "step 175960: train loss 2.3577, val loss 2.4837\n",
      "step 175970: train loss 2.3300, val loss 2.4560\n",
      "step 175980: train loss 2.4037, val loss 2.4802\n",
      "step 175990: train loss 2.4490, val loss 2.3993\n",
      "step 176000: train loss 2.4035, val loss 2.5860\n",
      "Generated text at iteration 176000\n",
      "\n",
      "Tes an a utèrinu qus le ere d'a direchesaster mes labre-Susu  dusoëyle pe cuiqunt duire qumait-ies l\n",
      "step 176010: train loss 2.4382, val loss 2.4064\n",
      "step 176020: train loss 2.4089, val loss 2.4679\n",
      "step 176030: train loss 2.3680, val loss 2.4937\n",
      "step 176040: train loss 2.4523, val loss 2.4871\n",
      "step 176050: train loss 2.3406, val loss 2.4114\n",
      "step 176060: train loss 2.3909, val loss 2.5183\n",
      "step 176070: train loss 2.3845, val loss 2.5606\n",
      "step 176080: train loss 2.4126, val loss 2.5001\n",
      "step 176090: train loss 2.4384, val loss 2.5020\n",
      "step 176100: train loss 2.3126, val loss 2.4854\n",
      "step 176110: train loss 2.4029, val loss 2.4622\n",
      "step 176120: train loss 2.3838, val loss 2.5222\n",
      "step 176130: train loss 2.4454, val loss 2.5208\n",
      "step 176140: train loss 2.4663, val loss 2.5888\n",
      "step 176150: train loss 2.3527, val loss 2.4829\n",
      "step 176160: train loss 2.4487, val loss 2.5018\n",
      "step 176170: train loss 2.3874, val loss 2.4884\n",
      "step 176180: train loss 2.4303, val loss 2.5181\n",
      "step 176190: train loss 2.4894, val loss 2.4143\n",
      "step 176200: train loss 2.4434, val loss 2.5562\n",
      "step 176210: train loss 2.4703, val loss 2.6294\n",
      "step 176220: train loss 2.4306, val loss 2.4744\n",
      "step 176230: train loss 2.5017, val loss 2.5401\n",
      "step 176240: train loss 2.4044, val loss 2.5428\n",
      "step 176250: train loss 2.4129, val loss 2.6166\n",
      "step 176260: train loss 2.3448, val loss 2.5759\n",
      "step 176270: train loss 2.4219, val loss 2.4208\n",
      "step 176280: train loss 2.4098, val loss 2.4057\n",
      "step 176290: train loss 2.4573, val loss 2.6096\n",
      "step 176300: train loss 2.3786, val loss 2.5710\n",
      "step 176310: train loss 2.5205, val loss 2.5342\n",
      "step 176320: train loss 2.3430, val loss 2.4864\n",
      "step 176330: train loss 2.4251, val loss 2.4175\n",
      "step 176340: train loss 2.3990, val loss 2.5014\n",
      "step 176350: train loss 2.4023, val loss 2.5494\n",
      "step 176360: train loss 2.3818, val loss 2.4397\n",
      "step 176370: train loss 2.4504, val loss 2.4789\n",
      "step 176380: train loss 2.3724, val loss 2.3914\n",
      "step 176390: train loss 2.4514, val loss 2.5189\n",
      "step 176400: train loss 2.4019, val loss 2.4957\n",
      "step 176410: train loss 2.4211, val loss 2.3314\n",
      "step 176420: train loss 2.3345, val loss 2.5010\n",
      "step 176430: train loss 2.3998, val loss 2.4091\n",
      "step 176440: train loss 2.4439, val loss 2.3938\n",
      "step 176450: train loss 2.3588, val loss 2.3553\n",
      "step 176460: train loss 2.3730, val loss 2.5489\n",
      "step 176470: train loss 2.4096, val loss 2.4935\n",
      "step 176480: train loss 2.4164, val loss 2.4614\n",
      "step 176490: train loss 2.4310, val loss 2.5653\n",
      "step 176500: train loss 2.4112, val loss 2.4160\n",
      "step 176510: train loss 2.4504, val loss 2.5543\n",
      "step 176520: train loss 2.4354, val loss 2.5556\n",
      "step 176530: train loss 2.3860, val loss 2.5003\n",
      "step 176540: train loss 2.4519, val loss 2.4841\n",
      "step 176550: train loss 2.4509, val loss 2.5094\n",
      "step 176560: train loss 2.4545, val loss 2.4986\n",
      "step 176570: train loss 2.4090, val loss 2.5603\n",
      "step 176580: train loss 2.4335, val loss 2.6342\n",
      "step 176590: train loss 2.3762, val loss 2.4725\n",
      "step 176600: train loss 2.5002, val loss 2.3874\n",
      "step 176610: train loss 2.4882, val loss 2.4231\n",
      "step 176620: train loss 2.4808, val loss 2.4773\n",
      "step 176630: train loss 2.3774, val loss 2.3976\n",
      "step 176640: train loss 2.4040, val loss 2.3982\n",
      "step 176650: train loss 2.4128, val loss 2.3878\n",
      "step 176660: train loss 2.3975, val loss 2.4274\n",
      "step 176670: train loss 2.4024, val loss 2.4879\n",
      "step 176680: train loss 2.3043, val loss 2.5385\n",
      "step 176690: train loss 2.3413, val loss 2.5361\n",
      "step 176700: train loss 2.3493, val loss 2.3886\n",
      "step 176710: train loss 2.4460, val loss 2.6333\n",
      "step 176720: train loss 2.4371, val loss 2.6276\n",
      "step 176730: train loss 2.4025, val loss 2.4733\n",
      "step 176740: train loss 2.4394, val loss 2.5135\n",
      "step 176750: train loss 2.3921, val loss 2.4141\n",
      "step 176760: train loss 2.3961, val loss 2.4776\n",
      "step 176770: train loss 2.3996, val loss 2.5812\n",
      "step 176780: train loss 2.3696, val loss 2.5740\n",
      "step 176790: train loss 2.3565, val loss 2.5180\n",
      "step 176800: train loss 2.4029, val loss 2.4419\n",
      "step 176810: train loss 2.3905, val loss 2.4281\n",
      "step 176820: train loss 2.3664, val loss 2.4979\n",
      "step 176830: train loss 2.3555, val loss 2.5880\n",
      "step 176840: train loss 2.3651, val loss 2.4208\n",
      "step 176850: train loss 2.3846, val loss 2.4617\n",
      "step 176860: train loss 2.3717, val loss 2.4684\n",
      "step 176870: train loss 2.4645, val loss 2.4786\n",
      "step 176880: train loss 2.4373, val loss 2.4586\n",
      "step 176890: train loss 2.4350, val loss 2.4940\n",
      "step 176900: train loss 2.4476, val loss 2.4679\n",
      "step 176910: train loss 2.3679, val loss 2.5596\n",
      "step 176920: train loss 2.4768, val loss 2.5131\n",
      "step 176930: train loss 2.3793, val loss 2.3316\n",
      "step 176940: train loss 2.4345, val loss 2.5369\n",
      "step 176950: train loss 2.4797, val loss 2.4069\n",
      "step 176960: train loss 2.4805, val loss 2.5802\n",
      "step 176970: train loss 2.3725, val loss 2.4275\n",
      "step 176980: train loss 2.4031, val loss 2.5736\n",
      "step 176990: train loss 2.3980, val loss 2.3763\n",
      "step 177000: train loss 2.4225, val loss 2.4849\n",
      "Generated text at iteration 177000\n",
      "\n",
      " J'Aber!\n",
      "\n",
      "Lenipe dreve?\n",
      "RSôà?où hourtygl'e é,\n",
      "\n",
      "Rù st  de, deurrrs qur ceufone ss QÔonteit ciesotrée \n",
      "step 177010: train loss 2.3504, val loss 2.4480\n",
      "step 177020: train loss 2.3844, val loss 2.5337\n",
      "step 177030: train loss 2.3992, val loss 2.4399\n",
      "step 177040: train loss 2.3867, val loss 2.3954\n",
      "step 177050: train loss 2.4221, val loss 2.4882\n",
      "step 177060: train loss 2.3854, val loss 2.3943\n",
      "step 177070: train loss 2.3119, val loss 2.4604\n",
      "step 177080: train loss 2.3583, val loss 2.5409\n",
      "step 177090: train loss 2.4126, val loss 2.5232\n",
      "step 177100: train loss 2.3803, val loss 2.4248\n",
      "step 177110: train loss 2.4109, val loss 2.5866\n",
      "step 177120: train loss 2.3561, val loss 2.5128\n",
      "step 177130: train loss 2.4659, val loss 2.4254\n",
      "step 177140: train loss 2.4454, val loss 2.4282\n",
      "step 177150: train loss 2.4044, val loss 2.4614\n",
      "step 177160: train loss 2.4871, val loss 2.4521\n",
      "step 177170: train loss 2.4305, val loss 2.4103\n",
      "step 177180: train loss 2.4683, val loss 2.4297\n",
      "step 177190: train loss 2.4025, val loss 2.5613\n",
      "step 177200: train loss 2.3682, val loss 2.5157\n",
      "step 177210: train loss 2.3325, val loss 2.5287\n",
      "step 177220: train loss 2.4577, val loss 2.4855\n",
      "step 177230: train loss 2.3693, val loss 2.4729\n",
      "step 177240: train loss 2.4213, val loss 2.4740\n",
      "step 177250: train loss 2.4679, val loss 2.4684\n",
      "step 177260: train loss 2.3918, val loss 2.3836\n",
      "step 177270: train loss 2.4422, val loss 2.5677\n",
      "step 177280: train loss 2.3934, val loss 2.4586\n",
      "step 177290: train loss 2.4624, val loss 2.5339\n",
      "step 177300: train loss 2.4133, val loss 2.4027\n",
      "step 177310: train loss 2.4004, val loss 2.5359\n",
      "step 177320: train loss 2.3805, val loss 2.4887\n",
      "step 177330: train loss 2.4735, val loss 2.4839\n",
      "step 177340: train loss 2.3881, val loss 2.6007\n",
      "step 177350: train loss 2.3651, val loss 2.5851\n",
      "step 177360: train loss 2.4113, val loss 2.4945\n",
      "step 177370: train loss 2.4154, val loss 2.6137\n",
      "step 177380: train loss 2.4028, val loss 2.4492\n",
      "step 177390: train loss 2.3631, val loss 2.5124\n",
      "step 177400: train loss 2.4547, val loss 2.4621\n",
      "step 177410: train loss 2.3407, val loss 2.4336\n",
      "step 177420: train loss 2.3596, val loss 2.4293\n",
      "step 177430: train loss 2.5023, val loss 2.5451\n",
      "step 177440: train loss 2.3774, val loss 2.4850\n",
      "step 177450: train loss 2.4924, val loss 2.5598\n",
      "step 177460: train loss 2.3691, val loss 2.4986\n",
      "step 177470: train loss 2.4424, val loss 2.5178\n",
      "step 177480: train loss 2.3524, val loss 2.5063\n",
      "step 177490: train loss 2.3737, val loss 2.4890\n",
      "step 177500: train loss 2.4100, val loss 2.4651\n",
      "step 177510: train loss 2.3760, val loss 2.3984\n",
      "step 177520: train loss 2.4142, val loss 2.4725\n",
      "step 177530: train loss 2.3631, val loss 2.4564\n",
      "step 177540: train loss 2.3846, val loss 2.4604\n",
      "step 177550: train loss 2.3041, val loss 2.4520\n",
      "step 177560: train loss 2.3655, val loss 2.5215\n",
      "step 177570: train loss 2.4006, val loss 2.4557\n",
      "step 177580: train loss 2.3286, val loss 2.5209\n",
      "step 177590: train loss 2.4094, val loss 2.6342\n",
      "step 177600: train loss 2.3873, val loss 2.4067\n",
      "step 177610: train loss 2.3715, val loss 2.6087\n",
      "step 177620: train loss 2.4320, val loss 2.4734\n",
      "step 177630: train loss 2.3594, val loss 2.5213\n",
      "step 177640: train loss 2.3634, val loss 2.5218\n",
      "step 177650: train loss 2.3240, val loss 2.4768\n",
      "step 177660: train loss 2.3926, val loss 2.4777\n",
      "step 177670: train loss 2.4528, val loss 2.5744\n",
      "step 177680: train loss 2.4119, val loss 2.5543\n",
      "step 177690: train loss 2.3752, val loss 2.4167\n",
      "step 177700: train loss 2.4464, val loss 2.5438\n",
      "step 177710: train loss 2.3997, val loss 2.5216\n",
      "step 177720: train loss 2.3657, val loss 2.3981\n",
      "step 177730: train loss 2.3310, val loss 2.5216\n",
      "step 177740: train loss 2.3470, val loss 2.4070\n",
      "step 177750: train loss 2.4223, val loss 2.4523\n",
      "step 177760: train loss 2.3925, val loss 2.5707\n",
      "step 177770: train loss 2.3579, val loss 2.6446\n",
      "step 177780: train loss 2.4742, val loss 2.5334\n",
      "step 177790: train loss 2.4591, val loss 2.4536\n",
      "step 177800: train loss 2.4230, val loss 2.4466\n",
      "step 177810: train loss 2.3930, val loss 2.4108\n",
      "step 177820: train loss 2.4233, val loss 2.5032\n",
      "step 177830: train loss 2.3603, val loss 2.5621\n",
      "step 177840: train loss 2.4620, val loss 2.4964\n",
      "step 177850: train loss 2.4135, val loss 2.6930\n",
      "step 177860: train loss 2.3168, val loss 2.4711\n",
      "step 177870: train loss 2.3003, val loss 2.3970\n",
      "step 177880: train loss 2.3769, val loss 2.4824\n",
      "step 177890: train loss 2.3659, val loss 2.4195\n",
      "step 177900: train loss 2.4090, val loss 2.3850\n",
      "step 177910: train loss 2.4273, val loss 2.3692\n",
      "step 177920: train loss 2.3655, val loss 2.4746\n",
      "step 177930: train loss 2.4015, val loss 2.4934\n",
      "step 177940: train loss 2.3776, val loss 2.4377\n",
      "step 177950: train loss 2.3538, val loss 2.4759\n",
      "step 177960: train loss 2.3697, val loss 2.4874\n",
      "step 177970: train loss 2.4227, val loss 2.5304\n",
      "step 177980: train loss 2.3566, val loss 2.4978\n",
      "step 177990: train loss 2.3985, val loss 2.5975\n",
      "step 178000: train loss 2.3765, val loss 2.5032\n",
      "Generated text at iteration 178000\n",
      "\n",
      "V\n",
      " chonesagiconnn flueses dous desu-Jens qut  quere  ces,  ventiarai ébles soëéa  Dgere\n",
      "Vneririele M\n",
      "step 178010: train loss 2.3816, val loss 2.5988\n",
      "step 178020: train loss 2.3728, val loss 2.4994\n",
      "step 178030: train loss 2.4414, val loss 2.5252\n",
      "step 178040: train loss 2.3832, val loss 2.5657\n",
      "step 178050: train loss 2.3634, val loss 2.5766\n",
      "step 178060: train loss 2.3855, val loss 2.4390\n",
      "step 178070: train loss 2.3999, val loss 2.4459\n",
      "step 178080: train loss 2.3688, val loss 2.4455\n",
      "step 178090: train loss 2.3253, val loss 2.5402\n",
      "step 178100: train loss 2.3427, val loss 2.5264\n",
      "step 178110: train loss 2.4057, val loss 2.4751\n",
      "step 178120: train loss 2.4169, val loss 2.4782\n",
      "step 178130: train loss 2.3736, val loss 2.5351\n",
      "step 178140: train loss 2.4490, val loss 2.5445\n",
      "step 178150: train loss 2.3596, val loss 2.6616\n",
      "step 178160: train loss 2.3643, val loss 2.3774\n",
      "step 178170: train loss 2.3146, val loss 2.5551\n",
      "step 178180: train loss 2.3763, val loss 2.4922\n",
      "step 178190: train loss 2.5057, val loss 2.4751\n",
      "step 178200: train loss 2.3750, val loss 2.4902\n",
      "step 178210: train loss 2.3358, val loss 2.5331\n",
      "step 178220: train loss 2.3551, val loss 2.4144\n",
      "step 178230: train loss 2.3951, val loss 2.4566\n",
      "step 178240: train loss 2.4611, val loss 2.5056\n",
      "step 178250: train loss 2.4038, val loss 2.4422\n",
      "step 178260: train loss 2.3682, val loss 2.5570\n",
      "step 178270: train loss 2.3618, val loss 2.3050\n",
      "step 178280: train loss 2.3517, val loss 2.5368\n",
      "step 178290: train loss 2.3905, val loss 2.4690\n",
      "step 178300: train loss 2.3817, val loss 2.5828\n",
      "step 178310: train loss 2.4094, val loss 2.5138\n",
      "step 178320: train loss 2.3494, val loss 2.4584\n",
      "step 178330: train loss 2.3642, val loss 2.4554\n",
      "step 178340: train loss 2.3529, val loss 2.5087\n",
      "step 178350: train loss 2.4116, val loss 2.3751\n",
      "step 178360: train loss 2.4558, val loss 2.3803\n",
      "step 178370: train loss 2.4873, val loss 2.5337\n",
      "step 178380: train loss 2.3455, val loss 2.4668\n",
      "step 178390: train loss 2.4330, val loss 2.4957\n",
      "step 178400: train loss 2.4318, val loss 2.5079\n",
      "step 178410: train loss 2.4670, val loss 2.5343\n",
      "step 178420: train loss 2.3352, val loss 2.4941\n",
      "step 178430: train loss 2.4219, val loss 2.4369\n",
      "step 178440: train loss 2.4267, val loss 2.5249\n",
      "step 178450: train loss 2.4701, val loss 2.5290\n",
      "step 178460: train loss 2.3756, val loss 2.4850\n",
      "step 178470: train loss 2.3890, val loss 2.3934\n",
      "step 178480: train loss 2.3488, val loss 2.3293\n",
      "step 178490: train loss 2.3827, val loss 2.4613\n",
      "step 178500: train loss 2.4155, val loss 2.5615\n",
      "step 178510: train loss 2.4615, val loss 2.4186\n",
      "step 178520: train loss 2.3375, val loss 2.5374\n",
      "step 178530: train loss 2.4107, val loss 2.4641\n",
      "step 178540: train loss 2.4077, val loss 2.6683\n",
      "step 178550: train loss 2.3908, val loss 2.4513\n",
      "step 178560: train loss 2.3563, val loss 2.4688\n",
      "step 178570: train loss 2.3449, val loss 2.4728\n",
      "step 178580: train loss 2.3996, val loss 2.5120\n",
      "step 178590: train loss 2.4968, val loss 2.4964\n",
      "step 178600: train loss 2.4373, val loss 2.4524\n",
      "step 178610: train loss 2.3567, val loss 2.5043\n",
      "step 178620: train loss 2.4200, val loss 2.3652\n",
      "step 178630: train loss 2.3964, val loss 2.4253\n",
      "step 178640: train loss 2.4249, val loss 2.5648\n",
      "step 178650: train loss 2.3928, val loss 2.4613\n",
      "step 178660: train loss 2.4451, val loss 2.5321\n",
      "step 178670: train loss 2.3909, val loss 2.5167\n",
      "step 178680: train loss 2.4215, val loss 2.4432\n",
      "step 178690: train loss 2.3681, val loss 2.4115\n",
      "step 178700: train loss 2.4291, val loss 2.4670\n",
      "step 178710: train loss 2.3884, val loss 2.4275\n",
      "step 178720: train loss 2.3868, val loss 2.4562\n",
      "step 178730: train loss 2.3717, val loss 2.4996\n",
      "step 178740: train loss 2.3163, val loss 2.3981\n",
      "step 178750: train loss 2.3952, val loss 2.4644\n",
      "step 178760: train loss 2.3895, val loss 2.3596\n",
      "step 178770: train loss 2.3846, val loss 2.4813\n",
      "step 178780: train loss 2.3156, val loss 2.5928\n",
      "step 178790: train loss 2.3865, val loss 2.4916\n",
      "step 178800: train loss 2.4047, val loss 2.4632\n",
      "step 178810: train loss 2.3585, val loss 2.3962\n",
      "step 178820: train loss 2.3883, val loss 2.4779\n",
      "step 178830: train loss 2.3146, val loss 2.5032\n",
      "step 178840: train loss 2.3832, val loss 2.4764\n",
      "step 178850: train loss 2.4090, val loss 2.6019\n",
      "step 178860: train loss 2.3368, val loss 2.5264\n",
      "step 178870: train loss 2.3751, val loss 2.5267\n",
      "step 178880: train loss 2.3485, val loss 2.4847\n",
      "step 178890: train loss 2.3580, val loss 2.4678\n",
      "step 178900: train loss 2.4405, val loss 2.4878\n",
      "step 178910: train loss 2.4153, val loss 2.4102\n",
      "step 178920: train loss 2.3889, val loss 2.4403\n",
      "step 178930: train loss 2.4034, val loss 2.5773\n",
      "step 178940: train loss 2.3079, val loss 2.4280\n",
      "step 178950: train loss 2.4242, val loss 2.4918\n",
      "step 178960: train loss 2.3523, val loss 2.4047\n",
      "step 178970: train loss 2.3810, val loss 2.4695\n",
      "step 178980: train loss 2.3244, val loss 2.5201\n",
      "step 178990: train loss 2.4891, val loss 2.6385\n",
      "step 179000: train loss 2.3548, val loss 2.4824\n",
      "Generated text at iteration 179000\n",
      "\n",
      "\n",
      "Et raunt   mme fie!7Ondoi le prane dulave, eseuvele C'i\n",
      "Sa  mé.\n",
      "\n",
      "De;\n",
      "\n",
      "V\n",
      "Pot touaise oceva se lant u\n",
      "step 179010: train loss 2.3649, val loss 2.3486\n",
      "step 179020: train loss 2.4322, val loss 2.4584\n",
      "step 179030: train loss 2.4365, val loss 2.4437\n",
      "step 179040: train loss 2.4411, val loss 2.4515\n",
      "step 179050: train loss 2.3897, val loss 2.4770\n",
      "step 179060: train loss 2.4001, val loss 2.4712\n",
      "step 179070: train loss 2.4611, val loss 2.5206\n",
      "step 179080: train loss 2.3800, val loss 2.4384\n",
      "step 179090: train loss 2.4306, val loss 2.5628\n",
      "step 179100: train loss 2.4362, val loss 2.4852\n",
      "step 179110: train loss 2.4520, val loss 2.4569\n",
      "step 179120: train loss 2.4083, val loss 2.4812\n",
      "step 179130: train loss 2.3152, val loss 2.3634\n",
      "step 179140: train loss 2.3968, val loss 2.5384\n",
      "step 179150: train loss 2.3914, val loss 2.5301\n",
      "step 179160: train loss 2.4926, val loss 2.4786\n",
      "step 179170: train loss 2.4402, val loss 2.4541\n",
      "step 179180: train loss 2.4465, val loss 2.5094\n",
      "step 179190: train loss 2.4161, val loss 2.5189\n",
      "step 179200: train loss 2.4532, val loss 2.5380\n",
      "step 179210: train loss 2.3829, val loss 2.4593\n",
      "step 179220: train loss 2.3616, val loss 2.4752\n",
      "step 179230: train loss 2.3623, val loss 2.3900\n",
      "step 179240: train loss 2.4774, val loss 2.6089\n",
      "step 179250: train loss 2.3781, val loss 2.4163\n",
      "step 179260: train loss 2.4392, val loss 2.3578\n",
      "step 179270: train loss 2.4391, val loss 2.5215\n",
      "step 179280: train loss 2.4599, val loss 2.4142\n",
      "step 179290: train loss 2.4042, val loss 2.4647\n",
      "step 179300: train loss 2.4614, val loss 2.4857\n",
      "step 179310: train loss 2.4626, val loss 2.4117\n",
      "step 179320: train loss 2.4835, val loss 2.4727\n",
      "step 179330: train loss 2.4197, val loss 2.5367\n",
      "step 179340: train loss 2.3867, val loss 2.4210\n",
      "step 179350: train loss 2.4032, val loss 2.5195\n",
      "step 179360: train loss 2.3448, val loss 2.4375\n",
      "step 179370: train loss 2.3883, val loss 2.4331\n",
      "step 179380: train loss 2.4412, val loss 2.4717\n",
      "step 179390: train loss 2.3574, val loss 2.4914\n",
      "step 179400: train loss 2.3030, val loss 2.4492\n",
      "step 179410: train loss 2.3779, val loss 2.4256\n",
      "step 179420: train loss 2.4147, val loss 2.4905\n",
      "step 179430: train loss 2.4111, val loss 2.4950\n",
      "step 179440: train loss 2.3951, val loss 2.3780\n",
      "step 179450: train loss 2.4365, val loss 2.4549\n",
      "step 179460: train loss 2.4107, val loss 2.5401\n",
      "step 179470: train loss 2.4053, val loss 2.4544\n",
      "step 179480: train loss 2.4090, val loss 2.4927\n",
      "step 179490: train loss 2.4177, val loss 2.4270\n",
      "step 179500: train loss 2.3611, val loss 2.4892\n",
      "step 179510: train loss 2.3988, val loss 2.4948\n",
      "step 179520: train loss 2.4052, val loss 2.6308\n",
      "step 179530: train loss 2.3931, val loss 2.5635\n",
      "step 179540: train loss 2.3917, val loss 2.4190\n",
      "step 179550: train loss 2.4467, val loss 2.5290\n",
      "step 179560: train loss 2.3789, val loss 2.3959\n",
      "step 179570: train loss 2.3197, val loss 2.4808\n",
      "step 179580: train loss 2.3736, val loss 2.3403\n",
      "step 179590: train loss 2.3564, val loss 2.4167\n",
      "step 179600: train loss 2.3876, val loss 2.3971\n",
      "step 179610: train loss 2.4112, val loss 2.4677\n",
      "step 179620: train loss 2.4046, val loss 2.5989\n",
      "step 179630: train loss 2.4266, val loss 2.5169\n",
      "step 179640: train loss 2.4744, val loss 2.4035\n",
      "step 179650: train loss 2.3861, val loss 2.5636\n",
      "step 179660: train loss 2.4189, val loss 2.5129\n",
      "step 179670: train loss 2.4155, val loss 2.5288\n",
      "step 179680: train loss 2.3976, val loss 2.4357\n",
      "step 179690: train loss 2.3931, val loss 2.4929\n",
      "step 179700: train loss 2.3347, val loss 2.5080\n",
      "step 179710: train loss 2.4094, val loss 2.5343\n",
      "step 179720: train loss 2.3372, val loss 2.3947\n",
      "step 179730: train loss 2.3973, val loss 2.5058\n",
      "step 179740: train loss 2.3434, val loss 2.5022\n",
      "step 179750: train loss 2.3255, val loss 2.5617\n",
      "step 179760: train loss 2.4502, val loss 2.5873\n",
      "step 179770: train loss 2.3328, val loss 2.5407\n",
      "step 179780: train loss 2.4755, val loss 2.4726\n",
      "step 179790: train loss 2.3386, val loss 2.4638\n",
      "step 179800: train loss 2.3888, val loss 2.5088\n",
      "step 179810: train loss 2.4397, val loss 2.5982\n",
      "step 179820: train loss 2.4292, val loss 2.4100\n",
      "step 179830: train loss 2.3933, val loss 2.5178\n",
      "step 179840: train loss 2.3845, val loss 2.4359\n",
      "step 179850: train loss 2.3563, val loss 2.4933\n",
      "step 179860: train loss 2.3816, val loss 2.4621\n",
      "step 179870: train loss 2.3971, val loss 2.4170\n",
      "step 179880: train loss 2.3547, val loss 2.4366\n",
      "step 179890: train loss 2.3824, val loss 2.4154\n",
      "step 179900: train loss 2.4283, val loss 2.4239\n",
      "step 179910: train loss 2.3982, val loss 2.5675\n",
      "step 179920: train loss 2.3355, val loss 2.5666\n",
      "step 179930: train loss 2.4136, val loss 2.5804\n",
      "step 179940: train loss 2.4374, val loss 2.6031\n",
      "step 179950: train loss 2.3809, val loss 2.4668\n",
      "step 179960: train loss 2.3515, val loss 2.6166\n",
      "step 179970: train loss 2.3652, val loss 2.5001\n",
      "step 179980: train loss 2.4183, val loss 2.5139\n",
      "step 179990: train loss 2.3749, val loss 2.4066\n",
      "step 180000: train loss 2.3716, val loss 2.4801\n",
      "Generated text at iteration 180000\n",
      "\n",
      "Toilar ce_t n as aiasomemmeacerns!\n",
      "L'éca mbspa de fét rs à megireui tutere paux!-bonent,\n",
      " LL'itonste\n",
      "step 180010: train loss 2.3869, val loss 2.5588\n",
      "step 180020: train loss 2.3726, val loss 2.5337\n",
      "step 180030: train loss 2.3583, val loss 2.4818\n",
      "step 180040: train loss 2.3879, val loss 2.5274\n",
      "step 180050: train loss 2.3630, val loss 2.4998\n",
      "step 180060: train loss 2.3414, val loss 2.5572\n",
      "step 180070: train loss 2.3900, val loss 2.4661\n",
      "step 180080: train loss 2.3866, val loss 2.6295\n",
      "step 180090: train loss 2.3340, val loss 2.5375\n",
      "step 180100: train loss 2.3971, val loss 2.5031\n",
      "step 180110: train loss 2.3666, val loss 2.4297\n",
      "step 180120: train loss 2.4446, val loss 2.5181\n",
      "step 180130: train loss 2.4090, val loss 2.5215\n",
      "step 180140: train loss 2.3536, val loss 2.5228\n",
      "step 180150: train loss 2.3846, val loss 2.4331\n",
      "step 180160: train loss 2.3729, val loss 2.6314\n",
      "step 180170: train loss 2.3941, val loss 2.4977\n",
      "step 180180: train loss 2.4068, val loss 2.5886\n",
      "step 180190: train loss 2.3975, val loss 2.4936\n",
      "step 180200: train loss 2.3937, val loss 2.5111\n",
      "step 180210: train loss 2.3452, val loss 2.4291\n",
      "step 180220: train loss 2.3704, val loss 2.4232\n",
      "step 180230: train loss 2.4049, val loss 2.5151\n",
      "step 180240: train loss 2.3873, val loss 2.4672\n",
      "step 180250: train loss 2.3971, val loss 2.4352\n",
      "step 180260: train loss 2.4138, val loss 2.4640\n",
      "step 180270: train loss 2.4192, val loss 2.4272\n",
      "step 180280: train loss 2.4359, val loss 2.4659\n",
      "step 180290: train loss 2.4022, val loss 2.5182\n",
      "step 180300: train loss 2.4056, val loss 2.3973\n",
      "step 180310: train loss 2.4096, val loss 2.5483\n",
      "step 180320: train loss 2.4342, val loss 2.5156\n",
      "step 180330: train loss 2.3918, val loss 2.4984\n",
      "step 180340: train loss 2.4116, val loss 2.5112\n",
      "step 180350: train loss 2.3158, val loss 2.5304\n",
      "step 180360: train loss 2.4210, val loss 2.5267\n",
      "step 180370: train loss 2.4231, val loss 2.4583\n",
      "step 180380: train loss 2.4462, val loss 2.4742\n",
      "step 180390: train loss 2.3357, val loss 2.4163\n",
      "step 180400: train loss 2.4084, val loss 2.3732\n",
      "step 180410: train loss 2.4148, val loss 2.4082\n",
      "step 180420: train loss 2.3268, val loss 2.3872\n",
      "step 180430: train loss 2.4249, val loss 2.4393\n",
      "step 180440: train loss 2.3521, val loss 2.4739\n",
      "step 180450: train loss 2.4058, val loss 2.5275\n",
      "step 180460: train loss 2.3841, val loss 2.4335\n",
      "step 180470: train loss 2.3154, val loss 2.4578\n",
      "step 180480: train loss 2.4069, val loss 2.3800\n",
      "step 180490: train loss 2.4496, val loss 2.5224\n",
      "step 180500: train loss 2.3421, val loss 2.4039\n",
      "step 180510: train loss 2.4613, val loss 2.4516\n",
      "step 180520: train loss 2.5246, val loss 2.5037\n",
      "step 180530: train loss 2.4136, val loss 2.5347\n",
      "step 180540: train loss 2.3561, val loss 2.5873\n",
      "step 180550: train loss 2.4343, val loss 2.4314\n",
      "step 180560: train loss 2.3860, val loss 2.5045\n",
      "step 180570: train loss 2.4422, val loss 2.4125\n",
      "step 180580: train loss 2.4342, val loss 2.5082\n",
      "step 180590: train loss 2.3868, val loss 2.4175\n",
      "step 180600: train loss 2.4405, val loss 2.4602\n",
      "step 180610: train loss 2.3949, val loss 2.4061\n",
      "step 180620: train loss 2.3491, val loss 2.5814\n",
      "step 180630: train loss 2.3798, val loss 2.4378\n",
      "step 180640: train loss 2.3548, val loss 2.4345\n",
      "step 180650: train loss 2.4130, val loss 2.5270\n",
      "step 180660: train loss 2.4234, val loss 2.4027\n",
      "step 180670: train loss 2.4060, val loss 2.4298\n",
      "step 180680: train loss 2.2919, val loss 2.5477\n",
      "step 180690: train loss 2.4341, val loss 2.4170\n",
      "step 180700: train loss 2.3842, val loss 2.5063\n",
      "step 180710: train loss 2.3732, val loss 2.4698\n",
      "step 180720: train loss 2.3954, val loss 2.5439\n",
      "step 180730: train loss 2.4018, val loss 2.5792\n",
      "step 180740: train loss 2.3412, val loss 2.5228\n",
      "step 180750: train loss 2.3149, val loss 2.4157\n",
      "step 180760: train loss 2.3344, val loss 2.6365\n",
      "step 180770: train loss 2.3699, val loss 2.4942\n",
      "step 180780: train loss 2.4487, val loss 2.4784\n",
      "step 180790: train loss 2.3963, val loss 2.2741\n",
      "step 180800: train loss 2.3546, val loss 2.4869\n",
      "step 180810: train loss 2.3832, val loss 2.5819\n",
      "step 180820: train loss 2.4282, val loss 2.4630\n",
      "step 180830: train loss 2.3801, val loss 2.4663\n",
      "step 180840: train loss 2.3387, val loss 2.3920\n",
      "step 180850: train loss 2.3274, val loss 2.4415\n",
      "step 180860: train loss 2.4064, val loss 2.4000\n",
      "step 180870: train loss 2.3864, val loss 2.3670\n",
      "step 180880: train loss 2.3716, val loss 2.4590\n",
      "step 180890: train loss 2.4353, val loss 2.4340\n",
      "step 180900: train loss 2.3218, val loss 2.5242\n",
      "step 180910: train loss 2.3363, val loss 2.4742\n",
      "step 180920: train loss 2.4208, val loss 2.5448\n",
      "step 180930: train loss 2.3866, val loss 2.4360\n",
      "step 180940: train loss 2.3539, val loss 2.3871\n",
      "step 180950: train loss 2.4564, val loss 2.5274\n",
      "step 180960: train loss 2.4008, val loss 2.4169\n",
      "step 180970: train loss 2.4855, val loss 2.4753\n",
      "step 180980: train loss 2.3292, val loss 2.6296\n",
      "step 180990: train loss 2.4304, val loss 2.3751\n",
      "step 181000: train loss 2.3631, val loss 2.4863\n",
      "Generated text at iteration 181000\n",
      "\n",
      "JU3·Je el'iegaile, nt s de crgele, tus plis êmaibeacrais\n",
      "LANeayret come\n",
      "Entomeri éde.. yesent,\n",
      "Leuty\n",
      "step 181010: train loss 2.4287, val loss 2.4722\n",
      "step 181020: train loss 2.3786, val loss 2.5057\n",
      "step 181030: train loss 2.3886, val loss 2.4680\n",
      "step 181040: train loss 2.3149, val loss 2.5439\n",
      "step 181050: train loss 2.4057, val loss 2.4772\n",
      "step 181060: train loss 2.3879, val loss 2.4740\n",
      "step 181070: train loss 2.4248, val loss 2.4365\n",
      "step 181080: train loss 2.3943, val loss 2.5721\n",
      "step 181090: train loss 2.3807, val loss 2.5426\n",
      "step 181100: train loss 2.3839, val loss 2.4723\n",
      "step 181110: train loss 2.3936, val loss 2.3995\n",
      "step 181120: train loss 2.3320, val loss 2.5112\n",
      "step 181130: train loss 2.4033, val loss 2.5441\n",
      "step 181140: train loss 2.4157, val loss 2.4844\n",
      "step 181150: train loss 2.4430, val loss 2.3752\n",
      "step 181160: train loss 2.2889, val loss 2.6057\n",
      "step 181170: train loss 2.3703, val loss 2.4232\n",
      "step 181180: train loss 2.4598, val loss 2.5210\n",
      "step 181190: train loss 2.3789, val loss 2.4964\n",
      "step 181200: train loss 2.4354, val loss 2.4943\n",
      "step 181210: train loss 2.4920, val loss 2.5000\n",
      "step 181220: train loss 2.4058, val loss 2.4913\n",
      "step 181230: train loss 2.5185, val loss 2.4645\n",
      "step 181240: train loss 2.4100, val loss 2.3912\n",
      "step 181250: train loss 2.3846, val loss 2.4679\n",
      "step 181260: train loss 2.3914, val loss 2.4850\n",
      "step 181270: train loss 2.4928, val loss 2.5153\n",
      "step 181280: train loss 2.3950, val loss 2.4230\n",
      "step 181290: train loss 2.4526, val loss 2.4794\n",
      "step 181300: train loss 2.3844, val loss 2.4452\n",
      "step 181310: train loss 2.4108, val loss 2.5963\n",
      "step 181320: train loss 2.3687, val loss 2.5043\n",
      "step 181330: train loss 2.4171, val loss 2.4561\n",
      "step 181340: train loss 2.3056, val loss 2.4234\n",
      "step 181350: train loss 2.3782, val loss 2.5156\n",
      "step 181360: train loss 2.3872, val loss 2.5421\n",
      "step 181370: train loss 2.3541, val loss 2.4159\n",
      "step 181380: train loss 2.3897, val loss 2.5381\n",
      "step 181390: train loss 2.3550, val loss 2.4548\n",
      "step 181400: train loss 2.4050, val loss 2.5559\n",
      "step 181410: train loss 2.3917, val loss 2.5264\n",
      "step 181420: train loss 2.3915, val loss 2.4895\n",
      "step 181430: train loss 2.3503, val loss 2.5251\n",
      "step 181440: train loss 2.3938, val loss 2.4795\n",
      "step 181450: train loss 2.3383, val loss 2.4322\n",
      "step 181460: train loss 2.3970, val loss 2.4488\n",
      "step 181470: train loss 2.4549, val loss 2.4788\n",
      "step 181480: train loss 2.3835, val loss 2.4668\n",
      "step 181490: train loss 2.3662, val loss 2.4738\n",
      "step 181500: train loss 2.3742, val loss 2.4808\n",
      "step 181510: train loss 2.3947, val loss 2.6224\n",
      "step 181520: train loss 2.3976, val loss 2.4100\n",
      "step 181530: train loss 2.4321, val loss 2.5513\n",
      "step 181540: train loss 2.3711, val loss 2.5285\n",
      "step 181550: train loss 2.3793, val loss 2.4791\n",
      "step 181560: train loss 2.4529, val loss 2.5399\n",
      "step 181570: train loss 2.3684, val loss 2.4996\n",
      "step 181580: train loss 2.3219, val loss 2.4628\n",
      "step 181590: train loss 2.3459, val loss 2.4608\n",
      "step 181600: train loss 2.4183, val loss 2.4998\n",
      "step 181610: train loss 2.4550, val loss 2.4055\n",
      "step 181620: train loss 2.3762, val loss 2.5427\n",
      "step 181630: train loss 2.3641, val loss 2.3835\n",
      "step 181640: train loss 2.5369, val loss 2.4041\n",
      "step 181650: train loss 2.3939, val loss 2.5473\n",
      "step 181660: train loss 2.4559, val loss 2.5740\n",
      "step 181670: train loss 2.3327, val loss 2.3762\n",
      "step 181680: train loss 2.4261, val loss 2.4611\n",
      "step 181690: train loss 2.3805, val loss 2.5705\n",
      "step 181700: train loss 2.4547, val loss 2.4757\n",
      "step 181710: train loss 2.4723, val loss 2.3510\n",
      "step 181720: train loss 2.4357, val loss 2.4542\n",
      "step 181730: train loss 2.4113, val loss 2.5186\n",
      "step 181740: train loss 2.3507, val loss 2.5895\n",
      "step 181750: train loss 2.3569, val loss 2.5623\n",
      "step 181760: train loss 2.3970, val loss 2.5273\n",
      "step 181770: train loss 2.3180, val loss 2.4533\n",
      "step 181780: train loss 2.3981, val loss 2.4545\n",
      "step 181790: train loss 2.3322, val loss 2.3901\n",
      "step 181800: train loss 2.4087, val loss 2.5547\n",
      "step 181810: train loss 2.4555, val loss 2.4991\n",
      "step 181820: train loss 2.4225, val loss 2.5278\n",
      "step 181830: train loss 2.3544, val loss 2.4623\n",
      "step 181840: train loss 2.3853, val loss 2.4846\n",
      "step 181850: train loss 2.4408, val loss 2.4424\n",
      "step 181860: train loss 2.4350, val loss 2.4610\n",
      "step 181870: train loss 2.2922, val loss 2.3773\n",
      "step 181880: train loss 2.4098, val loss 2.4476\n",
      "step 181890: train loss 2.4315, val loss 2.4230\n",
      "step 181900: train loss 2.4256, val loss 2.4528\n",
      "step 181910: train loss 2.4154, val loss 2.5079\n",
      "step 181920: train loss 2.3676, val loss 2.4198\n",
      "step 181930: train loss 2.3634, val loss 2.5432\n",
      "step 181940: train loss 2.3758, val loss 2.6075\n",
      "step 181950: train loss 2.4706, val loss 2.5341\n",
      "step 181960: train loss 2.4340, val loss 2.4924\n",
      "step 181970: train loss 2.4298, val loss 2.4359\n",
      "step 181980: train loss 2.3657, val loss 2.3404\n",
      "step 181990: train loss 2.4253, val loss 2.5906\n",
      "step 182000: train loss 2.3976, val loss 2.4573\n",
      "Generated text at iteration 182000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Aida ô clant saisieut le vau des euis t ll prrter, desol déeue lerboméan cobrabresonit en qunnant\n",
      "step 182010: train loss 2.4165, val loss 2.5393\n",
      "step 182020: train loss 2.3701, val loss 2.4596\n",
      "step 182030: train loss 2.3777, val loss 2.5539\n",
      "step 182040: train loss 2.3762, val loss 2.3501\n",
      "step 182050: train loss 2.3665, val loss 2.5284\n",
      "step 182060: train loss 2.4246, val loss 2.5490\n",
      "step 182070: train loss 2.4090, val loss 2.5689\n",
      "step 182080: train loss 2.3575, val loss 2.4974\n",
      "step 182090: train loss 2.4976, val loss 2.4463\n",
      "step 182100: train loss 2.3795, val loss 2.6189\n",
      "step 182110: train loss 2.4287, val loss 2.4706\n",
      "step 182120: train loss 2.4171, val loss 2.4104\n",
      "step 182130: train loss 2.3763, val loss 2.4791\n",
      "step 182140: train loss 2.3785, val loss 2.3837\n",
      "step 182150: train loss 2.3841, val loss 2.3872\n",
      "step 182160: train loss 2.3677, val loss 2.4696\n",
      "step 182170: train loss 2.3963, val loss 2.6204\n",
      "step 182180: train loss 2.4221, val loss 2.5180\n",
      "step 182190: train loss 2.5044, val loss 2.5306\n",
      "step 182200: train loss 2.4091, val loss 2.4399\n",
      "step 182210: train loss 2.3437, val loss 2.4411\n",
      "step 182220: train loss 2.4784, val loss 2.4997\n",
      "step 182230: train loss 2.3940, val loss 2.4372\n",
      "step 182240: train loss 2.4585, val loss 2.4153\n",
      "step 182250: train loss 2.4412, val loss 2.4733\n",
      "step 182260: train loss 2.4026, val loss 2.4021\n",
      "step 182270: train loss 2.4034, val loss 2.4378\n",
      "step 182280: train loss 2.3509, val loss 2.4399\n",
      "step 182290: train loss 2.3989, val loss 2.4644\n",
      "step 182300: train loss 2.3733, val loss 2.5420\n",
      "step 182310: train loss 2.3905, val loss 2.5176\n",
      "step 182320: train loss 2.4292, val loss 2.4120\n",
      "step 182330: train loss 2.4068, val loss 2.5976\n",
      "step 182340: train loss 2.4427, val loss 2.4944\n",
      "step 182350: train loss 2.4926, val loss 2.4713\n",
      "step 182360: train loss 2.4572, val loss 2.5132\n",
      "step 182370: train loss 2.3660, val loss 2.4889\n",
      "step 182380: train loss 2.4073, val loss 2.4137\n",
      "step 182390: train loss 2.4781, val loss 2.6076\n",
      "step 182400: train loss 2.4373, val loss 2.5522\n",
      "step 182410: train loss 2.3238, val loss 2.3920\n",
      "step 182420: train loss 2.3420, val loss 2.6322\n",
      "step 182430: train loss 2.4043, val loss 2.4120\n",
      "step 182440: train loss 2.3516, val loss 2.5208\n",
      "step 182450: train loss 2.3257, val loss 2.4454\n",
      "step 182460: train loss 2.3560, val loss 2.4115\n",
      "step 182470: train loss 2.4361, val loss 2.4597\n",
      "step 182480: train loss 2.3882, val loss 2.4085\n",
      "step 182490: train loss 2.3986, val loss 2.5196\n",
      "step 182500: train loss 2.4324, val loss 2.4913\n",
      "step 182510: train loss 2.4027, val loss 2.5334\n",
      "step 182520: train loss 2.2767, val loss 2.5690\n",
      "step 182530: train loss 2.3706, val loss 2.3680\n",
      "step 182540: train loss 2.3529, val loss 2.4476\n",
      "step 182550: train loss 2.4120, val loss 2.3907\n",
      "step 182560: train loss 2.3491, val loss 2.4623\n",
      "step 182570: train loss 2.4822, val loss 2.5503\n",
      "step 182580: train loss 2.4554, val loss 2.5019\n",
      "step 182590: train loss 2.4211, val loss 2.5737\n",
      "step 182600: train loss 2.3204, val loss 2.5505\n",
      "step 182610: train loss 2.4880, val loss 2.4615\n",
      "step 182620: train loss 2.4154, val loss 2.4795\n",
      "step 182630: train loss 2.4079, val loss 2.4964\n",
      "step 182640: train loss 2.4267, val loss 2.5493\n",
      "step 182650: train loss 2.3009, val loss 2.4118\n",
      "step 182660: train loss 2.3785, val loss 2.4593\n",
      "step 182670: train loss 2.3997, val loss 2.5220\n",
      "step 182680: train loss 2.4102, val loss 2.3944\n",
      "step 182690: train loss 2.3077, val loss 2.5552\n",
      "step 182700: train loss 2.4649, val loss 2.4370\n",
      "step 182710: train loss 2.3221, val loss 2.4198\n",
      "step 182720: train loss 2.4038, val loss 2.4471\n",
      "step 182730: train loss 2.4746, val loss 2.3854\n",
      "step 182740: train loss 2.4086, val loss 2.4716\n",
      "step 182750: train loss 2.3743, val loss 2.4866\n",
      "step 182760: train loss 2.3574, val loss 2.5458\n",
      "step 182770: train loss 2.4108, val loss 2.5447\n",
      "step 182780: train loss 2.3858, val loss 2.4440\n",
      "step 182790: train loss 2.4237, val loss 2.5161\n",
      "step 182800: train loss 2.4174, val loss 2.4441\n",
      "step 182810: train loss 2.3813, val loss 2.4943\n",
      "step 182820: train loss 2.3593, val loss 2.3999\n",
      "step 182830: train loss 2.3593, val loss 2.4893\n",
      "step 182840: train loss 2.4009, val loss 2.4356\n",
      "step 182850: train loss 2.3344, val loss 2.5292\n",
      "step 182860: train loss 2.4357, val loss 2.4128\n",
      "step 182870: train loss 2.4329, val loss 2.6011\n",
      "step 182880: train loss 2.3379, val loss 2.5440\n",
      "step 182890: train loss 2.3816, val loss 2.5494\n",
      "step 182900: train loss 2.3726, val loss 2.4551\n",
      "step 182910: train loss 2.3627, val loss 2.3902\n",
      "step 182920: train loss 2.4187, val loss 2.4181\n",
      "step 182930: train loss 2.4253, val loss 2.5030\n",
      "step 182940: train loss 2.3625, val loss 2.6844\n",
      "step 182950: train loss 2.4032, val loss 2.5912\n",
      "step 182960: train loss 2.4345, val loss 2.5344\n",
      "step 182970: train loss 2.4180, val loss 2.4959\n",
      "step 182980: train loss 2.4371, val loss 2.4249\n",
      "step 182990: train loss 2.3624, val loss 2.4330\n",
      "step 183000: train loss 2.4014, val loss 2.4498\n",
      "Generated text at iteration 183000\n",
      "\n",
      "Laus quitosen je!\n",
      "Te  l'ares, dét dis ci,»\n",
      "Des ne.\n",
      "Lysarespars ansumubes, qus ltefi! sirers  ans ns \n",
      "step 183010: train loss 2.3800, val loss 2.4292\n",
      "step 183020: train loss 2.4155, val loss 2.4535\n",
      "step 183030: train loss 2.3705, val loss 2.3706\n",
      "step 183040: train loss 2.3625, val loss 2.4447\n",
      "step 183050: train loss 2.4130, val loss 2.4441\n",
      "step 183060: train loss 2.2846, val loss 2.3431\n",
      "step 183070: train loss 2.3682, val loss 2.3861\n",
      "step 183080: train loss 2.4215, val loss 2.5302\n",
      "step 183090: train loss 2.4953, val loss 2.4920\n",
      "step 183100: train loss 2.4897, val loss 2.3971\n",
      "step 183110: train loss 2.3195, val loss 2.4223\n",
      "step 183120: train loss 2.4485, val loss 2.4278\n",
      "step 183130: train loss 2.4204, val loss 2.4027\n",
      "step 183140: train loss 2.4247, val loss 2.4920\n",
      "step 183150: train loss 2.4084, val loss 2.4238\n",
      "step 183160: train loss 2.3923, val loss 2.5793\n",
      "step 183170: train loss 2.3794, val loss 2.4788\n",
      "step 183180: train loss 2.3975, val loss 2.5688\n",
      "step 183190: train loss 2.3597, val loss 2.5022\n",
      "step 183200: train loss 2.4201, val loss 2.4737\n",
      "step 183210: train loss 2.4284, val loss 2.4128\n",
      "step 183220: train loss 2.4122, val loss 2.5059\n",
      "step 183230: train loss 2.4306, val loss 2.4921\n",
      "step 183240: train loss 2.4415, val loss 2.4048\n",
      "step 183250: train loss 2.4242, val loss 2.5517\n",
      "step 183260: train loss 2.4143, val loss 2.4625\n",
      "step 183270: train loss 2.4307, val loss 2.5095\n",
      "step 183280: train loss 2.3247, val loss 2.3619\n",
      "step 183290: train loss 2.3540, val loss 2.5130\n",
      "step 183300: train loss 2.4139, val loss 2.4519\n",
      "step 183310: train loss 2.3287, val loss 2.5554\n",
      "step 183320: train loss 2.3502, val loss 2.4354\n",
      "step 183330: train loss 2.3287, val loss 2.4979\n",
      "step 183340: train loss 2.3451, val loss 2.4717\n",
      "step 183350: train loss 2.3962, val loss 2.5509\n",
      "step 183360: train loss 2.4162, val loss 2.5806\n",
      "step 183370: train loss 2.4559, val loss 2.4904\n",
      "step 183380: train loss 2.3927, val loss 2.4928\n",
      "step 183390: train loss 2.4379, val loss 2.4799\n",
      "step 183400: train loss 2.4247, val loss 2.4760\n",
      "step 183410: train loss 2.3676, val loss 2.4580\n",
      "step 183420: train loss 2.3758, val loss 2.5350\n",
      "step 183430: train loss 2.3873, val loss 2.5082\n",
      "step 183440: train loss 2.4099, val loss 2.5921\n",
      "step 183450: train loss 2.4519, val loss 2.5270\n",
      "step 183460: train loss 2.4365, val loss 2.4427\n",
      "step 183470: train loss 2.3880, val loss 2.4866\n",
      "step 183480: train loss 2.3854, val loss 2.5068\n",
      "step 183490: train loss 2.4393, val loss 2.5012\n",
      "step 183500: train loss 2.3215, val loss 2.4940\n",
      "step 183510: train loss 2.4323, val loss 2.4897\n",
      "step 183520: train loss 2.3938, val loss 2.4312\n",
      "step 183530: train loss 2.3963, val loss 2.3934\n",
      "step 183540: train loss 2.3980, val loss 2.4054\n",
      "step 183550: train loss 2.3853, val loss 2.3254\n",
      "step 183560: train loss 2.3630, val loss 2.4212\n",
      "step 183570: train loss 2.4464, val loss 2.4319\n",
      "step 183580: train loss 2.4257, val loss 2.4209\n",
      "step 183590: train loss 2.4070, val loss 2.4170\n",
      "step 183600: train loss 2.3562, val loss 2.4269\n",
      "step 183610: train loss 2.4176, val loss 2.4662\n",
      "step 183620: train loss 2.3762, val loss 2.4576\n",
      "step 183630: train loss 2.3388, val loss 2.6306\n",
      "step 183640: train loss 2.4116, val loss 2.4824\n",
      "step 183650: train loss 2.4100, val loss 2.5540\n",
      "step 183660: train loss 2.3652, val loss 2.4457\n",
      "step 183670: train loss 2.4046, val loss 2.5665\n",
      "step 183680: train loss 2.3798, val loss 2.4981\n",
      "step 183690: train loss 2.3823, val loss 2.4896\n",
      "step 183700: train loss 2.4326, val loss 2.4564\n",
      "step 183710: train loss 2.4044, val loss 2.4691\n",
      "step 183720: train loss 2.4670, val loss 2.5318\n",
      "step 183730: train loss 2.4054, val loss 2.4378\n",
      "step 183740: train loss 2.3988, val loss 2.4709\n",
      "step 183750: train loss 2.3390, val loss 2.5018\n",
      "step 183760: train loss 2.4695, val loss 2.4138\n",
      "step 183770: train loss 2.4988, val loss 2.4147\n",
      "step 183780: train loss 2.3264, val loss 2.4263\n",
      "step 183790: train loss 2.3898, val loss 2.3314\n",
      "step 183800: train loss 2.3072, val loss 2.4961\n",
      "step 183810: train loss 2.3900, val loss 2.4119\n",
      "step 183820: train loss 2.3389, val loss 2.4351\n",
      "step 183830: train loss 2.4221, val loss 2.5429\n",
      "step 183840: train loss 2.4134, val loss 2.4487\n",
      "step 183850: train loss 2.3880, val loss 2.4755\n",
      "step 183860: train loss 2.4012, val loss 2.4761\n",
      "step 183870: train loss 2.3687, val loss 2.5317\n",
      "step 183880: train loss 2.3957, val loss 2.5061\n",
      "step 183890: train loss 2.3422, val loss 2.4776\n",
      "step 183900: train loss 2.4178, val loss 2.5074\n",
      "step 183910: train loss 2.4460, val loss 2.5050\n",
      "step 183920: train loss 2.3949, val loss 2.5071\n",
      "step 183930: train loss 2.4282, val loss 2.4922\n",
      "step 183940: train loss 2.4879, val loss 2.5617\n",
      "step 183950: train loss 2.4378, val loss 2.4491\n",
      "step 183960: train loss 2.4289, val loss 2.4633\n",
      "step 183970: train loss 2.4293, val loss 2.5206\n",
      "step 183980: train loss 2.3945, val loss 2.4919\n",
      "step 183990: train loss 2.4653, val loss 2.5386\n",
      "step 184000: train loss 2.3852, val loss 2.5058\n",
      "Generated text at iteration 184000\n",
      "\n",
      "Qus momeicent'e tr  ce ves.\n",
      "Lene,\n",
      "Hit cophot favoiorcous, rana  me n lûÀme me.\n",
      "TRx daie----UJelais, \n",
      "step 184010: train loss 2.3621, val loss 2.4339\n",
      "step 184020: train loss 2.4316, val loss 2.4792\n",
      "step 184030: train loss 2.3642, val loss 2.5964\n",
      "step 184040: train loss 2.3375, val loss 2.4840\n",
      "step 184050: train loss 2.4667, val loss 2.4740\n",
      "step 184060: train loss 2.3785, val loss 2.4331\n",
      "step 184070: train loss 2.3920, val loss 2.4453\n",
      "step 184080: train loss 2.3592, val loss 2.4917\n",
      "step 184090: train loss 2.3900, val loss 2.4501\n",
      "step 184100: train loss 2.4626, val loss 2.5342\n",
      "step 184110: train loss 2.3851, val loss 2.5029\n",
      "step 184120: train loss 2.3392, val loss 2.3768\n",
      "step 184130: train loss 2.3863, val loss 2.4464\n",
      "step 184140: train loss 2.3265, val loss 2.4175\n",
      "step 184150: train loss 2.3805, val loss 2.5139\n",
      "step 184160: train loss 2.4655, val loss 2.4471\n",
      "step 184170: train loss 2.3778, val loss 2.4346\n",
      "step 184180: train loss 2.4249, val loss 2.4206\n",
      "step 184190: train loss 2.4323, val loss 2.4215\n",
      "step 184200: train loss 2.3655, val loss 2.5041\n",
      "step 184210: train loss 2.4044, val loss 2.4642\n",
      "step 184220: train loss 2.4662, val loss 2.5504\n",
      "step 184230: train loss 2.3244, val loss 2.4611\n",
      "step 184240: train loss 2.3990, val loss 2.4236\n",
      "step 184250: train loss 2.3520, val loss 2.5413\n",
      "step 184260: train loss 2.4772, val loss 2.4824\n",
      "step 184270: train loss 2.4000, val loss 2.4661\n",
      "step 184280: train loss 2.4105, val loss 2.3777\n",
      "step 184290: train loss 2.3957, val loss 2.4015\n",
      "step 184300: train loss 2.4640, val loss 2.4534\n",
      "step 184310: train loss 2.3744, val loss 2.5775\n",
      "step 184320: train loss 2.3989, val loss 2.4776\n",
      "step 184330: train loss 2.3269, val loss 2.5210\n",
      "step 184340: train loss 2.4268, val loss 2.3247\n",
      "step 184350: train loss 2.3877, val loss 2.4537\n",
      "step 184360: train loss 2.3961, val loss 2.4005\n",
      "step 184370: train loss 2.4278, val loss 2.4113\n",
      "step 184380: train loss 2.4136, val loss 2.4392\n",
      "step 184390: train loss 2.4717, val loss 2.4608\n",
      "step 184400: train loss 2.4159, val loss 2.5103\n",
      "step 184410: train loss 2.4630, val loss 2.4719\n",
      "step 184420: train loss 2.4194, val loss 2.6672\n",
      "step 184430: train loss 2.3556, val loss 2.4457\n",
      "step 184440: train loss 2.4377, val loss 2.5481\n",
      "step 184450: train loss 2.4107, val loss 2.3851\n",
      "step 184460: train loss 2.4192, val loss 2.4871\n",
      "step 184470: train loss 2.4579, val loss 2.4880\n",
      "step 184480: train loss 2.3878, val loss 2.5550\n",
      "step 184490: train loss 2.4452, val loss 2.5049\n",
      "step 184500: train loss 2.4774, val loss 2.4515\n",
      "step 184510: train loss 2.3895, val loss 2.4655\n",
      "step 184520: train loss 2.4609, val loss 2.5683\n",
      "step 184530: train loss 2.4161, val loss 2.4698\n",
      "step 184540: train loss 2.3991, val loss 2.5593\n",
      "step 184550: train loss 2.3549, val loss 2.5472\n",
      "step 184560: train loss 2.4394, val loss 2.5232\n",
      "step 184570: train loss 2.4073, val loss 2.3501\n",
      "step 184580: train loss 2.3528, val loss 2.4521\n",
      "step 184590: train loss 2.3856, val loss 2.4929\n",
      "step 184600: train loss 2.4222, val loss 2.5604\n",
      "step 184610: train loss 2.2990, val loss 2.4221\n",
      "step 184620: train loss 2.4273, val loss 2.5156\n",
      "step 184630: train loss 2.3271, val loss 2.5800\n",
      "step 184640: train loss 2.4333, val loss 2.4662\n",
      "step 184650: train loss 2.4151, val loss 2.4705\n",
      "step 184660: train loss 2.3841, val loss 2.3565\n",
      "step 184670: train loss 2.3511, val loss 2.4382\n",
      "step 184680: train loss 2.4319, val loss 2.4685\n",
      "step 184690: train loss 2.4405, val loss 2.4773\n",
      "step 184700: train loss 2.4294, val loss 2.5223\n",
      "step 184710: train loss 2.3943, val loss 2.4266\n",
      "step 184720: train loss 2.4885, val loss 2.4836\n",
      "step 184730: train loss 2.3200, val loss 2.5018\n",
      "step 184740: train loss 2.3662, val loss 2.4833\n",
      "step 184750: train loss 2.3459, val loss 2.4208\n",
      "step 184760: train loss 2.4101, val loss 2.4835\n",
      "step 184770: train loss 2.3568, val loss 2.4905\n",
      "step 184780: train loss 2.4412, val loss 2.5450\n",
      "step 184790: train loss 2.4518, val loss 2.4848\n",
      "step 184800: train loss 2.4329, val loss 2.4213\n",
      "step 184810: train loss 2.5046, val loss 2.4527\n",
      "step 184820: train loss 2.3837, val loss 2.4411\n",
      "step 184830: train loss 2.3991, val loss 2.4906\n",
      "step 184840: train loss 2.3311, val loss 2.4418\n",
      "step 184850: train loss 2.4100, val loss 2.5273\n",
      "step 184860: train loss 2.3959, val loss 2.5041\n",
      "step 184870: train loss 2.4367, val loss 2.4237\n",
      "step 184880: train loss 2.4572, val loss 2.5160\n",
      "step 184890: train loss 2.3678, val loss 2.5310\n",
      "step 184900: train loss 2.3845, val loss 2.3682\n",
      "step 184910: train loss 2.4566, val loss 2.4668\n",
      "step 184920: train loss 2.3971, val loss 2.4139\n",
      "step 184930: train loss 2.4292, val loss 2.5213\n",
      "step 184940: train loss 2.4007, val loss 2.5207\n",
      "step 184950: train loss 2.4079, val loss 2.4702\n",
      "step 184960: train loss 2.3342, val loss 2.4866\n",
      "step 184970: train loss 2.3801, val loss 2.4353\n",
      "step 184980: train loss 2.3813, val loss 2.4155\n",
      "step 184990: train loss 2.3767, val loss 2.4206\n",
      "step 185000: train loss 2.3974, val loss 2.3919\n",
      "Generated text at iteration 185000\n",
      "\n",
      " az4;\n",
      "L'ha, ns;\n",
      " t!  D8h d prr leure l'e  t:ÔÆXlavosilezesouenderpe  ns\n",
      "\n",
      "Imbltit,\n",
      "\n",
      "Ete au qus   jou,\n",
      "step 185010: train loss 2.4272, val loss 2.5047\n",
      "step 185020: train loss 2.3606, val loss 2.4814\n",
      "step 185030: train loss 2.3608, val loss 2.4545\n",
      "step 185040: train loss 2.3819, val loss 2.5530\n",
      "step 185050: train loss 2.4032, val loss 2.5094\n",
      "step 185060: train loss 2.4187, val loss 2.4387\n",
      "step 185070: train loss 2.3672, val loss 2.4962\n",
      "step 185080: train loss 2.4630, val loss 2.4696\n",
      "step 185090: train loss 2.3763, val loss 2.3544\n",
      "step 185100: train loss 2.3896, val loss 2.5064\n",
      "step 185110: train loss 2.4168, val loss 2.4641\n",
      "step 185120: train loss 2.4976, val loss 2.4960\n",
      "step 185130: train loss 2.3423, val loss 2.5910\n",
      "step 185140: train loss 2.3873, val loss 2.4654\n",
      "step 185150: train loss 2.3524, val loss 2.4953\n",
      "step 185160: train loss 2.3996, val loss 2.4570\n",
      "step 185170: train loss 2.4898, val loss 2.4694\n",
      "step 185180: train loss 2.4944, val loss 2.4558\n",
      "step 185190: train loss 2.3480, val loss 2.4904\n",
      "step 185200: train loss 2.4662, val loss 2.4295\n",
      "step 185210: train loss 2.3680, val loss 2.5550\n",
      "step 185220: train loss 2.3601, val loss 2.3332\n",
      "step 185230: train loss 2.3581, val loss 2.5231\n",
      "step 185240: train loss 2.4313, val loss 2.4729\n",
      "step 185250: train loss 2.3462, val loss 2.4007\n",
      "step 185260: train loss 2.3653, val loss 2.4398\n",
      "step 185270: train loss 2.3688, val loss 2.4767\n",
      "step 185280: train loss 2.4574, val loss 2.4308\n",
      "step 185290: train loss 2.3753, val loss 2.4077\n",
      "step 185300: train loss 2.3674, val loss 2.5549\n",
      "step 185310: train loss 2.3481, val loss 2.4569\n",
      "step 185320: train loss 2.3672, val loss 2.5023\n",
      "step 185330: train loss 2.4174, val loss 2.3873\n",
      "step 185340: train loss 2.3666, val loss 2.5397\n",
      "step 185350: train loss 2.4223, val loss 2.4685\n",
      "step 185360: train loss 2.3888, val loss 2.5633\n",
      "step 185370: train loss 2.4484, val loss 2.4314\n",
      "step 185380: train loss 2.3800, val loss 2.4565\n",
      "step 185390: train loss 2.3821, val loss 2.4017\n",
      "step 185400: train loss 2.3701, val loss 2.5044\n",
      "step 185410: train loss 2.3400, val loss 2.4608\n",
      "step 185420: train loss 2.3833, val loss 2.4534\n",
      "step 185430: train loss 2.3930, val loss 2.4124\n",
      "step 185440: train loss 2.4250, val loss 2.5473\n",
      "step 185450: train loss 2.2989, val loss 2.4932\n",
      "step 185460: train loss 2.3765, val loss 2.5228\n",
      "step 185470: train loss 2.3507, val loss 2.4090\n",
      "step 185480: train loss 2.4572, val loss 2.5171\n",
      "step 185490: train loss 2.3767, val loss 2.4340\n",
      "step 185500: train loss 2.4333, val loss 2.5022\n",
      "step 185510: train loss 2.3576, val loss 2.3733\n",
      "step 185520: train loss 2.3916, val loss 2.5310\n",
      "step 185530: train loss 2.3768, val loss 2.5607\n",
      "step 185540: train loss 2.4034, val loss 2.4245\n",
      "step 185550: train loss 2.4857, val loss 2.4646\n",
      "step 185560: train loss 2.3014, val loss 2.5475\n",
      "step 185570: train loss 2.4533, val loss 2.5192\n",
      "step 185580: train loss 2.4043, val loss 2.5036\n",
      "step 185590: train loss 2.4569, val loss 2.5001\n",
      "step 185600: train loss 2.4141, val loss 2.4317\n",
      "step 185610: train loss 2.3478, val loss 2.5115\n",
      "step 185620: train loss 2.3806, val loss 2.4201\n",
      "step 185630: train loss 2.4368, val loss 2.5585\n",
      "step 185640: train loss 2.3658, val loss 2.5234\n",
      "step 185650: train loss 2.3640, val loss 2.4534\n",
      "step 185660: train loss 2.4805, val loss 2.4500\n",
      "step 185670: train loss 2.4304, val loss 2.5719\n",
      "step 185680: train loss 2.3837, val loss 2.5164\n",
      "step 185690: train loss 2.3614, val loss 2.4483\n",
      "step 185700: train loss 2.4470, val loss 2.5452\n",
      "step 185710: train loss 2.4042, val loss 2.3270\n",
      "step 185720: train loss 2.4557, val loss 2.4503\n",
      "step 185730: train loss 2.3847, val loss 2.4304\n",
      "step 185740: train loss 2.3999, val loss 2.4368\n",
      "step 185750: train loss 2.4030, val loss 2.5326\n",
      "step 185760: train loss 2.3597, val loss 2.4037\n",
      "step 185770: train loss 2.4730, val loss 2.5265\n",
      "step 185780: train loss 2.3498, val loss 2.5290\n",
      "step 185790: train loss 2.4111, val loss 2.4899\n",
      "step 185800: train loss 2.2828, val loss 2.5152\n",
      "step 185810: train loss 2.3668, val loss 2.4906\n",
      "step 185820: train loss 2.3310, val loss 2.4626\n",
      "step 185830: train loss 2.4573, val loss 2.5146\n",
      "step 185840: train loss 2.4411, val loss 2.4356\n",
      "step 185850: train loss 2.3633, val loss 2.4748\n",
      "step 185860: train loss 2.3790, val loss 2.4957\n",
      "step 185870: train loss 2.4763, val loss 2.5229\n",
      "step 185880: train loss 2.3708, val loss 2.5300\n",
      "step 185890: train loss 2.3701, val loss 2.3629\n",
      "step 185900: train loss 2.3944, val loss 2.4396\n",
      "step 185910: train loss 2.4056, val loss 2.4470\n",
      "step 185920: train loss 2.3727, val loss 2.4753\n",
      "step 185930: train loss 2.3771, val loss 2.3814\n",
      "step 185940: train loss 2.3894, val loss 2.4859\n",
      "step 185950: train loss 2.4273, val loss 2.5652\n",
      "step 185960: train loss 2.3650, val loss 2.3727\n",
      "step 185970: train loss 2.3738, val loss 2.4867\n",
      "step 185980: train loss 2.3599, val loss 2.4903\n",
      "step 185990: train loss 2.3465, val loss 2.4272\n",
      "step 186000: train loss 2.4275, val loss 2.3469\n",
      "Generated text at iteration 186000\n",
      "\n",
      "Lore,\n",
      "FvamêtouponietèblôGpl'in glarans  s hayies frtensavenout ceust ses aure chécha pretéchoa  lare\n",
      "step 186010: train loss 2.4470, val loss 2.4173\n",
      "step 186020: train loss 2.3767, val loss 2.5268\n",
      "step 186030: train loss 2.3912, val loss 2.4918\n",
      "step 186040: train loss 2.4093, val loss 2.5143\n",
      "step 186050: train loss 2.3734, val loss 2.4431\n",
      "step 186060: train loss 2.4409, val loss 2.4952\n",
      "step 186070: train loss 2.3450, val loss 2.4617\n",
      "step 186080: train loss 2.3551, val loss 2.4319\n",
      "step 186090: train loss 2.3886, val loss 2.5194\n",
      "step 186100: train loss 2.3878, val loss 2.6282\n",
      "step 186110: train loss 2.3869, val loss 2.4461\n",
      "step 186120: train loss 2.4362, val loss 2.4670\n",
      "step 186130: train loss 2.4138, val loss 2.4140\n",
      "step 186140: train loss 2.4606, val loss 2.4781\n",
      "step 186150: train loss 2.3849, val loss 2.5303\n",
      "step 186160: train loss 2.4429, val loss 2.5075\n",
      "step 186170: train loss 2.4147, val loss 2.4573\n",
      "step 186180: train loss 2.4679, val loss 2.5125\n",
      "step 186190: train loss 2.2992, val loss 2.4636\n",
      "step 186200: train loss 2.4116, val loss 2.4257\n",
      "step 186210: train loss 2.4499, val loss 2.4346\n",
      "step 186220: train loss 2.3718, val loss 2.4402\n",
      "step 186230: train loss 2.3999, val loss 2.4289\n",
      "step 186240: train loss 2.3696, val loss 2.5474\n",
      "step 186250: train loss 2.4590, val loss 2.5938\n",
      "step 186260: train loss 2.4524, val loss 2.4811\n",
      "step 186270: train loss 2.4421, val loss 2.6788\n",
      "step 186280: train loss 2.4279, val loss 2.4885\n",
      "step 186290: train loss 2.3899, val loss 2.4941\n",
      "step 186300: train loss 2.3806, val loss 2.4466\n",
      "step 186310: train loss 2.3766, val loss 2.4375\n",
      "step 186320: train loss 2.3935, val loss 2.4437\n",
      "step 186330: train loss 2.3483, val loss 2.4136\n",
      "step 186340: train loss 2.2833, val loss 2.4941\n",
      "step 186350: train loss 2.4106, val loss 2.4989\n",
      "step 186360: train loss 2.3953, val loss 2.3850\n",
      "step 186370: train loss 2.3680, val loss 2.3816\n",
      "step 186380: train loss 2.3527, val loss 2.5143\n",
      "step 186390: train loss 2.4604, val loss 2.4846\n",
      "step 186400: train loss 2.4170, val loss 2.4058\n",
      "step 186410: train loss 2.3967, val loss 2.4511\n",
      "step 186420: train loss 2.4678, val loss 2.5116\n",
      "step 186430: train loss 2.3620, val loss 2.4692\n",
      "step 186440: train loss 2.3786, val loss 2.3775\n",
      "step 186450: train loss 2.3955, val loss 2.4074\n",
      "step 186460: train loss 2.3826, val loss 2.4588\n",
      "step 186470: train loss 2.4296, val loss 2.5216\n",
      "step 186480: train loss 2.3801, val loss 2.4258\n",
      "step 186490: train loss 2.4167, val loss 2.4624\n",
      "step 186500: train loss 2.3918, val loss 2.4776\n",
      "step 186510: train loss 2.3759, val loss 2.4043\n",
      "step 186520: train loss 2.3880, val loss 2.5487\n",
      "step 186530: train loss 2.3656, val loss 2.6125\n",
      "step 186540: train loss 2.3615, val loss 2.4340\n",
      "step 186550: train loss 2.4648, val loss 2.4810\n",
      "step 186560: train loss 2.3655, val loss 2.5169\n",
      "step 186570: train loss 2.3157, val loss 2.5910\n",
      "step 186580: train loss 2.4314, val loss 2.5438\n",
      "step 186590: train loss 2.3903, val loss 2.5097\n",
      "step 186600: train loss 2.3451, val loss 2.4485\n",
      "step 186610: train loss 2.4067, val loss 2.5648\n",
      "step 186620: train loss 2.4837, val loss 2.4556\n",
      "step 186630: train loss 2.4490, val loss 2.5914\n",
      "step 186640: train loss 2.4174, val loss 2.4449\n",
      "step 186650: train loss 2.4431, val loss 2.4401\n",
      "step 186660: train loss 2.4346, val loss 2.5256\n",
      "step 186670: train loss 2.4734, val loss 2.4449\n",
      "step 186680: train loss 2.3916, val loss 2.5775\n",
      "step 186690: train loss 2.3978, val loss 2.5694\n",
      "step 186700: train loss 2.3681, val loss 2.5161\n",
      "step 186710: train loss 2.4929, val loss 2.4680\n",
      "step 186720: train loss 2.3872, val loss 2.4641\n",
      "step 186730: train loss 2.3719, val loss 2.4963\n",
      "step 186740: train loss 2.3726, val loss 2.5126\n",
      "step 186750: train loss 2.4176, val loss 2.3886\n",
      "step 186760: train loss 2.3792, val loss 2.4878\n",
      "step 186770: train loss 2.3679, val loss 2.3815\n",
      "step 186780: train loss 2.4755, val loss 2.5212\n",
      "step 186790: train loss 2.3474, val loss 2.5831\n",
      "step 186800: train loss 2.4027, val loss 2.5231\n",
      "step 186810: train loss 2.3719, val loss 2.4755\n",
      "step 186820: train loss 2.5122, val loss 2.5186\n",
      "step 186830: train loss 2.4113, val loss 2.3975\n",
      "step 186840: train loss 2.4283, val loss 2.4297\n",
      "step 186850: train loss 2.3609, val loss 2.5786\n",
      "step 186860: train loss 2.3994, val loss 2.4235\n",
      "step 186870: train loss 2.3817, val loss 2.5507\n",
      "step 186880: train loss 2.4409, val loss 2.4165\n",
      "step 186890: train loss 2.3486, val loss 2.4426\n",
      "step 186900: train loss 2.4493, val loss 2.4045\n",
      "step 186910: train loss 2.3966, val loss 2.4366\n",
      "step 186920: train loss 2.3712, val loss 2.5658\n",
      "step 186930: train loss 2.4087, val loss 2.4150\n",
      "step 186940: train loss 2.4153, val loss 2.4759\n",
      "step 186950: train loss 2.3306, val loss 2.4770\n",
      "step 186960: train loss 2.4049, val loss 2.5209\n",
      "step 186970: train loss 2.4469, val loss 2.5294\n",
      "step 186980: train loss 2.4496, val loss 2.5510\n",
      "step 186990: train loss 2.4173, val loss 2.3724\n",
      "step 187000: train loss 2.4105, val loss 2.3909\n",
      "Generated text at iteration 187000\n",
      "\n",
      "Ques sar, lans lufachengn dufleraneau, phébèt.\n",
      "L'ale cese Et  t,\n",
      "C'aimeuxx;\n",
      "Di chong fenèrse  c rent\n",
      "step 187010: train loss 2.3137, val loss 2.4807\n",
      "step 187020: train loss 2.4964, val loss 2.5043\n",
      "step 187030: train loss 2.3726, val loss 2.3627\n",
      "step 187040: train loss 2.3769, val loss 2.4152\n",
      "step 187050: train loss 2.4225, val loss 2.3954\n",
      "step 187060: train loss 2.4943, val loss 2.6456\n",
      "step 187070: train loss 2.4206, val loss 2.4850\n",
      "step 187080: train loss 2.4267, val loss 2.5289\n",
      "step 187090: train loss 2.4079, val loss 2.4287\n",
      "step 187100: train loss 2.3685, val loss 2.3845\n",
      "step 187110: train loss 2.3577, val loss 2.4439\n",
      "step 187120: train loss 2.3976, val loss 2.5146\n",
      "step 187130: train loss 2.3576, val loss 2.4338\n",
      "step 187140: train loss 2.3284, val loss 2.4327\n",
      "step 187150: train loss 2.3665, val loss 2.5618\n",
      "step 187160: train loss 2.4094, val loss 2.4218\n",
      "step 187170: train loss 2.4013, val loss 2.5018\n",
      "step 187180: train loss 2.4069, val loss 2.3701\n",
      "step 187190: train loss 2.3216, val loss 2.5498\n",
      "step 187200: train loss 2.4977, val loss 2.4474\n",
      "step 187210: train loss 2.3910, val loss 2.4544\n",
      "step 187220: train loss 2.4529, val loss 2.4716\n",
      "step 187230: train loss 2.3844, val loss 2.4807\n",
      "step 187240: train loss 2.3941, val loss 2.4475\n",
      "step 187250: train loss 2.3839, val loss 2.5424\n",
      "step 187260: train loss 2.4050, val loss 2.4407\n",
      "step 187270: train loss 2.3728, val loss 2.4525\n",
      "step 187280: train loss 2.3394, val loss 2.4195\n",
      "step 187290: train loss 2.4102, val loss 2.5561\n",
      "step 187300: train loss 2.4505, val loss 2.5135\n",
      "step 187310: train loss 2.4701, val loss 2.4727\n",
      "step 187320: train loss 2.3832, val loss 2.4668\n",
      "step 187330: train loss 2.3575, val loss 2.5892\n",
      "step 187340: train loss 2.3680, val loss 2.3461\n",
      "step 187350: train loss 2.3258, val loss 2.5060\n",
      "step 187360: train loss 2.2901, val loss 2.5185\n",
      "step 187370: train loss 2.3444, val loss 2.4943\n",
      "step 187380: train loss 2.4103, val loss 2.3555\n",
      "step 187390: train loss 2.4403, val loss 2.4925\n",
      "step 187400: train loss 2.3753, val loss 2.4948\n",
      "step 187410: train loss 2.4839, val loss 2.4070\n",
      "step 187420: train loss 2.3460, val loss 2.4935\n",
      "step 187430: train loss 2.4217, val loss 2.4653\n",
      "step 187440: train loss 2.4026, val loss 2.4829\n",
      "step 187450: train loss 2.4029, val loss 2.4203\n",
      "step 187460: train loss 2.4081, val loss 2.6494\n",
      "step 187470: train loss 2.3266, val loss 2.4339\n",
      "step 187480: train loss 2.3867, val loss 2.5524\n",
      "step 187490: train loss 2.4324, val loss 2.5369\n",
      "step 187500: train loss 2.3892, val loss 2.5057\n",
      "step 187510: train loss 2.4475, val loss 2.3418\n",
      "step 187520: train loss 2.3694, val loss 2.4969\n",
      "step 187530: train loss 2.3397, val loss 2.4028\n",
      "step 187540: train loss 2.4259, val loss 2.4284\n",
      "step 187550: train loss 2.3431, val loss 2.4893\n",
      "step 187560: train loss 2.4026, val loss 2.4212\n",
      "step 187570: train loss 2.4654, val loss 2.4915\n",
      "step 187580: train loss 2.3744, val loss 2.4222\n",
      "step 187590: train loss 2.4267, val loss 2.5398\n",
      "step 187600: train loss 2.4544, val loss 2.4212\n",
      "step 187610: train loss 2.4033, val loss 2.5545\n",
      "step 187620: train loss 2.4708, val loss 2.6223\n",
      "step 187630: train loss 2.3788, val loss 2.6576\n",
      "step 187640: train loss 2.3572, val loss 2.4183\n",
      "step 187650: train loss 2.4515, val loss 2.4146\n",
      "step 187660: train loss 2.4009, val loss 2.4131\n",
      "step 187670: train loss 2.3828, val loss 2.3849\n",
      "step 187680: train loss 2.4218, val loss 2.4275\n",
      "step 187690: train loss 2.3641, val loss 2.4387\n",
      "step 187700: train loss 2.4374, val loss 2.6462\n",
      "step 187710: train loss 2.4369, val loss 2.5445\n",
      "step 187720: train loss 2.3117, val loss 2.4237\n",
      "step 187730: train loss 2.4101, val loss 2.5666\n",
      "step 187740: train loss 2.3875, val loss 2.5014\n",
      "step 187750: train loss 2.4292, val loss 2.3918\n",
      "step 187760: train loss 2.4183, val loss 2.5071\n",
      "step 187770: train loss 2.3518, val loss 2.5859\n",
      "step 187780: train loss 2.4153, val loss 2.5067\n",
      "step 187790: train loss 2.4715, val loss 2.4246\n",
      "step 187800: train loss 2.3872, val loss 2.4466\n",
      "step 187810: train loss 2.4065, val loss 2.5058\n",
      "step 187820: train loss 2.3430, val loss 2.4267\n",
      "step 187830: train loss 2.3804, val loss 2.5051\n",
      "step 187840: train loss 2.4630, val loss 2.4964\n",
      "step 187850: train loss 2.4409, val loss 2.5055\n",
      "step 187860: train loss 2.3998, val loss 2.3464\n",
      "step 187870: train loss 2.2991, val loss 2.5045\n",
      "step 187880: train loss 2.3630, val loss 2.4549\n",
      "step 187890: train loss 2.3814, val loss 2.4344\n",
      "step 187900: train loss 2.3463, val loss 2.5771\n",
      "step 187910: train loss 2.4121, val loss 2.4697\n",
      "step 187920: train loss 2.3593, val loss 2.6310\n",
      "step 187930: train loss 2.3455, val loss 2.4096\n",
      "step 187940: train loss 2.3672, val loss 2.5041\n",
      "step 187950: train loss 2.3786, val loss 2.4898\n",
      "step 187960: train loss 2.3744, val loss 2.4449\n",
      "step 187970: train loss 2.4596, val loss 2.4514\n",
      "step 187980: train loss 2.4210, val loss 2.4464\n",
      "step 187990: train loss 2.3389, val loss 2.4796\n",
      "step 188000: train loss 2.4982, val loss 2.4789\n",
      "Generated text at iteration 188000\n",
      "\n",
      " n fe,\n",
      "\n",
      "\n",
      "D'Éèrsavont qures fuiche e:·1je  tefuren sidu  des  lai t n me detir pailéche PÈÉrdes qust \n",
      "step 188010: train loss 2.3860, val loss 2.5486\n",
      "step 188020: train loss 2.4522, val loss 2.5403\n",
      "step 188030: train loss 2.4017, val loss 2.5026\n",
      "step 188040: train loss 2.3476, val loss 2.6311\n",
      "step 188050: train loss 2.4006, val loss 2.4533\n",
      "step 188060: train loss 2.3494, val loss 2.5239\n",
      "step 188070: train loss 2.3719, val loss 2.5812\n",
      "step 188080: train loss 2.4609, val loss 2.4153\n",
      "step 188090: train loss 2.3751, val loss 2.5332\n",
      "step 188100: train loss 2.4262, val loss 2.5272\n",
      "step 188110: train loss 2.3338, val loss 2.5292\n",
      "step 188120: train loss 2.3869, val loss 2.6242\n",
      "step 188130: train loss 2.3711, val loss 2.5855\n",
      "step 188140: train loss 2.3720, val loss 2.4588\n",
      "step 188150: train loss 2.4348, val loss 2.4191\n",
      "step 188160: train loss 2.3787, val loss 2.4669\n",
      "step 188170: train loss 2.4236, val loss 2.4661\n",
      "step 188180: train loss 2.4503, val loss 2.5008\n",
      "step 188190: train loss 2.4081, val loss 2.5398\n",
      "step 188200: train loss 2.4432, val loss 2.4153\n",
      "step 188210: train loss 2.4250, val loss 2.4627\n",
      "step 188220: train loss 2.4161, val loss 2.4832\n",
      "step 188230: train loss 2.4174, val loss 2.5111\n",
      "step 188240: train loss 2.3467, val loss 2.5704\n",
      "step 188250: train loss 2.3827, val loss 2.5658\n",
      "step 188260: train loss 2.3768, val loss 2.4598\n",
      "step 188270: train loss 2.3704, val loss 2.5083\n",
      "step 188280: train loss 2.4095, val loss 2.3916\n",
      "step 188290: train loss 2.4311, val loss 2.5724\n",
      "step 188300: train loss 2.3804, val loss 2.5008\n",
      "step 188310: train loss 2.3635, val loss 2.5093\n",
      "step 188320: train loss 2.4865, val loss 2.5802\n",
      "step 188330: train loss 2.3779, val loss 2.4045\n",
      "step 188340: train loss 2.4404, val loss 2.4691\n",
      "step 188350: train loss 2.4072, val loss 2.4346\n",
      "step 188360: train loss 2.5009, val loss 2.4596\n",
      "step 188370: train loss 2.3716, val loss 2.5410\n",
      "step 188380: train loss 2.3884, val loss 2.4945\n",
      "step 188390: train loss 2.3325, val loss 2.4373\n",
      "step 188400: train loss 2.3912, val loss 2.3709\n",
      "step 188410: train loss 2.4199, val loss 2.3649\n",
      "step 188420: train loss 2.3959, val loss 2.4599\n",
      "step 188430: train loss 2.3746, val loss 2.6085\n",
      "step 188440: train loss 2.3698, val loss 2.4866\n",
      "step 188450: train loss 2.4285, val loss 2.4814\n",
      "step 188460: train loss 2.3631, val loss 2.5012\n",
      "step 188470: train loss 2.3994, val loss 2.7159\n",
      "step 188480: train loss 2.3035, val loss 2.4168\n",
      "step 188490: train loss 2.3690, val loss 2.4921\n",
      "step 188500: train loss 2.3892, val loss 2.6414\n",
      "step 188510: train loss 2.3686, val loss 2.6019\n",
      "step 188520: train loss 2.4042, val loss 2.5189\n",
      "step 188530: train loss 2.4360, val loss 2.4269\n",
      "step 188540: train loss 2.4501, val loss 2.5781\n",
      "step 188550: train loss 2.4291, val loss 2.4148\n",
      "step 188560: train loss 2.3825, val loss 2.5121\n",
      "step 188570: train loss 2.3661, val loss 2.6002\n",
      "step 188580: train loss 2.4348, val loss 2.4905\n",
      "step 188590: train loss 2.3884, val loss 2.5041\n",
      "step 188600: train loss 2.3536, val loss 2.6083\n",
      "step 188610: train loss 2.4041, val loss 2.4994\n",
      "step 188620: train loss 2.4216, val loss 2.4572\n",
      "step 188630: train loss 2.3433, val loss 2.4322\n",
      "step 188640: train loss 2.3391, val loss 2.4927\n",
      "step 188650: train loss 2.3996, val loss 2.5834\n",
      "step 188660: train loss 2.4341, val loss 2.4163\n",
      "step 188670: train loss 2.4014, val loss 2.4846\n",
      "step 188680: train loss 2.3512, val loss 2.4857\n",
      "step 188690: train loss 2.3901, val loss 2.5415\n",
      "step 188700: train loss 2.4225, val loss 2.5215\n",
      "step 188710: train loss 2.2947, val loss 2.5417\n",
      "step 188720: train loss 2.4035, val loss 2.4829\n",
      "step 188730: train loss 2.4541, val loss 2.4482\n",
      "step 188740: train loss 2.4837, val loss 2.4748\n",
      "step 188750: train loss 2.3957, val loss 2.3321\n",
      "step 188760: train loss 2.4013, val loss 2.4783\n",
      "step 188770: train loss 2.3789, val loss 2.5226\n",
      "step 188780: train loss 2.3968, val loss 2.3916\n",
      "step 188790: train loss 2.3608, val loss 2.4732\n",
      "step 188800: train loss 2.4983, val loss 2.4286\n",
      "step 188810: train loss 2.4596, val loss 2.4777\n",
      "step 188820: train loss 2.3578, val loss 2.4836\n",
      "step 188830: train loss 2.4425, val loss 2.5886\n",
      "step 188840: train loss 2.3473, val loss 2.5570\n",
      "step 188850: train loss 2.3818, val loss 2.3842\n",
      "step 188860: train loss 2.3820, val loss 2.4120\n",
      "step 188870: train loss 2.3949, val loss 2.4818\n",
      "step 188880: train loss 2.4160, val loss 2.4908\n",
      "step 188890: train loss 2.4689, val loss 2.5007\n",
      "step 188900: train loss 2.4242, val loss 2.5103\n",
      "step 188910: train loss 2.3977, val loss 2.6239\n",
      "step 188920: train loss 2.4025, val loss 2.4349\n",
      "step 188930: train loss 2.3674, val loss 2.3521\n",
      "step 188940: train loss 2.3188, val loss 2.4574\n",
      "step 188950: train loss 2.3671, val loss 2.4373\n",
      "step 188960: train loss 2.4701, val loss 2.3824\n",
      "step 188970: train loss 2.4359, val loss 2.4723\n",
      "step 188980: train loss 2.3925, val loss 2.6470\n",
      "step 188990: train loss 2.3659, val loss 2.4773\n",
      "step 189000: train loss 2.3898, val loss 2.5167\n",
      "Generated text at iteration 189000\n",
      "\n",
      "QUContant  s,  pe jesons ôdere.\n",
      "Je a âd;\n",
      "«Àîtrqus d'he le come lenouirois quemondes re déa  ttrese, \n",
      "step 189010: train loss 2.4735, val loss 2.4634\n",
      "step 189020: train loss 2.4248, val loss 2.4314\n",
      "step 189030: train loss 2.4686, val loss 2.4441\n",
      "step 189040: train loss 2.3485, val loss 2.5822\n",
      "step 189050: train loss 2.3340, val loss 2.4709\n",
      "step 189060: train loss 2.3724, val loss 2.4958\n",
      "step 189070: train loss 2.3770, val loss 2.4410\n",
      "step 189080: train loss 2.3485, val loss 2.4864\n",
      "step 189090: train loss 2.3830, val loss 2.4668\n",
      "step 189100: train loss 2.3164, val loss 2.5018\n",
      "step 189110: train loss 2.4156, val loss 2.4774\n",
      "step 189120: train loss 2.3668, val loss 2.4638\n",
      "step 189130: train loss 2.3985, val loss 2.3634\n",
      "step 189140: train loss 2.4134, val loss 2.5096\n",
      "step 189150: train loss 2.3971, val loss 2.4964\n",
      "step 189160: train loss 2.4036, val loss 2.4896\n",
      "step 189170: train loss 2.3777, val loss 2.6127\n",
      "step 189180: train loss 2.3408, val loss 2.5035\n",
      "step 189190: train loss 2.5048, val loss 2.4933\n",
      "step 189200: train loss 2.4141, val loss 2.4020\n",
      "step 189210: train loss 2.3548, val loss 2.4477\n",
      "step 189220: train loss 2.3852, val loss 2.4023\n",
      "step 189230: train loss 2.4305, val loss 2.5412\n",
      "step 189240: train loss 2.3723, val loss 2.5144\n",
      "step 189250: train loss 2.3964, val loss 2.3302\n",
      "step 189260: train loss 2.3800, val loss 2.5793\n",
      "step 189270: train loss 2.3934, val loss 2.4525\n",
      "step 189280: train loss 2.4577, val loss 2.4807\n",
      "step 189290: train loss 2.3203, val loss 2.4892\n",
      "step 189300: train loss 2.3967, val loss 2.4477\n",
      "step 189310: train loss 2.3665, val loss 2.5141\n",
      "step 189320: train loss 2.3732, val loss 2.5305\n",
      "step 189330: train loss 2.2903, val loss 2.5246\n",
      "step 189340: train loss 2.3865, val loss 2.4492\n",
      "step 189350: train loss 2.3992, val loss 2.5213\n",
      "step 189360: train loss 2.3717, val loss 2.4976\n",
      "step 189370: train loss 2.4638, val loss 2.3921\n",
      "step 189380: train loss 2.4472, val loss 2.6063\n",
      "step 189390: train loss 2.4083, val loss 2.4172\n",
      "step 189400: train loss 2.3598, val loss 2.4605\n",
      "step 189410: train loss 2.3730, val loss 2.4643\n",
      "step 189420: train loss 2.4149, val loss 2.4678\n",
      "step 189430: train loss 2.4118, val loss 2.5211\n",
      "step 189440: train loss 2.4408, val loss 2.6498\n",
      "step 189450: train loss 2.4308, val loss 2.3871\n",
      "step 189460: train loss 2.4541, val loss 2.3910\n",
      "step 189470: train loss 2.3782, val loss 2.4535\n",
      "step 189480: train loss 2.4043, val loss 2.4174\n",
      "step 189490: train loss 2.3576, val loss 2.5081\n",
      "step 189500: train loss 2.4677, val loss 2.4811\n",
      "step 189510: train loss 2.3876, val loss 2.4297\n",
      "step 189520: train loss 2.3805, val loss 2.4240\n",
      "step 189530: train loss 2.4677, val loss 2.6790\n",
      "step 189540: train loss 2.4168, val loss 2.3974\n",
      "step 189550: train loss 2.4059, val loss 2.5529\n",
      "step 189560: train loss 2.3833, val loss 2.3781\n",
      "step 189570: train loss 2.4483, val loss 2.4003\n",
      "step 189580: train loss 2.4124, val loss 2.3693\n",
      "step 189590: train loss 2.3749, val loss 2.6177\n",
      "step 189600: train loss 2.3415, val loss 2.5186\n",
      "step 189610: train loss 2.4341, val loss 2.6598\n",
      "step 189620: train loss 2.3657, val loss 2.4516\n",
      "step 189630: train loss 2.4613, val loss 2.4070\n",
      "step 189640: train loss 2.4237, val loss 2.4226\n",
      "step 189650: train loss 2.3548, val loss 2.4793\n",
      "step 189660: train loss 2.3659, val loss 2.4813\n",
      "step 189670: train loss 2.3323, val loss 2.4261\n",
      "step 189680: train loss 2.4232, val loss 2.4664\n",
      "step 189690: train loss 2.3842, val loss 2.5101\n",
      "step 189700: train loss 2.3057, val loss 2.4026\n",
      "step 189710: train loss 2.3728, val loss 2.5517\n",
      "step 189720: train loss 2.4827, val loss 2.4984\n",
      "step 189730: train loss 2.3977, val loss 2.3544\n",
      "step 189740: train loss 2.4620, val loss 2.4538\n",
      "step 189750: train loss 2.4125, val loss 2.5560\n",
      "step 189760: train loss 2.3575, val loss 2.6007\n",
      "step 189770: train loss 2.3953, val loss 2.4921\n",
      "step 189780: train loss 2.4147, val loss 2.5671\n",
      "step 189790: train loss 2.4186, val loss 2.4668\n",
      "step 189800: train loss 2.3552, val loss 2.4325\n",
      "step 189810: train loss 2.3960, val loss 2.5979\n",
      "step 189820: train loss 2.3825, val loss 2.4770\n",
      "step 189830: train loss 2.4180, val loss 2.4706\n",
      "step 189840: train loss 2.4634, val loss 2.5537\n",
      "step 189850: train loss 2.3326, val loss 2.6304\n",
      "step 189860: train loss 2.3583, val loss 2.4900\n",
      "step 189870: train loss 2.4096, val loss 2.4995\n",
      "step 189880: train loss 2.3899, val loss 2.5260\n",
      "step 189890: train loss 2.3818, val loss 2.5018\n",
      "step 189900: train loss 2.4166, val loss 2.4508\n",
      "step 189910: train loss 2.4822, val loss 2.5075\n",
      "step 189920: train loss 2.3824, val loss 2.4184\n",
      "step 189930: train loss 2.3986, val loss 2.4878\n",
      "step 189940: train loss 2.3453, val loss 2.4043\n",
      "step 189950: train loss 2.4295, val loss 2.4602\n",
      "step 189960: train loss 2.3269, val loss 2.5528\n",
      "step 189970: train loss 2.3747, val loss 2.3432\n",
      "step 189980: train loss 2.4529, val loss 2.4555\n",
      "step 189990: train loss 2.4030, val loss 2.4681\n",
      "step 190000: train loss 2.4218, val loss 2.4988\n",
      "Generated text at iteration 190000\n",
      "\n",
      "\n",
      "QJestomomèl, à darblomars me qus,\n",
      "Qut,\n",
      "Ces te,\n",
      "Some e!\n",
      "O s, yaroma cole lle chon vu qurt;\n",
      "Dut-drre,\n",
      "step 190010: train loss 2.4235, val loss 2.5622\n",
      "step 190020: train loss 2.4374, val loss 2.4301\n",
      "step 190030: train loss 2.3364, val loss 2.5755\n",
      "step 190040: train loss 2.3784, val loss 2.3755\n",
      "step 190050: train loss 2.4309, val loss 2.4269\n",
      "step 190060: train loss 2.4019, val loss 2.4118\n",
      "step 190070: train loss 2.3212, val loss 2.5894\n",
      "step 190080: train loss 2.3728, val loss 2.4238\n",
      "step 190090: train loss 2.3890, val loss 2.4928\n",
      "step 190100: train loss 2.3795, val loss 2.5029\n",
      "step 190110: train loss 2.4179, val loss 2.4578\n",
      "step 190120: train loss 2.3395, val loss 2.6004\n",
      "step 190130: train loss 2.4763, val loss 2.3856\n",
      "step 190140: train loss 2.3830, val loss 2.5270\n",
      "step 190150: train loss 2.4223, val loss 2.4208\n",
      "step 190160: train loss 2.3752, val loss 2.4804\n",
      "step 190170: train loss 2.4210, val loss 2.4352\n",
      "step 190180: train loss 2.4056, val loss 2.4476\n",
      "step 190190: train loss 2.3802, val loss 2.4449\n",
      "step 190200: train loss 2.3668, val loss 2.4826\n",
      "step 190210: train loss 2.3292, val loss 2.4466\n",
      "step 190220: train loss 2.5148, val loss 2.5004\n",
      "step 190230: train loss 2.4156, val loss 2.4701\n",
      "step 190240: train loss 2.4091, val loss 2.4566\n",
      "step 190250: train loss 2.4521, val loss 2.3700\n",
      "step 190260: train loss 2.3780, val loss 2.4336\n",
      "step 190270: train loss 2.3741, val loss 2.3932\n",
      "step 190280: train loss 2.3679, val loss 2.4302\n",
      "step 190290: train loss 2.4161, val loss 2.5462\n",
      "step 190300: train loss 2.3572, val loss 2.5125\n",
      "step 190310: train loss 2.4617, val loss 2.5281\n",
      "step 190320: train loss 2.3451, val loss 2.4807\n",
      "step 190330: train loss 2.4006, val loss 2.4972\n",
      "step 190340: train loss 2.3485, val loss 2.3528\n",
      "step 190350: train loss 2.3526, val loss 2.5070\n",
      "step 190360: train loss 2.4133, val loss 2.3942\n",
      "step 190370: train loss 2.5169, val loss 2.5029\n",
      "step 190380: train loss 2.4056, val loss 2.4947\n",
      "step 190390: train loss 2.3956, val loss 2.5062\n",
      "step 190400: train loss 2.4385, val loss 2.4399\n",
      "step 190410: train loss 2.3664, val loss 2.4367\n",
      "step 190420: train loss 2.3905, val loss 2.4488\n",
      "step 190430: train loss 2.3519, val loss 2.4409\n",
      "step 190440: train loss 2.4606, val loss 2.5326\n",
      "step 190450: train loss 2.4079, val loss 2.4420\n",
      "step 190460: train loss 2.2586, val loss 2.4000\n",
      "step 190470: train loss 2.3669, val loss 2.4572\n",
      "step 190480: train loss 2.4496, val loss 2.4452\n",
      "step 190490: train loss 2.4075, val loss 2.4885\n",
      "step 190500: train loss 2.4219, val loss 2.4793\n",
      "step 190510: train loss 2.3642, val loss 2.3678\n",
      "step 190520: train loss 2.3624, val loss 2.4638\n",
      "step 190530: train loss 2.4586, val loss 2.4483\n",
      "step 190540: train loss 2.3634, val loss 2.4588\n",
      "step 190550: train loss 2.3147, val loss 2.4305\n",
      "step 190560: train loss 2.4575, val loss 2.5154\n",
      "step 190570: train loss 2.4653, val loss 2.4040\n",
      "step 190580: train loss 2.4422, val loss 2.5008\n",
      "step 190590: train loss 2.3688, val loss 2.5198\n",
      "step 190600: train loss 2.4248, val loss 2.4201\n",
      "step 190610: train loss 2.3921, val loss 2.4625\n",
      "step 190620: train loss 2.4462, val loss 2.4630\n",
      "step 190630: train loss 2.4328, val loss 2.5010\n",
      "step 190640: train loss 2.4953, val loss 2.3962\n",
      "step 190650: train loss 2.3941, val loss 2.4945\n",
      "step 190660: train loss 2.3624, val loss 2.4294\n",
      "step 190670: train loss 2.3942, val loss 2.4341\n",
      "step 190680: train loss 2.3613, val loss 2.5143\n",
      "step 190690: train loss 2.3988, val loss 2.5273\n",
      "step 190700: train loss 2.3796, val loss 2.4902\n",
      "step 190710: train loss 2.3790, val loss 2.5495\n",
      "step 190720: train loss 2.3899, val loss 2.5288\n",
      "step 190730: train loss 2.3135, val loss 2.5118\n",
      "step 190740: train loss 2.4021, val loss 2.4959\n",
      "step 190750: train loss 2.4118, val loss 2.5000\n",
      "step 190760: train loss 2.3793, val loss 2.5490\n",
      "step 190770: train loss 2.4067, val loss 2.4848\n",
      "step 190780: train loss 2.4960, val loss 2.4339\n",
      "step 190790: train loss 2.3751, val loss 2.4124\n",
      "step 190800: train loss 2.3865, val loss 2.4781\n",
      "step 190810: train loss 2.3464, val loss 2.4420\n",
      "step 190820: train loss 2.4370, val loss 2.4846\n",
      "step 190830: train loss 2.3714, val loss 2.4276\n",
      "step 190840: train loss 2.4624, val loss 2.5206\n",
      "step 190850: train loss 2.3974, val loss 2.4254\n",
      "step 190860: train loss 2.4080, val loss 2.5608\n",
      "step 190870: train loss 2.3884, val loss 2.4428\n",
      "step 190880: train loss 2.4569, val loss 2.5088\n",
      "step 190890: train loss 2.3199, val loss 2.4671\n",
      "step 190900: train loss 2.3108, val loss 2.5992\n",
      "step 190910: train loss 2.3691, val loss 2.5032\n",
      "step 190920: train loss 2.3717, val loss 2.3883\n",
      "step 190930: train loss 2.4682, val loss 2.5300\n",
      "step 190940: train loss 2.3814, val loss 2.4064\n",
      "step 190950: train loss 2.4217, val loss 2.4579\n",
      "step 190960: train loss 2.3212, val loss 2.4680\n",
      "step 190970: train loss 2.4551, val loss 2.4415\n",
      "step 190980: train loss 2.4096, val loss 2.4695\n",
      "step 190990: train loss 2.4146, val loss 2.5219\n",
      "step 191000: train loss 2.3516, val loss 2.3745\n",
      "Generated text at iteration 191000\n",
      "\n",
      " qux an d'auioir ée,\n",
      "Fll'oue pertotaure us e!\n",
      "\n",
      "vit nu.\n",
      "Il'évindesce as l'es Dellis ds desou.\n",
      "\n",
      "L'hari\n",
      "step 191010: train loss 2.4159, val loss 2.4839\n",
      "step 191020: train loss 2.3050, val loss 2.4157\n",
      "step 191030: train loss 2.4327, val loss 2.4750\n",
      "step 191040: train loss 2.4486, val loss 2.3888\n",
      "step 191050: train loss 2.4454, val loss 2.5177\n",
      "step 191060: train loss 2.4691, val loss 2.5719\n",
      "step 191070: train loss 2.3877, val loss 2.4405\n",
      "step 191080: train loss 2.3361, val loss 2.3984\n",
      "step 191090: train loss 2.4366, val loss 2.3930\n",
      "step 191100: train loss 2.3829, val loss 2.3951\n",
      "step 191110: train loss 2.4635, val loss 2.6483\n",
      "step 191120: train loss 2.4371, val loss 2.5058\n",
      "step 191130: train loss 2.3578, val loss 2.5124\n",
      "step 191140: train loss 2.3969, val loss 2.4115\n",
      "step 191150: train loss 2.3379, val loss 2.3942\n",
      "step 191160: train loss 2.3845, val loss 2.4923\n",
      "step 191170: train loss 2.4162, val loss 2.5178\n",
      "step 191180: train loss 2.4097, val loss 2.5032\n",
      "step 191190: train loss 2.4116, val loss 2.5286\n",
      "step 191200: train loss 2.4154, val loss 2.4795\n",
      "step 191210: train loss 2.4449, val loss 2.4572\n",
      "step 191220: train loss 2.4364, val loss 2.3533\n",
      "step 191230: train loss 2.3450, val loss 2.5069\n",
      "step 191240: train loss 2.4098, val loss 2.3445\n",
      "step 191250: train loss 2.3838, val loss 2.5399\n",
      "step 191260: train loss 2.3741, val loss 2.3973\n",
      "step 191270: train loss 2.4241, val loss 2.5082\n",
      "step 191280: train loss 2.4890, val loss 2.5822\n",
      "step 191290: train loss 2.4436, val loss 2.5401\n",
      "step 191300: train loss 2.3833, val loss 2.4186\n",
      "step 191310: train loss 2.3276, val loss 2.4804\n",
      "step 191320: train loss 2.4949, val loss 2.4795\n",
      "step 191330: train loss 2.4124, val loss 2.4859\n",
      "step 191340: train loss 2.3789, val loss 2.4878\n",
      "step 191350: train loss 2.4223, val loss 2.5438\n",
      "step 191360: train loss 2.4153, val loss 2.4574\n",
      "step 191370: train loss 2.3800, val loss 2.4588\n",
      "step 191380: train loss 2.4589, val loss 2.4204\n",
      "step 191390: train loss 2.4092, val loss 2.4250\n",
      "step 191400: train loss 2.3912, val loss 2.4676\n",
      "step 191410: train loss 2.4153, val loss 2.5106\n",
      "step 191420: train loss 2.4204, val loss 2.4649\n",
      "step 191430: train loss 2.3981, val loss 2.4024\n",
      "step 191440: train loss 2.3770, val loss 2.5324\n",
      "step 191450: train loss 2.3778, val loss 2.4149\n",
      "step 191460: train loss 2.3826, val loss 2.4633\n",
      "step 191470: train loss 2.4162, val loss 2.4449\n",
      "step 191480: train loss 2.3795, val loss 2.5076\n",
      "step 191490: train loss 2.4554, val loss 2.3652\n",
      "step 191500: train loss 2.4244, val loss 2.5483\n",
      "step 191510: train loss 2.3998, val loss 2.4434\n",
      "step 191520: train loss 2.4027, val loss 2.4752\n",
      "step 191530: train loss 2.4733, val loss 2.5065\n",
      "step 191540: train loss 2.4117, val loss 2.6084\n",
      "step 191550: train loss 2.3073, val loss 2.5463\n",
      "step 191560: train loss 2.3837, val loss 2.4212\n",
      "step 191570: train loss 2.2960, val loss 2.4514\n",
      "step 191580: train loss 2.4317, val loss 2.4896\n",
      "step 191590: train loss 2.3970, val loss 2.5218\n",
      "step 191600: train loss 2.4254, val loss 2.4909\n",
      "step 191610: train loss 2.4445, val loss 2.5818\n",
      "step 191620: train loss 2.3922, val loss 2.4982\n",
      "step 191630: train loss 2.4105, val loss 2.5509\n",
      "step 191640: train loss 2.4198, val loss 2.5715\n",
      "step 191650: train loss 2.3827, val loss 2.4196\n",
      "step 191660: train loss 2.3718, val loss 2.5060\n",
      "step 191670: train loss 2.5081, val loss 2.5333\n",
      "step 191680: train loss 2.4316, val loss 2.3538\n",
      "step 191690: train loss 2.3728, val loss 2.4557\n",
      "step 191700: train loss 2.3459, val loss 2.4267\n",
      "step 191710: train loss 2.3832, val loss 2.4119\n",
      "step 191720: train loss 2.4069, val loss 2.4416\n",
      "step 191730: train loss 2.4338, val loss 2.5255\n",
      "step 191740: train loss 2.3664, val loss 2.4234\n",
      "step 191750: train loss 2.4274, val loss 2.5376\n",
      "step 191760: train loss 2.3346, val loss 2.5033\n",
      "step 191770: train loss 2.4557, val loss 2.3600\n",
      "step 191780: train loss 2.4007, val loss 2.5246\n",
      "step 191790: train loss 2.3557, val loss 2.4857\n",
      "step 191800: train loss 2.4497, val loss 2.4645\n",
      "step 191810: train loss 2.3600, val loss 2.4460\n",
      "step 191820: train loss 2.4001, val loss 2.3539\n",
      "step 191830: train loss 2.3854, val loss 2.4183\n",
      "step 191840: train loss 2.3358, val loss 2.4985\n",
      "step 191850: train loss 2.3971, val loss 2.4581\n",
      "step 191860: train loss 2.3618, val loss 2.4190\n",
      "step 191870: train loss 2.3769, val loss 2.5544\n",
      "step 191880: train loss 2.3982, val loss 2.4908\n",
      "step 191890: train loss 2.4194, val loss 2.5256\n",
      "step 191900: train loss 2.4424, val loss 2.4425\n",
      "step 191910: train loss 2.3758, val loss 2.4356\n",
      "step 191920: train loss 2.4507, val loss 2.5656\n",
      "step 191930: train loss 2.4246, val loss 2.4587\n",
      "step 191940: train loss 2.3569, val loss 2.4511\n",
      "step 191950: train loss 2.4166, val loss 2.4871\n",
      "step 191960: train loss 2.4134, val loss 2.4854\n",
      "step 191970: train loss 2.4195, val loss 2.3874\n",
      "step 191980: train loss 2.4445, val loss 2.4582\n",
      "step 191990: train loss 2.4433, val loss 2.5606\n",
      "step 192000: train loss 2.4317, val loss 2.3919\n",
      "Generated text at iteration 192000\n",
      "\n",
      "  Sols orgl'hue,\n",
      "Eugrere foux des he lû«ù fâcés jont endrs  s, cois\n",
      "\n",
      "En, n ule llayasa s.\n",
      "Site l sus\n",
      "step 192010: train loss 2.3950, val loss 2.5044\n",
      "step 192020: train loss 2.4034, val loss 2.4711\n",
      "step 192030: train loss 2.3577, val loss 2.5990\n",
      "step 192040: train loss 2.3727, val loss 2.5530\n",
      "step 192050: train loss 2.3638, val loss 2.5564\n",
      "step 192060: train loss 2.3807, val loss 2.5309\n",
      "step 192070: train loss 2.3951, val loss 2.4534\n",
      "step 192080: train loss 2.4230, val loss 2.5327\n",
      "step 192090: train loss 2.4146, val loss 2.4684\n",
      "step 192100: train loss 2.3374, val loss 2.6058\n",
      "step 192110: train loss 2.3189, val loss 2.4491\n",
      "step 192120: train loss 2.3463, val loss 2.4374\n",
      "step 192130: train loss 2.3934, val loss 2.4021\n",
      "step 192140: train loss 2.3717, val loss 2.4884\n",
      "step 192150: train loss 2.4196, val loss 2.4798\n",
      "step 192160: train loss 2.4099, val loss 2.5536\n",
      "step 192170: train loss 2.4185, val loss 2.4806\n",
      "step 192180: train loss 2.3933, val loss 2.4108\n",
      "step 192190: train loss 2.3787, val loss 2.5539\n",
      "step 192200: train loss 2.4230, val loss 2.4065\n",
      "step 192210: train loss 2.3106, val loss 2.4713\n",
      "step 192220: train loss 2.3725, val loss 2.5311\n",
      "step 192230: train loss 2.4043, val loss 2.4478\n",
      "step 192240: train loss 2.4527, val loss 2.4177\n",
      "step 192250: train loss 2.3835, val loss 2.5107\n",
      "step 192260: train loss 2.3750, val loss 2.4662\n",
      "step 192270: train loss 2.4205, val loss 2.4194\n",
      "step 192280: train loss 2.4224, val loss 2.4546\n",
      "step 192290: train loss 2.3725, val loss 2.5794\n",
      "step 192300: train loss 2.3484, val loss 2.5138\n",
      "step 192310: train loss 2.4394, val loss 2.4893\n",
      "step 192320: train loss 2.3423, val loss 2.4434\n",
      "step 192330: train loss 2.4275, val loss 2.4915\n",
      "step 192340: train loss 2.4278, val loss 2.4508\n",
      "step 192350: train loss 2.3483, val loss 2.3909\n",
      "step 192360: train loss 2.3935, val loss 2.4484\n",
      "step 192370: train loss 2.3474, val loss 2.4402\n",
      "step 192380: train loss 2.3911, val loss 2.4908\n",
      "step 192390: train loss 2.3550, val loss 2.5335\n",
      "step 192400: train loss 2.3469, val loss 2.5176\n",
      "step 192410: train loss 2.4401, val loss 2.4416\n",
      "step 192420: train loss 2.4538, val loss 2.5543\n",
      "step 192430: train loss 2.4210, val loss 2.5463\n",
      "step 192440: train loss 2.3960, val loss 2.4858\n",
      "step 192450: train loss 2.3469, val loss 2.4179\n",
      "step 192460: train loss 2.4064, val loss 2.3920\n",
      "step 192470: train loss 2.3903, val loss 2.5497\n",
      "step 192480: train loss 2.3374, val loss 2.5270\n",
      "step 192490: train loss 2.3511, val loss 2.4261\n",
      "step 192500: train loss 2.4152, val loss 2.3751\n",
      "step 192510: train loss 2.4025, val loss 2.4871\n",
      "step 192520: train loss 2.3193, val loss 2.5688\n",
      "step 192530: train loss 2.3887, val loss 2.5259\n",
      "step 192540: train loss 2.4081, val loss 2.4714\n",
      "step 192550: train loss 2.4340, val loss 2.4333\n",
      "step 192560: train loss 2.3187, val loss 2.5016\n",
      "step 192570: train loss 2.4182, val loss 2.4307\n",
      "step 192580: train loss 2.3813, val loss 2.5086\n",
      "step 192590: train loss 2.4118, val loss 2.4862\n",
      "step 192600: train loss 2.3994, val loss 2.5040\n",
      "step 192610: train loss 2.3221, val loss 2.3841\n",
      "step 192620: train loss 2.3624, val loss 2.5426\n",
      "step 192630: train loss 2.4355, val loss 2.4513\n",
      "step 192640: train loss 2.4041, val loss 2.4961\n",
      "step 192650: train loss 2.3871, val loss 2.5025\n",
      "step 192660: train loss 2.4088, val loss 2.5056\n",
      "step 192670: train loss 2.3892, val loss 2.4922\n",
      "step 192680: train loss 2.4442, val loss 2.4823\n",
      "step 192690: train loss 2.3816, val loss 2.5512\n",
      "step 192700: train loss 2.4032, val loss 2.3791\n",
      "step 192710: train loss 2.3317, val loss 2.4483\n",
      "step 192720: train loss 2.3851, val loss 2.4328\n",
      "step 192730: train loss 2.4068, val loss 2.4317\n",
      "step 192740: train loss 2.4362, val loss 2.3822\n",
      "step 192750: train loss 2.4245, val loss 2.4356\n",
      "step 192760: train loss 2.3733, val loss 2.5096\n",
      "step 192770: train loss 2.4545, val loss 2.4876\n",
      "step 192780: train loss 2.3776, val loss 2.4644\n",
      "step 192790: train loss 2.3965, val loss 2.4468\n",
      "step 192800: train loss 2.4329, val loss 2.4443\n",
      "step 192810: train loss 2.4506, val loss 2.4227\n",
      "step 192820: train loss 2.3786, val loss 2.4526\n",
      "step 192830: train loss 2.4405, val loss 2.4970\n",
      "step 192840: train loss 2.3134, val loss 2.4558\n",
      "step 192850: train loss 2.3736, val loss 2.4250\n",
      "step 192860: train loss 2.4003, val loss 2.4181\n",
      "step 192870: train loss 2.3918, val loss 2.4770\n",
      "step 192880: train loss 2.3757, val loss 2.4728\n",
      "step 192890: train loss 2.4186, val loss 2.4931\n",
      "step 192900: train loss 2.4192, val loss 2.5084\n",
      "step 192910: train loss 2.4064, val loss 2.4864\n",
      "step 192920: train loss 2.4595, val loss 2.6252\n",
      "step 192930: train loss 2.3672, val loss 2.5151\n",
      "step 192940: train loss 2.4261, val loss 2.4316\n",
      "step 192950: train loss 2.4042, val loss 2.5019\n",
      "step 192960: train loss 2.3918, val loss 2.4861\n",
      "step 192970: train loss 2.4832, val loss 2.4133\n",
      "step 192980: train loss 2.3488, val loss 2.4984\n",
      "step 192990: train loss 2.3702, val loss 2.3317\n",
      "step 193000: train loss 2.3847, val loss 2.3310\n",
      "Generated text at iteration 193000\n",
      "\n",
      "\n",
      "UN2Bieste Canoréoison,\n",
      "\n",
      "M][X2[02Âquanis unerte he deces, de bidione i donsos-traileuretet-bés r.. n\n",
      "step 193010: train loss 2.3409, val loss 2.4510\n",
      "step 193020: train loss 2.3329, val loss 2.3427\n",
      "step 193030: train loss 2.4056, val loss 2.5048\n",
      "step 193040: train loss 2.4103, val loss 2.5315\n",
      "step 193050: train loss 2.4157, val loss 2.4460\n",
      "step 193060: train loss 2.4354, val loss 2.5126\n",
      "step 193070: train loss 2.3584, val loss 2.3895\n",
      "step 193080: train loss 2.4140, val loss 2.7424\n",
      "step 193090: train loss 2.4095, val loss 2.4545\n",
      "step 193100: train loss 2.3333, val loss 2.5305\n",
      "step 193110: train loss 2.4016, val loss 2.4839\n",
      "step 193120: train loss 2.3880, val loss 2.4981\n",
      "step 193130: train loss 2.4559, val loss 2.5325\n",
      "step 193140: train loss 2.3676, val loss 2.4925\n",
      "step 193150: train loss 2.4464, val loss 2.5045\n",
      "step 193160: train loss 2.4206, val loss 2.4768\n",
      "step 193170: train loss 2.2790, val loss 2.4033\n",
      "step 193180: train loss 2.4169, val loss 2.4090\n",
      "step 193190: train loss 2.3423, val loss 2.4556\n",
      "step 193200: train loss 2.4269, val loss 2.4395\n",
      "step 193210: train loss 2.3739, val loss 2.4676\n",
      "step 193220: train loss 2.3830, val loss 2.4532\n",
      "step 193230: train loss 2.3622, val loss 2.4963\n",
      "step 193240: train loss 2.3772, val loss 2.3919\n",
      "step 193250: train loss 2.4190, val loss 2.5021\n",
      "step 193260: train loss 2.3161, val loss 2.4679\n",
      "step 193270: train loss 2.3905, val loss 2.5388\n",
      "step 193280: train loss 2.3919, val loss 2.4437\n",
      "step 193290: train loss 2.3575, val loss 2.5943\n",
      "step 193300: train loss 2.3218, val loss 2.5550\n",
      "step 193310: train loss 2.4139, val loss 2.4138\n",
      "step 193320: train loss 2.4188, val loss 2.4827\n",
      "step 193330: train loss 2.3402, val loss 2.4487\n",
      "step 193340: train loss 2.4139, val loss 2.4611\n",
      "step 193350: train loss 2.4009, val loss 2.5599\n",
      "step 193360: train loss 2.3705, val loss 2.5028\n",
      "step 193370: train loss 2.4259, val loss 2.4652\n",
      "step 193380: train loss 2.3240, val loss 2.4417\n",
      "step 193390: train loss 2.4125, val loss 2.5261\n",
      "step 193400: train loss 2.3221, val loss 2.4483\n",
      "step 193410: train loss 2.4286, val loss 2.4616\n",
      "step 193420: train loss 2.3697, val loss 2.3983\n",
      "step 193430: train loss 2.3790, val loss 2.5045\n",
      "step 193440: train loss 2.4238, val loss 2.3540\n",
      "step 193450: train loss 2.3691, val loss 2.4418\n",
      "step 193460: train loss 2.4693, val loss 2.4518\n",
      "step 193470: train loss 2.3670, val loss 2.5327\n",
      "step 193480: train loss 2.4391, val loss 2.5213\n",
      "step 193490: train loss 2.3514, val loss 2.6919\n",
      "step 193500: train loss 2.3660, val loss 2.5368\n",
      "step 193510: train loss 2.4364, val loss 2.3504\n",
      "step 193520: train loss 2.3892, val loss 2.4464\n",
      "step 193530: train loss 2.3852, val loss 2.4370\n",
      "step 193540: train loss 2.4577, val loss 2.4392\n",
      "step 193550: train loss 2.3701, val loss 2.6400\n",
      "step 193560: train loss 2.4168, val loss 2.5491\n",
      "step 193570: train loss 2.3758, val loss 2.4612\n",
      "step 193580: train loss 2.3198, val loss 2.4550\n",
      "step 193590: train loss 2.4503, val loss 2.4959\n",
      "step 193600: train loss 2.4609, val loss 2.4843\n",
      "step 193610: train loss 2.4401, val loss 2.3907\n",
      "step 193620: train loss 2.3588, val loss 2.4489\n",
      "step 193630: train loss 2.4131, val loss 2.5361\n",
      "step 193640: train loss 2.4369, val loss 2.3989\n",
      "step 193650: train loss 2.4425, val loss 2.4927\n",
      "step 193660: train loss 2.3223, val loss 2.4392\n",
      "step 193670: train loss 2.4199, val loss 2.4550\n",
      "step 193680: train loss 2.2657, val loss 2.4849\n",
      "step 193690: train loss 2.3332, val loss 2.3538\n",
      "step 193700: train loss 2.4220, val loss 2.5415\n",
      "step 193710: train loss 2.3949, val loss 2.4963\n",
      "step 193720: train loss 2.4774, val loss 2.4421\n",
      "step 193730: train loss 2.3602, val loss 2.5205\n",
      "step 193740: train loss 2.3852, val loss 2.4161\n",
      "step 193750: train loss 2.3106, val loss 2.5061\n",
      "step 193760: train loss 2.3644, val loss 2.3949\n",
      "step 193770: train loss 2.3473, val loss 2.5444\n",
      "step 193780: train loss 2.4354, val loss 2.3656\n",
      "step 193790: train loss 2.3902, val loss 2.4737\n",
      "step 193800: train loss 2.3613, val loss 2.5550\n",
      "step 193810: train loss 2.3912, val loss 2.5125\n",
      "step 193820: train loss 2.4022, val loss 2.4886\n",
      "step 193830: train loss 2.4015, val loss 2.4727\n",
      "step 193840: train loss 2.4103, val loss 2.4781\n",
      "step 193850: train loss 2.3420, val loss 2.5362\n",
      "step 193860: train loss 2.4024, val loss 2.3795\n",
      "step 193870: train loss 2.3329, val loss 2.5084\n",
      "step 193880: train loss 2.4318, val loss 2.4370\n",
      "step 193890: train loss 2.3577, val loss 2.4307\n",
      "step 193900: train loss 2.4035, val loss 2.5307\n",
      "step 193910: train loss 2.3864, val loss 2.4527\n",
      "step 193920: train loss 2.4062, val loss 2.3751\n",
      "step 193930: train loss 2.4375, val loss 2.4921\n",
      "step 193940: train loss 2.4225, val loss 2.4691\n",
      "step 193950: train loss 2.3791, val loss 2.5065\n",
      "step 193960: train loss 2.4178, val loss 2.4729\n",
      "step 193970: train loss 2.4901, val loss 2.4633\n",
      "step 193980: train loss 2.3623, val loss 2.4484\n",
      "step 193990: train loss 2.3720, val loss 2.3570\n",
      "step 194000: train loss 2.4073, val loss 2.6081\n",
      "Generated text at iteration 194000\n",
      "\n",
      "L'éparfau ten,\n",
      "DRSïFhalu s  lu'e; ieuc  vimmu\n",
      "Pére a de lachau  des\n",
      "Etet-\n",
      " ffes rt  itaureuie,\n",
      "Lan.\n",
      "\n",
      "step 194010: train loss 2.4531, val loss 2.4183\n",
      "step 194020: train loss 2.3742, val loss 2.4980\n",
      "step 194030: train loss 2.4636, val loss 2.4668\n",
      "step 194040: train loss 2.3954, val loss 2.4958\n",
      "step 194050: train loss 2.3987, val loss 2.5266\n",
      "step 194060: train loss 2.3980, val loss 2.4485\n",
      "step 194070: train loss 2.4574, val loss 2.4167\n",
      "step 194080: train loss 2.4404, val loss 2.3910\n",
      "step 194090: train loss 2.3564, val loss 2.4925\n",
      "step 194100: train loss 2.4064, val loss 2.5207\n",
      "step 194110: train loss 2.3711, val loss 2.4005\n",
      "step 194120: train loss 2.4190, val loss 2.4328\n",
      "step 194130: train loss 2.4064, val loss 2.4783\n",
      "step 194140: train loss 2.3201, val loss 2.4738\n",
      "step 194150: train loss 2.5171, val loss 2.5360\n",
      "step 194160: train loss 2.3797, val loss 2.4169\n",
      "step 194170: train loss 2.5101, val loss 2.5736\n",
      "step 194180: train loss 2.4370, val loss 2.4005\n",
      "step 194190: train loss 2.3775, val loss 2.4732\n",
      "step 194200: train loss 2.4385, val loss 2.5237\n",
      "step 194210: train loss 2.3203, val loss 2.5353\n",
      "step 194220: train loss 2.3715, val loss 2.5301\n",
      "step 194230: train loss 2.3929, val loss 2.5105\n",
      "step 194240: train loss 2.3282, val loss 2.5465\n",
      "step 194250: train loss 2.3239, val loss 2.4593\n",
      "step 194260: train loss 2.3573, val loss 2.4338\n",
      "step 194270: train loss 2.4533, val loss 2.4130\n",
      "step 194280: train loss 2.3470, val loss 2.3635\n",
      "step 194290: train loss 2.4566, val loss 2.4801\n",
      "step 194300: train loss 2.3181, val loss 2.5022\n",
      "step 194310: train loss 2.3403, val loss 2.4403\n",
      "step 194320: train loss 2.3743, val loss 2.5520\n",
      "step 194330: train loss 2.4476, val loss 2.4690\n",
      "step 194340: train loss 2.4077, val loss 2.5474\n",
      "step 194350: train loss 2.3810, val loss 2.6538\n",
      "step 194360: train loss 2.3586, val loss 2.4722\n",
      "step 194370: train loss 2.4024, val loss 2.5604\n",
      "step 194380: train loss 2.4251, val loss 2.5272\n",
      "step 194390: train loss 2.3980, val loss 2.4914\n",
      "step 194400: train loss 2.4080, val loss 2.5151\n",
      "step 194410: train loss 2.4066, val loss 2.4521\n",
      "step 194420: train loss 2.3538, val loss 2.5055\n",
      "step 194430: train loss 2.4880, val loss 2.4807\n",
      "step 194440: train loss 2.3415, val loss 2.4374\n",
      "step 194450: train loss 2.3157, val loss 2.5288\n",
      "step 194460: train loss 2.3731, val loss 2.4177\n",
      "step 194470: train loss 2.3922, val loss 2.4121\n",
      "step 194480: train loss 2.4044, val loss 2.4706\n",
      "step 194490: train loss 2.4148, val loss 2.5524\n",
      "step 194500: train loss 2.3873, val loss 2.4951\n",
      "step 194510: train loss 2.4047, val loss 2.4030\n",
      "step 194520: train loss 2.4550, val loss 2.5271\n",
      "step 194530: train loss 2.3626, val loss 2.5729\n",
      "step 194540: train loss 2.3339, val loss 2.4790\n",
      "step 194550: train loss 2.3209, val loss 2.4575\n",
      "step 194560: train loss 2.3908, val loss 2.4467\n",
      "step 194570: train loss 2.4388, val loss 2.4578\n",
      "step 194580: train loss 2.4296, val loss 2.3918\n",
      "step 194590: train loss 2.4162, val loss 2.5630\n",
      "step 194600: train loss 2.4357, val loss 2.5531\n",
      "step 194610: train loss 2.3893, val loss 2.3864\n",
      "step 194620: train loss 2.3475, val loss 2.5769\n",
      "step 194630: train loss 2.3791, val loss 2.5341\n",
      "step 194640: train loss 2.4727, val loss 2.4673\n",
      "step 194650: train loss 2.3979, val loss 2.4270\n",
      "step 194660: train loss 2.3805, val loss 2.5545\n",
      "step 194670: train loss 2.5144, val loss 2.4609\n",
      "step 194680: train loss 2.4596, val loss 2.6220\n",
      "step 194690: train loss 2.3782, val loss 2.5731\n",
      "step 194700: train loss 2.3273, val loss 2.4130\n",
      "step 194710: train loss 2.3983, val loss 2.5698\n",
      "step 194720: train loss 2.4233, val loss 2.5039\n",
      "step 194730: train loss 2.4347, val loss 2.4442\n",
      "step 194740: train loss 2.4317, val loss 2.5333\n",
      "step 194750: train loss 2.4162, val loss 2.4758\n",
      "step 194760: train loss 2.5672, val loss 2.4434\n",
      "step 194770: train loss 2.3523, val loss 2.4641\n",
      "step 194780: train loss 2.4090, val loss 2.5454\n",
      "step 194790: train loss 2.4288, val loss 2.3634\n",
      "step 194800: train loss 2.4391, val loss 2.5686\n",
      "step 194810: train loss 2.3687, val loss 2.5212\n",
      "step 194820: train loss 2.3071, val loss 2.4061\n",
      "step 194830: train loss 2.3827, val loss 2.4372\n",
      "step 194840: train loss 2.4508, val loss 2.4776\n",
      "step 194850: train loss 2.4667, val loss 2.4499\n",
      "step 194860: train loss 2.4453, val loss 2.4873\n",
      "step 194870: train loss 2.3487, val loss 2.5090\n",
      "step 194880: train loss 2.4385, val loss 2.5306\n",
      "step 194890: train loss 2.3950, val loss 2.4830\n",
      "step 194900: train loss 2.4320, val loss 2.5764\n",
      "step 194910: train loss 2.4077, val loss 2.4334\n",
      "step 194920: train loss 2.4810, val loss 2.4741\n",
      "step 194930: train loss 2.3142, val loss 2.4938\n",
      "step 194940: train loss 2.3881, val loss 2.4058\n",
      "step 194950: train loss 2.3925, val loss 2.4956\n",
      "step 194960: train loss 2.4281, val loss 2.4461\n",
      "step 194970: train loss 2.3685, val loss 2.4943\n",
      "step 194980: train loss 2.3558, val loss 2.5118\n",
      "step 194990: train loss 2.4006, val loss 2.5179\n",
      "step 195000: train loss 2.3687, val loss 2.5071\n",
      "Generated text at iteration 195000\n",
      "\n",
      "Par  la saroi.\n",
      "\n",
      "\n",
      "Etuins d.\n",
      "ITosoëys, s, ve parireau cisiens cles sce ut l-\n",
      "Deuss llederéce le; jeres\n",
      "step 195010: train loss 2.4274, val loss 2.4606\n",
      "step 195020: train loss 2.3817, val loss 2.4638\n",
      "step 195030: train loss 2.4015, val loss 2.5114\n",
      "step 195040: train loss 2.3580, val loss 2.3889\n",
      "step 195050: train loss 2.4031, val loss 2.4419\n",
      "step 195060: train loss 2.3880, val loss 2.5019\n",
      "step 195070: train loss 2.3953, val loss 2.4345\n",
      "step 195080: train loss 2.3984, val loss 2.4666\n",
      "step 195090: train loss 2.3914, val loss 2.4782\n",
      "step 195100: train loss 2.3842, val loss 2.4549\n",
      "step 195110: train loss 2.3949, val loss 2.4787\n",
      "step 195120: train loss 2.3378, val loss 2.5967\n",
      "step 195130: train loss 2.3604, val loss 2.5646\n",
      "step 195140: train loss 2.3823, val loss 2.4609\n",
      "step 195150: train loss 2.3915, val loss 2.4189\n",
      "step 195160: train loss 2.3941, val loss 2.4471\n",
      "step 195170: train loss 2.4423, val loss 2.4626\n",
      "step 195180: train loss 2.3687, val loss 2.5019\n",
      "step 195190: train loss 2.3863, val loss 2.5697\n",
      "step 195200: train loss 2.3842, val loss 2.5509\n",
      "step 195210: train loss 2.3804, val loss 2.5486\n",
      "step 195220: train loss 2.3643, val loss 2.4327\n",
      "step 195230: train loss 2.3596, val loss 2.4271\n",
      "step 195240: train loss 2.3602, val loss 2.4328\n",
      "step 195250: train loss 2.3316, val loss 2.4547\n",
      "step 195260: train loss 2.3700, val loss 2.4970\n",
      "step 195270: train loss 2.4308, val loss 2.4842\n",
      "step 195280: train loss 2.4000, val loss 2.5415\n",
      "step 195290: train loss 2.4300, val loss 2.5538\n",
      "step 195300: train loss 2.4441, val loss 2.3735\n",
      "step 195310: train loss 2.3934, val loss 2.5582\n",
      "step 195320: train loss 2.3608, val loss 2.4828\n",
      "step 195330: train loss 2.3820, val loss 2.5319\n",
      "step 195340: train loss 2.4012, val loss 2.3948\n",
      "step 195350: train loss 2.4211, val loss 2.4387\n",
      "step 195360: train loss 2.3614, val loss 2.5696\n",
      "step 195370: train loss 2.4285, val loss 2.3702\n",
      "step 195380: train loss 2.2714, val loss 2.5051\n",
      "step 195390: train loss 2.4001, val loss 2.5258\n",
      "step 195400: train loss 2.3526, val loss 2.5010\n",
      "step 195410: train loss 2.4126, val loss 2.4901\n",
      "step 195420: train loss 2.3669, val loss 2.4368\n",
      "step 195430: train loss 2.3683, val loss 2.4533\n",
      "step 195440: train loss 2.4363, val loss 2.5685\n",
      "step 195450: train loss 2.4553, val loss 2.5605\n",
      "step 195460: train loss 2.4035, val loss 2.4214\n",
      "step 195470: train loss 2.4816, val loss 2.5153\n",
      "step 195480: train loss 2.4163, val loss 2.3777\n",
      "step 195490: train loss 2.4072, val loss 2.5174\n",
      "step 195500: train loss 2.4109, val loss 2.5179\n",
      "step 195510: train loss 2.4241, val loss 2.3736\n",
      "step 195520: train loss 2.4347, val loss 2.4616\n",
      "step 195530: train loss 2.3972, val loss 2.5241\n",
      "step 195540: train loss 2.3552, val loss 2.4748\n",
      "step 195550: train loss 2.3570, val loss 2.5066\n",
      "step 195560: train loss 2.3519, val loss 2.5307\n",
      "step 195570: train loss 2.3953, val loss 2.4058\n",
      "step 195580: train loss 2.3805, val loss 2.4530\n",
      "step 195590: train loss 2.4268, val loss 2.4730\n",
      "step 195600: train loss 2.3618, val loss 2.4350\n",
      "step 195610: train loss 2.4294, val loss 2.4088\n",
      "step 195620: train loss 2.3947, val loss 2.4510\n",
      "step 195630: train loss 2.4590, val loss 2.5831\n",
      "step 195640: train loss 2.3882, val loss 2.4796\n",
      "step 195650: train loss 2.3597, val loss 2.4398\n",
      "step 195660: train loss 2.3418, val loss 2.4452\n",
      "step 195670: train loss 2.4416, val loss 2.5742\n",
      "step 195680: train loss 2.3781, val loss 2.4614\n",
      "step 195690: train loss 2.4040, val loss 2.5297\n",
      "step 195700: train loss 2.3819, val loss 2.4945\n",
      "step 195710: train loss 2.4755, val loss 2.5145\n",
      "step 195720: train loss 2.4211, val loss 2.5381\n",
      "step 195730: train loss 2.3937, val loss 2.5045\n",
      "step 195740: train loss 2.3520, val loss 2.4532\n",
      "step 195750: train loss 2.3979, val loss 2.4329\n",
      "step 195760: train loss 2.3773, val loss 2.4298\n",
      "step 195770: train loss 2.3706, val loss 2.4667\n",
      "step 195780: train loss 2.3935, val loss 2.4064\n",
      "step 195790: train loss 2.3344, val loss 2.4798\n",
      "step 195800: train loss 2.4899, val loss 2.4055\n",
      "step 195810: train loss 2.3880, val loss 2.4315\n",
      "step 195820: train loss 2.3522, val loss 2.5026\n",
      "step 195830: train loss 2.3589, val loss 2.4613\n",
      "step 195840: train loss 2.4352, val loss 2.4733\n",
      "step 195850: train loss 2.3407, val loss 2.5171\n",
      "step 195860: train loss 2.3199, val loss 2.3212\n",
      "step 195870: train loss 2.4127, val loss 2.5237\n",
      "step 195880: train loss 2.3611, val loss 2.5154\n",
      "step 195890: train loss 2.4298, val loss 2.3711\n",
      "step 195900: train loss 2.3774, val loss 2.4204\n",
      "step 195910: train loss 2.3291, val loss 2.5231\n",
      "step 195920: train loss 2.4573, val loss 2.4245\n",
      "step 195930: train loss 2.4143, val loss 2.4844\n",
      "step 195940: train loss 2.3705, val loss 2.5077\n",
      "step 195950: train loss 2.4132, val loss 2.5611\n",
      "step 195960: train loss 2.3667, val loss 2.6072\n",
      "step 195970: train loss 2.3638, val loss 2.4323\n",
      "step 195980: train loss 2.3940, val loss 2.5188\n",
      "step 195990: train loss 2.3511, val loss 2.5313\n",
      "step 196000: train loss 2.3690, val loss 2.5888\n",
      "Generated text at iteration 196000\n",
      "\n",
      "Maibe aiture ccui,»-cuve Tuvomous pour.\n",
      "J'iless les luesurinstrons  SErimol'omen prmbrant t noume!\n",
      "\n",
      "\n",
      "step 196010: train loss 2.4078, val loss 2.5081\n",
      "step 196020: train loss 2.4055, val loss 2.4478\n",
      "step 196030: train loss 2.3921, val loss 2.4213\n",
      "step 196040: train loss 2.4024, val loss 2.3938\n",
      "step 196050: train loss 2.3653, val loss 2.4171\n",
      "step 196060: train loss 2.4280, val loss 2.5900\n",
      "step 196070: train loss 2.3603, val loss 2.5416\n",
      "step 196080: train loss 2.3956, val loss 2.4449\n",
      "step 196090: train loss 2.3985, val loss 2.4345\n",
      "step 196100: train loss 2.3198, val loss 2.6178\n",
      "step 196110: train loss 2.2966, val loss 2.5281\n",
      "step 196120: train loss 2.4360, val loss 2.3744\n",
      "step 196130: train loss 2.4215, val loss 2.4824\n",
      "step 196140: train loss 2.3978, val loss 2.5932\n",
      "step 196150: train loss 2.4389, val loss 2.4828\n",
      "step 196160: train loss 2.4615, val loss 2.4651\n",
      "step 196170: train loss 2.3793, val loss 2.5336\n",
      "step 196180: train loss 2.4306, val loss 2.4282\n",
      "step 196190: train loss 2.3982, val loss 2.5607\n",
      "step 196200: train loss 2.3702, val loss 2.3557\n",
      "step 196210: train loss 2.4596, val loss 2.4594\n",
      "step 196220: train loss 2.3123, val loss 2.3918\n",
      "step 196230: train loss 2.3886, val loss 2.4789\n",
      "step 196240: train loss 2.3745, val loss 2.5397\n",
      "step 196250: train loss 2.3979, val loss 2.4823\n",
      "step 196260: train loss 2.3584, val loss 2.4942\n",
      "step 196270: train loss 2.4335, val loss 2.3836\n",
      "step 196280: train loss 2.4215, val loss 2.4510\n",
      "step 196290: train loss 2.4206, val loss 2.4135\n",
      "step 196300: train loss 2.3764, val loss 2.3474\n",
      "step 196310: train loss 2.4102, val loss 2.3829\n",
      "step 196320: train loss 2.4336, val loss 2.4490\n",
      "step 196330: train loss 2.3775, val loss 2.5032\n",
      "step 196340: train loss 2.3765, val loss 2.5134\n",
      "step 196350: train loss 2.4676, val loss 2.4496\n",
      "step 196360: train loss 2.3687, val loss 2.5234\n",
      "step 196370: train loss 2.5000, val loss 2.4541\n",
      "step 196380: train loss 2.4222, val loss 2.6403\n",
      "step 196390: train loss 2.2887, val loss 2.4874\n",
      "step 196400: train loss 2.4207, val loss 2.3797\n",
      "step 196410: train loss 2.3632, val loss 2.5359\n",
      "step 196420: train loss 2.4215, val loss 2.5507\n",
      "step 196430: train loss 2.3572, val loss 2.4801\n",
      "step 196440: train loss 2.3943, val loss 2.4502\n",
      "step 196450: train loss 2.4231, val loss 2.5222\n",
      "step 196460: train loss 2.3906, val loss 2.3982\n",
      "step 196470: train loss 2.4232, val loss 2.4488\n",
      "step 196480: train loss 2.4027, val loss 2.4182\n",
      "step 196490: train loss 2.4062, val loss 2.4767\n",
      "step 196500: train loss 2.4231, val loss 2.4626\n",
      "step 196510: train loss 2.3579, val loss 2.4519\n",
      "step 196520: train loss 2.3223, val loss 2.5282\n",
      "step 196530: train loss 2.3995, val loss 2.4952\n",
      "step 196540: train loss 2.3647, val loss 2.5719\n",
      "step 196550: train loss 2.3795, val loss 2.4761\n",
      "step 196560: train loss 2.3586, val loss 2.3916\n",
      "step 196570: train loss 2.4233, val loss 2.4500\n",
      "step 196580: train loss 2.4270, val loss 2.4310\n",
      "step 196590: train loss 2.4473, val loss 2.4578\n",
      "step 196600: train loss 2.4693, val loss 2.4731\n",
      "step 196610: train loss 2.3837, val loss 2.6820\n",
      "step 196620: train loss 2.3964, val loss 2.4618\n",
      "step 196630: train loss 2.4910, val loss 2.3893\n",
      "step 196640: train loss 2.4318, val loss 2.5370\n",
      "step 196650: train loss 2.3809, val loss 2.2995\n",
      "step 196660: train loss 2.3529, val loss 2.4380\n",
      "step 196670: train loss 2.3245, val loss 2.4926\n",
      "step 196680: train loss 2.3668, val loss 2.4965\n",
      "step 196690: train loss 2.3867, val loss 2.4358\n",
      "step 196700: train loss 2.4505, val loss 2.4759\n",
      "step 196710: train loss 2.4673, val loss 2.4012\n",
      "step 196720: train loss 2.3447, val loss 2.4447\n",
      "step 196730: train loss 2.4878, val loss 2.3669\n",
      "step 196740: train loss 2.3994, val loss 2.4458\n",
      "step 196750: train loss 2.4379, val loss 2.5210\n",
      "step 196760: train loss 2.3756, val loss 2.5593\n",
      "step 196770: train loss 2.4295, val loss 2.4591\n",
      "step 196780: train loss 2.4288, val loss 2.4203\n",
      "step 196790: train loss 2.4473, val loss 2.5174\n",
      "step 196800: train loss 2.4498, val loss 2.4516\n",
      "step 196810: train loss 2.4359, val loss 2.5764\n",
      "step 196820: train loss 2.4643, val loss 2.4799\n",
      "step 196830: train loss 2.4347, val loss 2.5343\n",
      "step 196840: train loss 2.3481, val loss 2.4344\n",
      "step 196850: train loss 2.4878, val loss 2.3977\n",
      "step 196860: train loss 2.3908, val loss 2.4607\n",
      "step 196870: train loss 2.4230, val loss 2.4039\n",
      "step 196880: train loss 2.4535, val loss 2.4881\n",
      "step 196890: train loss 2.3904, val loss 2.4884\n",
      "step 196900: train loss 2.3698, val loss 2.4531\n",
      "step 196910: train loss 2.3044, val loss 2.4313\n",
      "step 196920: train loss 2.4337, val loss 2.4278\n",
      "step 196930: train loss 2.3885, val loss 2.5965\n",
      "step 196940: train loss 2.4124, val loss 2.5844\n",
      "step 196950: train loss 2.4348, val loss 2.4008\n",
      "step 196960: train loss 2.3853, val loss 2.4984\n",
      "step 196970: train loss 2.4722, val loss 2.5108\n",
      "step 196980: train loss 2.4172, val loss 2.5143\n",
      "step 196990: train loss 2.4180, val loss 2.5120\n",
      "step 197000: train loss 2.3468, val loss 2.4687\n",
      "Generated text at iteration 197000\n",
      "\n",
      "Cocons ndest,\n",
      "Tie greux é?\n",
      "Fisammit mpoue foux, las sess, jendrbestupomuë\n",
      "Phes u lanoi:à péme ve t d\n",
      "step 197010: train loss 2.4600, val loss 2.4864\n",
      "step 197020: train loss 2.3945, val loss 2.5832\n",
      "step 197030: train loss 2.4353, val loss 2.5087\n",
      "step 197040: train loss 2.3791, val loss 2.3855\n",
      "step 197050: train loss 2.4662, val loss 2.4890\n",
      "step 197060: train loss 2.3765, val loss 2.4564\n",
      "step 197070: train loss 2.3286, val loss 2.4934\n",
      "step 197080: train loss 2.4298, val loss 2.5666\n",
      "step 197090: train loss 2.3755, val loss 2.3933\n",
      "step 197100: train loss 2.4138, val loss 2.5734\n",
      "step 197110: train loss 2.4947, val loss 2.4688\n",
      "step 197120: train loss 2.4994, val loss 2.4784\n",
      "step 197130: train loss 2.3872, val loss 2.5381\n",
      "step 197140: train loss 2.3954, val loss 2.4439\n",
      "step 197150: train loss 2.4036, val loss 2.4526\n",
      "step 197160: train loss 2.4049, val loss 2.5583\n",
      "step 197170: train loss 2.4372, val loss 2.4741\n",
      "step 197180: train loss 2.4411, val loss 2.5803\n",
      "step 197190: train loss 2.3379, val loss 2.5321\n",
      "step 197200: train loss 2.4329, val loss 2.4817\n",
      "step 197210: train loss 2.3542, val loss 2.4820\n",
      "step 197220: train loss 2.4733, val loss 2.4003\n",
      "step 197230: train loss 2.4002, val loss 2.3799\n",
      "step 197240: train loss 2.4528, val loss 2.5672\n",
      "step 197250: train loss 2.4815, val loss 2.4665\n",
      "step 197260: train loss 2.4132, val loss 2.4759\n",
      "step 197270: train loss 2.3737, val loss 2.4746\n",
      "step 197280: train loss 2.3790, val loss 2.5322\n",
      "step 197290: train loss 2.4162, val loss 2.4122\n",
      "step 197300: train loss 2.4536, val loss 2.5527\n",
      "step 197310: train loss 2.3465, val loss 2.4508\n",
      "step 197320: train loss 2.4018, val loss 2.4951\n",
      "step 197330: train loss 2.3587, val loss 2.3953\n",
      "step 197340: train loss 2.4111, val loss 2.5353\n",
      "step 197350: train loss 2.4508, val loss 2.4702\n",
      "step 197360: train loss 2.3694, val loss 2.4505\n",
      "step 197370: train loss 2.4213, val loss 2.4764\n",
      "step 197380: train loss 2.3850, val loss 2.4458\n",
      "step 197390: train loss 2.3485, val loss 2.4235\n",
      "step 197400: train loss 2.4463, val loss 2.4750\n",
      "step 197410: train loss 2.3375, val loss 2.5172\n",
      "step 197420: train loss 2.4009, val loss 2.4507\n",
      "step 197430: train loss 2.3528, val loss 2.4553\n",
      "step 197440: train loss 2.3811, val loss 2.4596\n",
      "step 197450: train loss 2.3979, val loss 2.4843\n",
      "step 197460: train loss 2.3664, val loss 2.4646\n",
      "step 197470: train loss 2.4183, val loss 2.6134\n",
      "step 197480: train loss 2.4430, val loss 2.4786\n",
      "step 197490: train loss 2.4156, val loss 2.4766\n",
      "step 197500: train loss 2.3535, val loss 2.5336\n",
      "step 197510: train loss 2.4456, val loss 2.4283\n",
      "step 197520: train loss 2.4427, val loss 2.4511\n",
      "step 197530: train loss 2.4379, val loss 2.3892\n",
      "step 197540: train loss 2.3805, val loss 2.4399\n",
      "step 197550: train loss 2.4612, val loss 2.3995\n",
      "step 197560: train loss 2.3213, val loss 2.5433\n",
      "step 197570: train loss 2.3284, val loss 2.4907\n",
      "step 197580: train loss 2.3623, val loss 2.4220\n",
      "step 197590: train loss 2.4413, val loss 2.5369\n",
      "step 197600: train loss 2.3693, val loss 2.5793\n",
      "step 197610: train loss 2.3663, val loss 2.4526\n",
      "step 197620: train loss 2.4589, val loss 2.4774\n",
      "step 197630: train loss 2.4087, val loss 2.5747\n",
      "step 197640: train loss 2.4176, val loss 2.4817\n",
      "step 197650: train loss 2.4103, val loss 2.5314\n",
      "step 197660: train loss 2.3903, val loss 2.4793\n",
      "step 197670: train loss 2.3352, val loss 2.5221\n",
      "step 197680: train loss 2.4159, val loss 2.4380\n",
      "step 197690: train loss 2.3629, val loss 2.5021\n",
      "step 197700: train loss 2.3829, val loss 2.5155\n",
      "step 197710: train loss 2.3484, val loss 2.3947\n",
      "step 197720: train loss 2.3657, val loss 2.5708\n",
      "step 197730: train loss 2.4451, val loss 2.4287\n",
      "step 197740: train loss 2.4449, val loss 2.5550\n",
      "step 197750: train loss 2.4418, val loss 2.3320\n",
      "step 197760: train loss 2.3318, val loss 2.3751\n",
      "step 197770: train loss 2.4105, val loss 2.5317\n",
      "step 197780: train loss 2.4209, val loss 2.4527\n",
      "step 197790: train loss 2.3190, val loss 2.5012\n",
      "step 197800: train loss 2.4509, val loss 2.4218\n",
      "step 197810: train loss 2.3956, val loss 2.5589\n",
      "step 197820: train loss 2.4199, val loss 2.4776\n",
      "step 197830: train loss 2.4053, val loss 2.4543\n",
      "step 197840: train loss 2.3162, val loss 2.4536\n",
      "step 197850: train loss 2.3972, val loss 2.5212\n",
      "step 197860: train loss 2.4359, val loss 2.3907\n",
      "step 197870: train loss 2.3898, val loss 2.4786\n",
      "step 197880: train loss 2.4062, val loss 2.5082\n",
      "step 197890: train loss 2.3776, val loss 2.4488\n",
      "step 197900: train loss 2.3022, val loss 2.5764\n",
      "step 197910: train loss 2.4504, val loss 2.5024\n",
      "step 197920: train loss 2.4157, val loss 2.5291\n",
      "step 197930: train loss 2.3451, val loss 2.4395\n",
      "step 197940: train loss 2.3750, val loss 2.5724\n",
      "step 197950: train loss 2.3064, val loss 2.5199\n",
      "step 197960: train loss 2.4193, val loss 2.4950\n",
      "step 197970: train loss 2.4341, val loss 2.4255\n",
      "step 197980: train loss 2.3492, val loss 2.4968\n",
      "step 197990: train loss 2.3217, val loss 2.4406\n",
      "step 198000: train loss 2.3863, val loss 2.4396\n",
      "Generated text at iteration 198000\n",
      "\n",
      "De  savants\n",
      "Mêtoma veris he!\n",
      "L'OUEntinn ux.\n",
      "U4von géaièt vone s Dins ve d'iroprere le mprt nt ques s\n",
      "step 198010: train loss 2.3678, val loss 2.4726\n",
      "step 198020: train loss 2.4033, val loss 2.4429\n",
      "step 198030: train loss 2.3872, val loss 2.3102\n",
      "step 198040: train loss 2.3593, val loss 2.4215\n",
      "step 198050: train loss 2.3839, val loss 2.4619\n",
      "step 198060: train loss 2.4003, val loss 2.5677\n",
      "step 198070: train loss 2.3511, val loss 2.4072\n",
      "step 198080: train loss 2.3074, val loss 2.4932\n",
      "step 198090: train loss 2.3416, val loss 2.4060\n",
      "step 198100: train loss 2.4191, val loss 2.4949\n",
      "step 198110: train loss 2.3206, val loss 2.4362\n",
      "step 198120: train loss 2.3993, val loss 2.3815\n",
      "step 198130: train loss 2.3385, val loss 2.4879\n",
      "step 198140: train loss 2.4426, val loss 2.4780\n",
      "step 198150: train loss 2.3335, val loss 2.4808\n",
      "step 198160: train loss 2.4194, val loss 2.4624\n",
      "step 198170: train loss 2.3293, val loss 2.4326\n",
      "step 198180: train loss 2.4377, val loss 2.4365\n",
      "step 198190: train loss 2.3772, val loss 2.5210\n",
      "step 198200: train loss 2.3917, val loss 2.4554\n",
      "step 198210: train loss 2.3826, val loss 2.4301\n",
      "step 198220: train loss 2.4228, val loss 2.4101\n",
      "step 198230: train loss 2.4458, val loss 2.5014\n",
      "step 198240: train loss 2.4234, val loss 2.5388\n",
      "step 198250: train loss 2.3854, val loss 2.5294\n",
      "step 198260: train loss 2.4222, val loss 2.4584\n",
      "step 198270: train loss 2.4164, val loss 2.6646\n",
      "step 198280: train loss 2.3735, val loss 2.4683\n",
      "step 198290: train loss 2.3783, val loss 2.4183\n",
      "step 198300: train loss 2.5137, val loss 2.5238\n",
      "step 198310: train loss 2.3474, val loss 2.5671\n",
      "step 198320: train loss 2.4179, val loss 2.4515\n",
      "step 198330: train loss 2.3373, val loss 2.4138\n",
      "step 198340: train loss 2.4097, val loss 2.4725\n",
      "step 198350: train loss 2.4050, val loss 2.6097\n",
      "step 198360: train loss 2.3393, val loss 2.5101\n",
      "step 198370: train loss 2.3876, val loss 2.5505\n",
      "step 198380: train loss 2.4217, val loss 2.4363\n",
      "step 198390: train loss 2.4199, val loss 2.4297\n",
      "step 198400: train loss 2.3461, val loss 2.5304\n",
      "step 198410: train loss 2.3822, val loss 2.5536\n",
      "step 198420: train loss 2.3857, val loss 2.4949\n",
      "step 198430: train loss 2.3677, val loss 2.4284\n",
      "step 198440: train loss 2.3981, val loss 2.4421\n",
      "step 198450: train loss 2.3942, val loss 2.4470\n",
      "step 198460: train loss 2.3672, val loss 2.5475\n",
      "step 198470: train loss 2.4271, val loss 2.3532\n",
      "step 198480: train loss 2.3538, val loss 2.4510\n",
      "step 198490: train loss 2.4099, val loss 2.5549\n",
      "step 198500: train loss 2.4285, val loss 2.5377\n",
      "step 198510: train loss 2.4418, val loss 2.5705\n",
      "step 198520: train loss 2.3514, val loss 2.4387\n",
      "step 198530: train loss 2.3622, val loss 2.4807\n",
      "step 198540: train loss 2.3570, val loss 2.4350\n",
      "step 198550: train loss 2.3042, val loss 2.4543\n",
      "step 198560: train loss 2.3387, val loss 2.4015\n",
      "step 198570: train loss 2.3842, val loss 2.4239\n",
      "step 198580: train loss 2.3797, val loss 2.4697\n",
      "step 198590: train loss 2.3995, val loss 2.4840\n",
      "step 198600: train loss 2.3685, val loss 2.5310\n",
      "step 198610: train loss 2.3819, val loss 2.4411\n",
      "step 198620: train loss 2.4230, val loss 2.4700\n",
      "step 198630: train loss 2.3574, val loss 2.4729\n",
      "step 198640: train loss 2.3836, val loss 2.4878\n",
      "step 198650: train loss 2.3837, val loss 2.4394\n",
      "step 198660: train loss 2.3851, val loss 2.4393\n",
      "step 198670: train loss 2.4408, val loss 2.4276\n",
      "step 198680: train loss 2.3929, val loss 2.4949\n",
      "step 198690: train loss 2.4176, val loss 2.5049\n",
      "step 198700: train loss 2.4336, val loss 2.4979\n",
      "step 198710: train loss 2.3812, val loss 2.5257\n",
      "step 198720: train loss 2.3649, val loss 2.4330\n",
      "step 198730: train loss 2.4106, val loss 2.4352\n",
      "step 198740: train loss 2.4199, val loss 2.4740\n",
      "step 198750: train loss 2.4285, val loss 2.4761\n",
      "step 198760: train loss 2.4524, val loss 2.4038\n",
      "step 198770: train loss 2.4503, val loss 2.5658\n",
      "step 198780: train loss 2.3972, val loss 2.4091\n",
      "step 198790: train loss 2.3647, val loss 2.4399\n",
      "step 198800: train loss 2.4162, val loss 2.5275\n",
      "step 198810: train loss 2.4229, val loss 2.4270\n",
      "step 198820: train loss 2.4342, val loss 2.4871\n",
      "step 198830: train loss 2.3602, val loss 2.5645\n",
      "step 198840: train loss 2.4271, val loss 2.4846\n",
      "step 198850: train loss 2.3585, val loss 2.4168\n",
      "step 198860: train loss 2.4035, val loss 2.5805\n",
      "step 198870: train loss 2.4600, val loss 2.4296\n",
      "step 198880: train loss 2.4208, val loss 2.4236\n",
      "step 198890: train loss 2.3487, val loss 2.5692\n",
      "step 198900: train loss 2.3975, val loss 2.4667\n",
      "step 198910: train loss 2.4289, val loss 2.4991\n",
      "step 198920: train loss 2.3246, val loss 2.5551\n",
      "step 198930: train loss 2.3892, val loss 2.5096\n",
      "step 198940: train loss 2.4895, val loss 2.4899\n",
      "step 198950: train loss 2.3599, val loss 2.5011\n",
      "step 198960: train loss 2.4486, val loss 2.5988\n",
      "step 198970: train loss 2.4188, val loss 2.4921\n",
      "step 198980: train loss 2.3991, val loss 2.4783\n",
      "step 198990: train loss 2.4441, val loss 2.4600\n",
      "step 199000: train loss 2.3713, val loss 2.4930\n",
      "Generated text at iteration 199000\n",
      "\n",
      "IIleugurtreuit   nces it;\n",
      "T\n",
      "Ouredes  las pends,\n",
      "OUITon ces  de susie, D'e ouyénimer giscô qu t moin \n",
      "step 199010: train loss 2.4400, val loss 2.4944\n",
      "step 199020: train loss 2.3976, val loss 2.3630\n",
      "step 199030: train loss 2.4334, val loss 2.3849\n",
      "step 199040: train loss 2.4183, val loss 2.4884\n",
      "step 199050: train loss 2.4166, val loss 2.3801\n",
      "step 199060: train loss 2.3459, val loss 2.4832\n",
      "step 199070: train loss 2.4342, val loss 2.5619\n",
      "step 199080: train loss 2.4111, val loss 2.4443\n",
      "step 199090: train loss 2.4016, val loss 2.5464\n",
      "step 199100: train loss 2.3523, val loss 2.6785\n",
      "step 199110: train loss 2.3476, val loss 2.3833\n",
      "step 199120: train loss 2.4574, val loss 2.5100\n",
      "step 199130: train loss 2.4382, val loss 2.3571\n",
      "step 199140: train loss 2.3476, val loss 2.4950\n",
      "step 199150: train loss 2.3855, val loss 2.5084\n",
      "step 199160: train loss 2.4036, val loss 2.5564\n",
      "step 199170: train loss 2.2603, val loss 2.5429\n",
      "step 199180: train loss 2.3731, val loss 2.4845\n",
      "step 199190: train loss 2.3767, val loss 2.5895\n",
      "step 199200: train loss 2.3899, val loss 2.4294\n",
      "step 199210: train loss 2.4377, val loss 2.5081\n",
      "step 199220: train loss 2.3643, val loss 2.4901\n",
      "step 199230: train loss 2.4326, val loss 2.6095\n",
      "step 199240: train loss 2.3629, val loss 2.4670\n",
      "step 199250: train loss 2.4731, val loss 2.5166\n",
      "step 199260: train loss 2.3248, val loss 2.4796\n",
      "step 199270: train loss 2.3070, val loss 2.4242\n",
      "step 199280: train loss 2.3499, val loss 2.4735\n",
      "step 199290: train loss 2.4416, val loss 2.4767\n",
      "step 199300: train loss 2.4433, val loss 2.3797\n",
      "step 199310: train loss 2.3940, val loss 2.4780\n",
      "step 199320: train loss 2.3949, val loss 2.4494\n",
      "step 199330: train loss 2.3860, val loss 2.3974\n",
      "step 199340: train loss 2.3552, val loss 2.4940\n",
      "step 199350: train loss 2.3463, val loss 2.4474\n",
      "step 199360: train loss 2.3423, val loss 2.6571\n",
      "step 199370: train loss 2.3762, val loss 2.4568\n",
      "step 199380: train loss 2.3777, val loss 2.5544\n",
      "step 199390: train loss 2.4135, val loss 2.5543\n",
      "step 199400: train loss 2.3180, val loss 2.4519\n",
      "step 199410: train loss 2.4416, val loss 2.5588\n",
      "step 199420: train loss 2.4641, val loss 2.3748\n",
      "step 199430: train loss 2.3885, val loss 2.4583\n",
      "step 199440: train loss 2.3211, val loss 2.3728\n",
      "step 199450: train loss 2.3780, val loss 2.5695\n",
      "step 199460: train loss 2.3585, val loss 2.5627\n",
      "step 199470: train loss 2.4323, val loss 2.4718\n",
      "step 199480: train loss 2.3453, val loss 2.5561\n",
      "step 199490: train loss 2.4170, val loss 2.4148\n",
      "step 199500: train loss 2.3964, val loss 2.4053\n",
      "step 199510: train loss 2.4167, val loss 2.4653\n",
      "step 199520: train loss 2.3621, val loss 2.4383\n",
      "step 199530: train loss 2.4058, val loss 2.4545\n",
      "step 199540: train loss 2.3587, val loss 2.4930\n",
      "step 199550: train loss 2.3090, val loss 2.4401\n",
      "step 199560: train loss 2.4512, val loss 2.4830\n",
      "step 199570: train loss 2.3935, val loss 2.4972\n",
      "step 199580: train loss 2.4050, val loss 2.3786\n",
      "step 199590: train loss 2.3898, val loss 2.4188\n",
      "step 199600: train loss 2.3675, val loss 2.3681\n",
      "step 199610: train loss 2.3962, val loss 2.4498\n",
      "step 199620: train loss 2.4173, val loss 2.5845\n",
      "step 199630: train loss 2.4508, val loss 2.4808\n",
      "step 199640: train loss 2.4345, val loss 2.4389\n",
      "step 199650: train loss 2.3324, val loss 2.5371\n",
      "step 199660: train loss 2.4119, val loss 2.5980\n",
      "step 199670: train loss 2.4074, val loss 2.5243\n",
      "step 199680: train loss 2.5014, val loss 2.5399\n",
      "step 199690: train loss 2.4419, val loss 2.4170\n",
      "step 199700: train loss 2.4157, val loss 2.5388\n",
      "step 199710: train loss 2.3715, val loss 2.4625\n",
      "step 199720: train loss 2.4167, val loss 2.5189\n",
      "step 199730: train loss 2.4359, val loss 2.5312\n",
      "step 199740: train loss 2.3457, val loss 2.4992\n",
      "step 199750: train loss 2.4121, val loss 2.4960\n",
      "step 199760: train loss 2.3503, val loss 2.4051\n",
      "step 199770: train loss 2.2858, val loss 2.5264\n",
      "step 199780: train loss 2.3444, val loss 2.4706\n",
      "step 199790: train loss 2.4690, val loss 2.3854\n",
      "step 199800: train loss 2.3463, val loss 2.5126\n",
      "step 199810: train loss 2.4900, val loss 2.4925\n",
      "step 199820: train loss 2.4151, val loss 2.5428\n",
      "step 199830: train loss 2.4175, val loss 2.4766\n",
      "step 199840: train loss 2.3989, val loss 2.3807\n",
      "step 199850: train loss 2.3361, val loss 2.3795\n",
      "step 199860: train loss 2.3738, val loss 2.5077\n",
      "step 199870: train loss 2.4414, val loss 2.4659\n",
      "step 199880: train loss 2.3901, val loss 2.4069\n",
      "step 199890: train loss 2.4681, val loss 2.5048\n",
      "step 199900: train loss 2.3757, val loss 2.5419\n",
      "step 199910: train loss 2.4422, val loss 2.5137\n",
      "step 199920: train loss 2.4030, val loss 2.4634\n",
      "step 199930: train loss 2.3884, val loss 2.5515\n",
      "step 199940: train loss 2.4139, val loss 2.5454\n",
      "step 199950: train loss 2.3537, val loss 2.5619\n",
      "step 199960: train loss 2.3228, val loss 2.5681\n",
      "step 199970: train loss 2.3830, val loss 2.4859\n",
      "step 199980: train loss 2.3984, val loss 2.5292\n",
      "step 199990: train loss 2.3719, val loss 2.4111\n"
     ]
    }
   ],
   "source": [
    "max_iters = 200000\n",
    "batch_size = 4\n",
    "eval_interval = 10\n",
    "learning_rate = 1e-4\n",
    "eval_iters = 20\n",
    "\n",
    "@torch.no_grad() # no gradient is computed here\n",
    "def estimate_loss():\n",
    "    \"\"\" Estimate the loss on eval_iters batch of train and val sets.\"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# re-create the model\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_list = []\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        loss_list.append((losses['train'], losses['val']))\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # toute les 1000 itérations, on génère du texte\n",
    "    if iter % 1000 == 0:\n",
    "        print (f'Generated text at iteration {iter}')\n",
    "        generated = m.generate(initial_prompt, 100)\n",
    "        print (decode(generated[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49c92138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the loss from loss_list wich contain couple of train and val loss\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(loss_list)\n",
    "plt.legend(['train', 'val'])\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512395a0",
   "metadata": {},
   "source": [
    "Once the network has been trained for 100 iterations, we can generate a sequence of characters.  \n",
    "\n",
    "#### Questions:  \n",
    "- What is the effect of training?  \n",
    "- Increase the number of iterations to 1,000 and then to 10,000. Note the obtained loss and the generated sentence. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff8e0a2",
   "metadata": {},
   "source": [
    "### **Observations** : \n",
    "\n",
    "- Training with only 1000 iterations does not make it possible to build a coherent sentences, the loss decreases slowly .But, we noticed that the more we increase the maximum number of itteration the more the weights of adjusting well in order to improve the generation of text. In the example that appears during training above we can see that the quality of the text generated is better and better. But from 100,000 iterations we reach the limits of this model since the loss no longer decreases.\n",
    "\n",
    "Note: I decreased learning to try avoid falling into a local minimum.\n",
    "\n",
    "- 1000ème iteration : **\"VxÈâ2a0MérïHuÀmXQÉy;M_,È-Ô_2O3U0çl] CSËP6Â]LxqùNCùg\n",
    "yqt!ëÂeru Ôo2ÉFj-Ékèc;GÀHfW'HqêNËlR0iÊçoEëè«·Wûw\"**\n",
    "\n",
    "- End of the training (200 000 itérations) : **\"T·[entis suis l'ét siroupes s, prboneu'ymbre es, pâoysousé. nt rple las, he-but!\n",
    "EIù[ïhe Couct:ême d\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4071b998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ûCrblan,\n",
      "Dire pes loibas vamone Loit pl'auetes e  mme.\n",
      "\n",
      "L'i out  aix mbreunt ne  Di vagre,\n",
      "\n",
      "Avome mil\n"
     ]
    }
   ],
   "source": [
    "idx = torch.ones((1,1), dtype=torch.long)*3\n",
    "idx = idx.to(device)\n",
    "print (decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfd33b1",
   "metadata": {},
   "source": [
    "## Single Head Attention  \n",
    "\n",
    "We will now implement the basic attention mechanism. For each pair of words in the sequence, this mechanism combines:  \n",
    "- **Q** (*query*): the information being searched for,  \n",
    "- **K** (*key*): the information retrieved,  \n",
    "- **V** (*value*): a result vector calculated from the attention mechanism.  \n",
    "\n",
    "![single head attention](images/single_head_attention.png)  \n",
    "\n",
    "### Masking  \n",
    "\n",
    "However, since we are using the model to generate sequences, we must not use characters that come after the current character—these are precisely the characters we aim to predict during training. *The future should not be used to predict the future.*  \n",
    "\n",
    "To enforce this constraint, we integrate a **masking matrix** into the process. This matrix ensures that:  \n",
    "- For the first character in the sequence, only that character is available for prediction (no context).  \n",
    "- For the second character, only the first and second characters can be used.  \n",
    "- For the third character, only the first three characters are accessible, and so on.  \n",
    "\n",
    "This results in a **lower triangular matrix**, where each row is normalized (rows sum to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d15fbb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "T = 8\n",
    "\n",
    "# first version of the contraints with matrix multiplication\n",
    "# create a lower triangular matrix\n",
    "weights0 = torch.tril(torch.ones(T,T))\n",
    "# normalize each row\n",
    "weights0 = weights0 / weights0.sum(1, keepdim=True) \n",
    "print (weights0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1eb4a7",
   "metadata": {},
   "source": [
    "The [`softmax`](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html) function provides another way to achieve normalization.  \n",
    "\n",
    "#### Question:  \n",
    "- Verify that applying `softmax` results in the same lower triangular matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "75455f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "weights = torch.tril(torch.ones(T,T))\n",
    "weights = weights.masked_fill(tril== 0, float('-inf'))\n",
    "weights = nn.functional.softmax(weights, dim=-1)\n",
    "print (weights)\n",
    "\n",
    "# Verification\n",
    "assert torch.allclose(weights, weights0) # c'est bien les meme matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f76c8b",
   "metadata": {},
   "source": [
    "### Implementation  \n",
    "\n",
    "We can now implement the attention layer based on the following formula:  \n",
    "\n",
    "![attention_formula](images/attention_formula.png)  \n",
    "\n",
    "This involves computing the **queries (Q)**, **keys (K)**, and **values (V)**, applying the **masking mechanism**, and using the **softmax function** to normalize the attention scores before computing the weighted sum of values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02681533",
   "metadata": {},
   "source": [
    "#### Questions:  \n",
    "\n",
    "- Create the **key**, **query**, and **value** layers as linear layers of dimension `C × head_size`.  \n",
    "- Apply these layers to `x`.  \n",
    "- Compute the attention weights:  \n",
    "  ```python\n",
    "  weights = query @ key.transpose(-2, -1)\n",
    "  ```\n",
    "  (Transpose the second and third dimensions of `key` to enable matrix multiplication).  \n",
    "- Apply the **normalization factor** (typically, divide by `sqrt(head_size)`).  \n",
    "- Apply the **triangular mask** and the **softmax** function to `weights`.  \n",
    "- Apply the **value** layer to `x`.  \n",
    "- Compute the final output:  \n",
    "  ```python\n",
    "  out = weights @ value(x)\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "129fe994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3297e-01, -3.7066e-01,  6.4770e-01, -1.1214e+00,  8.8342e-01,\n",
       "         -8.7100e-01, -1.7236e-02, -4.6525e-01, -6.3489e-01,  5.9552e-01,\n",
       "         -1.5879e-01,  6.0928e-01,  5.1799e-02,  1.0595e+00,  9.8379e-02,\n",
       "          3.8483e-01],\n",
       "        [-1.2690e-01, -3.4209e-01,  6.5743e-01, -3.6344e-01,  4.8228e-01,\n",
       "         -5.7327e-01, -3.0033e-01, -1.1472e-01,  8.1547e-02,  4.3500e-01,\n",
       "         -7.8594e-02,  6.2029e-01,  1.4072e-01,  5.1213e-01,  1.0453e-01,\n",
       "          7.9567e-01],\n",
       "        [-1.3559e-01, -4.1535e-01,  6.6877e-01, -4.1148e-01,  4.5904e-01,\n",
       "         -4.1167e-01, -1.3116e-01,  2.5674e-02, -2.4614e-02,  3.8201e-01,\n",
       "         -1.1345e-01,  7.0348e-01,  1.7121e-01,  4.3305e-01,  1.3716e-01,\n",
       "          6.0820e-01],\n",
       "        [-2.1374e-01, -2.3117e-01,  4.6133e-01, -3.3051e-01,  4.2616e-01,\n",
       "         -2.5731e-01, -1.9569e-01,  2.3020e-02,  6.1510e-02,  3.5505e-01,\n",
       "          3.4234e-04,  3.4482e-01, -5.7847e-02,  2.2601e-01,  2.5193e-01,\n",
       "          4.6992e-01],\n",
       "        [-1.9575e-01, -2.4886e-01,  4.4925e-01, -3.5522e-01,  4.2965e-01,\n",
       "         -2.2099e-01, -1.7013e-01, -4.1464e-03,  1.8215e-02,  3.8178e-01,\n",
       "          1.5110e-02,  3.5021e-01, -3.9465e-02,  1.9653e-01,  2.3248e-01,\n",
       "          3.8511e-01],\n",
       "        [-1.8995e-01, -2.7133e-01,  2.4278e-01, -1.2034e-01,  3.8374e-01,\n",
       "          6.8903e-02, -1.0562e-01,  6.4337e-02,  1.6859e-01,  4.6546e-01,\n",
       "          1.0428e-01,  1.3401e-02, -1.7694e-01, -1.2036e-01,  1.9612e-01,\n",
       "          1.9643e-01],\n",
       "        [-3.2521e-01, -2.2683e-01,  2.1169e-02, -1.2547e-01,  5.1751e-01,\n",
       "          6.9877e-02,  5.3734e-02,  2.3458e-01,  2.6321e-01,  4.6544e-01,\n",
       "          9.7477e-02, -3.2432e-01, -4.2883e-01, -9.1366e-02,  2.3787e-01,\n",
       "          1.9319e-01],\n",
       "        [-2.0351e-01, -5.7124e-02, -1.4550e-01, -1.7803e-01,  3.9084e-01,\n",
       "          2.5735e-01, -1.1606e-01,  4.4205e-02,  2.2042e-01,  3.6113e-01,\n",
       "          1.8795e-01,  8.8660e-03, -9.8878e-02, -3.9207e-01,  1.9376e-02,\n",
       "         -1.2815e-01]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_size = 16\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "## YOUR CODE HERE\n",
    "# define the Key layer  \n",
    "key = nn.Linear(C, head_size)\n",
    "# define the Query layer\n",
    "query = nn.Linear(C, head_size)\n",
    "# define the Value layer\n",
    "value =  nn.Linear(C, head_size)\n",
    "# apply each layer to the input\n",
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x)  # (B, T, head_size)\n",
    "v = value(x)  # (B, T, head_size)\n",
    "# compute the normalize product between Q and K \n",
    "weights = q @ k.transpose(-2, -1) / math.sqrt(head_size)# (B, T, head_size) @ (B, 16, head_size) -> (B, T, T)\n",
    "# apply the mask (lower triangular matrix)\n",
    "weights = weights.masked_fill(tril== 0, float('-inf'))\n",
    "# apply the softmax\n",
    "weights = nn.functional.softmax(weights, dim=-1)\n",
    "###\n",
    "out  = weights @ value(x) # (B, T, head_size)\n",
    "\n",
    "# print the result\n",
    "weights[0]\n",
    "out[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db591771",
   "metadata": {},
   "source": [
    "### Questions:  \n",
    "\n",
    "- Copy your code into `gpt_single_head.py`:  \n",
    "  - Define the **key**, **query**, and **value** layers in the **constructor** of the `Head` class.  \n",
    "  - Implement the **computations** in the `forward` function.  \n",
    "- Train the model.  \n",
    "- What are the **training** and **validation** losses?  \n",
    "- Does the generated text appear **better** compared to the previous model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c043812e",
   "metadata": {},
   "source": [
    "## Multi-Head Attention  \n",
    "\n",
    "Multi-head attention is simply the parallel computation of multiple **single-head attention** mechanisms. Each **single-head attention** output is concatenated to form the output of the **multi-head attention** module. In the original paper's illustration, the number of heads in the **multi-head attention** is denoted as `h`.  \n",
    "\n",
    "To allow for **weighted combinations** of each single-head attention output, a **linear transformation layer** is added after concatenation.  \n",
    "\n",
    "![multi head attention](images/multi_head_attention.png)  \n",
    "\n",
    "#### Questions:  \n",
    "\n",
    "- In the **constructor**, create a list containing `num_heads` instances of the `Head` module using PyTorch’s [`ModuleList`](https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html).  \n",
    "- In the `forward` function:  \n",
    "  - Apply each **single-head attention** to the input.  \n",
    "  - Concatenate the results using PyTorch’s [`cat`](https://pytorch.org/docs/stable/generated/torch.cat.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fab8977",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        ## YOUR CODE HERE\n",
    "        ## list of num_heads modules of type Head\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        ###\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## YOUR CODE HERE\n",
    "        ## apply each head in self.heads to x and concat the results \n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a39989",
   "metadata": {},
   "source": [
    "#### Questions:  \n",
    "\n",
    "1. **Copy** the file `gpt_single_head.py` and rename it as `gpt_multi_head.py`.  \n",
    "2. **Add** the `MultiHeadAttention` module in `gpt_multi_head.py`.  \n",
    "3. At the **beginning of the file**, add a parameter:  \n",
    "   ```python\n",
    "   n_head = 4\n",
    "   ```\n",
    "4. In the `BigramLanguageModel` module, **replace** the `Head` module with `MultiHeadAttention`, using the parameters:  \n",
    "   ```python\n",
    "   num_heads = n_head\n",
    "   head_size = n_embd // n_head\n",
    "   ```\n",
    "   This ensures the total number of parameters remains **the same**.  \n",
    "5. **Retrain the model** and note:  \n",
    "   - The total number of **parameters**  \n",
    "   - The **training** and **validation** losses obtained  \n",
    "\n",
    "**Expected Output Example:**  \n",
    "```\n",
    "0.009893 M parameters  \n",
    "step 4999: train loss 2.1570, val loss 2.1802  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333d3f27",
   "metadata": {},
   "source": [
    "## Adding a FeedForward Computation Layer  \n",
    "\n",
    "After the **attention layers**, which collect information from the sequence, a **computation layer** is added to combine all the gathered information.  \n",
    "\n",
    "This layer is a simple **Multi-Layer Perceptron (MLP)** with:  \n",
    "- One **hidden layer**,  \n",
    "- A **ReLU non-linearity** using [`ReLU`](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html).  \n",
    "\n",
    "### Architecture:  \n",
    "\n",
    "<img src=\"images/multi_ff.png\" alt=\"multi feedforward\" width=\"200\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915f8148",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple MLP with RELU \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ca5ef7",
   "metadata": {},
   "source": [
    "#### Questions:  \n",
    "\n",
    "1. **Add** the `FeedForward` module to your `gpt_multi_head.py` file.  \n",
    "2. **Integrate** this `FeedForward` layer **after** the **multi-head attention** module.  \n",
    "3. **Retrain the model** and note:  \n",
    "   - The total **number of parameters**  \n",
    "   - The **training** and **validation** losses obtained  \n",
    "\n",
    "**Expected Output Example:**  \n",
    "```\n",
    "0.010949 M parameters  \n",
    "step 4999: train loss 2.1290, val loss 2.1216  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc16dfb3",
   "metadata": {},
   "source": [
    "## Stacking Blocks  \n",
    "\n",
    "The network we have built so far represents just **one block** of the final model. Now, we can **stack multiple blocks** of **multi-head attention** to create a **deeper** network.  \n",
    "\n",
    "### Architecture:  \n",
    "![multi feedforward](images/multi_bloc.png)  \n",
    "\n",
    "The following code defines a **block**:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbdecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" A single bloc of multi-head attention \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sa(x)\n",
    "        x = self.ffwd(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3144fff7",
   "metadata": {},
   "source": [
    "#### Questions:  \n",
    "\n",
    "- Add the `Block` module to `gpt_multi_head.py`.  \n",
    "- Modify the `BigramLanguageModel` code to include **three** instances of `Block(n_embd, n_head=4)`, using a [`Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) container **instead of** `MultiHeadAttention` and `FeedForward`.  \n",
    "- Retrain the model and note:  \n",
    "  - The **number of parameters**  \n",
    "  - The **training** and **validation** losses obtained  \n",
    "\n",
    "**Expected Output Example:**  \n",
    "```\n",
    "0.019205 M parameters  \n",
    "step 4999: train loss 2.2080, val loss 2.2213  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be02d77a",
   "metadata": {},
   "source": [
    "## Improving Training  \n",
    "\n",
    "If we want to continue increasing the **network size**, we need to incorporate layers that **enhance training stability** and **improve generalization** (reducing overfitting). These layers include:  \n",
    "\n",
    "- **Skip connections** (or **residual connections**)  \n",
    "- **Normalization layers**  \n",
    "- **Dropout**  \n",
    "\n",
    "### Updated Architecture:  \n",
    "\n",
    "<img src=\"images/multi_skip_norm.png\" alt=\"multi feedforward\" width=\"200\">\n",
    "\n",
    "---\n",
    "\n",
    "#### Questions:  \n",
    "\n",
    "1. In the `Block` module, **add a skip connection** by summing the input at each step:  \n",
    "   ```python\n",
    "   x = x + self.sa(self.ln1(x))\n",
    "   x = x + self.ffwd(self.ln2(x))\n",
    "   ```  \n",
    "   \n",
    "2. In the `Block` module, **add two** [`LayerNorm`](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) layers of size `n_embd`:  \n",
    "   - **Before** the `Multi-Head Attention` layer.  \n",
    "   - **Before** the `FeedForward` layer.  \n",
    "\n",
    "3. **After the sequence of 3 blocks**, add a **LayerNorm** layer of size `n_embd`.  \n",
    "\n",
    "4. Define a variable at the **beginning of the file**:  \n",
    "   ```python\n",
    "   dropout = 0.2\n",
    "   ```\n",
    "   Then add a [`Dropout`](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) layer:  \n",
    "   - **After** the `ReLU` activation in `FeedForward`.  \n",
    "   - **After** the `Multi-Head Attention` layer in `MultiHeadAttention`.  \n",
    "   - **After** the `softmax` layer in the single-head attention `Head`.  \n",
    "\n",
    "5. **Retrain the model** and note:  \n",
    "   - The **number of parameters**  \n",
    "   - The **training** and **validation** losses  \n",
    "\n",
    "---\n",
    "\n",
    "**Expected Output Example:**  \n",
    "```\n",
    "0.019653 M parameters  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4733a17",
   "metadata": {},
   "source": [
    "## Conclusion  \n",
    "\n",
    "The key components of **GPT-2** are now in place. The next step is to **scale up** the model and train it on a **much larger** dataset. For comparison, the parameters of [GPT-2](https://huggingface.co/transformers/v2.11.0/model_doc/gpt2.html) are:  \n",
    "\n",
    "- **`vocab_size = 50257`** → GPT-2 models **subword tokens**, while we model **characters**. For us, `vocab_size = 100`.  \n",
    "- **`n_positions = 1024`** → The maximum **context size**. For us, it's `block_size = 8`.  \n",
    "- **`n_embd = 768`** → The **embedding dimension**. For us, it's `n_embd = 32`.  \n",
    "- **`n_layer = 12`** → The number of **blocks**. For us, it's `3`.  \n",
    "- **`n_head = 12`** → The number of **multi-head attention layers**. For us, it's `4`.  \n",
    "\n",
    "Overall, **GPT-2** consists of **1.5 billion parameters** and was trained on **8 million web pages**, totaling **40 GB of text**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Training Results**  \n",
    "```text\n",
    "10.816613 M parameters  \n",
    "step 0: train loss 4.7847, val loss 4.7701  \n",
    "step 4999: train loss 0.2683, val loss 2.1161  \n",
    "time: 31m47.910s   \n",
    "```\n",
    "\n",
    "### **Generated Text Sample:**  \n",
    "\n",
    "```text\n",
    "Le pêcheur où l'homme en peu de Carevante  \n",
    "Sa conter des chosses qu'en ses yoitn!  \n",
    "\n",
    "Ils sont là-hauts parler à leurs ténèbres  \n",
    "A ceux qu'on rêve aux oiseaux des cheveux,  \n",
    "Et celus qu'on tourna jamais sous le front;  \n",
    "Ils se disent tu mêle aux univers.  \n",
    "J'ai vu Jean vu France, potte; petits contempler,  \n",
    "Et petié calme au milibre et versait,  \n",
    "M'éblouissant, emportant, écoute, ingorancessible,  \n",
    "On meurt s'efferayait.....--Pas cont âme parle en Apparia!  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9498389b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
